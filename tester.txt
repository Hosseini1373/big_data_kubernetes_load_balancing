['Huimin Ma · Liang Wang · Changshui Zhang · \nFei Wu · Tieniu Tan · Yaonan Wang · \nJianhuang Lai · Yao Zhao (Eds.)LNCS 13022\n4th Chinese Conference, PRCV 2021\nBeijing, China, October 29 – November 1, 2021\nProceedings, Part IVPattern Recognition \nand Computer Vision\n', 'Lecture Notes in Computer Science 13022\nFounding Editors\nGerhard Goos\nKarlsruhe Institute of T echnology, Karlsruhe, Germany\nJuris Hartmanis\nCornell University, Ithaca, NY, USA\nEditorial Board Members\nElisa Bertino\nPurdue University, W est Lafayette, IN, USA\nWen Gao\nPeking University, Beijing, China\nBernhard Steffen\nTU Dortmund University, Dortmund, Germany\nGerhard Woeginger\nRWTH Aachen, Aachen, Germany\nMoti Yung\nColumbia University, New Y ork, NY, USA', 'More information about this subseries at http://www.springer.com/series/7412', 'Huimin Ma ·Liang Wang ·Changshui Zhang ·\nFei Wu ·Tieniu Tan ·Yaonan Wang ·\nJianhuang Lai ·Yao Zhao (Eds.)\nPattern Recognition\nand Computer Vision\n4th Chinese Conference, PRCV 2021\nBeijing, China, October 29 – November 1, 2021Proceedings, Part IV', 'Editors\nHuimin Ma\nUniversity of Science and Technology Beijing\nBeijing, China\nChangshui Zhang\nTsinghua UniversityBeijing, China\nTieniu Tan\nChinese Academy of SciencesBeijing, China\nJianhuang Lai\nSun Yat-Sen UniversityGuangzhou, Guangdong, ChinaLiang Wang\nChinese Academy of SciencesBeijing, China\nFei Wu\nZhejiang University\nHangzhou, China\nYaonan Wang\nHunan UniversityChangsha, China\nYao Zhao\nBeijing Jiaotong University\nBeijing, China\nISSN 0302-9743 ISSN 1611-3349 (electronic)\nLecture Notes in Computer ScienceISBN 978-3-030-88012-5 ISBN 978-3-030-88013-2 (eBook)https://doi.org/10.1007/978-3-030-88013-2\nLNCS Sublibrary: SL6 – Image Processing, Computer Vision, Pattern Recognition, and Graphics\n© Springer Nature Switzerland AG 2021\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the\nmaterial is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or informationstorage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now\nknown or hereafter developed.\nThe use of general descriptive names, registered names, trademarks, service marks, etc. in this publicationdoes not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant\nprotective laws and regulations and therefore free for general use.\nThe publisher, the authors and the editors are safe to assume that the advice and information in this book arebelieved to be true and accurate at the date of publication. Neither the publisher nor the authors or the editorsgive a warranty, expressed or implied, with respect to the material contained herein or for any errors or\nomissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in\npublished maps and institutional afﬁliations.\nThis Springer imprint is published by the registered company Springer Nature Switzerland AG\nThe registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland', 'Preface\nWelcome to the proceedings of the 4th Chinese Conference on Pattern Recognition and\nComputer Vision (PRCV 2021) held in Beijing, China!\nPRCV was established to further boost the impact of the Chinese community in\npattern recognition and computer vision, which are two core areas of artiﬁcial intelli-\ngence, and further improve the quality of academic communication. Accordingly, PRCV\nis co-sponsored by four major academic societies of China: the China Society of Imageand Graphics (CSIG), the Chinese Association for Artiﬁcial Intelligence (CAAI), the\nChina Computer Federation (CCF), and the Chinese Association of Automation (CAA).\nPRCV aims at providing an interactive communication platform for researchers\nfrom academia and from industry. It promotes not only academic exchange but also\ncommunication between academia and industry. In order to keep track of the frontierof academic trends and share the latest research achievements, innovative ideas, and\nscientiﬁc methods, international and local leading experts and professors are invited to\ndeliver keynote speeches, introducing the latest advances in theories and methods in theﬁelds of pattern recognition and computer vision.\nPRCV 2021 was hosted by University of Science and Technology Beijing, Beijing\nJiaotong University, and the Beijing University of Posts and Telecommunications. Wereceived 513 full submissions. Each submission was reviewed by at least three reviewers\nselected from the Program Committee and other qualiﬁed researchers. Based on the\nreviewers’ reports, 201 papers were ﬁnally accepted for presentation at the conference,including 30 oral and 171 posters. The acceptance rate was 39.2%. PRCV took place\nduring October 29 to November 1, 2021, and the proceedings are published in this\nvolume in Springer’s Lecture Notes in Computer Science (LNCS) series.\nWe are grateful to the keynote speakers, Larry Davis from the University of Maryland,\nUSA, Yoichi Sato from the University of Tokyo, Japan, Michael Black from the Max\nPlanck Institute for Intelligent Systems, Germany, Songchun Zhu from Peking University\nand Tsinghua University, China, and Bo Xu from the Institute of Automation, Chinese\nAcademy of Sciences, China.\nWe give sincere thanks to the authors of all submitted papers, the Program Committee\nmembers and the reviewers, and the Organizing Committee. Without their contributions,\nthis conference would not have been possible. Special thanks also go to all of the sponsors', 'vi Preface\nand the organizers of the special forums; their support helped to make the conference a\nsuccess. We are also grateful to Springer for publishing the proceedings.\nOctober 2021 Tieniu Tan\nYaonan Wang\nJianhuang Lai\nYao Zhao\nHuimin Ma\nLiang Wang\nChangshui Zhang\nFei Wu', 'Organization\nSteering Committee Chair\nTieniu Tan Institute of Automation, Chinese Academy of Sciences, China\nSteering Committee\nXilin Chen Institute of Computing Technology, Chinese Academy of Sciences,\nChina\nChenglin Liu Institute of Automation, Chinese Academy of Sciences, ChinaYong Rui Lenovo, ChinaHongbing Zha Peking University, ChinaNanning Zheng Xi’an Jiaotong University, ChinaJie Zhou Tsinghua University, China\nSteering Committee Secretariat\nLiang Wang Institute of Automation, Chinese Academy of Sciences, China\nGeneral Chairs\nTieniu Tan Institute of Automation, Chinese Academy of Sciences, ChinaYaonan Wang Hunan University, ChinaJianhuang Lai Sun Yat-sen University, ChinaYao Zhao Beijing Jiaotong University, China\nProgram Chairs\nHuimin Ma University of Science and Technology Beijing, ChinaLiang Wang Institute of Automation, Chinese Academy of Sciences, ChinaChangshui Zhang Tsinghua University, ChinaFei Wu Zhejiang University, China\nOrganizing Committee Chairs\nXucheng Yin University of Science and Technology Beijing, ChinaZhanyu Ma Beijing University of Posts and Telecommunications, ChinaZhenfeng Zhu Beijing Jiaotong University, ChinaRuiping Wang Institute of Computing Technology, Chinese Academy of Sciences,\nChina', 'viii Organization\nSponsorship Chairs\nNenghai Yu University of Science and Technology of China, China\nXiang Bai Huazhong University of Science and Technology, China\nYue Liu Beijing Institute of Technology, China\nJinfeng Yang Shenzhen Polytechnic, China\nPublicity Chairs\nXiangwei Kong Zhejiang University, ChinaTao Mei JD.com, China\nJiaying Liu Peking University, China\nDan Zeng Shanghai University, China\nInternational Liaison Chairs\nJingyi Yu ShanghaiTech University, ChinaXuelong Li Northwestern Polytechnical University, ChinaBangzhi Ruan Hong Kong Baptist University, China\nT utorial Chairs\nWeishi Zheng Sun Yat-sen University, ChinaMingming Cheng Nankai University, China\nShikui Wei Beijing Jiaotong University, China\nSymposium Chairs\nHua Huang Beijing Normal University, ChinaYuxin Peng Peking University, China\nNannan Wang Xidian University, China\nDoctoral Forum Chairs\nXi Peng Sichuan University, ChinaHang Su Tsinghua University, China\nHuihui Bai Beijing Jiaotong University, China\nCompetition Chairs\nNong Sang Huazhong University of Science and Technology, ChinaWangmeng Zuo Harbin Institute of Technology, China\nXiaohua Xie Sun Yat-sen University, China', 'Organization ix\nSpecial Issue Chairs\nJiwen Lu Tsinghua University, China\nShiming Xiang Institute of Automation, Chinese Academy of Sciences, China\nJianxin Wu Nanjing University, China\nPublication Chairs\nZhouchen Lin Peking University, ChinaChunyu Lin Beijing Jiaotong University, China\nHuawei Tian People’s Public Security University of China, China\nRegistration Chairs\nJunjun Yin University of Science and Technology Beijing, China\nYue Ming Beijing University of Posts and Telecommunications, ChinaJimin Xiao Xi’an Jiaotong-Liverpool University, China\nDemo Chairs\nXiaokang Yang Shanghai Jiaotong University, China\nXiaobin Zhu University of Science and Technology Beijing, China\nChunjie Zhang Beijing Jiaotong University, China\nWebsite Chairs\nChao Zhu University of Science and Technology Beijing, ChinaZhaofeng He Beijing University of Posts and Telecommunications, China\nRunmin Cong Beijing Jiaotong University, China\nFinance Chairs\nWeiping Wang University of Science and Technology Beijing, China\nLifang Wu Beijing University of Technology, ChinaMeiqin Liu Beijing Jiaotong University, China', 'x Organization\nProgram Committee\nJing Dong Chinese Academy of Sciences, China\nRan He Institute of Automation, Chinese Academy of Sciences, ChinaXi Li Zhejiang University, China\nSi Liu Beihang University, China\nXi Peng Sichuan University, ChinaYu Qiao Chinese Academy of Sciences, China\nJian Sun Xi’an Jiaotong University, China\nRongrong Ji Xiamen University, ChinaXiang Bai Huazhong University of Science and Technology, ChinaJian Cheng Institute of Automation, Chinese Academy of Sciences, China\nMingming Cheng Nankai University, China\nJunyu Dong Ocean University of China, ChinaWeisheng Dong Xidian University, China\nYuming Fang Jiangxi University of Finance and Economics, China\nJianjiang Feng Tsinghua University, ChinaShenghua Gao ShanghaiTech University, ChinaMaoguo Gong Xidian University, China\nYahong Han Tianjin University, China\nHuiguang He Institute of Automation, Chinese Academy of Sciences, ChinaShuqiang Jiang Institute of Computing Technology, China Academy of Science,\nChina\nLianwen Jin South China University of Technology, ChinaXiaoyuan Jing Wuhan University, ChinaHaojie Li Dalian University of Technology, China\nJianguo Li Ant Group, China\nPeihua Li Dalian University of Technology, ChinaLiang Lin Sun Yat-sen University, China\nZhouchen Lin Peking University, China\nJiwen Lu Tsinghua University, ChinaSiwei Ma Peking University, ChinaDeyu Meng Xi’an Jiaotong University, China\nQiguang Miao Xidian University, China\nLiqiang Nie Shandong University, ChinaWanli Ouyang The University of Sydney, AustraliaJinshan Pan Nanjing University of Science and Technology, China\nNong Sang Huazhong University of Science and Technology, China\nShiguang Shan Institute of Computing Technology, Chinese Academy of Sciences,\nChina\nHongbin Shen Shanghai Jiao Tong University, China\nLinlin Shen Shenzhen University, ChinaMingli Song Zhejiang University, ChinaHanli Wang Tongji University, China\nHanzi Wang Xiamen University, China\nJingdong Wang Microsoft, China', 'Organization xi\nNannan Wang Xidian University, China\nJianxin Wu Nanjing University, ChinaJinjian Wu Xidian University, China\nYihong Wu Institute of Automation, Chinese Academy of Sciences, China\nGuisong Xia Wuhan University, ChinaYong Xia Northwestern Polytechnical University, China\nShiming Xiang Chinese Academy of Sciences, China\nXiaohua Xie Sun Yat-sen University, ChinaJufeng Yang Nankai University, China\nWankou Yang Southeast University, China\nYang Yang University of Electronic Science and Technology of China, ChinaYilong Yin Shandong University, China\nXiaotong Yuan Nanjing University of Information Science and Technology, China\nZhengjun Zha University of Science and Technology of China, China\nDaoqiang Zhang Nanjing University of Aeronautics and Astronautics, China\nZhaoxiang Zhang Institute of Automation, Chinese Academy of Sciences, ChinaWeishi Zheng Sun Yat-sen University, China\nWangmeng Zuo Harbin Institute of Technology, China\nReviewers\nBai Xiang\nBai XiaoCai Shen\nCai Yinghao\nChen Zailiang\nChen Weixiang\nChen Jinyu\nChen Yifan\nCheng Gong\nChu Jun\nCui Chaoran\nCui Hengfei\nCui ZheDeng Hongxia\nDeng Cheng\nDing Zihan\nDong Qiulei\nDong Yu\nDong Xue\nDuan Lijuan\nFan Bin\nFan Yongxian\nFan Bohao\nFang YuchunFeng Jiachang\nFeng JiaweiFu Bin\nFu Ying\nGao Hongxia\nGao Shang-Hua\nGao Changxin\nGao Guangwei\nGao Yi\nGe Shiming\nGe Yongxin\nGeng Xin\nGong ChenGong Xun\nGu Guanghua\nGu Yu-Chao\nGuo Chunle\nGuo Jianwei\nGuo Zhenhua\nHan Qi\nHan Linghao\nHe Hong\nHe Mingjie\nHe ZhaofengHe Hongliang\nHong JinchengHu Shishuai\nHu Jie\nHu Yang\nHu Fuyuan\nHu Ruyun\nHu Yangwen\nHuang Lei\nHuang Sheng\nHuang Dong\nHuang Huaibo\nHuang JiangtaoHuang Xiaoming\nJi Fanfan\nJi Jiayi\nJi Zhong\nJia Chuanmin\nJia Wei\nJia Xibin\nJiang Bo\nJiang Peng-Tao\nKan Meina\nKang Wenxiong', 'xii Organization\nLei Na\nLei ZhenLeng Lu\nLi Chenglong\nLi ChunleiLi Hongjun\nLi Shuyan\nLi Xia\nLi Zhiyong\nLi GuanbinLi Peng\nLi Ruirui\nLi ZechaoLi Zhen\nLi Ce\nLi Changzhou\nLi Jia\nLi JianLi Shiying\nLi Wanhua\nLi YongjieLi Yunfan\nLiang Jian\nLiang Yanjie\nLiao Zehui\nLin ZihangLin Chunyu\nLin Guangfeng\nLiu HengLiu Li\nLiu Wu\nLiu Yiguang\nLiu Zhiang\nLiu ChongyuLiu Li\nLiu Qingshan\nLiu YunLiu Cheng-Lin\nLiu Min\nLiu Risheng\nLiu Tiange\nLiu WeifengLiu Xiaolong\nLiu Yang\nLiu ZhiLiu Zhou\nLu ShaopingLu Haopeng\nLuo Bin\nLuo GenMa Chao\nMa Wenchao\nMa Cheng\nMa Wei\nMei JieMiao Yongwei\nNie Liqiang\nNie XiushanNiu Xuesong\nNiu Yuzhen\nOuyang Jianquan\nPan Chunyan\nPan ZhiyuPan Jinshan\nPeng Yixing\nPeng JunQian Wenhua\nQin Binjie\nQu Yanyun\nRao Yongming\nRen WenqiRui Song\nShen Chao\nShen HaifengShen Shuhan\nShen Tiancheng\nSheng Lijun\nShi Caijuan\nShi WuShi Zhiping\nShi Hailin\nShi LukuiSong Chunfeng\nSu Hang\nSun Xiaoshuai\nSun Jinqiu\nSun ZhanliSun Jun\nSun Xian\nSun ZhenanTan Chaolei\nTan XiaoyangTang Jin\nTu Zhengzheng\nWang FudongWang Hao\nWang Limin\nWang Qinfen\nWang Xingce\nWang XinnianWang Zitian\nWang Hongxing\nWang JiapengWang Luting\nWang Shanshan\nWang Shengke\nWang Yude\nWang ZileiWang Dong\nWang Hanzi\nWang JinjiaWang Long\nWang Qiufeng\nWang Shuqiang\nWang Xingzheng\nWei Xiu-ShenWei Wei\nWen Jie\nWu YadongWu Hong\nWu Shixiang\nWu Xia\nWu Yongxian\nWu YuweiWu Xinxiao\nWu Yihong\nXia DaoxunXiang Shiming\nXiao Jinsheng\nXiao Liang\nXiao Jun\nXie XingyuXu Gang\nXu Shugong\nXu Xun', 'Organization xiii\nXu Zhenghua\nXu LixiangXu Xin-Shun\nXu Mingye\nXu YongXue Nan\nYan Bo\nYan DongmingYan Junchi\nYang Dong\nYang GuanYang Peipei\nYang Wenming\nYang Yibo\nYang Lu\nYang JinfuYang Wen\nYao Tao\nYe MaoYin Ming\nYin FeiYou Gexin\nYu YeYu Qian\nYu Zhe\nZeng LinganZeng Hui\nZhai Yongjie\nZhang AiwuZhang Chi\nZhang Jie\nZhang ShuZhang Wenqiang\nZhang Yunfeng\nZhang Zhao\nZhang Hui\nZhang LeiZhang Xuyao\nZhang Yongfei\nZhang DingwenZhang Honggang\nZhang LinZhang Mingjin\nZhang ShanshanZhang Xiao-Yu\nZhang Yanming\nZhang YuefengZhao Cairong\nZhao Yang\nZhao YuqianZhen Peng\nZheng Wenming\nZheng FengZhong Dexing\nZhong Guoqiang\nZhou Xiaolong\nZhou Xue\nZhou QuanZhou Xiaowei\nZhu Chaoyang\nZhu XiangpingZou Yuexian\nZuo Wangmeng', 'Contents - Part IV\nMachine Learning, Neural Network and Deep Learning\nEdge-Wise One-Level Global Pruning on NAS Generated Networks . . . . . . . . . . 3\nQiantai Feng, Ke Xu, Y uhai Li, Y uxin Sun, and Dong W ang\nC o n v o l u t i o nT e l l sW h e r et oL o o k ........................................ 1 6\nFan Xu, Lijuan Duan, Y uanhua Qiao, and Ji Chen\nRobust Single-Step Adversarial Training with Regularizer . . . . . . . . . . . . . . . . . . . 29\nLehui Xie, Y aopeng W ang, Jia-Li Yin, and Ximeng Liu\nTexture-Guided U-Net for OCT-to-OCTA Generation . . . . . . . . . . . . . . . . . . . . . . . 42\nZiyue Zhang, Zexuan Ji, Qiang Chen, Songtao Y uan, and W en Fan\nLearning Key Actors and Their Interactions for Group Activity Recognition . . . 53\nY utai Duan and Jianming W ang\nAttributed Non-negative Matrix Multi-factorization for Data\nR e p r e s e n t a t i o n ........................................................ 6 6\nJie W ang, Y anfeng Sun, Jipeng Guo, Y ongli Hu, and Baocai Yin\nImproved Categorical Cross-Entropy Loss for Training Deep Neural\nNetworks with Noisy Labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\nPanle Li, Xiaohui He, Dingjun Song, Zihao Ding, Mengjia Qiao,\nXijie Cheng, and Runchuan Li\nA Residual Correction Approach for Semi-supervised Semantic\nS e g m e n t a t i o n ......................................................... 9 0\nHaoliang Li and Huicheng Zheng\nHypergraph Convolutional Network with Hybrid Higher-Order Neighbors . . . . . 103\nJiahao Huang, Fangyuan Lei, Senhong W ang, Song W ang,\nand Qingyun Dai\nText-Aware Single Image Specular Highlight Removal . . . . . . . . . . . . . . . . . . . . . . 115\nShiyu Hou, Chaoqun W ang, W eize Quan, Jingen Jiang,\nand Dong-Ming Y an\nMinimizing Wasserstein-1 Distance by Quantile Regression for GANs\nModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\nYingying Chen, Xinwen Hou, and Y u Liu', 'xvi Contents - Part IV\nA Competition of Shape and Texture Bias by Multi-view Image\nR e p r e s e n t a t i o n ........................................................ 1 4 0\nLingwei Kong, Jianzong W ang, Zhangcheng Huang, and Jing Xiao\nLearning Indistinguishable and Transferable Adversarial Examples . . . . . . . . . . . 152\nWu Zhang, Junhua Zou, Y exin Duan, Xingyu Zhou, and Zhisong Pan\nEfﬁcient Object Detection and Classiﬁcation of Ground Objects\nfrom Thermal Infrared Remote Sensing Image Based on Deep Learning . . . . . . . 165\nFalin Wu, Guopeng Zhou, Jiaqi He, Haolun Li, Y ushuang Liu,\nand Gongliu Y ang\nMEMA-NAS: Memory-Efﬁcient Multi-Agent Neural Architecture Search . . . . . 176\nQi Kong, Xin Xu, and Liangliang Zhang\nAdversarial Decoupling for Weakly Supervised Semantic Segmentation . . . . . . . 188\nGuoying Sun, Meng Y ang, and W enfeng Luo\nTowards End-to-End Embroidery Style Generation: A Paired Dataset\nand Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\nJingwen Y e, Yixin Ji, Jie Song, Zunlei Feng, and Mingli Song\nEfﬁcient and Real-Time Particle Detection via Encoder-Decoder Network . . . . . 214\nY uanyuan W ang, Ling Ma, Lihua Jian, and Huiqin Jiang\nFlexible Projection Search Using Optimal Re-weighted Adjacency\nfor Unsupervised Manifold Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\nY uting T ao, Haifeng Zhao, and Y an Zhang\nFabric Defect Detection via Multi-scale Feature Fusion-Based Saliency . . . . . . . 240\nZhoufeng Liu, Ning Huang, Chunlei Li, Zijing Guo, and Chengli Gao\nImproving Adversarial Robustness of Detector via Objectness\nR e g u l a r i z a t i o n ........................................................ 2 5 2\nJiayu Bao, Jiansheng Chen, Hongbing Ma, Huimin Ma, Cheng Y u,\nand Yiqing Huang\nIPE Transformer for Depth Completion with Input-Aware Positional\nEmbeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\nBocen Li, Guozhen Li, Haiting W ang, Lijun W ang, Zhenfei Gong,\nXiaohua Zhang, and Huchuan Lu\nEnhanced Multi-view Matrix Factorization with Shared Representation . . . . . . . 276\nSheng Huang, Y unhe Zhang, Lele Fu, and Shiping W ang', 'Contents - Part IV xvii\nMulti-level Residual Attention Network for Speckle Suppression . . . . . . . . . . . . . 288\nY u Lei, Shuaiqi Liu, Luyao Zhang, Ling Zhao, and Jie Zhao\nSuppressing Style-Sensitive Features via Randomly Erasing for Domain\nGeneralizable Semantic Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300\nSiwei Su, Haijian W ang, and Meng Y ang\nMAGAN: Multi-attention Generative Adversarial Networks\nfor Text-to-Image Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312\nXibin Jia, Qing Mi, and Qi Dai\nDual Attention Based Network with Hierarchical ConvLSTM for Video\nO b j e c tS e g m e n t a t i o n ................................................... 3 2 3\nZongji Zhao and Sanyuan Zhao\nDistance-Based Class Activation Map for Metric Learning . . . . . . . . . . . . . . . . . . 336\nY eqing Shen, Huimin Ma, Xiaowen Zhang, Tianyu Hu, and Y uhan Dong\nReading Pointer Meter Through One Stage End-to-End Deep Regression . . . . . . 348\nZhenzhen Chao, Y aobin Mao, and Yi Han\nDeep Architecture Compression with Automatic Clustering of Similar\nN e u r o n s .............................................................. 3 6 1\nXiang Liu, W enxue Liu, Li-Na W ang, and Guoqiang Zhong\nAttention Guided Spatio-Temporal Artifacts Extraction for Deepfake\nD e t e c t i o n ............................................................. 3 7 4\nZhibing W ang, Xin Li, Rongrong Ni, and Y ao Zhao\nLearn the Approximation Distribution of Sparse Coding with Mixture\nSparsity Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387\nLi Li, Xiao Long, Liansheng Zhuang, and Shafei W ang\nAnti-occluded Person Re-identiﬁcation via Pose Restoration and Dual\nChannel Feature Distance Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399\nBin Wu, Keyang Cheng, Chunyun Meng, and Sai Liang\nDynamic Runtime Feature Map Pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411\nPei Zhang, T ailin Liang, John Glossner, Lei W ang, Shaobo Shi,and Xiaotong Zhang', 'xviii Contents - Part IV\nSpecial Session: New Advances in Visual Perception and\nUnderstanding\nMulti-branch Graph Network for Learning Human-Object Interaction . . . . . . . . . 425\nT ongtong Wu, Xu Zhang, Fuqing Duan, and Liang Chang\nF D E A :F a c eD a t a s e tw i t hE t h n i c i t yA t t r i b u t e .............................. 4 3 7\nJun Chen, Ting Liu, Fu-Zhao Ou, and Y uan-Gen W ang\nTMD-FS: Improving Few-Shot Object Detection with Transformer\nMulti-modal Directing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447\nYing Y uan, Lijuan Duan, W enjian W ang, and Qing En\nFeature Matching Network for Weakly-Supervised Temporal Action\nL o c a l i z a t i o n .......................................................... 4 5 9\nPeng Dou, W ei Zhou, Zhongke Liao, and Haifeng Hu\nLiDAR-Based Symmetrical Guidance for 3D Object Detection . . . . . . . . . . . . . . . 472\nHuazhen Chu, Huimin Ma, Haizhuang Liu, and Rongquan W ang\nFew-Shot Segmentation via Complementary Prototype Learning\nand Cascaded Reﬁnement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484\nHanxiao Luo, Hui Li, Qingbo Wu, Hongliang Li, King Ngi Ngan,\nFanman Meng, and Linfeng Xu\nCouple Double-Stage FPNs with Single Pipe-Line for Solar Speckle\nImages Deblurring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496\nFuhai Li, Murong Jiang, and Lei Y ang\nMulti-scale Image Partitioning and Saliency Detection for Single Image\nB l i n dD e b l u r r i n g ...................................................... 5 0 8\nJiaqian Y an, Y u Shi, Xia Hua, Zhigao Huang, and Ruzhou Li\nCETransformer: Casual Effect Estimation via Transformer Based\nR e p r e s e n t a t i o nL e a r n i n g ................................................ 5 2 4\nZhenyu Guo, Shuai Zheng, Zhizhe Liu, Kun Y an, and Zhenfeng Zhu\nAn Efﬁcient Polyp Detection Framework with Suspicious Targets Assisted\nT r a i n i n g .............................................................. 5 3 6\nZhipeng Zhang, Li Xiao, Fuzhen Zhuang, Ling Ma, Y uan Chang,\nY uanyuan W ang, Huiqin Jiang, and Qing He\nInvertible Image Compressive Sensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548\nBingfeng Sun and Jian Zhang', 'Contents - Part IV xix\nGradient-Free Neural Network Training Based on Deep Dictionary\nL e a r n i n gw i t ht h eL o gR e g u l a r i z e r ....................................... 5 6 1\nYing Xie, Zhenni Li, and Haoli Zhao\nAuthor Index ......................................................... 5 7 5', 'Machine Learning, Neural Network\nand Deep Learning', 'Edge-Wise One-Level Global Pruning\non NAS Generated Networks\nQiantai Feng1,2,K eX u1,2, Yuhai Li3, Yuxin Sun4, and Dong Wang1,2(B)\n1Institute of Information Science, Beijing Jiaotong University, Beijing 100044, China\n2Beijing Key Laboratory of Advanced Information Science and Network Technology,\nBeijing 100044, China\nwangdong@bjtu.edu.cn\n3Science and Technology on Electro-Optical Information Security Control\nLaboratory, Tianjin 300308, China\n4Tianjin University, Tianjin 300072, China\nAbstract. In recent years, there has been a lot of studies in neural\narchitecture search (NAS) in the ﬁeld of deep learning. Among them,the cell-based search method, such as [ 23,27,32,36], is one of the most\npopular and widely discussed topics, which usually stacks less cells in\nsearch process and more in evaluation. Although this method can reducethe resource consumption in the process of search, the diﬀerence in the\nnumber of cells may inevitably cause a certain degree of redundancy\nin network evaluation. In order to mitigate the computational cost, wepropose a novel algorithm called Edge-Wise One-Level Global Pruning\n(EOG-Pruning). The proposed approach can prune out weak edges from\nthe cell-based network generated by NAS globally, by introducing anedge factor to represent the importance of each edge, which can not\nonly greatly improve the inference speed of the model with reducing the\nnumber of edges, but also promote the model accuracy. Experimentalresults show that networks pruned by EOG-Pruning achieve signiﬁcant\ni m p r o v e m e n ti na c c u r a c ya n ds p e e d u pr a t eo nC P Ui nc o m m o nw i t h\n50% pruning rate on CIFAR. Speciﬁcally, we reduced the test error rateby 1.58% and 1.34% on CIFAR-100 for DARTS (2nd-order) and PC-\nDARTS.\nKeywords: Neural architecture search\n·Network pruning\n1 Introduction\nIn the ﬁeld of machine learning, research based on deep learning has gradually\nbecome the mainstream. As the networks go deeper and larger, how to design\nSupported by Beijing Natural Science Foundation (4202063), Fundamental Research\nFunds for the Central Universities (2020JBM020), Research Founding of Electro-Optical Information Security Control Laboratory, National Key Research and Devel-\nopment Program of China under Grant 2019YFB2204200, BJTU-Kuaishou Research\nGrant.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 3–15, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_1', '4Q . F e n g e t a l .\nhigh-performance network structure for a certain task under certain constraints\nhas become a topic of interest to researchers. Neural network architecture search(NAS) is one of the methods to reduce the reliance on human expertise and arti-\nﬁcial priors, and obtain the optimal model by exploring the model architecture\nin a large search space by speciﬁc search algorithms. At the beginning, most ofthe work focuses on how to get a complete network [ 1,29]. A large number of\nmodels are sampled in the search space and these models are individually trained\nand evaluated.\nInspired by the success of stacking the same cell or block [ 13,15] in images\nclassiﬁcation task, more attention has been paid to ﬁnding the well behaved\nand robust cell. Some heuristic methods are usually used, such as reinforcementlearning [ 36], evolutionary learning [ 27,32], etc. However, they take a lot of\ncomputational cost, e.g., thousands of GPU-days, so these methods are diﬃcult\nto use widely. In order to reduce the amount of computation overhead, a series of\ngradient-based methods have been proposed [ 23,28]. So as to avoid the problem\nof excessive memory cost during search process, the approach such as DARTS[23], which becomes the most popular gradient-based method recently, uses less\ncells for search and more cells for evaluation.\nHowever, there are some problems for us with this cell-based NAS strategy.\nThe ﬁrst is the deeper network in evaluation is not necessarily compact and\nmay has a great redundancy, because all cells are the same, and the number of\nstacking cells and some conﬁguration are artiﬁcially set, without adapting to theactual task. For instance, in DARTS [ 23], there are 20 cells with 2 predecessor\ninputs stacking together on CIFAR, following NASNet [ 36], AmoebaNet [ 27],\nPNAS [ 6], etc. Although manual design conﬁguration can narrow the search\nspace for lower computational cost and keep the stability of networks, it causes\nthe over-parameterized and redundancy. The second is the depth gap mentioned\nin PDARTS [ 6]. An optimal narrow network in search processs cannot guarantee\nthe optimality of deeper network in evaluation. So it is necessary to optimize\nthe structure of the model.\nThe weakness of cell-based network mentioned above leads us naturally to try\na further structure search and compression. The traditional compression meth-\nods, like unstructured or structured pruning, can indeed compress the model,but they cannot verify the redundancy of the searched network and cannot ﬁnd\na better substructure.\nIn this work, We propose Edge-Wise One-Level Global Pruning on Architec-\nture Search (EOG-Pruning), a novel algorithm to prune the cell-based network\nof NAS. We allocate a weight for each edge in a cell-based network. After com-\npression through edge-wise structured pruning, some of the less important onesare removed by the weight size, so that the size of the model will be cut down\nand the inference performance will be accelerated. Furthermore, the obvious\nimprovement of accuracy has been widely found in our solid experiments.\nWe conduct experiments on popular image classiﬁcation benchmark, i.e.,\nCIFAR-10, CIFAR-100 [ 16], ImageNet [ 8], with mainstream networks based on\nDARTS, like DARTS [ 23], P-DARTS [ 6], PC-DARTS [ 33]. Through a series of', 'Edge-Wise One-Level Global Pruning on NAS Generated Networks 5\nvalidation and contrast tests, we can verify the our algorithm is feasible and eﬀec-\ntive. Speciﬁcally, we reduce the test error rate by 1.58% and 1.34% on CIFAR-100for DARTS (2nd-order) and PC-DARTS. Moreover, the performance of EOG-\nPruning under diﬀerent pruning rates is veriﬁed, and the accuracy of pruning\nrate below 50% is often higher than that of the original.\n2 Related Work\n2.1 Neural Architecture Search\nNeural network architecture search technology hopes to replace human experts\nwith prior knowledge of artiﬁcial experience by ﬁnding an optimal model that\nmeets the needs through speciﬁc algorithms in a design search space. The cur-\nrent method has achieved competitive performance in tasks, in the ﬁelds ofimage classiﬁcation [ 36], object detection [ 10,34] and so on. The NAS approach\ndesigns a search space containing the model architecture, and looks for the opti-\nmal architecture or hyperparameters through a speciﬁc search algorithm, e.g.,\nrandom search [ 21], meta learning [ 20], Bayesian optimization [ 2], evolution-\nary algorithms [ 32], reinforcement learning [ 1], gradient-based algorithms [ 23].\nIn architecture search, the pioneering work comes from Google Brain, usingreinforcement learning [ 35] to search in 2016. Later, more and more companies\nand research institutes are beginning to join the research, with various search\napproach to explore diﬀerent designed search space. Unlike the previous work[1,29], most of resent works focus on ﬁnding an optimal cell or block structure\n[13,15], and some search methods achieve great performance, such as reinforce-\nment learning [ 36], evolutionary learning [ 27,32]. Following works aim at solving\nthe calculation consumption in search process, e.g., PNASNet [ 22], ENAS [ 26],\nProxylessNAS [ 5], DARTS [ 23].\nDARTS is a gradient-based one-shot neural network search algorithm. By\ncombining the network weight and the architecture parameter for bi-level opti-\nmization, an optimal model can be searched out, which can be applied to convo-lutional and recurrent neural networks. The performance on some tasks is even\nbetter than some non-diﬀerentiable search techniques. The models searched on\nthe small data set (CIFAR-10) can be migrated to ImageNet for search. DARTSwill search the architecture in a narrow network while evaluating in a larger one\nwith more cells stacking together, e.g., 20 cells in CIFAR evaluation, bringing\non redundancy inevitably.\n2.2 Network Pruning\nThe network pruning technology reduces the model by removing some redundant\nparameters or structures, and realizes the acceleration of the model. Deep neural\nnetworks have been widely used, and even replaced many traditional machine\nlearning methods. It is also widely used in some mobile devices or some embeddeddevices. Although deep learning has better performance, it still has problems', '6Q . F e n g e t a l .\nsuch as high computational cost and slow inference speeds, so that it is not easy\nto deploy on lightweight devices. In order to solve this problem, methods suchas pruning and quantiﬁcation of neural networks have been proposed, which can\nreduce the size of the network, accelerate the speed of inference.\nPruning is classiﬁed according to the granularity of pruning from ﬁne to\ncoarse, which can be roughly divided into unstructured pruning and structured\npruning. Unstructured pruning [ 12,18] mainly constructs a sparse weight matrix\nby setting independent weight parameters to 0. However, this method is cur-rently not well supported on general hardware computing devices, but the sparse\nweight cannot achieve the acceleration of the target. So research began to lean\ntowards structured pruning [ 14,19,24,25]. Through pruning on the channel or\non the layer, the calculation amount and parameter amount of the model can\nbe reduced. This method does not destroy the basic structure, such as channel\npruning does not destroy the structure of the convolution layer, and it is easily\nsupported by various computing devices and acceleration libraries.\n3 Edge-Wise One-Level Global Pruning on DARTS-Based\nNetwork\nNowadays, there are many cell-based networks with diﬀerent methods, and\nDARTS is one of the popular methods among them. So our approach focus\non the DARTS-based networks [ 6,23,33] with the same search space, to make\nour approach more practical and convincing.\nDARTS-based networks regard as our baseline framework, from which we\nwork on extracting more concise one by remove weak edges. There are Lcells\nin a network, and each cell is represented as a directed acyclic graph (DAG)\nwithNnodes. The predeﬁned space of operations denoted by O, including all\npossibilities of edges.\nAfter search procedure, one operation has been chosen from Oto connect\neach pair of nodes ( i,j). Each node jkeeps only two strong predecessors i≤j,\nwhere 0 ≤i≤j≤N−1, so there are 2 NLedges in a network.\n3.1 Edge Weight Assignment\nAs shown in Fig. 1, we introduce a factor αfor every edges, indicating the impor-\ntance of the edge. After training process, the edge with small edge weight αwill\nbe pruned. In order to get close to the ﬁnal pruned network, αis initialized to\na number in [0 ,1], e.g.,αis set to 1-pruning rate in our experiments. Then we\nwant to keep alpha in a reasonable range, so we simply crop the αvalue to [0 ,1]\nin the training procedure, and it is formulated as:\nα∗=⎧\n⎨\n⎩11≤α\nα0≤α<1\n0α≤0(1)', 'Edge-Wise One-Level Global Pruning on NAS Generated Networks 7\nFig. 1. Method of adding weight to two predecessors. Orange and blue mean weights\nand nodes, respectively. (Color ﬁgure online)\nThen we multiply α∗with the output of that edge f(i,j)(x(i)):\nx(j)=/summationdisplay\ni<jα∗(i,j)f(i,j)(x(i)) (2)\nWith one exception, we don’t want to aﬀect operation skip-connection , because\nit has a signiﬁcant impact on recognition accuracy at the evaluation stage\n[6]. What’s more, as an operation without parameters, skip-connection won’t\naﬀect the model inference speed too much. Assuming that the number of skip-\nconnection is sand pruning rate is p, there are [ p(NL−s)] edges left in a network\nafter the pruning procedure.\n3.2 One-Level Structure Learning\nIn structure learning procedure, EOG-Pruning aims to optimize the training\nloss, by learning the edge weights αand the network weights w.\nAs we introduce edge weight α, the goal of structure learning is to solve\nfollowing objective function, which seems like the goal of DARTS:\nmin\nαLtrain/parenleftBig\nω∗(α),α/parenrightBig\n(3)\ns.t.ω∗(α) = argminωLtrain(ω,α) (4)\nIn the actual training process, the quantity gap between αandwis obvious, so\nDARTS proposed bi-level optimization to alleviate the problem. As pointed out\nin the [ 3], bi-level optimization takes more computational overhead, leading to\ncritical weakness known as considerable inaccuracy of gradient estimation and\ninstability. So we follow the one-level optimization method just like [ 4], which\nadds some regularization ( e.g., Cutout [ 9], AutoAugment [ 7], etc.) to a small\ndata set. One-level optimization involves updating αandwsimultaneously in\neach step:', '8Q . F e n g e t a l .\nInitialized Model Training Trained Model Pruning Pruned Model Fine-tuning Final ModelStructure Learning Precision Recovery\nFig. 2. The pipeline of EOG-Pruning. After a robust cell was found in search process,\na network of stacked cells will be put into the pipeline. It will go through three stages:\ninitialized, pruned and ﬁne-tuned. Training process will change distribution of edgeweight, and the edge with small αwill be pruned. Blue and green indicate the stage\nand the treatment of a network. (Color ﬁgure online)\nωt+1←ωt−ξw·∇ωLtrain/parenleftBig\nωt,αt/parenrightBig\n(5)\nαt+1←αt−ξα·∇αLtrain/parenleftBig\nωt,αt/parenrightBig\n(6)\nIt balances the optimization process between αandw, while alleviating over-\nﬁtting phenomenon. What’s more, the learning rates ξαandξwofαandw\ncorrespondingly need to be set to diﬀerent sizes to ﬁx the issue above.\n3.3 Procedure of EOG-Pruning\nAs shown in Fig. 2, the process of EOG-Pruning can be summarized as three\nsteps: training, pruning and ﬁne-tuning. The network learns structure through\none-level optimization training, and restores accuracy through ﬁne-tuning. There\nare two mainstream strategies to restore the recovery precision. One way is toprune once and retrain the network until restore the accuracy of the baseline, and\nthe other way is to prune and retrain the model iteratively [ 19]. For simplicity\nand eﬀectiveness, We just adopt the ﬁrst method.\n3.4 Comparison with Previous Work\nNetwork Slimming [ 24] associates a scaling factor γwhich is reused from a batch\nnormalization layer with each channel in convolutional layers, and the channels\nwith tiny scaling factor values will be removed. EOG-Pruning is more coarse-\ngrained pruning paradigm, suitable for networks composed of cells and edges,without L1 regularization on α.P C - D A R T S[ 33] also associates a factor with\neach edge for training stability, so the purpose is diﬀerent from us.\n4 Experiments\n4.1 Datasets\nWe demonstrate the eﬀectiveness of EOG-Pruning on several datasets, including\nCIFAR-10, CIFAR-100 [ 16] and ImageNet [ 8]. Each of CIFAR-10 and CIFAR-\n100 datasets consists of 50K/10K training/testing 32 ×32 natural colour images,', 'Edge-Wise One-Level Global Pruning on NAS Generated Networks 9\nwith 6K/0.6K images per class. ImageNet dataset contains 1.2 million training\nimages and 50K validation images of 1K classes. Following the mobile setting inDARTS [ 23], the input image size is ﬁxed to be 224 ×224 and the number of\nmulti-add operations does not exceed 600M in the testing stage.\nTable 1. Results for EOG-Pruning on CIFAR-10, CIFAR-100 and ImageNet\nDataset Model Test error Params FLOPs Latency Edge# Conv# Pool#\n(%) (M) (M) (s)\nCIFAR10 DARTS\n(1st-order)(Baseline)3 3.3 511 12.89 100 403 10\nDARTS(1st-order)(50%\nPruned)2.54( −0.46) 2.4(27.3%) 371(27.4%) 7.35(1.8 ×)50 227 4\nDARTS\n(2nd-order)(Baseline)2.76 3.3 539 13.69 118 439 10\nDARTS(2nd-order)(50%\nPruned)2.53( −0.23) 2.4(27.3%) 380(29.5%) 7.56(1.8 ×)59 237 6\nPDARTS (Baseline) 2.50 3.4 542 13.23 124 435 4\nPDARTS (50%\nPruned)2.42( −0.08) 2.3(32.4%) 385(29.0%) 7.71 (1.7 ×)62 239 4\nPC-DARTS\n(Baseline)2.57 3.6 567 13.85 142 459 20\nPC-DARTS (50%Pruned) 2.34( −0.23) 2.5(30.6%) 393(30.7%) 7.57(1.8 ×)71 255 8\nGOLD-NAS-H 2.70 2.5 402 – – – –\nCIFAR100 DARTS\n(1st-order)(Baseline)17.76 3.3 511 12.89 100 403 10\nDARTS(1st-order)(50%\nPruned)17.19( −0.57) 2.4(27.3%) 371 (27.4%) 7.35(1.8 ×)50 227 4\nDARTS\n(2nd-order)(Baseline)17.54 3.3 539 13.69 118 439 10\nDARTS(2nd-order)(50%\nPruned)15.96( −1.58) 2.4(27.3%) 384(28.8%) 7.96(1.7 ×)59 243 6\nPDARTS (Baseline) 16.55 3.4 542 13.23 124 435 4\nPDARTS (50%\nPruned)16.76(+0.21) 2.4(29.4%) 386(28.8%) 7.98(1.7 ×)62 241 4\nPC-DARTS\n(Baseline)17.01†3.6 567 13.85 142 459 20\nPC-DARTS (50%Pruned) 15.67( −1.34) 2.5(30.6%) 393(30.7%) 7.57(1.8 ×)71 245 11\nImageNet DARTS (Baseline) 26.7 4.7 574 6.93 82 298 10\nDARTS (30%\nPruned)26.05( −0.65) 4.4(6.4%) 460(19.9%) 5.66(1.2 ×)57 224 6\nDARTS (50%\nPruned)27.64(+0.97) 3.9(17.0%) 393(31.5%) 4.18(1.7 ×)41 160 6\n4.2 EOG-Pruning on CIFAR-10 and CIFAR-100\nCompare with the evaluation process of DARTS [ 23], we just add an extra joint\ntraining procedures of αandwtraining before the normal evaluation training.\nThe training process is equivalent to modifying the learning rates ξαandξw,', '10 Q. Feng et al.\nFig. 3. Distribution of αin DARTS on CIFAR-10 and ImageNet separately after\ntraining.\nwhile adding AutoAugment [ 7] strategy to train on the basis of all conﬁgurations\nin DARTS. We initialize the network randomly, and simply set αto 1−p.α\nandwin the network of 20 cells is trained jointly for 50 epochs by one-level\noptimization with batch size 128. The initial learning rates for αandware\n0.025 and 0.01, accordingly, using Cosine Annealing . Cutout regularization [ 9]\nof length 16, drop-path [ 17] of probability 0.3 and auxiliary towers [ 30] of weight\n0.4 are applied. When the joint training process is over, the model will be prunedby manual pruning percentage p. Then we ﬁne-tune the pruned model for 600\nepochs with the previous conﬁguration, and set the learning rate of wto 0.025.\nThe results of EOG-Pruning on CIFAR-10 and CIFAR-100 are summarized\nin Table 1. The pruning rate is set to 50%. We also test the model inference\nspeed. We deﬁne that latency is the average 10 rounds of time required for\ninference once when batch size is equal to 1, and the speedup rate means thespeed of baseline model divided by the speed of pruned model. The speed test\nis performed on the Intel(R) Xeon(R) CPU E5-2609.\nAs demonstrated in Table 1, the networks pruned by EOG-Pruning surpass\nthe baselines in speed and accuracy generally, with 50% pruning rate, achieving\n0.23% improvement of accuracy on CIFAR-10 basically and 1.8 speedup ratein common. Specially, the accuracy of DARTS (2nd-order) and PC-DARTS on\nCIFAR100 is 1.58% and 1.34% higher than the original respectively. Only the\nPDARTS on CIFAR-100 has a slightly decrease in accuracy. We also compare ourresult to GOLD-NAS on CIFAR-10, which adopts a progressive pruning strategy\nin search process. Under similar parameters scale, EOG-Pruning performs better.\nThe distribution of αin DARTS is shown in Fig. 3(a). Most of αare chieﬂy\ndistribute between 0.4 and 0.6. It shows that the training procedure makes a\ncertain degree of discrimination conductive to pruning.\n4.3 EOG-Pruning on ImageNet\nWe embrace the same pruning framework of CIFAR to ImageNet. In training\nprocess, we optimize αandwjointly for 10 epochs, and ﬁne-tuning for 250', 'Edge-Wise One-Level Global Pruning on NAS Generated Networks 11\nFig. 4. The structure of 50% pruned percentage of DARTS (2nd-order) on CIFAR-10\nand ImageNet, separately, containing 20/14 cells in the network. The red, blue, black,dotted lines indicate the pruned edges, preserved edges, concatenation, skip-connection,\nrespectively. The orange, purple, pink circles mean the nodes in cell, the predecessors\nand output of each cell, respectively. (Color ﬁgure online)\nFig. 5. Eﬀectiveness of diﬀerent pruning rates based on PC-DARTS. The learning rates\nξαandξware all set to 0.025 for brevity.\nepochs after pruning. The network conﬁguration of DARTS is adopted in our\ntraining and ﬁne-tuning procedure, except that we don’t employ the learning\nrate warmup [ 11] at the beginning empirically. The network of 14 cells is trained\nwith batch size 128. The learning rates ξαandξware 0.025 and 0.01 in training,\nand in ﬁne-tuning, learning rate of wis set to 0.5. Both of αandware optimized\nby SGD with the momentum of 0.9 and the weight decay of 3 ×10−5. Additional\nenhancements including label smoothing [ 31] and auxiliary loss tower [ 30]a r e\napplied during training.\nThe results of EOG-Pruning on ImageNet are summarized in Table 1.W e\ncan observe that our method outperforms the baseline with diﬀerent pruning\nrate. The distribution of αis shown in Fig. 3(b), and it’s easy to distinguish\nthe relatively unimportant edges. We can see that the pruned edges are mainlydistributed in the front of the network.', '12 Q. Feng et al.\n4.4 Visualization\nThe visualization of DARTS (2nd-order) is shown in Fig. 4. The models both\nmaintain good connectivity with 50% pruning rate. Their performance indicatesthat some edges are redundant.\n4.5 Ablation Study\nEﬀectiveness of Pruned Percentage. In EOG-Pruning, we should set the\npruning rate before the pruning process. So it is necessary for us to ﬁgure out the\nrelationship between the pruning rate and the model accuracy, in order to make\na desired trade-oﬀ. The results of diﬀerent pruning rate based on PC-DARTSare summarized in Fig. 5. If we want to obtain higher accuracy than the original\non CIFAR-10, we should set the pruning rate below 50%, and large pruning rate\nmay result in loss of accuracy. Moreover, we can observe that the latency changesmore consistently with FLOPs than the number of parameters.\nDiﬀerent Strategies of One-Level Structure Learning. In EOG-Pruning,\nwe suppose One-Level Structure Learning, which should set diﬀerent learning\nrates for structure factors αand network weights w, and use AutoAugment [ 7]\nto boost the performance of models. The results of ablation test based on PC-\nDARTS with 50% pruning rate are summarized in Table 2. The advantages of\nthese two methods can be proved by experiments.\nTable 2. Diﬀerent strategies of one-level optimization on PC-DARTS with 50% prun-\ning rate.\nModel Acc. Diﬀ. lr AutoAug.\n(%) (M) (M)\nPC-DARTS (Baseline) 97.43 – –\nPC-DARTS (50% Pruned) 97.66 ✓ ✓\nPC-DARTS (50% Pruned) 97.53 ✗ ✓\nPC-DARTS (50% Pruned) 97.15 ✓ ✗\nPC-DARTS (50% Pruned) 97.31 ✗ ✗\n5 Conclusion\nWe propose a pioneering pruning approach called EOG-Pruning (Edge-Wise,\nOne-Level, Global) for the cell-based NAS networks to reduce the redundancy\ncaused by model stacking. It imposes some factors to indicate the importanceof edges in the network. The factors will be sorted after the training process,', 'Edge-Wise One-Level Global Pruning on NAS Generated Networks 13\nand the edges with the factors lower than the manual threshold will be pruned.\nOur approach not only reduces the model size and achieves speedup, but alsofurther improves the accuracy proved by our experiments on various datasets.\nReferences\n1. Baker, B., Gupta, O., Naik, N., Raskar, R.: Designing neural network architec-\ntures using reinforcement learning. In: Submitted to International Conference on\nLearning Representations (2016)\n2. Bergstra, J., Bardenet, R., Bengio, Y., K´ egl, B.: Algorithms for Hyper-parameter\noptimization. In: Proceedings of the 24th International Conference on Neural Infor-\nmation Processing Systems, pp. 2546–2554. NIPS 2011, Curran Associates Inc.,Red Hook, NY, USA (2011), event-place: Granada, Spain\n3. Bi, K., Hu, C., Xie, L., Chen, X., Wei, L., Tian, Q.: Stabilizing DARTS\nwith Amended Gradient Estimation on Architectural Parameters. arXiv e-prints\narXiv:1910.11831 (2019)\n4. Bi, K., Xie, L., Chen, X., Wei, L., Tian, Q.: GOLD-NAS: Gradual, One-Level,\nDiﬀerentiable. arXiv e-prints arXiv:2007.03331 (2020)\n5. Cai, H., Zhu, L., Han, S.: ProxylessNAS: Direct Neural Architecture Search on\nTarget Task and Hardware. arXiv e-prints arXiv:1812.00332\n6. Chen, X., Xie, L., Wu, J., Tian, Q.: Progressive Diﬀerentiable Architecture\nSearch: Bridging the Depth Gap between Search and Evaluation. arXiv e-prints\narXiv:1904.12760 (2019)\n7. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: AutoAugment: learning\naugmentation strategies from data. In: 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 113–123. IEEE, Long Beach, CA,USA, June 2019. https://doi.org/10.1109/CVPR.2019.00020 ,https://ieeexplore.\nieee.org/document/8953317/\n8. Deng, J., Dong, W., Socher, R., Li, L., Li, K., Fei-Fei, L.: ImageNet: a large-scale\nhierarchical image database. In: 2009 IEEE Conference on Computer Vision and\nPattern Recognition, pp. 248–255 (2009). https://doi.org/10.1109/CVPR.2009.\n5206848\n9. DeVries, T., Taylor, G.W.: Improved Regularization of Convolutional Neural Net-\nworks with Cutout. arXiv e-prints arXiv:1708.04552 (2017)\n10. Ghiasi, G., Lin, T.Y., Pang, R., Le, Q.V.: NAS-FPN: Learning Scalable Feature\nPyramid Architecture for Object Detection. arXiv e-prints arXiv:1904.07392 ,A p r i l\n2019\n11. Goyal, P., et al.: Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.\narXiv e-prints arXiv:1706.02677 (2017)\n12. Han, S., Mao, H., Dally, W.J.: Deep compression: compressing deep neural net-\nworks with pruning. ICLR, trained quantization and huﬀman coding (2016)\n13. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 770–778 (2016). https://doi.org/10.1109/CVPR.2016.90\n14. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural\nnetworks. In: 2017 IEEE International Conference on Computer Vision (ICCV),\npp. 1398–1406 (2017). https://doi.org/10.1109/ICCV.2017.155', '14 Q. Feng et al.\n15. Huang, G., Liu, Z., Maaten, L.V.D., Weinberger, K.Q.: Densely connected con-\nvolutional networks. In: 2017 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 2261–2269 (2017). https://doi.org/10.1109/CVPR.2017.\n243\n16. Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images.\nHandb. Syst. Autoimmune Dis. 1(4) (2009)\n17. Larsson, G., Maire, M., Shakhnarovich, G.: Fractalnet: Ultra-deep neural networks\nwithout residuals. In: ICLR (2017)\n18. LeCun, Y., Denker, J.S., Solla, S.A.: Optimal brain damage. In: Touretzky, D.S.\n(ed.) Advances in Neural Information Processing Systems 2, pp. 598–605. Morgan-Kaufmann (1990). http://papers.nips.cc/paper/250-optimal-brain-damage.pdf\n19. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning ﬁlters for eﬃ-\ncient ConvNets. In: International Conference on Learning Representations (ICLR)\n(2016)\n20. Li, K., Malik, J.: Learning to Optimize. arXiv e-prints arXiv:1606.01885 (2016)\n21. Li, L., Talwalkar, A.: Random Search and Reproducibility for Neural Architecture\nSearch. arXiv e-prints arXiv:1902.07638 (2019)\n22. Liu, C., et al.: Progressive Neural Architecture Search. arXiv e-prints\narXiv:1712.00559 (2017)\n23. Liu, H., Simonyan, K., Yang, Y.: DARTS: Diﬀerentiable Architecture Search. arXiv\ne-prints arXiv:1806.09055 (2018)\n24. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning eﬃcient convolu-\ntional networks through network slimming. In: Proceedings of the IEEE Interna-tional Conference on Computer Vision (ICCV) (2017)\n25. Luo, J.H., Wu, J., Lin, W.: ThiNet: a ﬁlter level pruning method for deep neural\nnetwork compression. In: ICCV, pp. 5068–5076 (2017). https://doi.org/10.1109/\nICCV.2017.541\n26. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Eﬃcient neural architecture\nsearch via parameter sharing. In: ICML, pp. 4092–4101 (2018). http://proceedings.\nmlr.press/v80/pham18a.html\n27. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image classi-\nﬁer architecture search. In: AAAI, pp. 4780–4789 (2019). https://doi.org/10.1609/\naaai.v33i01.33014780\n28. Shin*, R., Packer*, C., Song, D.: Diﬀerentiable neural network architecture search.\nIn: In International Conference on Learning Representationss-Workshops (2018).\nhttps://openreview.net/forum?id=BJ-MRKkwG\n29. Suganuma, M., Shirakawa, S., Nagao, T.: A genetic programming approach to\ndesigning convolutional neural network architectures. In: IJCAI, pp. 5369–5373\n(2018). https://doi.org/10.24963/ijcai.2018/755\n30. Szegedy, C., et al.: Going deeper with convolutions. In: 2015 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 1–9 (2015). https://doi.\norg/10.1109/CVPR.2015.7298594\n31. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the incep-\ntion architecture for computer vision. In: CVPR, pp. 2818–2826 (2016). https://\ndoi.org/10.1109/CVPR.2016.308\n32. Xie, L., Yuille, A.: Genetic CNN. In: 2017 IEEE International Conference on Com-\nputer Vision (ICCV), pp. 1388–1397 (2017). https://doi.org/10.1109/ICCV.2017.\n154\n33. Xu, Y., et al.: PC-DARTS: partial channel connections for memory-eﬃcient archi-\ntecture search. In: Submitted to International Conference on Learning Represen-\ntations (2020). https://openreview.net/forum?id=BJlS634tPr', 'Edge-Wise One-Level Global Pruning on NAS Generated Networks 15\n34. Yao, L., Xu, H., Zhang, W., Liang, X., Li, Z.: SM-NAS: Structural-to-Modular\nNeural Architecture Search for Object Detection. arXiv e-prints arXiv:1911.09929\n(2019)\n35. Zoph, B., Le, Q.V.: Neural Architecture Search with Reinforcement Learning.\narXiv e-prints arXiv:1611.01578 (2016)\n36. Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning transferable architectures\nfor scalable image recognition. In: Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), June 2018', 'Convolution Tells Where to Look\nFan Xu1,2, Lijuan Duan1,3,4(B), Yuanhua Qiao5, and Ji Chen2\n1Faculty of Information Technology, Beijing University of Technology, Beijing, China\nljduan@bjut.edu.cn\n2Peng Cheng Laboratory, Shenzhen, China\n3Beijing Key Laboratory of Trusted Computing, Beijing, China\n4National Engineering Laboratory for Key Technologies of Information Security\nLevel Protection, Beijing, China\n5College of Mathematics and Physics, Beijing University of Technology,\nBeijing, China\nAbstract. Many attention models have been introduced to boost the\nrepresentational power of convolutional neural networks (CNNs). Most ofthem are self-attention models which generate an attention mask based\non current features, like spatial attention and channel attention model.\nHowever, these attention models may not achieve good results whenthe current features are the low-level features of CNNs. In this work,\nwe propose a new lightweight attention unit, feature diﬀerence (FD)\nmodel, which utilizes the diﬀerence between two feature maps to gener-ate the attention mask. The FD module can be integrated into most of\nthe state-of-the-art CNNs like ResNets and VGG just by adding some\nshortcut connections, which does not introduce any additional parame-ters and layers. Extensive experiments show that the FD model can help\nimprove the performance of the baseline on four benchmarks, includ-\ning CIFAR10, CIFAR100, ImageNet-1K, and VOC PASCAL. Note thatResNet44 (6.10% error) with FD model achieves better results than\nResNet56 (6.24% error), while the former has fewer parameters than\nthe latter one by 29%.\nKeywords: Feature representation\n·Attention model ·Image\nclassiﬁcation\n1 Introduction\nConvolutional neural networks (CNNs) have become one of the most eﬀective\nmethods in the ﬁeld of image processing. Because of their powerful feature\nextraction capabilities [ 6,9,14,16,22,23]. Recently, some researchers introduce\nthe attention mechanism to tell the convolutional neural network where to look,\nwhich signiﬁcantly improve the feature extraction power of CNNs [ 4,24,25,28],\nlike channel attention [ 4] and spatial attention [ 28].\nFrom Fig. 1(e), we can observe that the attention values applied to the dif-\nferent channels of the low-level feature do not reﬂect the prominent diﬀerences\nbetween the diﬀerent classes. From Fig. 1(b), we see that the spatial attention\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 16–28, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_2', 'Convolution Tells Where to Look 17\nOriginal imageCAM of CAM of CAM of \n(a) (b) (c) (d)\nFig. 1. The results of (b), (c), (d) and (e) are based on ResNet50. (a) An image from\nthe validation set of ImageNet-1K. (b) The grad-CAM [ 19] visualization of the 5th\nconvolutional outputs (conv1). (c) The grad-CAM visualization of the 6th convolutional\noutputs (conv2). (d) The grad-CAM visualization of |conv1−conv2|.\nmask based on the low-level feature can not tell the position of the object well.\nBoth of the two attention model generate attention mask based on current fea-\ntures. However, the low-level features of CNNs are general for all objects which\ndo not reﬂect diﬀerences between diﬀerent categories well [ 29]. So it is not easy\nto obtain the excellent attention mask when these two attention models are\napplied for the low-level features and the input image has a very complicated\nbackground. What is more, Fig. 1(c) is obtained after a convolutional opera-\ntion is applied to the feature of Fig. 1(b). If we subtract the Fig. 1(b) from the\nFig.1(c), we can get the Fig. 1(d). We observed that the Fig. 1(d) could show the\nposition of the object than Fig. 1(b) or the Fig. 1(c) better. Base on that, in this\npaper, we propose a novel attention model that can locate the object quickly\nbased on low-level features of CNNs, which we term the “Feature Diﬀerence”\n(FD) module. The FD module can tell the network which areas in a feature map\nshould be enhanced and which ones should be suppressed based on the diﬀerence\nbetween two features, especially for low-level features. The structure of the FDmodule we proposed is shown in Fig. 2. Given an input feature X, we get the\noutput feature Fafter a convolutional operation is applied. So, there should be\na diﬀerence between the output Fand input Xbecause of the convolutional\noperation. The diﬀerence can reﬂect the eﬀect of the convolutional operation on\nthe input Xand the response of input Xto it. The areas with the high response\nFig. 2. The architecture of a feature diﬀerence module. It infers attention mask along\nboth of channel and spatial dimensions.', '18 F. Xu et al.\nshould be enhanced, while the area with the low response should be suppressed.\nTherefore, we can let the convolutional layer itself tell the network where to lookby mapping the response to an attention mask and using it to reﬁne the output\nfeature.\nIn order to validate the eﬀects of the proposed FD module, we embed it\ninto some very popular CNNs to build the FDNets and test the performance of\nFDNets on four benchmark datasets (CIFAR-10 [ 5], CIFAR-100 [ 5], ImageNet-\n1K [1], and VOC PASCAL). We ﬁnd that FDNets achieve good performance\non those datasets, which demonstrates that the FD module can help improve\nthe performance of CNNs without additional parameters. In summary, our work\nmakes the following contributions:\n1. The proposed FD model is tiny and can be embedded into most of the popular\nCNNs easily, such as ResNets and VGG, with no additional parameters.\n2. We verify the performance of the FD model on four benchmark datasets,\nincluding CIFAR10, CIFAR100, ImageNet-1K, and VOC PASCAL.\n2 Related Work\nConvolutional Neural Network. VGGNets [ 20] generate a larger receptive\nﬁeld by stacking multiple small ﬁlter kernels, which signiﬁcantly reduce thes\ntraining parameters of CNNs. VGGNets reach a depth of 16 or 19 layers, butdeeper networks cause the problem of vanishing/exploding gradients, which\nmakes the networks diﬃcult to train and optimize. Inspired by the idea of con-\ntrol gates in long short-term memory networks, ResNets [ 3] add the input of\none layer to the output with a shortcut connection, which makes information\nof the low layer transmitted to the higher one completely. And it also solves\nthe problem that the deep neural network is diﬃcult to train and achieve bet-ter performance than HighWay [ 21]. ResNeXt [ 26] adopt grouped convolution\ninstead to reduce the parameters of the convolutional layer so that larger fea-\nture dimensions can be used for better results. SKNet [ 8] employs the channel\nattention model to select the convolution kernel, which makes the network use\nconvolutional kernels with diﬀerent receptive ﬁeld sizes based on input featureadaptively.\nAttention Mechanism. The visual attention mechanism plays a signiﬁcant\nrole in human vision, which greatly improves the eﬃciency and accuracy of\nvisual information processing [ 2,11,13,18]. In recent years, many researchers\nhave tried to introduce the attention mechanism to computer vision tasks to ﬁnd\nand enhance those signiﬁcant foreground while ignoring those background [ 7,10,\n15,27]. The residual attention network [ 24] proposes a bottom-up and top-down\nstructure to generates the attention mask. However, due to its complexity, it is\nnot easy to combine it with CNNs that contain no residual learning, especially\nwhen CNNs have few layers. Hu [ 4] proposes a channel attention module to\nrecalibrate the feature adaptively with the squeeze-and-excitation block. On that\nwork, Sanghyun [ 25] proposes the convolutional block attention module (CBAM)\nwhich infers attention maps both along the channel and spatial dimensions.', 'Convolution Tells Where to Look 19\n3 Feature Diﬀerence Module\nIn a convolutional layer of CNNs, given an input feature X, we get the output\nFafter a convolutional operator:\nF=f{X,W}, (1)\nwhere XandFare the input and output features of a convolutional layer and\nf(·) represents the convolutional operator; Wdenotes the trainable parameters\nof a set of convolutional kernels (for simplicity, we omit the biases). When aconvolutional operator is applied to input X, it changes the input Xand causes\na diﬀerence between output Fand input X. Therefore, we consider that the\ndiﬀerence between output Fand input Xreﬂects the eﬀect of the convolutional\noperator on the input Xand also represents the response of input Xto the\nconvolutional operator. Because of the backpropagation algorithm, the value of\nthe response is related to the label. So, those areas that are highly responsiveto the convolutional operator should be enhanced, while others that are low\nresponsive to the convolutional operator should be suppressed. Based on that,\nwe design the feature diﬀerence (FD) module to measures the response of inputXto the convolutional operator by calculating the diﬀerence between input X\nand output F. And then an attention mask is generated to reﬁne the output\nfeature maps Fbased on those response. The structure of the FD module is\nshown in Fig. 2.\n3.1 Feature Diﬀerence\nIn order to measure the diﬀerence between two feature maps conveniently, we\nassume that the pixel value of any position in input X∈R\nH×W×Ccan be\ndenoted as pi,j,k, where i∈[0,W),j∈[0,H),k∈[0,C). After applying\nthe convolutional operator to the input X, we obtain the output feature maps\nF∈RH×W×C(specify that the convolutional operator does not change the size\nof input X). The pixel value of any position in F is denoted as p/prime\ni,j,k. If the pixel\nvalues in some positions do not change after the convolutional operator, that is\np/prime\ni,j,k=pi,j,k, we can assume the convolutional operator has no eﬀect to those\npixels. If the pixel values in some positions changed, that is p/prime\ni,j,k/negationslash=pi,j,k,w e\nthink that the convolutional operator has eﬀect on this pixel. The changes of\npixel values in input Xreﬂects the response of the input Xto the convolutional\noperator. Inspired by smooth L1loss, we measure the eﬀect of the convolutional\noperator on input Xby the following equation:\nD(X)=smooth (F−X), (2)\nin which\nsmooth (x)=/braceleftbigg0.5\nβx2if|x|<1\n|x|−0.5β otherwise(3)\nwhere βis a hyperparameter and it is set to 1 in this work.', '20 F. Xu et al.\nWhenDi,j,k(x) is large, it indicates that the convolutional operator has more\neﬀect on this pixel and when Di,j,k(x) is small, it indicates that the convolutional\noperator has less eﬀect on that one. In particular, when Di,j,k(x) equals to 0, it\nindicates that the convolutional operator has no eﬀect on it.\nDtells CNNs that in the current feature maps, which areas should be focused\non and enhanced, and which areas should be suppressed. So, we have the follow-\ning way to reﬁne the output feature maps F:\nM(X)=σ(D(X)), (4)\n˜F=M(X)⊗F, (5)\nwhere ⊗denotes element-wise multiplication. σis an activation function and we\nwill discuss it in the next section. ˜Fis reﬁned feature maps by attention mask\nM.F,Mand˜Fhave the same size.\n3.2 FD Networks\nFig. 3. The original residual module (left) and the FD-residual module (right).\nThe proposed FD module is very tiny and easy to implement, so we can embed\nFD module into some popular CNNs easily to build FDNets. To illustrate that,\nwe describe how to embed the FD module into ResNets [ 3] and VGG to build\nFDNets in this section.\nThe FD-residual block structure that makes up FD-ResNets is shown in Fig. 3\nand the FD-VGG block structure that makes up FD-VGG is shown in Fig. 4.\nSince we use Eq. 2to calculate the diﬀerence between two feature maps, we\nneed to ensure that they have the same size. Therefore, when the convolutional\noperator does not change the size of the input feature, we use the FD module toreﬁne the output feature. Note that we just introduced some new short connec-\ntions and operations on the original module to build the FD model, so it is no\nadditional parameters.', 'Convolution Tells Where to Look 21\nFig. 4. The original VGG module (left) and the FD-VGG module (right).\n4 Experiments\nIn this section, we embed the FD module into some benchmark CNNs to build\nFDNets and evaluate the performance of FDNets on four datasets (CIFAR-\n10 [5], (CIFAR-100 [ 5], VOC 2012), and ImageNet-1K [ 1]). In order to perform an\napple-to-apple comparison, all results are reproduced in the PyTorch framework.Firstly, we conduct a simple ablation analysis to determine the ﬁnal structure of\nFDNets, which mainly includes the position of the FD module. Then we compare\nthe performance of FDNets with original ones on CIFAR-10 and CIFAR-100 andImageNet-1K datasets, and we also compare FDNets with other state-of-the-art\nmodels, including some other attention models. In addition, an experiment is\ndone on VOC PASCAL object detection data to verify the generalization of theFD model.\nTable 1. Test error (%) on CIFAR-10 and CIFAR-100 and all results are reproduced\nin the PyTorch framework. Note that ResNets have same parameters with ResNets-FD\nDescription CIFAR-10 CIFAR100\nResNet-32 [ 3] 7.16 29.13\nResNet-32 + FD 6.77 28.75\nResNet-44 6.55 28.16\nResNet-44 + FD 6.10 27.83\nResNet-56 6.24 27.30\nResNet-56 + FD 5.82 27.11\nResNet-110 5.82 26.26\nResNet-110 + FD 5.60 25.99', '22 F. Xu et al.\nTable 2. The architecture of CNNs for ImageNet-1K dataset. ( The third column )\noriginal ResNet-34. ( The fourth column ) FD-ResNet-34. ( T h eﬁ f t hc o l u m n )o r i g -\ninal ResNet-50. ( The sixth column ) FD-ResNet-50. “FD( F1,F2)”means that FD\nmodule is used in this block to calculate the feature diﬀerence between feature F1and\nfeature F2and then F2will be reﬁned with it.\nLayer Output size ResNet-34 FD-ResNet-34 ResNet-50 FD-ResNet-50\nconv1 112 ×112 7×7, 16, stride 2\n3×3 max pooling, 16, stride 2\nconv2 x56 ×56/bracketleftBigg\n3×3,64\n3×3,64/bracketrightBigg\n×3⎡\n⎢⎢⎣F1,3×3,64\nF2,3×3,64\nFD (F1,F2)⎤\n⎥⎥⎦×3⎡\n⎢⎢⎣1×1,64\n3×3,64\n1×1,256⎤\n⎥⎥⎦×3⎡\n⎢⎢⎢⎢⎣F1,1×1,64\nF2,3×3,64\n1×1,256\nFD (F1,F2)⎤\n⎥⎥⎥⎥⎦×3\nconv3 x28 ×28/bracketleftBigg\n3×3,128\n3×3,128/bracketrightBigg\n×4⎡\n⎢⎢⎣F1,3×3,128\nF2,3×3,128\nFD (F1,F2)⎤\n⎥⎥⎦×4⎡\n⎢⎢⎣1×1,128\n3×3,128\n1×1,512⎤\n⎥⎥⎦×4⎡\n⎢⎢⎢⎢⎣F1,1×1,128\nF2,3×3,128\n1×1,512\nFD (F1,F2)⎤\n⎥⎥⎥⎥⎦×4\nconv4 x14 ×14/bracketleftBigg\n3×3,256\n3×3,256/bracketrightBigg\n×6⎡\n⎢⎢⎣F1,3×3,256\nF2,3×3,256\nFD (F1,F2)⎤\n⎥⎥⎦×6⎡\n⎢⎢⎣1×1,256\n3×3,256\n1×1,1024⎤\n⎥⎥⎦×6⎡\n⎢⎢⎢⎢⎣F1,1×1,256\nF2,3×3,256\n1×1,1024\nFD (F1,F2)⎤\n⎥⎥⎥⎥⎦×6\nconv5 x7×7/bracketleftBigg\n3×3,512\n3×3,512/bracketrightBigg\n×3⎡\n⎢⎢⎣F1,3×3,512\nF2,3×3,512\nFD (F1,F2)⎤\n⎥⎥⎦×3⎡\n⎢⎢⎣1×1,512\n3×3,512\n1×1,2048⎤\n⎥⎥⎦×3⎡\n⎢⎢⎢⎢⎣F1,1×1,512\nF2,3×3,512\n1×1,2048\nFD (F1,F2)⎤\n⎥⎥⎥⎥⎦×3\n1×1 Global average pool, 1000-d fc, softmax\nParam. 11.69M 25.56M\n4.1 Results on CIFAR-10 and CIFAR-100\nImplementation . The CIFAR-10 dataset contains 60,000, 32 ×32 color images\nof 10 classes. We use ResNets [ 3] as our plain network to build FD-ResNets. In\norder to perform apple-to-apple comparison, we keep most of the training meth-\nods same as ResNets [ 3] when training the FD-ResNets.\nIn this section, we embed FD module into ResNet-32, ResNet-44, ResNet-\n56, and ResNet-110 to build FD-ResNets and compare the performance of FD-\nResNets with original ResNets on CIFAR-10 and CIFAR-100 datasets. We keep\nthe same training strategy on CIFAR-100 as CIFAR-10. The experimental resultsare shown in Table 1. We ﬁnd that FD-ResNets achieve better results than the\noriginal ResNets both on CIFAR-10 as CIFAR-100 dataset. What is more, FD-\nResNet-44 (6.10% error) even achieves better results than ResNet-56 (6.24%error) by 0.14%. FD-ResNet-44 and ResNet-44 have the same number of param-\neters. ResNet-56 has more parameters than FD-ResNet-44 by ∼28.7%, which\nmeans that the FD model helps the network achieve better results with fewerparameters.\n4.2 ImageNet Classiﬁcation\nImplementation. The ImageNet-1K contains 1.2 million training, 50,000 vali-\ndation and 100,000 test images of 1000 classes. We use ResNets [ 3] as the base', 'Convolution Tells Where to Look 23\nTable 3. Comparisons of single-crop validation error rates (%) on ImageNet-1K\ndataset. The VGG and FD-VGG are trained with batch normalization for 100 epochs\nDescription top-1 top-5\nResNet-34 [ 3] 26.69 8.60\nResNet-34 + FD 26.41 8.49\nResNet-50 [ 3] 24.56 7.50\nResNet-50 + FD 24.15 7.29\nResNet-101 [ 3]23.38 6.88\nResNet-101 + FD 22.73 6.48\nResNext-50 [ 26]22.85 6.48\nResNext-50+ FD 22.49 6.34\nVgg16 [ 20] 25.93 8.02\nVgg16 + FD 25.48 7.81\nTable 4. Classiﬁcation results (error (%)) on ImageNet-1K using the light-weight\nnetwork.\nDescription top-1 top-5\nMobileNet [ 17]34.85 13.6\nMobileNet + FD 34.32 13.15\nnetwork to build FD-ResNets and evaluate the performance of FD-ResNets on\nthe ImageNet-1K dataset [ 1]. The structures of networks are shown in Table 2.\nWe evaluate the networks on the validation dataset like others [ 3,4,25]. To ensure\na fair comparison, we use the same training strategy as ResNet [ 3], including the\nsame data augmentation scheme. A 224 ×224 crop is randomly sampled from an\naugment image or its horizontal ﬂip during the training and a single crop with\na size of 224 ×224 from the center of original images is sampled for testing. The\nper-pixel value of images is scaled to [0, 1], following the mean value subtractedand standard variance divided both for training and testing. We use SGD with\na momentum of 0.9 to train the networks with the mini-batch size of 256 in 4\nGPUs. We set the initial learning rate to 0.1, and then it is divided by 10 at 30,60, 90 epochs. The total training epochs are 90 if there is no special statement.\nResults on ImageNet-1K. We compare the performance of FD module based\non ResNets. The experimental results are shown in Table 3. Note that the FD\nmodule we proposed does no bring additional parameters. From the results,\nwe have the those following observations: Firstly, FD-ResNets perform betterthan the original ResNets, including, ResNet34, ResNet50, and ResNet101. As\nthe depth of the network increases, the improvement brought by the FD model\nbecomes more obvious. For example, the FD model results in only 0.28% (Top1error) on ResNet34, while it is 0.65% for ResNet101. It suggests that multiple', '24 F. Xu et al.\nTable 5. Comparisons of single-crop validation error rates (%) and complexity on\nImageNet-1K dataset and the best result are shown in bold\nDescription top-1 error (%) top-5 error (%) Param GFLOPs\nResNeXt-50 [ 26] 22.85 6.48 25.03M 3.768\nResNeXt-50 + BAM [ 12]22.56 6.40 25.39M 3.85\nResNeXt-50 + CBAM [ 25]21.92 5.91 27.56M 3.774\nSENet [ 4] 21.91 6.04 27.56M 3.771\nFDNet (ours) 22.49 6.34 25.03M 3.768\nResNet-50 [ 3] 24.56 7.50 25.56 3.858\nSENet [ 4] 23.14 6.70 28.09M 3.860\nFDNet (ours) 24.15 7.29 25.56M 3.858\nSENet + FD (ours) 22.84 6.43 28.09M 3.860\nuses of the FD model lead to better performance, which is consistent with the\nconclusion of the ablation study.\nSecondly, The FD model also achieves good results on networks with grouped\nconvolutional layers like ResNeXt [ 26].\nThirdly, not only do we test the performance of the FD model on CNNs with\nresidual connections, but we also do it on networks without residual connections.On the VGG16, the FD model achieve a boost of 0.45% (Top 1 error).\nIn addition, since the FD model is an extremely lightweight model and it\nbrings almost no additional parameters and very little computation, embeddingit into some lightweight models is a natural choice like MobileNetv2 [ 17]. We\nonly embed the FD model into the depthwise convolutional layers. Experimen-\ntal results are shown in Table 4. FD-MobileNet achieves better results than the\noriginal one, which fully demonstrates the potential of the FD model in low-end\ndevices like mobile phones, etc.\nComparision with Other Attention Models In this section, we compare\nthe performance of the FD model with other state-of-the-art attention models,including BAM [ 12], CBAM [ 25], and SE [ 4] attention models, and all three\nattention models include channel attention block. What is more, the BAM and\nCBAM are based on the SE model, and the FD model is the only one in allfour attention models that bring no additional parameters. The experimental\nresults are shown in Table 5. Firstly, like other attention models, the FD model\nhelp to improve the performance of the baseline, and it achieves better results\nthan BAM with fewer parameters. Secondly, Although the FD model does not\nachieve better results than CBAM or SE, it is diﬀerent from other attentionmodels. So, it is a natural choice to combine the FD model with other attention\nmodels without additional parameters. We embed the FD model into SENet and\nachieve a better results. These experiments indicate that the FD-model can not', 'Convolution Tells Where to Look 25\nonly be embedded into a backbone network but also work with other attention\nmodels to further improve the performance of CNNs.\n4.3 VOC 2012 Object Detection\nTable 6. Object detection mAP(%) on the VOC 2012 test set.\nDetector Backbone mAP@.5\nFaster-RCNN ResNet101 73.6\nFaster-RCNN ResNet101 + FD 74.2\nFig. 5. Example images illustrating how the FD model works. The low-level feature is\nfrom the ﬁrst residual block of FD-ResNet101 and the high-level feature is from the\nlast residual block.\nWe also conduct experiments on the detection task to prove the generalization of\nthe FD model. In this section, the performance of the FD model is evaluated onVOC PASCAL dataset. We use the Faster RCNN [ 14] as the detector to compare\nthe eﬀects of diﬀerent backbone networks. All methods are trained on VOC 2007\nand VOC 2012 trainval sets plus VOC 2007 test set and tested on VOC 2012test set For an apple-to-apple comparison, the same experimental strategy is\nemployed as the faster RCNN [ 14], and we reproduced all the results on the\nPyTorch platform. The experimental results are summarized in Table 6.W ec a n\nsee that compared to ResNet101 as the backbone network, the FD-ResNet101\nachieves better results. Note that ResNet101 and FD-ResNet101 have the same\nparameters, which means the FD model can help improve the performance ofthe faster RCNN detector with little additional computation. That is extremely\nimportant when the speed of a detector can aﬀect its application.\n5 Analysis and Interpretation\nTo understand how the FD model works, we visualize the attention mask gen-\nerated by the FD model, as shown in Fig. 1. We observer the following ﬁndings:', '26 F. Xu et al.\nFirstly, the FD model can locate the object well base on the low-level features\nwhile most of the self-attention models can not do it because they generate anattention mask just based on the feature (b).\nSecondly, for the high-level feature, the diﬀerence between two features is\nminimal, which leads to that most of the value in the attention mask is closeto 0.5. At this time, the FD model cannot locate the object. However, we can\nsee from (e) that the current feature can locate the object itself. Therefore, for\nthe high-level feature, no additional attention model is needed to ﬁnd the objectwhen the low-level feature already does it well. Besides, the output of the FD\nmodel closing to zero indicates that the current convolutional operation does not\nchange the value of the feature. It means that this convolutional layer do notlearn new features, so it can be removed from the network.\n6 Conclusion\nIn this paper, we propose the feature diﬀerence (FD) module, a new attention\nblock to boost the representation power of CNNs, which is diﬀerent from the pre-\nvious works like SE [ 4] and CBAM [ 25]. It uses the diﬀerence between the input\nfeature and the output feature of a convolutional layer to generate the attention\nmask. Extensive experiments are conducted to evaluate the performance of the\nFD module on four benchmark datasets, which indicates that the FD model\nhelps boost the representational power of a network and it also works well with\nother attention models, like channel attention model, to further improve theclassiﬁcation accuracy of a network. Besides, we ﬁnd that the FD model has dif-\nferent eﬀects between low-level features and high-level features, which suggests\nthat we can use a small number of attention blocks to locate objects quickly. Wehope that this discovery will inspire the future design of the attention model.\nFinally, the FD model can be used to evaluate the eﬀect of the convolutional\noperation on features, which could help to ﬁnd those convolutional layers thathave little eﬀect on the accuracy of recognition for the network pruning.\nAcknowledgements. This research is partially sponsored by Key Project of Beijing\nMunicipal Education Commission (No. KZ201910005008).\nReferences\n1. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: a large-scale\nhierarchical image database. In: IEEE Conference on Computer Vision and PatternRecognition, 2009. CVPR 2009, pp. 248–255. IEEE (2009)\n2. Fox, M.D., Corbetta, M., Snyder, A.Z., Vincent, J.L., Raichle, M.E.: Spontaneous\nneuronal activity distinguishes human dorsal and ventral attention systems. Proc.Natl. Acad. Sci. 103(26), 10046–10051 (2006)\n3. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pp. 770–778 (2016)', 'Convolution Tells Where to Look 27\n4. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pp. 7132–7141\n(2018)\n5. Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images.\nTech. rep, Citeseer (2009)\n6. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-\nvolutional neural networks. In: International Conference on Neural InformationProcessing Systems, pp. 1097–1105 (2012)\n7. Larochelle, H., Hinton, G.E.: Learning to combine foveal glimpses with a third-\norder Boltzmann machine. In: Advances in Neural Information Processing Systems,\npp. 1243–1251 (2010)\n8. Li, X., Wang, W., Hu, X., Yang, J.: Selective kernel networks. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 510–519\n(2019)\n9. Liu, N., Han, J.: Dhsnet: deep hierarchical saliency network for salient object\ndetection. In: Computer Vision and Pattern Recognition, pp. 678–686 (2016)\n10. Nuechterlein, K.H., Parasuraman, R., Jiang, Q.: Visual sustained attention: image\ndegradation produces rapid sensitivity decrement over time. Science 220(4594),\n327–329 (1983)\n11. Pardo, J.V., Fox, P.T., Raichle, M.E.: Localization of a human system for sustained\nattention by positron emission tomography. Nature 349(6304), 61 (1991)\n12. Park, J., Woo, S., Lee, J., Kweon, I.S.: BAM: bottleneck attention module. In:\nBritish Machine Vision Conference 2018, BMVC 2018, Newcastle, UK, 3–6 Septem-\nber 2018, p. 147. BMVA Press (2018). http://bmvc2018.org/contents/papers/0092.\npdf\n13. Petersen, S.E., Posner, M.I.: The attention system of the human brain: 20 years\nafter. Ann. Rev. Neurosci. 35, 73–89 (2012)\n14. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-CNN: towards real-time object\ndetection with region proposal networks. In: International Conference on Neural\nInformation Processing Systems, pp. 91–99 (2015)\n15. Rensink, R.A., O’Regan, J.K., Clark, J.J.: To see or not to see: the need for atten-\ntion to perceive changes in scenes. Psychol. Sci. 8(5), 368–373 (1997)\n16. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomed-\nical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.\n(eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015).\nhttps://doi.org/10.1007/978-3-319-24574-4\n28\n17. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv 2:\ninverted residuals and linear bottlenecks. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 4510–4520 (2018)\n18. Schneider, W., Shiﬀrin, R.M.: Controlled and automatic human information pro-\ncessing: I. detection, search, and attention. Psychol. Rev. 84(1), 1 (1977)\n19. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-\ncam: Visual explanations from deep networks via gradient-based localization. In:\nProceedings of the IEEE International Conference on Computer Vision, pp. 618–626 (2017)\n20. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition. In: Bengio, Y., LeCun, Y. (eds.) 3rd International Conferenceon Learning Representations, ICLR 2015, San Diego, CA, USA, 7–9 May 2015,\nConference Track Proceedings (2015). http://arxiv.org/abs/1409.1556\n21. Srivastava, R.K., Greﬀ, K., Schmidhuber, J.: Highway networks. arXiv preprint\narXiv:1505.00387 (2015)', '28 F. Xu et al.\n22. Tan, M., et al.: Mnasnet: platform-aware neural architecture search for mobile. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npp. 2820–2828 (2019)\n23. Tan, M., Le, Q.V.: Eﬃcientnet: rethinking model scaling for convolutional neural\nnetworks. In: Chaudhuri, K., Salakhutdinov, R. (eds.) Proceedings of the 36th\nInternational Conference on Machine Learning, ICML 2019, 9–15 June 2019, LongBeach, California, USA. Proceedings of Machine Learning Research, vol. 97, pp.\n6105–6114. PMLR (2019). http://proceedings.mlr.press/v97/tan19a.html\n24. Wang, F., et al.: Residual attention network for image classiﬁcation. In: Proceed-\nings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n3156–3164 (2017)\n25. Woo, S., Park, J., Lee, J.Y., Kweon, I.S.: Cbam: Convolutional block attention\nmodule. In: Proceedings of the European Conference on Computer Vision (ECCV)\n(2018)\n26. Xie, S., Girshick, R., Doll´ ar, P., Tu, Z., He, K.: Aggregated residual transformations\nfor deep neural networks. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 1492–1500 (2017)\n27. Xu, K., et al.: Show, attend and tell: Neural image caption generation with visual\nattention. In: International Conference on Machine Learning, pp. 2048–2057 (2015)\n28. Zagoruyko, S., Komodakis, N.: Paying more attention to attention: improving the\nperformance of convolutional neural networks via attention transfer. In: 5th Inter-\nnational Conference on Learning Representations, ICLR 2017, Toulon, France, 24–\n26 April 2017, Conference Track Proceedings. OpenReview.net (2017). https://\nopenreview.net/forum?id=Sks9\najex\n29. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.\nIn: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS,vol. 8689, pp. 818–833. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-\n10590-1\n53', 'Robust Single-Step Adversarial Training\nwith Regularizer\nLehui Xie1,2, Yaopeng Wang1,2, Jia-Li Yin1,2, and Ximeng Liu1,2(B)\n1College of Mathematics and Computer Science, Fuzhou University,\nFuzhou 350108, China\n2Fujian Provincial Key Laboratory of Information Security of Network Systems,\nFuzhou University, Fuzhou 350108, China\nAbstract. High cost of training time caused by multi-step adversarial\nexample generation is a major challenge in adversarial training. Pre-\nvious methods try to reduce the computational burden of adversarial\ntraining using single-step adversarial example generation schemes, whichcan eﬀectively improve the eﬃciency but also introduce the problem of\n“catastrophic overﬁtting”, where the robust accuracy against Fast Gra-\ndient Sign Method (FGSM) can achieve nearby 100% whereas the robustaccuracy against Projected Gradient Descent (PGD) suddenly drops to\n0% over a single epoch. To address this issue, we focus on single-step\nadversarial training scheme in this paper and propose a novel Fast Gradi-ent Sign Method with PGD Regularization (FGSMPR) to boost the eﬃ-\nciency of adversarial training without catastrophic overﬁtting. Our core\nobservation is that single-step adversarial training can not simultane-ously learn robust internal representations of FGSM and PGD adversar-\nial examples. Therefore, we design a PGD regularization term to encour-\nage similar embeddings of FGSM and PGD adversarial examples. Theexperiments demonstrate that our proposed method can train a robust\ndeep network for L\n∞-perturbations with FGSM adversarial training and\nreduce the gap to multi-step adversarial training.\nKeywords: Deep learning ·Adversarial training ·Adversarial defense\n1 Introduction\nDeep learning has shown outstanding success in near all machine learning ﬁelds.\nHowever, it has been proved that deep neural networks are vulnerable to adver-\nsarial examples, i.e., small disturbances to the input signal, which are usually\ninvisible to the human eyes, are enough to induce large changes in model out-put [17]. This phenomenon has aroused people’s concerns about the safety of\ndeep learning in the adversarial environment, where malicious attackers may\nsigniﬁcantly degrade the robustness of deep learning based applications. To mit-igate the harm caused by the adversarial examples, numerous defensive methods\nThe ﬁrst author is a student.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 29–41, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_3', '30 L. Xie et al.\nFig. 1. (CIFAR-10) Visualization of the FGSM and PGD robustness of the model\ntrained with FAST-FGSM AT (dashed), FGSMPR AT (solid). All statistics are evalu-\nated against FGSM attacks and 50 steps PGD attacks with 10 random restarts on thetest dataset. FAST-FGSM AT occurs catastrophic overﬁtting at 180 epochs, charac-\nterized by a sudden drop of PGD robustness and a rapid increase of FGSM robustness.\nFGSMPR AT (ours) does not suﬀer from catastrophic overﬁtting and maintains stablerobustness during the whole training process.\nincluding pre-processing based [ 6], modiﬁed networks based [ 3] and detection\nbased [ 13] have been proposed. Among these methods, adversarial training [ 12]\nis one of the most powerful approaches for robust defense against adversarial\nattacks since [ 2] a set of purportedly robust defenses by the adaptive attack.\nAdversarial Training (AT) aims to augment each small batch of training data\nwith adversarial examples for learning a robust model. It is generally considered\nto be more expensive than traditional training because it is necessary to con-struct adversarial examples through ﬁrst-order methods such as projection gra-\ndient descent (PGD). To combat the increased computational overhead of PGD\nAT, a recent line of work focused on improving the eﬃciency of AT. [ 20]p r o -\nposed to perform multi-step PGD adversarial attacks by chopping oﬀ redundant\ncomputations during backpropagation when computing adversarial examples toobtain additional speedup. [ 15] proposed a variant of Ksteps PGD AT with\na single-step Fast Gradient Sign Method (FGSM) AT overhead, called “FREE\nAT”, which can update model weights as well as input perturbations simulta-neously by using a single backpropagation in a way that is less expensive than\nPGD AT overheads. Inspired by [ 15,19] found that previously non-robustness\nFGSM AT, with a random initialization, could reach similar robustness to PGDAT, called “FAST-FGSM AT”. However, FGSM-based AT suﬀers from catas-\ntrophic overﬁtting where the robustness against PGD attacks increases in the\nearly stage of training, but suddenly drops to 0 over a single epoch, as shownin Fig. 1. Several methods have been proposed to prevent the overﬁtting of AT', 'Robust Single-Step Adversarial Training with Regularizer 31\n[1,11,18,19]. However, these methods are either computationally ineﬃcient or\ndecrease the robustness accuracy.\nIn this paper, we ﬁrst analyze the reason why FGSM AT suﬀers catastrophic\noverﬁtting in the training process. We observe that FGSM AT is prone to learn\nspurious functions that excessively ﬁt the FGSM adversarial data distributionbut have undeﬁned behavior oﬀ the FGSM adversarial data manifold. Then we\ndiscuss the diﬀerence behind the logits output between the FGSM and PGD\nadversarial examples in the model trained with FGSM AT and PGD AT, wherewe show that the logits become signiﬁcantly diﬀerent when the FGSM AT trained\nmodel suﬀers from overﬁtting, while the robust model trained with PGD AT\nremains stable. We additionally provide for this case an experimental analysisthat helps to explain why the FGSM AT trained model generates vastly dif-\nferent logit outputs for single-step and multi-step adversarial examples when\ncatastrophic overﬁtting occurs. Finally, we propose a novel Fast Gradient Sign\nMethod with PGD regularization (FGSMPR), in which a PGD regularization\nitem is utilized to prompt the model to learn logits that are a function of the trulyrobust features in the image and ignore the spurious features, thus preventing\ncatastrophic overﬁtting, as shown in Fig. 1.\nThe contribution of this paper is summarized as follows:\n– We analyze the reason why FGSM AT suﬀers from catastrophic overﬁtting\nand demonstrate that the logit distribution of the FGSM AT trained model\nevaluated against FGSM and PGD adversarial examples have signiﬁcant dif-\nference when suﬀering from catastrophic overﬁtting.\n– We propose a Fast Gradient Sign Method with PGD regularization\n(FGSMPR), which can eﬀectively prevent FGSM AT from catastrophic over-\nﬁtting by explicitly minimizing the diﬀerence in the logit of the model againstFGSM and PGD adversarial examples.\n– The extensive experiments show that the FGSMPR can learn a robust model\ncomparable to PGD AT with low computational overhead while in relief ofcatastrophic overﬁtting. Specially, the FGSMPR takes only 30 min to train a\nCIFAR-10 model with 46% robustness against 50 steps PGD attacks.\n2 Related Work\n2.1 Adversarial Training\nPrevious work [ 12] formalized the training of adversarial robust model into the\nfollowing non-convex non-concave min-max robust optimization problem:\nmin\nθE(x,y)∼D[max\nδ∈SL(θ,x+δ, y)]. (1)\nThe parameter θof the network is learned by Eq. 1on the example ( x,y)∼D,\nwhere Dis the data generating distribution. Sdenotes the region within the', '32 L. Xie et al.\nFig. 2. (CIFAR-10) Visualization of the FGSM and PGD accuracy/loss of the model\ntrained with FGSM AT, FAST-FGSM AT, PGD-7 AT and tested against FGSM adver-\nsarial attacks and 50 steps PGD attack with 10 random restarts during the trainingprocess. All results are averaged over three independent runs. FGSM AT and FAST-\nFGSM AT occurs catastrophic overﬁtting around 30 and 180 epochs, respectively,\ncharacterized by a sudden drop in PGD accuracy and FGSM loss and a rapid increasein PGD loss and FGSM accuracy.\n/epsilon1perturbation range under the /lscript∞threat model for each example, i.e., S={δ:\n/bardblδ/bardbl∞≤/epsilon1}, which is usually chosen so that it contains only visually imperceptible\nperturbations. The procedure for AT is to use adversarial attacks to approximate\nthe internal maximization over S.\nFGSM AT. One of the earliest versions of AT used the FGSM attack to ﬁnd\nadversarial examples x/primeto approximate the internal maximization, formalized\nas follows [ 5]:\nx/prime=x+/epsilon1·sign(∇xL(θ,x,y)). (2)\nFGSM AT is cheap since it only relies on computing the gradient once. However,\nthe FGSM AT is easily defeated by multi-step adversarial attacks.\nPGD AT. PGD attacks [ 12] used multi-step gradient projection descent to\napproximate the inner maximization, which is more accurate than FGSM butcomputationally expensive, formalized as follows:\nx\nt+1=Πx+S/parenleftbig\nxt+αsign (∇xL(θ,x,y))/parenrightbig\n, (3)\nwhere x0initialized as the clean input x,Πrefers to the projection operator,\nwhich ensures projecting the adversarial examples back to the ball within the\nradius /epsilon1of the clean data point. The number of iterations Kin the PGD attacks\n(PGD- K) determines the strength of the attack and the computational cost.\nFurther, Nrandom restarts are usually employed to verify robustness under\nstrong attacks (PGD- K-N).', 'Robust Single-Step Adversarial Training with Regularizer 33\n2.2 Single-Step Adversarial Training\nFREE AT [ 15], a single-step training method that generates adversarial exam-\nples while updating network weights, is quite similar to FGSM AT. By deeply\nanalyzing the diﬀerences between FREE AT and FGSM AT, [ 19] found that an\nimportant property of FREE AT is that the perturbation of the previous sign\nof gradient is used as the initial perturbation of the next iteration. Based on\nthis observation, [ 19] proposed a FAST-FGSM AT with almost the same robust-\nness as the PGD AT, but the spent time close to the normal training by adding\nnon-zero initialization perturbations to FGSM AT and further combining somestandard techniques [ 14,16] to accelerate the training. However, FAST-FGSM\nAT suﬀers from catastrophic overﬁtting, where the robustness for PGD adver-\nsarial examples suddenly drop to 0% over a single epoch.\nA recent line of work foucs on addressing the catastrophic overﬁtting prob-\nlem in single-step AT. [ 19] used the early stopping method to stop training the\nmodel when the model robustness decreases beyond a threshold. [ 18] introduced\ndropout layers after each non-linear layer of the model and further decay its\ndropout probability as the training progresses. [ 11] monitored the FGSM AT\nprocess and performed PGD AT with a few batches to help the FGSM modelrecover its robustness when the robustness decreases beyond a threshold. [ 1]p r o -\nposed the Gradient Alignment (GradAlign) regularization item that maximizes\nthe gradient alignment based on the connection between FAST-FGSM AT over-ﬁtting and local linearization of the model as a way to prevent the occurrence of\ncatastrophic overﬁtting. Although these methods provide a better understand-\ning of catastrophic overﬁtting prevention, but still cannot essentially explain the\nproblem of catastrophic overﬁtting. Moreover, these methods can improve the\nrobustness of single-step AT models to some extent, but sacriﬁce a large amountof computational overhead and lose the eﬃcient advantage of single-step AT,\neven up to the training time of multi-step AT.\n3 Proposed Approach\n3.1 Observation\nTo investigate catastrophic overﬁtting, we begin by recording the robust accuracy\nof FGSM AT on CIFAR-10 [ 9]. We evaluate the robust accuracy of the model\nagainst 50 steps PGD attacks with 10 random restarts (PGD-50-10) for stepsizeα=2/255 and maximum perturbation /epsilon1=8/255. Figure 2visualizes the\naccuracy and loss of the FGSM AT trained, FAST-FGSM AT trained, and PGD-\n7 AT trained model and evaluated against FGSM and PGD-50-10 attack duringthe training phase. As we can see, when FGSM AT and FAST-FGSM AT occur\ncatastrophic overﬁtting around 30 and 180 epochs respectively, the robustness\nagainst PGD-50-10 attack of the model trained with FGSM AT and FAST-FGSM AT begin to drop suddenly, whereas the accuracy against FGSM increases\nrapidly. However, for the robust PGD-7 AT, the accuracy and loss of the model\ntend to stabilize after a certain number of epochs.', '34 L. Xie et al.\nWe maintain that the reason the models trained using FGSM AT suﬀer from\ncatastrophic overﬁtting is that it is prone to learn spurious functions that ﬁtthe FGSM data distribution but have undeﬁned behavior oﬀ the FGSM data\nmanifold. Therefore, the FGSM AT is highly susceptible to overﬁtting due to\na single-step adversarial perturbation, resulting in a sudden drop in the PGDrobustness of the model, while the FGSM accuracy increases instantaneously.\nTo study the diﬀerences in the performance of the models trained with FGSM\nAT and PGD-7 AT for evaluating at the FGSM and PGD adversarial examples,we utilize a distance function Lto measure the diﬀerence between the output\nof the model evaluated at single-step and multi-step adversarial attacks. For a\nmodel that take inputs xand output logits f(x), we have:\nL(f(x\nfgsm),f(xpgd)), (4)\nwhere xfgsmandxpgdare adversarial examples crafted by FGSM and PGD-7,\nrespectively. Here, we choose L2forL. For a well-generalized and robustness\nmodel, we assume that the logit f(xfgsm)a n d f(xpgd) of the model evaluated\nat FGSM and PGD adversarial examples should be as similar as possible, i.e.,\n||f(xfgsm)−f(xpgd)||2should be very small.\nTo demonstrate our intuition, we ﬁrstly train several CIFAR-10 models using\nFGSM AT and PGD-7 for 200 epochs. For each model, we compute the diﬀer-\nence between the output of the model evaluated at FGSM and PGD-7 adversar-ial examples by using Eq. 4, and performed data processing using a logarithmic\nfunction to visualize the diﬀerences more clearly, as shown in Fig. 3. In Fig. 3\n(b), it can be observed that there is no signiﬁcant diﬀerence in the logits fromFGSM and PGD adversarial examples during the early phase of training, which\nmatches our intuition. Once catastrophic overﬁtting occurs, the gap between\nthe logit of the model evaluated at single-step and multi-step adversarial attacksare increasing rapidly around 30 and 180 epochs respectively, which is consis-\ntent with PGD loss. In contrast, the PGD-7 AT does not suﬀer catastrophic\noverﬁtting and the diﬀerence of the logit of the model is keeping stable. Thisphenomenon will also appear on the simple MNIST dataset [ 10], but it is not as\nclear as CIFAR-10, as shown in Fig. 3(a).\n3.2 PGD Regularization\nBased on the analysis in Sect. 3.1, the only FGSM adversarial loss is not enough\nfor the model to learn the robust features of both single-step and multi-stepadversarial examples. To solve this problem, inspired by [ 8], we use the logit\npairing to encourage the model to learn robust internal representation of FGSM\nand PGD adversarial examples so that the logit outputs f(x\nfgsm)a n d f(xpgd)o f\nthe model for FGSM and PGD adversarial examples to be as similar as possible:\nλ1\nmm/summationdisplay\ni=1L(f(xfgsm\ni;θ),f(xpgd\ni;θ)), (5)', 'Robust Single-Step Adversarial Training with Regularizer 35\n(a) MNIST\n (b) CIFAR-10\nFig. 3. Visualization of the L2distance of logit of the FGSM AT trained, FAST-\nFGSM AT trained, PGD-7 AT trained model and evaluated against FGSM and PGD\nadversarial attack. (a) In MNIST, when the model is not robust, the diﬀerence in\nL2distance starts to ﬂuctuate, while PGD AT is relatively smooth. (b) In CIFAR10,\nFGSM AT and FAST-FGSM AT occurs catastrophic overﬁtting around 30 and 180\nepochs, respectively, and is characterized by a rapid increase of L2distance.\nwhere LisL2norm; xfgsm\niandxpgd\niare adversarial examples crafted by FGSM\nand PGD attacks, respectively; λis a hyparameter to balance FGSM loss\nand PGD regularization item. Combining with the proposed regularization, the\nFGSM AT can learn a robustness model comparable with PGD-7 AT, as vali-\ndated in Sect. 4.\nWe hold that PGD regularization works well because it provides an addi-\ntional prior that regularizes the model toward a more accurate understanding\nof adversarial examples. If we train the model with only the single-step FGSMadversarial loss, it is prone to learn spurious functions that excessively ﬁt the\nFGSM adversarial data distribution but have undeﬁned behavior oﬀ the FGSM\ndata manifold (e.g., multi-step adversarial examples). PGD regularization forcesthe explanations of the FGSM adversarial example and multi-step adversarial\nexample to be similar. This is essentially a prior encouraging the model to learn\nlogits that are a function of the truly signiﬁcant features in the image and ignorethe spurious features.\n3.3 Training Route\nThe overall training procedure of the FGSMPR AT is summarized in\nAlgorithm 1. We ﬁrst perform FGSM adversarial attack to generate FGSM\nadversarial examples x\nfgsm\niand compute FGSM AT loss fgsm lossusing cross-\nentropy. Then, we perform PGD adversarial attack for mexamples, from a batch\nof natural examples, to generate mPGD adversarial examples. After generat-\ning FGSM and PGD adversarial examples, the regularization loss reglossofm\nFGSM and PGD adversarial examples are calculated using Eq. 5and used as\npart of the total loss totalloss. Finally, the parameter θof the model is updated\nusing a proper optimizer (e.g., stochastic gradient descent). The hyperparameter', '36 L. Xie et al.\nAlgorithm 1: FGSMPR AT\nInput : Training data ( X,Y), perturbation bound /epsilon1, learning rate γ,\nhyparameter α, λ.\nOutput :T r a i n e dm o d e l f(·) with parameter θ\n1for epoch = 1 ... Nepoch do\n2 for i = 1 ... Bdo\n3 // Perform FGSM adversarial attack\n4 xfgsm\ni=xi+α·sign(∇δ/lscript(fθ(xi),yi))\n5 fgsm loss=J(fθ,xfgsm\ni,yi)\n6 // Perform PGD adversarial attack\n7 for k = 1 ... Kdo\n8 δ=δ+α·sign(∇δ/lscript(fθ(xi+δ),yi))\n9 δ= max(min( δ, /epsilon1),−/epsilon1))\n10 end for\n11 xpgd\ni=xi+δ\n12 regloss=λ1\nm/summationtextm\nj=1L(f(xfgsm\ni,j;θ),f(xpgd\ni,j;θ))\n13 totalloss=fgsm loss+regloss\n14 Update model parameter θbased on totalloss\n15 end for\n16end for\n17return f(·).\nλshall be properly chosen to balance FGSM loss fgsm lossand PGD regular-\nization reglossitem. In practice, we take α=/epsilon1/K,K=3a n d m= 1. In other\nwords, we only pick a single example from a batch for generating a PGD-3 adver-\nsarial example, which is then used for regularization to encourage the model to\nlearn similar logit output. The experiments show that a single PGD adversarialexample for regularization is enough to learn a robustness model.\n4 Experiments\nIn this section, we demonstrate that the proposed FGSMPR is robust against\nstrong PGD attacks. All experiments are run on a single RTX 2070, in which\nwe use half-precision computation recommended in [ 19] to speed up the training\nof CIFAR-10 model, which was incorporated with the Apes amp package at the\nO1 optimization level for all CIFAR-10 experiments.\nAttacks: We attack all models using PGD attacks with Kiterations and 10\nrandom restarts on both cross-entropy loss (PGD-K-10) and the Carlini-Wagner\nloss (CW-K-10) [ 4]. All PGD attacks used at evaluation for MNIST [ 10] are run\nwith 10 random restarts for 20/40 iterations. All PGD attacks used at evaluation\nfor CIFAR-10 [ 9] are run with 10 random restarts for 20/50 steps.\nPerturbation: For MNIST, we set the maximum perturbation /epsilon1to 0.3 and the\nPGD step size αto 0.1. For CIFAR-10, we set the maximum perturbation /epsilon1to\n8/255 and the PGD step size αto 2/255.', 'Robust Single-Step Adversarial Training with Regularizer 37\nTable 1. Validation accuracy (%) and robustness of MNIST models trained with\nFGSM AT, FAST-FGSM AT, GradAlign AT, FREE AT, PGD-40 AT, FGSMPR AT\nwithout early stopping and the corresponding training time. All statistics are evaluatedagainst PGD/CW attacks with 20/40 iterations and 10 random restarts for α=0.1,\n/epsilon1=0.3 over three independent runs. The bold indicates the best performance except\nfor PGD-40 AT.\nMethod Standard\naccuracyPGD-20-10 PGD-40-10 CW-20-10 CW-40-10 Training time\n(s)\nFGSM AT 97.53 ±0.39 39.31 ±20.68 12.45 ±12.15 40.14 ±20.64 13.46 ±12.89 481.03 ±0.81\nFA S T - FG S M\nAT98.52 ±0.34 42.60 ±12.47 11.19 ±6.52 43.41 ±12.55 11.85 ±7.26 491.24 ±1.71\nGradAlign AT 99.05 ±0.03 91.42 ±0.57 75.94 ±3.23 91.23 ±0.51 75.86 ±3.16 633.96 ±3.55\nFREE AT 98.49 ±0.05 92.90 ±0.20 90.06 ±0.36 92.70 ±0.15 89.85 ±0.32 175.45 ±1.99\nPGD-40 AT 99.16 ±0.03 94.72 ±0.08 92.52 ±0.14 94.75 ±0.03 92.65 ±0.10 3652.39 ±1.00\nFGSMPR AT\n(ours)98.35 ±0.09 93.77 ±0.32 90.83 ±0.49 93.65 ±0.26 90.56 ±0.55 626.57 ±1.68\nTable 2. Validation accuracy (%) and robustness of CIFAR-10 models trained with\nFGSM AT, FAST-FGSM AT, GradAlign AT, FREE AT, PGD-7 AT, FGSMPR AT\nwithout early stopping and the corresponding training time. All statistics are evaluatedagainst PGD/CW attacks with 20/50 iterations and 10 random restarts for α=2/255,\n/epsilon1=8/255 over three independent runs. The bold indicates the best performance except\nfor PGD-7 AT.\nMethod Standard\naccuracyPGD-20-10 PGD-50-10 CW-20-10 CW-50-10 Training time\n(m)\nFGSM AT 88.51 ±1.27 0.01 ±0.17 0.00 ±0.00 0.01 ±0.11 0.00 ±0.00 119.04 ±0.41\nFA S T - FG S M\nAT90.33 ±0.42 0.92 ±0.49 0.32 ±0.25 0.52 ±0.30 00.17 ±0.08 123.81 ±0.18\nGradAlign AT 82.82 ±0.13 32.94 ±0.83 32.50 ±0.80 32.94 ±0.83 32.52 ±0.81 486.20 ±0.67\nFREE AT 82.32 ±0.12 46.97 ±0.05 46.07 ±0.82 45.77 ±0.23 45.64 ±0.24 61.94 ±0.15\nPGD-7 AT 84.75 ±0.87 48.33 ±0.62 47.99 ±0.66 47.80 ±0.31 47.59 ±0.38 493.41 ±0.04\nFGSMPR AT\n(ours)83.31 ±0.40 47.59 ±0.51 47.19 ±0.42 46.98 ±0.18 46.79 ±0.20 211.65 ±0.61\nComparisions: We compare the performance of our proposed method\n(FGSMPR) with FGSM: standard FGSM AT [ 5]; FAST-FGSM AT: FGSM AT\nwith a random initialization [ 19]; FREE AT: recently proposed single-step AT\nmethod [ 15]; GradAlign AT: recently proposed method solving catastrophic over-\nﬁtting [ 1]; PGD- KAT: AT with a Kiterations PGD attack [ 12].\nEvaluation: We demonstrate that the performance of models against PGD- K-\n10/CW- K-10 adversarial attacks under white-box settings. For all experiments,\nthe mean and standard deviation over three independent runs are reported.\n4.1 Results on MNIST\nFirst, we conduct a study to demonstrate that our proposed approach is highly\nworking in MNIST benchmark dataset [ 10]. We train models for MNIST dataset\nwith the same architecture used by [ 19], using FGSM AT, FAST-FGSM AT,', '38 L. Xie et al.\nFig. 4. Visualization of the accuracy of the CIFAR-10 model trained for FGSM AT,\nFAST-FGSM AT, GradAlign AT, FREE AT, PGD-7 AT, and FGSMPR AT. All thestatistics are tested against 50 steps PGD attacks with 10 random restarts for α=\n2/255,/epsilon1=8/255. Catastrophic overﬁtting for the FGSM and FAST-FGSM AT occur\naround 30 and 180 epochs, respectively, and is characterized by a sudden drop in thePGD accuracy.\nFREE AT, PGD-40 AT, FGSMPR AT. Except that the AT free replays each\nbatch of m= 8 for a total of 7 epochs, all other models are trained for 50 epochs.\nFor the proposed method, we set the hyparameter λ,Kandmto (0.1,3,1).\nThe experimental results are provided in Table 1. It can be observed that our\nproposed FGSMPR AT is more robust against both PGD and CW attacks on the\nMNIST dataset than the GradAlign AT and FREE AT, and is second only to thePGD AT model with a small diﬀerence. In the course of testing the robustness\nof FAST-FGSM AT on the MNIST dataset, we found an interesting problem\nwhere increasing the number of MNIST training epochs to 50 also resulted in\ncatastrophic overﬁtting, although this phenomenon was previously found only\nin CIFAR-10. Besides, the GradAlign AT [ 1] can keep the model from suﬀering\ncatastrophic overﬁtting to some extent, but it is far inferior to other comparison\nmethods in defending against the higher iteration adversarial attacks.\n4.2 Results on CIF AR-10\nTo verify whether AT scheme suﬀers from catastrophic overﬁtting, we train\n200 epoch for all CIFAR-10 models using the Preact ResNet-18 [ 7] architecture\nwithout early stopping, especially the FREE AT replays each batch m=8\ntimes for a total of 25 epochs as recommend in [ 15]. For the FGSMPR, we set\nthe hyparameter λ,Kandmto (0.5,3,1). The experimental results are provided\nin Table 2. It can be observed that FGSMPR AT is quite similar to PGD-7 AT\nwhile our training time is half of PGD-7 AT. To demonstrate that the proposed\nFGSMPR does not suﬀer from catastrophic overﬁtting, we takes 211 min to', 'Robust Single-Step Adversarial Training with Regularizer 39\nFig. 5. Accuracy of the model trained for FGSM AT, FAST-FGSM AT, GradAlign\nAT, FREE AT, PGD-7 AT and FGSMPR AT with early stopping. All the statistics\nare evaluated against 50 steps PGD attacks with 10 random restarts for diﬀerent l∞-\nperturbation /epsilon1.\ntrain a CIFAR-10 model for 200 epochs, which is longer than time for FREE\nAT. However, our method was able to achieve 46% robustness by training 30\nepochs in only 30 min, which is half less than FREE AT. Further, we visualize\nthe robustness of the training process of diﬀerent AT method and tested against\na 10 random restart PGD-50 attack, as shown in Fig. 4. It can be observed that\nthe robustness of the FGSMPR against PGD has steadily increased, which is\nonly 0.8% behind PGD-7 AT and does not suﬀer from catastrophic overﬁtting\neven when trained to 200 epochs. Instead, FAST-FGSM AT started to have atrend similar to PGD AT, but there is a sharp drop in robustness around 180\nepochs when occuring catastrophic overﬁtting. The GradAlign AT was proposed\nto prevent the FGSM AT from catastrophic overﬁtting, but the accuracy stilldropped by more than 10% and took more than two times longer compared to\nour FGSMPR AT. Besides, we also test the model’s robustness under diﬀerent\nl\n∞perturbation where all models are trained with early stopping. In the case\nof larger l∞perturbations, FGSMPR AT is essentially indistinguishable from\nPGD-7 AT, and even slightly better than PGD-7 AT, as shown in Fig. 5.\n5 Conclusion\nIn this paper, we analyze and address the catastrophic overﬁtting in FGSM\nAT. We empirically show that FGSM AT is prone to learn spurious functionsthat ﬁt the FGSM adversarial data distribution but have undeﬁned behavior\noﬀ the FGSM data manifold. We therefore exploit the diﬀerence behind the\nlogits between the FGSM and PGD adversarial examples in the model trainedwith FGSM AT and PGD AT, where the logit becomes signiﬁcantly diﬀerent', '40 L. Xie et al.\nwhen FGSM AT suﬀers from overﬁtting, while PGD AT remains stable. Based\non these observations, we propose a novel FGSMPR AT, where a PGD regu-larization term is used to encourage the model to learn similar embeddings of\nFGSM and PGD adversarial examples. The extensive experiments show that the\nFGSMPR can eﬀectively keep FGSM AT from catastrophic overﬁtting with alow computational cost.\nReferences\n1. Andriushchenko, M., Flammarion, N.: Understanding and improving fast adver-\nsarial training. Virtual, Online (2020)\n2. Athalye, A., Carlini, N., Wagner, D.: Obfuscated gradients give a false sense of\nsecurity: circumventing defenses to adversarial examples. In: International confer-\nence on machine learning, pp. 274–283. Stockholm, Sweden (2018)\n3. Buckman, J., Roy, A., Raﬀel, C., Goodfellow, I.: Thermometer encoding: one hot\nway to resist adversarial examples. In: International Conference on Learning Rep-\nresentations. Vancouver, BC, Canada (2018)\n4. Carlini, N., Wagner, D.: Towards evaluating the robustness of neural networks. In:\n2017 IEEE Symposium on Security and Privacy (sp), pp. 39–57. IEEE (2017)\n5. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial\nexamples. San Diego, CA, United states (2015)\n6. Guo, C., Rana, M., Cisse, M., Van Der Maaten, L.: Countering adversarial images\nusing input transformations. Vancouver, BC, Canada (2018)\n7. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks. In:\nLeibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9908, pp.\n630–645. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46493-0 38\n8. Kannan, H., Kurakin, A., Goodfellow, I.: Adversarial logit pairing (2018)\n9. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny\nimages. https://www.cs.toronto.edu/ kriz/learning-features-2009-TR.pdf (2009)\n10. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to\ndocument recognition. Proc. IEEE 86(11), 2278–2324 (1998)\n11. Li, B., Wang, S., Jana, S., Carin, L.: Towards understanding fast adversarial train-\ning. arXiv preprint arXiv:2006.03089 (2020)\n12. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning\nmodels resistant to adversarial attacks (2018)\n13. Metzen, J.H., Genewein, T., Fischer, V., Bischoﬀ, B.: On detecting adversarial\nperturbations. Toulon, France (2017)\n14. Narang, S., et al.: Mixed precision training. Vancouver, BC, Canada (2018)15. Shafahi, A., et al.: Adversarial training for free! In: Advances in Neural Information\nProcessing Systems, pp. 3358–3369 (2019)\n16. Smith, L.N.: Cyclical learning rates for training neural networks. In: 2017 IEEE\nWinter Conference on Applications of Computer Vision (WACV), pp. 464–472.\nIEEE (2017)\n17. Szegedy, C., et al.: Intriguing properties of neural networks. Banﬀ, AB, Canada\n(2014)\n18. Vivek, B., Babu, R.V.: Single-step adversarial training with dropout scheduling.\nIn: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pp. 947–956. IEEE (2020)', 'Robust Single-Step Adversarial Training with Regularizer 41\n19. Wong, E., Rice, L., Kolter, J.Z.: Fast is better than free: Revisiting adversarial\ntraining. Addis Ababa, Ethiopia (2020)\n20. Zhang, D., Zhang, T., Lu, Y., Zhu, Z., Dong, B.: You only propagate once: accel-\nerating adversarial training via maximal principle. In: Advances in Neural Infor-\nmation Processing Systems, pp. 227–238 (2019)', 'Texture-Guided U-Net for OCT-to-OCTA\nGeneration\nZiyue Zhang1, Zexuan Ji1(B),Q i a n gC h e n1, Songtao Yuan2,a n dW e nF a n2\n1School of Computer Science and Engineering, Nanjing University of Science and\nTechnology, Nanjing 210094, China\njizexuan@njust.edu.cn\n2Department of Ophthalmology, The First Aﬃliated Hospital with Nanjing Medical\nUniversity, Nanjing 210029, China\nAbstract. As a new imaging modality, optical coherence tomography\nangiography (OCTA) can fully explore the characteristics of retinal\nblood ﬂow. Considering the inconvenience of acquiring OCTA images\nand inevitable mechanical artifacts, we introduce deep learning to gener-ate OCTA from OCT. In this paper, we propose a texture-guided down-\nand up-sampling model based on U-Net for OCT-to-OCTA generation.\nA novel texture-guided sampling block is proposed by combining theextracted texture features and content-adaptive convolutions. The cor-\nresponding down-sampling and up-sampling operations would preserve\nmore textural details during the convolutions and deconvolutions, respec-tively. Then a deeply-supervised texture-guided U-Net is constructed by\nsubstituting the traditional convolution with the texture-guided sam-\npling blocks. Moreover, the image Euclidean distance is utilized to con-struct the loss function, which is more robust to noise and could explore\nmore useful similarities involved in OCT and OCTA images. The dataset\ncontaining paired OCT and OCTA images from 489 eyes diagnosedwith various retinal diseases is used to evaluate the performance of the\nproposed network. The results based on cross validation experiments\ndemonstrate the stability and superior performances of the proposedmodel comparing with state-of-the-art semantic segmentation models\nand GANs.\nKeywords: Optical coherence tomography angiography\n·Generation ·\nDeep learning\n1 Introduction\nAs an important imaging modality, Optical coherence tomography (OCT) has\nbecome more and more popular in the evaluation and diagnosis of retinal dis-\nThe authors declare no conﬂicts of interest. This work was supported in part by\nNational Science Foundation of China under Grants No. 62072241, in part by Nat-ural Science Foundation of Jiangsu Province under Grant No. BK20180069, in part by\nSix talent peaks project in Jiangsu Province under Grant No. SWYY-056, and in part\nby National Institutes of Health Grant No. P30-EY026877.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 42–52, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_4', 'Texture-Guided U-Net for OCT-to-OCTA Generation 43\nease. The technology of OCT is not only noninvasive but also can help imaging\nintra-ocular structures, which makes it rather valuable in practice. Thanks toOCT, optical coherence tomography angiography (OCTA) takes a step forward\nfor diagnosing retinal disease. With the help of OCTA, a map of blood ﬂow can\nbe obtained by comparing the decorrelation signal between serial OCT B-scans,which are taken at the same location [ 1]. Therefore, comparing with ﬂuorescein\nangiography imaging, OCTA is able to noninvasively visualize both superﬁcial\ncapillary and deep capillary of retina, which enables better diagnosis and detec-tion of retinal disease free from side eﬀects [ 2]. Theoretically, the OCT hardware\nis able to obtain the corresponding OCTA data. However, some hardware and\nsoftware modiﬁcations on the existing OCT machines are needed to acquire theOCTA data [ 3]. Some OCTA scanning protocols easily introduce background\nnoise and thus degrade OCTA image quality [ 4,5]. In addition, OCTA technol-\nogy need to perform the acquisition multiply, which leads to motion artifacts\nand degradation in image quality [ 6]. Moreover, due to the time diﬀerence in the\ndevelopment of OCT and OCTA modalities, there are a large number of OCTimages in the hospital lacking corresponding OCTA data. It is of great signiﬁ-\ncance to explore the relationship between OCT and their corresponding OCTA\nimages, thereby handily acquiring OCTA image from its counterpart.\nVarious OCTA algorithms have been proposed to measure OCT signal\nchanges. However, traditional OCTA algorithms have a limited ability to make\ngood use of the OCT signal variation [ 7]. Nowadays, deep learning shows strong\ncapability in ophthalmology and achieves robust performances in detection of\ndiabetic retinopathy, optic disc with glaucoma, macular oedema and age-related\nmacular degeneration [ 8]. Although it is inconvenient to obtain OCTA images,\nthe similar structure can explore the relationship between OCTA and OCT\nimages. This inspires researchers to explore the potential of deep learning to gen-\nerate OCTA images from the corresponding OCT images. Liu et al. [ 9] employed\na deep learning based method for OCTA predicting on the datasets, which\nare generated by a conventional method. And the results demonstrated that\ndeep learning has the ability to surpass traditional methods. According to our\nexperience, there are two main methods for cross-modal image-to-image trans-\nlation, which are Generative adversarial networks (GANs) based methods andsegmentation based method. Generative adversarial networks [ 10–15] (GANs)\nare widely used for image generation and image-to-image translation. Consid-\nering that OCT and OCTA images are spatially aligned, the GANs [ 7,16]h a s\nbeen designed for paired images to perform cross-modal translation. However,\nit has been proven that GAN-generated ones, share some common systematic\nﬂaws, which prevents GANs from achieving realistic image synthesis [ 17].\nThe other method for cross-modal translation is segmentation based method.\nU-Net [ 18] is a famous network with the structure of encoder-decoder and is\nwidely used for medical images segmentation. Lee et al. [ 3] employed U-Net\nto generate blood ﬂow from single OCT images, whose results are signiﬁcantly\nsuperior than experts and clinicians. Yang et al. [ 19] utilized a fovea attention\nmechanism together with a residual neural network to visualize capillaries around', '44 Z. Zhang et al.\nthe foveal avascular zone better. Down-sampling and up-sampling are inevitable\nfor encoder-decoder based models to produce the output which have the samesize with the input. However, sampling is the primary cause of lost details.\nExcept for preserving details by simple concatenation like U-Net, some other\nmethods focus on local spatial changes while sampling [ 20–23]. Considering that\nrich texture information in the input medical images will be missing during\nconvolutions, we combine the extracted texture features and content-adaptive\nconvolutions to guide U-Net to generate more detailed images.\nCommonly used metrics for evaluating similarity between images are mean-\nsquare error (MSE), structural similarity (SSIM) and peak signal-to-noise ratio.\nThese metrics evaluate pixel-to-pixel diﬀerence and are vulnerable to noise. Dueto OCT and OCTA images are naturally highly noisy, it is inappropriate to\nemploy these pixel-to-pixel metrics as loss function. Image Euclidean Distance\n(IMED) [ 24] measures the spatial relationships of pixels, which is more robust\nto small perturbations. IMED is reasonable and agrees with human vision per-\nception for image evaluation. The loss function of U-Net is cross entropy loss,which is commonly used for image segmentation tasks. We replace it with IMED\nloss to guide U-Net to generate OCTA from OCT images.\nConsequently, the main contributions of this study are summarized as follows:\n– A novel texture-guided sampling block is proposed by combining the extracted\ntexture features and content-adaptive convolutions, where the weights of con-\nvolution kernels are multiplied by learnable textural features extracted frominput images. Therefore, the corresponding down-sampling and up-sampling\noperations would preserve more textural details during the convolutions and\ndeconvolutions, respectively. Then a deeply-supervised texture-guided U-Netis constructed by substituting the traditional convolution with the texture-\nguided sampling blocks.\n– The image Euclidean distance (IMED) is utilized to construct the loss func-\ntion, which is more robust to noise and could explore more useful similaritiesinvolved in OCT and OCTA images.\n– We evaluate the performance of our method on a large dataset collected from\nNanjing Provincial People’s Hospital between March 2018 and September\n2019. A 3-fold cross validation is performed. The results based on cross val-idation experiments demonstrate the stability and superior performances of\nthe proposed model compared with state-of-the-art semantic segmentation\nmodels and GANs.\n2M e t h o d\nCNNs are made up of convolutions of which the weights are spatially shared.\n[22] proposed a pixel-adaptive convolution (PAC), which simply and eﬀectively\nintroduced pixel-level features to standard convolutions. The ﬁlter weights inPAC are multiplied with a kernel, which are spatially varying and obtained\nfrom local pixel features. Mostayed et al. [ 23] introduced an economical solu-\ntion, which is eliminating the concatenation operations in U-Net and using the', 'Texture-Guided U-Net for OCT-to-OCTA Generation 45\nFig. 1. The workﬂow of texture-guided down-sampling and up-sampling based on U-\nNet. The architecture consists of two parts, which is preprocessing of textural features(below) and modiﬁed U-Net (above). For convenience, we set down-sampling factor f\nto be 4. In the modiﬁed U-Net, we replace the standard convolution and deconvolution\nlayers with texture-guided down-sampling (T-Down block) and up-sampling (T-Upblock).\nencoder features as guiding features. However, this architecture is eﬃcient with\nlimited input image size and cannot make good use of medical images with high\nresolution. In medical images, texture analysis is regarded as a useful tool to dis-\ntinguish between diﬀerent pathological areas, and texture information is proven\nto be more valuable and eﬀective than human vision [ 25]. Thus we combine\nthe texture feature and pixel-adaptive convolution to guide U-Net to generate\nmore detailed images. We propose a deeply-supervised U-Net substituting the\ntraditional convolution for a pixel-adaptive convolution, where the weights ofconvolution kernels are multiplied by learnable textural features extracted from\ninput images. Figure 1depicts the workﬂow of our texture-guided U-Net. The\nframework includes the preprocessing of textural features and U-Net with pixel-adaptive convolution and deconvolution.\nWe ﬁrst conduct texture feature extraction on the input images, which\naims at guiding the process of both down-sampling and up-sampling with moredetailed and latent feature. In this paper, we utilize discrete wavelet transform,\nwhich has the advantage of capturing localized frequency and spatial informa-\ntion [26]. Notably, our texture-guided method is a rather generic framework, so\nany particular pixel-based texture extraction method, even edge, color and line\nfeatures, can be applied to preserve detail information while generation.\nThe results of texture extraction are reasonable to visual perception, like\nlow- and high-frequency components obtained from wavelet transform. However,', '46 Z. Zhang et al.\nstatic features being used to guide down-sampling and up-sampling directly often\nleads to unstable training. We conduct three convolution layers on extractedtexture features before they are fed into texture-guided down-sampling and up-\nsampling blocks. To get diﬀerent scale of texture features later, we use same-size\nconvolutions to keep the size by setting the padding of convolution to be ⌊ks/2⌋,\nwhere ksis kernel size. Denoting that the down-sampling factor is fand the input\nimage has the size of I, the size of inner-most feature map of U-Net is I/2\nf.T h e\noutput features of last same-size convolution are divided into fgroups with each\ngroup scaled to certain size. Our texture-guided down-sampling and up-sampling\nblocks are implemented by pixel-adaptive convolution and deconvolution. So we\nneed the guided feature of down-sampling to be the same size with input featuremap, and that of up-sampling to be the same size with output feature map. So\nwe resize the fgroups of guided features to I,I/2,..., I/2\nfrespectively.\nFig. 2. The texture-adaptive weights (a) and texture-guided down-sampling (b) and\nup-sampling (c) blocks. The blue cube denotes extracted texture feature with size I\nand channel c. Each s×swindow is compressed to single channel by kernel function,\nlike Gaussian. With each channel in weights multiplied by single channel guidance\n(gray dashed square), weights vary from window to window. For texture-guided down-\nsampling (b) and up-sampling (c) blocks, we take the down-sampling from ItoI/2\nand the inverse up-sampling for examples. This is accomplished by standard convolu-\ntion and deconvolution with texture-guided weights (red and yellow cubes) which are\nderived from blocks in (a) (Color ﬁgure online).\nThe preprocessing of textural features outputs learnable texture features\nwhich are then fed into pixel-adaptive convolution and deconvolution.\nThe standard spatial convolution with image features v=(v1,..., vn),vi∈\nRcand weights W∈Rc/prime×c×s×scan be deﬁned as\nv/prime\ni=/summationdisplay\nj∈Ω(i)W[pi−pj]vj+b (1)\nwhere pi=(x,y)Tdenotes pixel coordinates, Ω(·) denotes an s×sconvolution\nwindow, as shown in Fig. 2with the dashed box. And b∈Rc/primedenotes biases.\nFollowing [ 22], [pi−pj] indexes the spatial dimension of an array with a two-\ndimensional spatial oﬀset. Considering that standard convolution uses spatially\nshared weights which ignore adaptation of each pixel, we use extracted texture', 'Texture-Guided U-Net for OCT-to-OCTA Generation 47\nfeatures as guidance to make proper adjustment to the weights. Each s×s\nwindow in the extracted texture features is compressed to single channel bykernel function, like Gaussian. With each channel in weights multiplied by single\nchannel guidance, weights vary from window to window. When the texture-\nadaptive weights are utilized in standard convolution and deconvolution, weget the texture-guided down- and up-sampling blocks, as shown in Fig. 2. Since\nthe spatially varying kernel function K∈R\nc/prime×c×s×sis utilized to modify the\ninvariant convolution, the convolution with kernel obtained from pixel featurescan be written as [ 22]\nv\n/prime\ni=/summationdisplay\nj∈Ω(i)K(fi,fj)W[pi−pj]vj+b (2)\nwhere Kis a Gaussian kernel function.\nSince U-Net [ 18] is a widely-used and acknowledged image segmentation net-\nwork, which contains couples of down-sampling and up-sampling operations, we\nuse U-Net as our backbone. Similar with any other convolution-based networks,\nU-Net makes adjustment to channels in the purpose of preserving details. Ourtexture-guided sampling focuses on pixels, thereby introducing extracted texture\nfeatures or manual features at each pixel. Compared to PacNet [ 22], we make\nuse of the original image to get guided texture features by themselves instead\nof depending on paired extra modal. It should be noted that self-extracted fea-\ntures not only bring in latent information of original images, but also meet therequirement of pixel pairing at the same time.\nWhile Euclidean distance is known as the most popular image metrics, tradi-\ntional Euclidean distance and most of its variants are inappropriate for evaluat-ing image distance. Image Euclidean Distance (IMED) [ 24] measures the spatial\nrelationships of pixels, which is more robust to small perturbations. IMED is\nreasonable and agrees with human vision perception for image evaluation.\nLetx,ybe two M×Nimages, the Euclidean distance with metric coeﬃcients\nd\n2\nE(x,y) is written by\nd2\nE(x,y)=(x−y)TG(x−y) (3)\nwhere G=(gij)MN×MNdenotes metric matrix. When Gis a identity\nmatrix, Eq. ( 3) denotes the traditional Euclidean distance. Similarly, it becomes\nthe weighted Euclidean distance if Gis a diagonal matrix. Following [ 24], we also\nutilize the Gaussian function to construct the metric coeﬃcients. The IMED met-\nric is able to provide intuitively reasonable results, and we use this metric as lossfunction.\n3 Experiments\n3.1 Dataset and Metrics\nIn this paper, a large dataset collected from Nanjing Provincial People’s Hospital\nbetween March 2018 and September 2019 was used to evaluate the performance', '48 Z. Zhang et al.\nof the proposed network. The dataset contains 489 projects with 6 mm ×6\nmm FOV. The data were collected using a commercial 70 kHz spectral-domainOCT system with a center wavelength of 840 nm (RTVue-XR, Optovue, CA).\nThe dataset contains paired OCT and OCTA images from 489 eyes and can\nbe categorized into NORMAL, OTHERS and various retinal diseases, includ-ing Age-related Macular Degeneration (AMD), Choroidal Neovascularization\n(CNV), Central Serous Chorioretinopathy (CSC), Diabetic Retinopathy (DR)\nand Retinal Vein Occlusion (RVO). Since 400 B-scans are collected for each eye,the dataset includes 195600 OCT and OCTA images each. We performed a 3-\nfold cross validation based on eyes, thus each train set and test set contain 326\nand 163 eyes respectively.\nWe utilized U-Net as the backbone of the generator and wavelet transform as\ntexture extraction method. The input of U-Net is OCT images with the original\nsize 640 ×400 and the feature map will be down-sampled to 40 ×25 at bottleneck\nwithfset to 4. We utilized Adam solver [ 27] to optimize the proposed texture-\nguided U-net and IMED [ 24] as loss function. We set the batch size to 1 and\ninitialized the learning rate as 0.0002.\nThe evaluation metrics include mean-square error (MSE), structural similar-\nity (SSIM) [ 28] and feature similarity (FSIM) [ 29]. All the metrics are evaluated\nwith the ground truth (GT), averaged over all test B-scans. While MSE focuses\non average diﬀerence of two images, SSIM focuses on the structural information\nbetween two images. Because luminance and contrast vary a lot, SSIM utilizesthe local metrics to evaluate. FSIM [ 29] employs two complementary metrics,\nphase congruency and gradient magnitude, to compute the local similarity map.\n3.2 Results\nThe intensity of each B-scan ranges from 0 to 255. Mean MSE of generated scans\non three folds are 6.858, 6.950 and 7.001, respectively, which reveals that ourmodel performed stably.\nThe intensity of each B-scan ranges from 0 to 255. Mean MSE of generated\nscans on three folds are 6.858, 6.950 and 7.001, respectively, which reveals thatour model performed stably. Compared with other methods of segmentation\nand GANs, our method achieved minimum mean and var of MSE and maximum\nSSIM and FSIM, as shown in Table 1. Note that U-Net is specially designed\nfor segmenting detailed medical images and other methods, like ReﬁneNet [ 30],\nDeepLabv3+ [ 31]a n dF C N[ 32], are for general segmentation tasks. Though the\noverall performance of U-Net is comparable, the corresponding MSE ﬂuctuates\na lot. The variance of U-Net is 2.885, and ours keep it low at 1.714, which proves\nour model to be stable and reliable.\nFigure 3presents comparisons on several methods of segmentation and GAN.\nWe present OCT, GT OCTA and generated OCTA projections of one case for\neach category. The generated results are from test datasets and are projected bysum. While generation results of segmentation methods share similar structures\nwith the ground-truth, there still exist the situation of lost details. ReﬁneNet\n[30], for example, exploits various levels of detail from the scale of 1/32 to 1/4.', 'Texture-Guided U-Net for OCT-to-OCTA Generation 49OCT GT RefineNet DeepLabv3+ FCN U-Net GAN CycleGAN cGAN Pix2Pix Ours\nNORMAL AMD CNV CSC DR RVO OTHERS\nFig. 3. Comparisons on several methods of segmentation and GAN. We present OCT,\nGT OCTA and generated OCTA projections of one case for each category.', '50 Z. Zhang et al.\nTable 1. Comparison on other methods of segmentation and GAN. Following [ 15], we\ncondition on an input image for cGAN and generate a corresponding output image.\nMethods MSE SSIM FSIM\nReﬁneNet [ 30]15.612 ±15.377 0.908 ±3.667e–4 0.963 ±1.100e–3\nDeepLabv3+ [ 31]8.911 ±2.344 0.929 ±4.947e–5 0.944 ±1.700e–3\nFCN [ 32] 7.896 ±2.740 0.938 ±6.958e–5 0.976 ±2.587e–5\nU-Net [ 3] 7.925 ±2.885 0.941 ±6.501e–5 0.977 ±2.315e–5\nGAN [ 10] 26.750 ±19.317 0.870 ±1.839e–2 0.959 ±6.593e–5\nCycleGAN [ 13]21.999 ±12.292 0.894 ±1.032e–4 0.957 ±8.469e–5\ncGAN [ 14] 14.252 ±4.622 0.912 ±9.435e–5 0.972 ±2.509e–5\nPix2Pix [ 15] 12.054 ±10.957 0.922 ±1.689e–4 0.972 ±3.865e–5\nOurs 6.858 ±1.714 0.947 ±4.496e–5 0.978 ±2.496e–5\nDetails could be spoiled when fusing both small and large scales of feature map\nto obtain a high-resolution prediction. This makes ReﬁneNet inappropriate tofulﬁll the task of natural textures generations. DeepLabv3+ [ 31] applies atrous\nconvolution at multiple scales which leads to fuzzy blood capillary. Focusing on\nobject boundaries from a macro perspective, DeepLabv3+ [ 31] is also incapable\nto generate rich textures. Traditional segmentation networks, like FCN [ 32]a n d\nU-Net [ 18], achieve relatively detailed results. They fail to manage blurry large\nblood vessels and broken small blood vessels, and our model keeps the boundariesof blood vessels sharp and clear.\nB-scans generated by GAN based models suﬀer from checkerboard artifact.\nConsequently, the corresponding generation results contain inevitable stripes inthe projected blood ﬂow maps. It is worth noting that some GANs methods\nwhich heavily depend on the distribution of the data tend to reverse the color of\nthe blood vessels in the ﬂow maps. For example, the ﬂow maps obtained by GAN[10] and CycleGAN [ 13] mark blood vessels with black (see Fig. 3), yet others\nincluding the ground-truth mark them with white. These kind of unsupervised\nmethods focus on the distribution which is an advantage for image style transfer,\nlike transferring the light OCT to the dark OCTA. However, it is incapable to\nseize the subtle diﬀerences between the images. Consequently, GAN [ 10]a n d\nCycleGAN [ 13] suﬀer from inferior generation quality most. cGAN [ 14] greatly\nreduces the artifacts by conditioning on extra information, as shown in Fig. 3.\nWith the help of supervision information, pix2pix achieves the most ideal resultsamong four GAN based models. Comparatively, our results are free from artifacts\nwhich indicates that our model can achieve desirable performance.\n4 Conclusion\nIn this paper, we presented a texture-guided U-Net architecture which is able\nto generate more detailed images by texture-guided down- and up-sampling and', 'Texture-Guided U-Net for OCT-to-OCTA Generation 51\nachieves very good performance on generation of retinal ﬂow maps. Visual results\nof generated OCTA images demonstrate that the proposed model can accuratelyexplore the unseen features of OCT and translate them into blood ﬂow signals.\nWhile our main counterpart [ 3] costs 6 days of training time, our texture-guided\nU-Net only takes a very prominent training time of 4 h each fold to achievecomparable and stable results. What’s more, the eﬃcient space consumption of\n2.290M makes our texture-guided U-Net superior to embed in commercial med-\nical systems. The architecture provides an eﬃcient way to generate inaccessibleand paired images, which will be investigated as our future work.\nReferences\n1. de Carlo, T.E., Romano, A., Waheed, N.K., Duker, J.S.: A review of optical\ncoherence tomography angiography (octa). Int. J. Retina Vitreous 1(1), 5 (2015).\nhttps://doi.org/10.1186/s40942-015-0005-8\n2. Dsw, T., Gsw, T., Agrawal, R., et al.: Optical coherence tomographic angiography\nin type 2 diabetes and diabetic retinopathy, JAMA Ophthalmol. 135(4), 306–312\n(2017). https://doi.org/10.1001/jamaophthalmol.2016.5877\n3. Lee, C.S., et al.: Generating retinal ﬂow maps from structural optical coher-\nence tomography with artiﬁcial intelligence, CoRR abs/1802.08925 (2018).\narXiv:1802.08925\n4. Rabiolo, A., et al.: Macular perfusion parameters in diﬀerent angiocube sizes: does\nthe size matter in quantitative optical coherence tomography angiography? Invest.\nOpthalmol. Vis. Sci. 59, 231 (2018). https://doi.org/10.1167/iovs.17-22359\n5. Kadomoto, S., Uji, A., Muraoka, Y., Akagi, T., Tsujikawa, A.: Enhanced visual-\nization of retinal microvasculature in optical coherence tomography angiography\nimaging via deep learning. J. Clin. Med. 9, 1322 (2020). https://doi.org/10.3390/\njcm9051322\n6. Zhang, Q., et al.: Wide-ﬁeld optical coherence tomography based microangiography\nfor retinal imaging. Sci. Rep. 6, 22017 (2016). https://doi.org/10.1038/srep22017\n7. Jiang, Z., et al.: Comparative study of deep learning models for optical coherence\ntomography angiography. Biomed. Opt. Express 11(3), 1580–1597 (2020). https://\ndoi.org/10.1364/BOE.387807\n8. Ting, D.: Artiﬁcial intelligence and deep learning in ophthalmology, Br. J. Oph-\nthalmol. 103 (2018) bjophthalmol-2018. https://doi.org/10.1136/bjophthalmol-\n2018-313173\n9. Xi, L.: A deep learning based pipeline for optical coherence tomography angiogra-\nphy. J. Biophotonics 12(2019). https://doi.org/10.1002/jbio.201900008\n10. Goodfellow, I.J.: Generative adversarial networks (2014). arXiv:1406.2661\n11. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein GAN (2017). arXiv:1701.07875\n12. Radford, M., Metz, L., Chintala, S.: Unsupervised representation learning with\ndeep convolutional generative adversarial networks (2015). arXiv:1511.06434\n13. Zhu, J.-Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation\nusing cycle-consistent adversarial networks (2017). arXiv:1703.10593\n14. Zhu, J.-Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation\nusing cycle-consistent adversarial networks (2017). arXiv:1703.10593\n15. Isola, P., Zhu, J.-Y., Zhou, T., Efros, A.A.: Image-to-image translation with con-\nditional adversarial networks (2016). arXiv:1611.07004', '52 Z. Zhang et al.\n16. Li, P.L., et al.: Deep learning algorithm for generating optical coherence tomog-\nraphy angiography (OCTA) maps of the retinal vasculature. In: Zelinski, M.E.,\nTaha, T.M., Howe, J., Awwal, A.A.S., Iftekharuddin, K.M. (eds.), Applications ofMachine Learning 2020, vol. 11511, International Society for Optics and Photonics,\nSPIE, 2020, pp. 39–49. https://doi.org/10.1117/12.2568629\n17. Wang, S.-Y., Wang, O., Zhang, R., Owens, A., Efros, A.A.: CNN-generated images\nare surprisingly easy to spot... for now (2019). arXiv:1912.11035\n18. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation, CoRR abs/1505.04597 (2015). arXiv:1505.04597\n19. Yang, J., Liu, P., Duan, L., Hu, Y., Liu, J.: Deep learning enables extraction of\ncapillary-level angiograms from single oct volume (2019). arXiv:1906.07091\n20. Saeedan, F., Weber, N., Goesele, M., Roth, S.: Detail-preserving pooling in deep\nnetworks, CoRR abs/1804.04076 (2018). arXiv:1804.04076\n21. Weber, N., Waechter, M., Amend, S.C., Guthe, S., Goesele, M.: Rapid, detail-\npreserving image downscaling, ACM Trans. Graph. 35(6) (2016). https://doi.\norg/10.1145/2980179.2980239\n22. Su, H., Jampani, V., Sun, D., Gallo, O., Learned-Miller, E.G., Kautz, J.:\nPixel-adaptive convolutional neural networks, CoRR abs/1904.05373 (2019).\narXiv:1904.05373\n23. Mostayed, A., Wee, W., Zhou, X.: Content-adaptive u-net architecture for medical\nimage segmentation. In: International Conference on Computational Science and\nComputational Intelligence (CSCI), pp. 698–702 (2019)\n24. Wang, L., Zhang, Y., Feng, J.: On the Euclidean distance of images. IEEE Trans.\nPattern Anal. Mach. Intell. 27(8), 1334–1339 (2005)\n25. Nailon, W.H.: Texture analysis methods for medical image characterisation.\nBiomed. Imaging 75, 100 (2010)\n26. Humeau-Heurtier, A.: Texture feature extraction methods: a survey. IEEE Access\n7, 8975–9000 (2019)\n27. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2014).\narXiv:1412.6980\n28. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:\nfrom error visibility to structural similarity. IEEE Trans. Image Process. 13(4),\n600–612 (2004)\n29. Zhang, L., Zhang, L., Mou, X., Zhang, D.: FSIM: a feature similarity index for\nimage quality assessment. IEEE Trans. Image Process. 20(8), 2378–2386 (2011)\n30. Lin, G., Milan, A., Shen, C., Reid, I.: Reﬁnenet: Multi-path reﬁnement networks\nfor high-resolution semantic segmentation (2016). arXiv:1611.06612\n31. Chen, L.-C., Zhu, Y., Papandreou, G., Schroﬀ, F., Adam, H.: Encoder-decoder\nwith atrous separable convolution for semantic image segmentation (2018).\narXiv:1802.02611\n32. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation (2014). arXiv:1411.4038', 'Learning Key Actors and Their\nInteractions for Group Activity\nRecognition\nYutai Duan and Jianming Wang(B)\nTianGong University, Tianjin 300387, China\nwangjianming@tiangong.edu.cn\nAbstract. Group activity recognition is a challenging task. Group activ-\nities usually involves many actors, while only some key actors play a\ndecisive role in group activity recognition. Therefore, extraction of keyactors and their interactions is an important problem in group activity\nrecognition. To tackle this problem, we propose a new method based on\ngraph convolutional layers and self-attention mechanisms to extract theSubgraphs of the Actor Relation Graph (SARG). SARG is a scene rep-\nresentation that only contains key actors and their interactions, which\nis used to enhance the importance of key actors in each group activity.First, the actor relation graph was generated via the appearance and\nlocation information of the actors; it was further analyzed by using a\ngraph convolutional layer. Second, we use the graph convolutional layer\nto generate the self-attention features for each participant and extract the\nactor relation subgraphs that can ascertain group activities. Finally, wefuse actor relation subgraphs, actor relation graphs and original features\nto recognize group activities. We evaluate our model over two datasets:\nthe collective activity dataset and the volleyball dataset. SARG has anaverage improvement of 4%, compared to 8 benchmarks.\nKeywords: Group activity recognition\n·Graph convolution ·\nSubgraph extraction\n1 Introduction\nGroup activity recognition is an important task, often with many people active\nin the scene but only a small subset contributing to an actual event [ 1]. There-\nfore, extracting key actors is a crucial and challenging problem in group activ-\nity recognition [ 2,3]. In sports video analysis and video surveillance, the model\nshould always focus on the key actors. For example, in Fig. 1(a), group activity\nis only deﬁned by looking at the actions of actors 1,2 and 3. However, other\nactors in the same frame, such as actors 4,5 and 6 in Fig. 1(a), often have the\nsame action labels, such as standing or waiting, but are not as important. It is\ncrucial for group activity recognition that modeling extracts key actors and their\ninteractions, which is shown in Figs. 1(b) and 1(c). However, the dataset does\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 53–65, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_5', '54 Y. Duan and J. Wang\nnot supply the model with any information about the importance of individual\nactors. Therefore, designing a method that can extract information about keyactors is a challenging problem.\nFig. 1. Group activities are often determined by a part of key actors and their inter-\nactions. Our model performs group activity reasoning by extracting actor relationship\nsubgraph and ﬁltering irrelevant actors.\nTo tackle this problem, we propose a new method to extract the actor relation\nsubgraph, creating a scene representation that only contains the key actors and\ntheir interactions. Our method is inspired by [ 4,5], which extracts hierarchical\nstructure in graph classiﬁcation tasks. Speciﬁcally, based on the method of [ 4],\nwe ﬁrst construct an actor relation graph (ARG), which reﬂects the relationship\nof various positions and appearance among actors. ARG is supplemented with a\ngraph convolutional layer (GCN) to generate whole scene representation. In thesecond step, we generate action representation vectors for each actor. Next, a\ngraph convolution layer is added to generate the self-attention score of each actor.\nFurthermore, we extract the subgraph of the actor graph according to the self-attention score, to capture the key actors and their interactions. Finally, actor\nrelation subgraphs, actor relation graphs and original features are combined for\ngroup activity recognition.\nIn summary, the contributions of this paper can be summarized as: (1) We\npropose a novel actor relation subgraph extracting framework for group activityrecognition, by abstracting group activity recognition as a graph classiﬁcation\ntask. (2) We designed multiple model variants and provided ablation studies,\ndemonstrating that Subgraphs of the Actor Relation Graph (SARG) can improvethe performance of the model and oﬀer additional information. (3) We evaluate\nour model using two datasets: the collective activity dataset and the volleyball\ndataset. SARG has an average improvement of 4%, compared to 8 benchmarks.\n2 Related Work\n2.1 Group Activity Recognition\nGroup activity recognition is an important topic in video understanding [ 2,3],\nwhich has received more and more attention in recent years. Group activity', 'Learning Key Actors and Their Interactions for Group Activity Recognition 55\nrecognition has many applications, such as video surveillance and sports video\nanalysis. Diﬀerent from the action recognition of individuals, group activityrecognition is based on the comprehension of individual actions while concur-\nrently inferring the spatiotemporal connections between actors to complete the\nrecognition of group activities [ 6–8].\nGroup activity recognition uses an image as holistic input and uses a deep\nmodel to extract information for classiﬁcation. The application of deep learning\ntechnology [ 9] has eﬀectively improved the model performance of group activ-\nity recognition. In recent years, some approaches have demonstrated the posi-\ntive inﬂuence of interaction networks on actors in group activity recognition [ 8];\nhowever, many of the methods are not ﬂexible because a manual constructionis required in advance. Other methods use LSTM, RNN, or complex message\npropagation methods [ 10,11], resulting in a large amount of computational cost.\n2.2 Graph Neural Networks\nIn recent years, because most conventional convolution methods cannot process\ndata in non-Euclidean spaces, generalizing a convolution operator to graph datahas become an interesting topic.\n[12,13] proposes to use GCN for semi-supervised node classiﬁcation. For\ngraph-level problems, the graph pooling layer [ 5,14] provides sampling strate-\ngies and introduces new parameters to ﬁt the data. Meanwhile, pooling layers\nare also considered to be eﬀective in extracting hierarchical structures. In this\nwork, we use a graph convolution layer to generate the high-level representationfor graph, and then use graph pooling layers to extract subgraphs.\n3 Approach and Framework\n3.1 Preliminaries\nOur goal is to identify group activities by explicitly exploiting the information of\nkey actors and their interactions. Therefore, we generate scene representations\nby extracting the SARG. We begin by extracting image features, generating fea-tures and extracting the bounding box vector for each actor. Then, we construct\nthe ARG and supplement it with GCN layers. We follow the strategy of ARG\nconstruction used in [ 4]. The next step is to use the method we designed, namely,\nto extract the SARG. Finally, we design a series of feature fusing methods to\nfuse the original information, ARG, and SARG to complete the group activity\nrecognition process. We just described a brief introduction to the feature extrac-tion method, the ARG construction strategy and using GCN to learn ARG. In\nSects. 3.2and3.3, we will focus on how to extract subgraphs and fuse multi-scale\nfeatures.', '56 Y. Duan and J. Wang\nFig. 2. An overview of our network framework for group activity recognition.\nFeature Extraction. We ﬁrst sample a ﬁxed set of Kframes from a given\nvideo sequence clip and generate the actor’s feature vector. We use VGG [ 15]o r\nInception-v3 [ 16] to extract a multi-scale feature map for each frame. Roialign\n[17] is used to extract the bounding box for each actor. To align the feature\ndimension, we use a full connection layer to obtain d-dimensional features for\neach actor. The total number of actors appearing in Kframes is N, and the\nfeature vector of Nactors are concatenated to obtain the feature matrix X∈\nRN×fof the actor.\nARG Construction Strategy. We follow the ARG construction strategy\nG(A,X)i n[4]. Each node in the actor graph represents an actor; its adjacency\nmatrix is represented by Aand the edge weight is determined by the appearance\ninformation and position information between actors. The calculation equation\nof edge weight is as follows:\nAij=fs/parenleftbig\nxs\ni,xs\nj/parenrightbig\nexp/parenleftbig\nfa/parenleftbig\nxa\ni,xa\nj/parenrightbig/parenrightbig\n/summationtextN\nj=1fs/parenleftbig\nxs\ni,xs\nj/parenrightbig\nexp/parenleftbig\nfa/parenleftbig\nxa\ni,xa\nj/parenrightbig/parenrightbig (1)\nwhere fa/parenleftbig\nxa\ni,xa\nj/parenrightbig\nrepresents the appearance relation value between two actors,\nand the position relation is performed by fs/parenleftbig\nxs\ni,xs\nj/parenrightbig\n. The appearance relation\nvalues and position relation values are calculated as follows:\nfa/parenleftbig\nxa\ni,xa\nj/parenrightbig\n=(xa\ni)Txaj\n√\nd,fs/parenleftbig\nxs\ni,xs\nj/parenrightbig\n=I/parenleftbig\nd/parenleftbig\nxs\ni,xs\nj/parenrightbig\n≤μ/parenrightbig\n(2)\nwhere√\ndacts as a normalization factor. We get the adjacency matrices A\nandX, which will go through the graph convolution layer to learn the relation\nrepresentation H(1)between the actors.', 'Learning Key Actors and Their Interactions for Group Activity Recognition 57\nLearning Relation Representation by Graph Convolutional Layers.\nAfter obtaining the actor relation adjacency matrix Aand the actor feature\nmatrix X, we perform relational reasoning on them. We use the graph convo-\nlution layer [ 12,13,18]. A graph convolution layer is a kind of message passing\nmodel that takes an adjacency matrix and a feature matrix as input to obtainthe output of new feature matrix. Formally, we can write the process as:\nH\n(l+1)=σ/parenleftBig\nAH(l)W(l)/parenrightBig\n(3)\nwhere A∈RN×Nis the matrix representation of graph with a structural rela-\ntionship. H(l)∈RN×fis the feature matrix of nodes, which is the output of\nthelthlayer, and H(0)=X.W∈Rf×dare the learnable parameters for which\ndimension dcan be adjusted. In this work we adjust dequal to f.σ(·)i sa n\nactivation function, and we adopt RELU in this work. In [ 4], multiple graphs\nG=/parenleftbig\nG1,G2,···,GN g/parenrightbig\nare constructed as:\nH(l+1 ,N g)=σ/parenleftBig\nAiH(l,N g)W(l,N g)/parenrightBig\n, (4)\nwhere Ngis a hyper-parameter. We extend the work by extracting multiple sub-\ngraphs G∫=/parenleftBig\nG1\ns,G2\ns,···,GN gs/parenrightBig\n. Every actor relation subgraph is constructed\nin the same way according to Sect. 3.2, but with unshared weights. We use the\nsame temporal modeling as [ 4] to extract 3 frames from a group of videos.\n3.2 Extracting the SARG\nIn the ﬁeld of Graph Neural Networks, extracting the hierarchical structure of\ngraphs via a pooling layer is an eﬀective method for graph classiﬁcation [ 5,\n19]. When the ARG is established, the group activity recognition problem is\nessentially abstracted into the graph classiﬁcation problem. In this work, weintroduce a method of subgraph extraction to obtain a scene representation\nonly containing key actors.\nAction Feature. After reasoning by a graph convolutional layer, we ﬁnd that\nH\n(1)contains a relational representation. We extract the SARG based on this\nanalysis, which contains key actors and their interactions. Subgraph extraction\nis based on the self-attention scores of each actor about individual actions. Cur-\nrent models [ 4,7] for individual action classiﬁcation use a full connection layer\nclassiﬁcation. This process transforms the features extracted from the network\ninto action features. We simulate this process by passing the feature matrix H(1)\nthrough a full connection layer to generate a behavior vector for each actor. The\noutput dimension of the full connection layer will remain d.\nSelf-attention Mask. An attention mechanism is a widely used new method\nin the ﬁeld of deep learning [ 20,21]. This mechanism allows the model to focus', '58 Y. Duan and J. Wang\nmore on important features and less on unimportant ones. In particular, the\nself-attention mechanism allows input features to generate attention accordingto their own criteria. We generate the action self-attention score of each actor\nbased on the action feature matrix. Speciﬁcally, we investigate two methods we\ndesigned to calculate the action self-attention score S∈R\nN×1of each actor.\nEach scalar SiinS∈RN×1represents the action self-attention score of the ith\nactor.\n(1) GCN Attention Encoder: Generally, predicting individual actions in a group\nactivity requires reference to the local entities. Based on these observations, weuse a graph convolutional network to calculate the actor’s action self-attention\nscoreS∈R\nN×1, which can propagate the signals from local entities to target\nentities. The calculation equation is as follows:\nS=sigmod/parenleftBig\nˆAHΘ att/parenrightBig\n(5)\nwhere ˆA=A+I, which makes the node emphasize its own features more when\ncalculating action self-attention. Additionally, Θ∈RN×1is a trainable param-\neter. Self-attention is obtained by using a graph convolution based on graph\nfeatures and topology.\n(2) FC Attention Encoder: We can generate action self-attention for each actor\nvia a full connection neural network. We choose a two-layer full connected neuralnetwork to compute the action self-attention.\nS=F\nattn(H) (6)\nwhere Fattn(·) is a two-layer full connected neural network, with an input of\nH∈RN×dand an output of S∈RN×1. The number of neurons in the ﬁrst layer\nis 1/2d, and the number of neurons in the second layer should be 1. Generated\nfrom attention mechanisms using a full connection neural network, the result isonly based on the action vectors of each actor.\nExtracting Subgraph. For subgraph extraction, we follow the rules of node\nselection, which retains a portion of nodes of the input graph even when graphs of\nvarying sizes and structures are inputted [ 22]. We ﬁrst ranked the self-attention\nscoreSthat we obtained from the largest to the smallest. The ranked scores yield\na ﬁxed ratio of nodes. The ratio k∈(0,1] is a hyper-parameter that determines\nthe number of nodes to keep. In this work, we set K=0.5. The top ⌈kN⌉nodes\nare selected based on S.\nidx = top-rank (S,⌈kN⌉),S\nmask=Sidx (7)', 'Learning Key Actors and Their Interactions for Group Activity Recognition 59\nwhere top-rank (·,·) is the function that returns indices of the top ⌈kN⌉values.\nThe·idxis an indexing operation. Smask is the feature attention mask. The\nsubgraph is extracted in the following ways (shown in Fig. 2):\nHs=Hidx ,:,Aout=Aidx ,idx (8)\nwhere Xidx ,:is the row-wise indexed feature matrix and Aidx ,idxis the row-wise\nand column-wise indexed adjacency matrix. The ⊙is the broadcasted element-\nwise product. Furthermore, according to the method in [ 5], the feature matrix of\nthe subgraph can be multiplied by the attention mechanism and used to expand\nand contract the matrix.\nHs=Hs⊙Smask (9)\nSo far, we have completed the extraction of the subgraph, which contains a new\nfeature matrix and a new actor relation adjacency matrix. We set the extraction\nratioK=0.5, which means that every time the actor relation subgraph is\nextracted, half of the irrelevant actors/nodes will be ﬁltered.\n3.3 Feature Fusion\nThe overall network framework is shown in Fig. 2. The original feature matrix\nH(0)∈ RN×f, containing all actor features, is extracted by a backbone and\nRoIAlign [ 17]. The relation representation of data is learned by using a graph\nconvolutional layer, the feature matrix H(1)∈RN×dwith a relation represen-\ntation is obtained. Finally, after the subgraph extraction module, the feature\nmatrix H(1)\ns∈ R⌈kN⌉× dof the actor relation subgraph is obtained. We fuse\nH(0),H(1)andH(1)\nsin diﬀerent ways to describe scene-level representation. In\nthis paper, we investigate three feature fusion methods we designed.\nMaxpool Fusing. The ﬁrst method uses the uniﬁed maxpool method, which\nis used in [ 4]. Maxpool is a graph representation method commonly used in\nthe graph classiﬁcation task, which is regarded as an approach that represents\noutstanding features. To use the maxpool method, we need to set the dimension\nofH(1)\nsequal to H(1)to make the matrices additive. To this end, we generate a\nzero matrix 0N×d∈RN×dand construct a new subgraph feature matrix H(1)/prime\ns\nas follows:\nH(1)/prime\ns=distribute/parenleftBig\n0N×d,H(1)\ns,idx/parenrightBig\n(10)\nwhere distribute/parenleftBig\n0N×d,H(1)\ns,idx/parenrightBig\nis the operation that distributes row vectors\ninH(1)\nsinto0N×dfeature matrix according to their corresponding indices stored\nin idx. Each row in the feature matrix represents an actor. According to Eq. 10,\nwe can ﬁnd that if actors are ﬁltered during subgraph extraction, the featurevector corresponding to actors is 0. If retained, the feature is retained. The\nscene-level representation is generated as follows:\nZ=X+N g/summationdisplay\ni=1(H(1,i)+H(1,i)/prime\ns),Z/prime=Nmax\ni=1zi (11)', '60 Y. Duan and J. Wang\nwhere maxN\ni=1is the function that returns max value of column dimension.\nAddpool Fusing. For the graph classiﬁcation problem, [ 23] proves that addpool\nis more capable than maxpool for distinguishable non-isomorphic graphs. The\nactor representations are added together to generate scene-level representation\nas follows:\nZ=X+N/summationdisplay\ni=1(H(1,i)+H(1,i)/prime\ns),Z/prime=N/summationdisplay\ni=1Zi (12)\nGraph Feature Fusion Network. We also evaluate the learnable approach,\nwhich is inspired by [ 18]. It can be written as:\nZ=N g/summationdisplay\ni=1(relu/parenleftBig/bracketleftBig\nX,H(1,i),H(1,i)/prime\ns/bracketrightBig\nWp,i+bp,i/parenrightBig\n)i,Z/prime=N/summationdisplay\ni=1Zi (13)\nwhere [ ·,·] is the concatenation operation and Wp,iandbp,iare learnable param-\neters that project the concatenated vector to the same dimension as Z∈RN×d,\nfollowed by a RELU non-linearity.\n3.4 Training Loss\nThe ﬁnal scene-level representation, Z∈RN×dandZ/prime∈R1×d,a r ef e dt ot w o\nclassiﬁers consisting of a full connection layer to generate individual actions andgroup activity predictions. The whole process can be trained in a single end-\nto-end model, combined with cross-entropy loss commonly used in classiﬁcation\nproblems. The ﬁnal loss is the sum of an individual action prediction and activity\nprediction loss:\nL=L\n1/parenleftbig\nyG,ˆyG/parenrightbig\n+λL2/parenleftbig\nyI,ˆyI/parenrightbig\n(14)\nwhere L1andL2are the cross-entropy elements, ˆ yGand ˆyIrepresent the pre-\ndictions of group activity and individual action, yGandyIrepresent the labels\nof group activity and individual action. The weight λis a hyper-parameter that\nis used to balance the loss of activity and action.\n4 Experiments\nIn this section, we evaluate the performance of our proposed model and its\nvariants; then we compare our model to several benchmarks. We conduct exper-\niments on two widely adopted datasets, the Volleyball dataset [ 6] and the Col-\nlective Activity dataset [ 24]. We use the ADMA optimizer and follow the hyper-\nparameter settings in [ 4]. Additionally, we set d= 1024, f= 1024 and K=0.5.', 'Learning Key Actors and Their Interactions for Group Activity Recognition 61\nTable 1. Ablation studies about diﬀerent types of self-attention calculation methods.\nMethod Accuracy (%)\nNo subgraph extraction 89.1\nGCN attention encoder 89.8\nFC attention encoder 89.5\nTable 2. Ablation studies about diﬀerent types of feature fusion methods.\nMethod Accuracy (%)\nBase model (with GCN attention encoder) 89.8\nBase model + Addpool 90.1\nBase model + Fusion Net 90.1\n4.1 Ablation Studies\nTo understand the contribution of proposed model components to model perfor-\nmance, we conduct detailed ablation experiments on the collective dataset. In\nthis experiment, we take recognition accuracy as an evaluation metric.\nAction Self-attention Computation. Firstly, we investigate the inﬂuence of\ndiﬀerent ways of calculating the attention value on performance. To eliminateother inﬂuences, our baseline is based on a single frame, N\ng= 1, without using\ntemporal modeling. The base model is only reasoned by a GCN layer but with\nno subgraph extraction. We choose VGG-16 as the backbone network. In thissection, we use maxpool feature fusion.\nThe results are shown in Table 1. We can observe that, compared with the\nbaseline, extracting subgraphs can bring signiﬁcant performance improvement.The performance improvements illustrate that oﬀering the scene representation\nof key actors can bring identiﬁable information. Compared with the relation\ngraph between all actors, the subgraph only containing key actors also providesadditional information for group activity recognition. Moreover, the attention\nvalues calculated using GCN are better than using the full connection layer.\nWe conjecture those individual actions in group activities require the reference\nof surrounding entities, thus generating more representative attention. In the\nfollowing experiments, we use GCN attention encoder to compute the appearancerelation value.\nFeature Fusion Methods. In this section, we investigated the inﬂuence of\ndiﬀerent feature fusion methods based on model performance. We set the base\nmodel with the GCN attention encoder as the base model. The results are shown', '62 Y. Duan and J. Wang\nin Table 2. We can observe that the Addpool and Fusion Net we designed have\ngood performance. We conjecture that the three fusion methods we designedpotentially highlight the expressiveness of key actors. Moreover, Addpool and\nFusion Net have better performance, which compute the representation by sum\nthe features of all nodes/actors. [ 23] proved that the add aggregator is more\neﬀective than the max/mean aggregator. In the following experiments, we use\nFusion Net to fuse features.\nTable 3. Ablation study of number of graphs and subgraphs.\nARG [ 4]T h en u m b e ro fg r a p h 148 16 32\nAccuracy (%) 89.189.489.5 89.8 89.3\nOURS T h en u m b e ro fg r a p h 148 16 32\nAccuracy (%) 90.190.4 90.7 90.5 90.2\nMultiple Graphs and Subgraphs. We also investigate the eﬀect of the num-\nber of graphs on model performance. The number of graphs is equal to thenumber of subgraphs. The results are shown in Table 3, where we compare with\nARG, we conduct experiments using the published ARG code and following\nthe author’s hyper-parameter settings. Subgraph extraction brings signiﬁcantimprovement. As we increase the number of graphs, the accuracy of ARG and\nour model improve. Compared with ARG, our model achieves optimal perfor-\nmance with only half the number of graphs in the ARG. Our best performanceis 1% higher than that of ARG. The results demonstrate that the addition of\nsubgraph of key actors brings additional information to group activity recogni-\ntion.\n4.2 Compared with SOTA\nIn this section, we compare our best model (with K=3a n d Temporal modeling )\nwith existing SOTA methods. We performed experiments on diﬀerent backbone\nnetworks, including Inception-v3 and VGG. The results are shown in Table 4,\nand our model shows encouraging results.\nThe results show that our results surpass all existing methods, thus estab-\nlishing a new state-of-the-art, including all backbone. This indicates that our\nmethod is generalizable. Our method extracts the key actors of group activitiesand their interactions, which are identiﬁable information in group activity recog-\nnition; thus, our method is better than the ARG method [ 4]. Our method is also\nsuperior to the hierarchical relational network and the method using RNN [ 6],\nmainly because we can extract key actor’s action information.', 'Learning Key Actors and Their Interactions for Group Activity Recognition 63\nTable 4. Comparison with SOTA on the volleyball dataset and collective dataset.\nVolleyball dataset Collective dataset\nMethod Backbone Accuracy (%) Method Backbone Accuracy (%)\nCERN [ 25] VGG16 83.3 HDTM [ 6] AlexNet 81.9\nStageNet [ 8]VGG16 89.3 SIM [ 26] VGG16 83.3\nHRN [ 7] VGG16 87.6 SBGAR [ 27]VGG16 89.3\nSSU [ 28] Inception-v3 90.6 CERN [ 25] VGG16 87.6\nARG [ 4] VGG16 91.9 StageNet [ 8]Inception-v3 90.6\nARG [ 4] Inception-v3 92.5 ARG [ 4] Inception-v3 91.0\nPRL [ 29] VGG16 91.4 ARG [ 4] VGG16 90.1\nOURS Inception-v3 92.8 OURS Inception-v3 91.3\nOURS VGG16 92.4 OURS VGG16 91.1\nOURS VGG19 92.5 OURS VGG19 91.0\nIn general, instead of explicitly giving key actors more attention, our app-\nroach increases the importance of key actors. Our method provides a more rec-\nognizable representation of the scene only containing key actors for the net-\nwork, which distinguishes (characterizes) the diﬀerent activities. Compared withARG [ 4], our approach improves the VGG network more than it improves the\nInception-v3 network, which we conjecture is due to the strong ﬁtting capability\nof the Inception-v3 network. The improvement on VGG validates the eﬀective-\nness of our approach, demonstrating that SARG can bring additional useful\ninformation.\n5 Conclusion\nWe proposed a novel end-to-end deep learning architecture and designed a sub-graph extraction method to learn key actors and their interactions in group activ-\nity recognition. Based on the actor action self-attention method, the subgraph is\nextracted from the actor relationship graph to obtain a new scene representationthat only contains the key actors and their interactions. Our method provides\nadditional information for group activity recognition and distinguishes diﬀerent\ngroup activities. In addition, we design a variation of attention calculation andthree methods for information fusion. Additional ablation experiments demon-\nstrated that: (1)Our method can learn and extract information about key actors\nand their interactions. (2) The subgraph of key actors brings additional infor-mation to group activity recognition. We evaluate our model over two datasets:\nthe collective activity dataset and the volleyball dataset. SARG has an average\nimprovement of 4%, compared to 8 benchmarks.', '64 Y. Duan and J. Wang\nReferences\n1. Ramanathan, V., et al.: Detecting events and key actors in multi-person videos. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2016\n2. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotem-\nporal features with 3d convolutional networks. IEEE (2015)\n3. Wang, L., Li, W., Li, W., Van Gool, L.: Appearance-and-relation networks for video\nclassiﬁcation. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (2017)\n4. Wu, J., Wang, L., Wang, L., Guo, J., Wu, G.: Learning actor relation graphs for\ngroup activity recognition. In: CVPR (2019)\n5. Lee, J., Lee, I., Kang, J.: Self-attention graph pooling. In: 36th International Con-\nference on Machine Learning (2019)\n6. Ibrahim, M., Muralidharan, S., Deng, Z., Vahdat, A., Mori, G.: A hierarchical\ndeep temporal model for group activity recognition. In: 2016 IEEE Conference onComputer Vision and Pattern Recognition (CVPR) (2016)\n7. Ibrahim, M.S., Mori, G.: Hierarchical relational networks for group activity recog-\nnition and retrieval. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.)ECCV 2018. LNCS, vol. 11207, pp. 742–758. Springer, Cham (2018). https://doi.\norg/10.1007/978-3-030-01219-9\n44\n8. Qi, M., Jie, Q., Li, A., Wang, Y., Luo, J., Gool, L.V.: Stagnet: an attentive semantic\nRNN for group activity recognition. In: European Conference on Computer Vision\n(2018)\n9. Lecun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436 (2015)\n10. Liu, J., Shahroudy, A., Xu, D., Wang, G.: Spatio-temporal LSTM with trust gates\nfor 3d human action recognition. In: ECCV (2016)\n11. Si, C., Chen, W., Wang, W., Wang, L., Tan, T.: An attention enhanced graph\nconvolutional LSTM network for skeleton-based action recognition. In: 2019\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n(2019)\n12. Hamilton, W.L., Ying, R., Leskovec, J.: Inductive representation learning on large\ngraphs. In: NIPS (2017)\n13. Kipf, T.N.: Max Welling. Semi-supervised classiﬁcation with graph convolutional\nnetworks (2016). arXiv preprint: arXiv:1609.02907\n14. Ying, R., You, J., Morris, C., Ren, X., Hamilton, W.L., Leskovec, J.: Hierarchical\ngraph representation learning with diﬀerentiable pooling. In: 32th Conference onNeural Information Processing Systems (2018)\n15. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition. arXiv (2014)\n16. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the incep-\ntion architecture for computer vision. In: IEEE, pp. 2818–2826 (2016)\n17. He, K., Gkioxari, G., Doll´ ar, P., Girshick, R.: Mask R-CNN. IEEE (2017)\n18. Veliˇ ckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li` o, P., Bengio, Y.: Graph\nattention networks. Int. Conf. Learn. Represent. (2018) (accepted as poster)\n19. Cheung, M., Shi, J., Jiang, L.Y., Wright, O., Moura, J.: Pooling in Graph Convo-\nlutional Neural Networks. IEEE (2020)\n20. Parikh, A.P., T¨ ackstr¨ om, O., Das, D., Uszkoreit, J.: A decomposable attention\nmodel for natural language inference (2016). arXiv preprint: arXiv:1606.01933', 'Learning Key Actors and Their Interactions for Group Activity Recognition 65\n21. Zhang, H., Goodfellow, I., Metaxas, D., Odena, A.: Self-attention generative adver-\nsarial networks. In: International Conference on Machine Learning, pp. 7354–7363.\nPMLR (2019)\n22. Gao, H., Ji, S.: Graph u-nets. In: 36th International Conference on Machine Learn-\ning (2019)\n23. Xu, K., Hu, W. Leskovec, J., Jegelka., S.: How powerful are graph neural networks?\nIn: International Conference on Learning Representations (2019)\n24. Choi, W., Shahid, K., Savarese, S.: What are they doing?: collective activity clas-\nsiﬁcation using spatio-temporal relationship among people. In: IEEE International\nConference on Computer Vision Workshops (2009)\n25. Shu, T., Todorovic, S. Zhu, S.C.: CERN: conﬁdence-energy recurrent network for\ngroup activity recognition. In: 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) (2017)\n26. Deng, Z., Vahdat, A., Hu, H., Mori, G.: Structure Inference Machines: Recurrent\nNeural Networks For Analyzing Relations In Group Activity Recognition. IEEE\n(2016)\n27. Xin, L., Chuah, M.C.: SBGAR: semantics based group activity recognition. In:\nIEEE International Conference on Computer Vision (2017)\n28. Bagautdinov, T., Alahi, A., Fleuret, F., Fua, P., Savarese, S.: Social scene under-\nstanding: end-to-end multi-person action localization and collective activity recog-nition. IEEE (2016)\n29. Hu, G., Cui, B., He, Y., Yu, S.: Progressive relation learning for group activ-\nity recognin. In: 2020 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) (2020)', 'Attributed Non-negative Matrix\nMulti-factorization for Data\nRepresentation\nJie Wang, Yanfeng Sun(B), Jipeng Guo, Yongli Hu, and Baocai Yin\nBeijing Key Laboratory of Multimedia and Intelligent Software Technology,\nBeijing Institute of Artiﬁcial Intelligence, Faculty of Information Technology,\nBeijing University of Technology, Beijing, China\n{wangjie,guojipeng }@emails.bjut.edu.cn, {yfsun,huyongli,ybc }@bjut.edu.cn\nAbstract. Non-negative matrix factorization (NMF) is an important\nmethod of latent data representation learning. Most of the existing NMFmethods focus only on one single factorization and obtain one clustering\nsolution. However, real data are usually complex and can be described\nfrom multiple attributes or sub-features. For example, face image con-sists of genders attribute and expressions attribute. And, the various\nattributes provide complementary information of data. Failing to explore\nmulti-attribute representation and exploit the complementary informa-tion, it may be diﬃcult to learn discriminative representation. In order to\nsolve the above issue and obtain richer low-dimensional representations,\nwe propose the Attributed Non-negative Matrix Multi-Factorization forData Representation (ANMMF) model which simultaneously learns mul-\ntiple low-dimensional representations from original data. By utilizing\nHilbert Schmidt Independence Criterion (HSIC) to constrain the pair-wise attributes, ANMMF enforces that each low-dimensional attribute\nrepresentation is independent, which eﬀectively mines complementary\nmulti-attribute information embed in the original data. Further, graphLaplacian regularization is constrained to maintain the local geometri-\ncal structure. The low dimensional multi-attribute representation infor-\nmation embedded in the original data is fused to improve the cluster-ing results. Finally, we develop the iterative updating schemes for the\nANMMF model optimization, and extensive experiments on real-world\ndatabases demonstrate that our method has the most advanced perfor-mance compared with other related algorithms.\nKeywords: Non-negative matrix factorization\n·Multi-attribute\nrepresentation ·Hilbert Schmidt Independence Criterion ·Graph\nLaplacian constraint\nThe ﬁrst author is a student.\nElectronic supplementary material The online version of this chapter ( https://\ndoi.org/10.1007/978-3-030-88013-2 6) contains supplementary material, which is avail-\nable to authorized users.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 66–77, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_6', 'Attributed Non-negative Matrix Multi-factorization for Data Representation 67\n1 Introduction\nHigh-dimensional data are becoming increasingly common in many ﬁelds, such\nas biomedical engineering, pattern recognition, computer vision and image engi-\nneering [ 4,7,18]. Thus, learning eﬀective low-dimensional representation of orig-\ninal data is necessary. Some dimension reduction methods have achieved greatsuccesses, such as Principal Component Analysis (PCA) [ 1], Linear Discriminant\nAnalysis (LDA) [ 13], and Locality Preserving Projection (LPP) [ 10]. However,\nthey are diﬃcult to provide a straightforward physical meaning. Non-negativematrix factorization (NMF) [ 12] gradually became the most popular dimension\nreduction tools owing to its straightforward interpretability of non-negative fac-\ntorization results. The main idea of NMF algorithm can be simply described asfollows: for any non-negative data matrix X, NMF algorithm seeks to ﬁnd two\nnon-negative matrices UandV,s ot h a t X≈UV\nTis satisﬁed. Thus, the NMF\nallows only non-subtractive combinations of non-negative components. This non-negativity constraints result in the parts-based representation of NMF.\nDue to the good properties of NMF, various methods [ 11,16] are proposed\nto extend the standard NMF by adding additional constraints. Sparse NMF\n(SNMF) [ 6] introduces a penalty constraint to encode sparsity in the factor\nmatrices, leading to a local-based representation. Orthogonal NMF [ 5] reduces\nthe redundant information of data representation by inducing the orthogonality\nconstraints on the factor matrices. The geometrical structure of data is of great\nimportance for learning eﬀective representation for unsupervised tasks. Thus,graph regularized NMF (GNMF) [ 2] preserves the local geometrical structure in\nthe low-dimensional representation by the graph Laplacian regularization.\nAlthough NMF and GNMF made the great successes in many applications,\nboth tend to intuitively utilize the features of original data as a whole and they\nonly obtain one pair of factorization factors from original data. However, real\ndata is complex and can be described from multiple attributes and aspects. Con-cretely, one data representation can be regarded as a description of original data\nfrom one aspect. For example, face images include facial expressions, races, skin\ncolors, facial features, decorations and other features. Because each attributecontains speciﬁc information, it is crucial to explore the diverse and comple-\nmentary information among multiple attributes to learn a more discriminative\nlow-dimensional representation.\nTo address the aforementioned issues, we propose an Attributed Non-negative\nMatrix Multi-Factorization (ANMMF) model for data representation, whichdescribes data from diﬀerent attributes and fuses the low-dimensional represen-\ntations of these attributes to improve the clustering results. Speciﬁcally, a non-\nnegative matrix Xis decomposed into multiple pairs of factors, thus obtaining\nmultiple low-dimensional representations. Then, Hilbert Schmidt Independence\nCriterion (HSIC) [ 9] is used to increase the diversity between diﬀerent represen-\ntations, which explores more complementary information. Each low-dimensionalrepresentation is enforced to be independent and correspond to a particular\nattribute. Similar to GNMF, the multiple representations learned by ANMMF\nalso preserve the local geometric structure by the graph Laplacian regularization.', '68 J. Wang et al.\n2 Related Work\nIn this section, we brieﬂy review the closely related works, i.e., Non-negative\nMatrix Factorization (NMF) [ 12] and Graph regularized NMF (GNMF) [ 2].\n2.1 NMF\nGiven a non-negative data matrix X=[x1,x2,···,xn]∈RD×n\n+, each column\nofXis a observed sample vector. U∈RD×m\n+is regarded as the basis matrix,\nV∈Rn×m\n+is regarded as the representation coeﬃcient matrix. NMF model can\nbe mathematically expressed by the following objective function:\nmin\nU≥0,V≥0/bardblX−UVT/bardbl2\nF, (1)\nwhere /bardbl·/bardbl2\nFis Frobenius norm of a matrix. Owing to the non-negativity of factor\nmatrices, NMF learns a parts-based data representation, which is easy to explain\nin many real applications.\n2.2 GNMF\nIn fact, the geometrical structure of data is of great importance for signiﬁcantly\nimproving the learning performance of data representation. To incorporate the\ngeometric structure into the NMF, Graph regularized NMF (GNMF) [ 2]w a s\nproposed, in which geometric information was encoded by an aﬃnity graph and\npreserved by the graph Laplacian regularization. As shown in [ 2], the objective\nfunction of GNMF model is as follows:\nmin\nU≥0,V≥0/bardblX−UVT/bardbl2+λtr(VTLV), (2)\nwhere λis a non-negative regularization parameter and is employed to balance\nthe reconstruction error and regularized term. L=D−Wis the graph Laplacian\nmatrix corresponding to the aﬃnity matrix W∈Rn×n,i nw h i c h Dis a diagonal\nmatrix and Djj=n/summationtext\ni=1W ij.\n3 The Proposed Method\nIn this section, we ﬁrst present our novel clustering framework, Then, we design\nan optimization algorithm for our approach.\n3.1 Motivation and Objective Function\nLearning a informative data representation is a crucial prerequisite for subse-\nquent tasks. Although NMF and GNMF are promising to learn eﬀective data\nrepresentation, they still have some limitations. For example, for complex face', 'Attributed Non-negative Matrix Multi-factorization for Data Representation 69\nimages, it is diﬃcult for NMF and GNMF to learn the representation of multiple\nattributes due to the only one factorization and capture the diverse informationamong them. In fact, diﬀerent representations describe samples from diﬀerent\nattributes and aspects, which can provide rich information.\nTo understand and represent data thoroughly and in-depth, we propose a ﬂex-\nible and robust non-negative matrix factorization model to eﬀectively solve the\ndata multi-representation problem, which is named as Attributed Non-negative\nMatrix Multi-Factorization (ANMMF). This model learns multiple representa-tions from original data, i.e., multi-attribute representation {V\ni}V\ni=1.E a c h Vi\nrepresents an aspect/attribute of the data, and the latent information of each\nattribute can be fully explored, i.e., ViandVj, are enforced to be independent\nto each other. From the perspective of model optimization, we note that the loss\nfunction ( 1) and ( 2) are non-convex in both variables U,V. This indicates that\nthe solution for NMF or GNMF is not unique, i.e., the non-negative matrix fac-\ntorization of data Xis not unique. To this end, we propose the following matrix\nfactorization model:\nV/summationdisplay\ni=1/bardblX−UiVT\ni/bardbl2F+λ1V/summationdisplay\ni/negationslash=jHSIC( Vi,Vj),\ns.t. Ui≥0,Vi≥0,∀i∈{1,2,···,V},(3)\nwhere λ1≥0 is the trade-oﬀ parameter. TheV/summationtext\ni/negationslash=jHSIC( Vi,Vj) is co-regularizing\nterm which ensures that the correlation of diﬀerent representations is minimum.\nThus, the co-regularizing term exploits complementary information from vari-\nous aspects. Here, we enforce diﬀerent representations {Vi}V\ni=1to be diverse by\nadopting the Hilbert-Schmidt Independence Criterion (HSIC) [ 9]. One empirical\nversion of HSIC is deﬁned as follows:\nDeﬁnition 1. Consider a series of nobservations A×B={(a1,b1),\n(a2,b2),···,(an,bn)}, an estimator of HSIC , written as HSIC( A,B),i s\ngiven by:\nHSIC( A,B)=(n−1)−2tr(KAHK BH), (4)\nwhere KAand KBare the Gram matrices with kA,ij=kA(ai,aj),kB,ij=\nkB(bi,bj),i nw h i c h kAandkBare the kernel functions. hij=δij−1\nncenters\nthe Gram matrix to have zero mean in the feature space.\nFor more details of HSIC, please refer to the related paper [ 9]. As in GNMF,\nwe also explore the local graph structure, which explicitly enforces the represen-\ntations to meet the geometrical structure. For the sake of simplicity, we assume\nthat diﬀerent representations obey the same geometrical structure. Thus, theﬁnal model of ANMMF is deﬁned as:\nO= V/summationdisplay\ni=1/bardblX−UiVT\ni/bardbl2F+λ1V/summationdisplay\ni/negationslash=jHSIC( Vi,Vj)+λ2V/summationdisplay\ni=1tr(VT\niLV i)\ns.t. Ui≥0,Vi≥0,∀i∈{1,2,···,V},(5)', '70 J. Wang et al.\nwhere λ1≥0a n d λ2≥0 are the trade-oﬀ parameters to balance the diversity\namong low-dimensional representations and graph regularization term for allattributes. Vis the number of diﬀerent low dimensional representations.\nHere, we analyze the relations between NMF, GNMF and proposed ANMMF.\nRecall the objective function of ANMMF in ( 5), if we set V=1a n d λ\n1=0 ,\nthe above formula degenerates into the GNMF. When λ1,λ2are both 0 and\nV= 1, the above formula degenerates into the basic NMF. We also ﬁnd when\nλ2= 0, the above model degenerates to MCNMF [ 17]. Thus, NMF, GNMF and\nMCNMF can be seen as special cases of the proposed ANMMF. Theoretically,\nthe proposed method ANMMF will have a better performance.\n3.2 Model Optimization\nObviously, the objective function in Eq. ( 5) is non-convex in both of UiandVi\ntogether, so the global minimum and closed-form solution cannot be obtained.\nHere, we provide an iterative strategy to optimize the objective function.\nFirst of all, the objective function of ( 5) can be rewritten as follows:\nO=V/summationdisplay\ni=1/bardblX−UiVT\ni/bardbl2F+λ1V/summationdisplay\ni/negationslash=jHSIC( Vi,Vj)+λ2V/summationdisplay\ni=1tr(VT\niLV i)\n=V/summationdisplay\ni=1/parenleftbig\ntr(XXT)−2tr(XV iUT\ni)+t r ( UiVT\niViUTi)/parenrightbig\n+λ1V/summationdisplay\ni/negationslash=jHSIC( Vi,Vj)+λ2V/summationdisplay\ni=1tr(VT\niLV i),(6)\nwhere the second equation holds because of the matrix computation properties\ntr(AB)=t r ( BA) and tr( A)=t r ( AT). letΦiandΨibe the Lagrangian multi-\npliers for non-negative constraints Ui≥0a n d Vi≥0, respectively. Let φcdand\nψldbe the lagrange multiplier for constraint ucd≥0a n d vld≥0, respectively,\nandΦi=[φcd],Ψi=[ψld]. Then the Lagrange function is given as follows:\nL=V/summationdisplay\ni=1/parenleftbig\ntr(XXT)−2tr(XV iUT\ni)+t r ( UiVT\niViUTi)/parenrightbig\n+λ2V/summationdisplay\ni=1tr(VT\niLV i)\n+λ1V/summationdisplay\nj=1 ,j/negationslash=iHSIC( Vi,Vj)+V/summationdisplay\ni=1/parenleftbig\ntr(ΦiUT\ni)+t r ( ΨiVT\ni)/parenrightbig\n,\n(7)\nWith the alternating optimization strategy, we can approximately optimize equa-\ntion ( 7) in the manner of minimizing with respect to one pair of factorization\nfactors {Ui,Vi}once at a time while ﬁxing the others. Speciﬁcally, with all but\none{Ui,Vi}ﬁxed.\nIn this paper, we use the simple and eﬀective inner product nucleus to rep-\nresent HSIC, i.e., Ki=ViVT\ni. Since ( n−1)−2is an independent constant from', 'Attributed Non-negative Matrix Multi-factorization for Data Representation 71\nthe coeﬃcient matrix Viwe are seeking, it is ignored for the convenience of\noptimization. It can be further written as follows:\nV/summationdisplay\nj=1 ,j/negationslash=iHSIC( Vi,Vj)=V/summationdisplay\nj=1 ,j/negationslash=itr(HK iHK j)=V/summationdisplay\nj=1 ,j/negationslash=itr(VT\niHK jHV i)\n=t r ( VT\niKV i),(8)\nwhere K=V/summationtext\nj=1 ,j/negationslash=itr(HK jH). The variables UiandVican be solved alternately\nby alternating minimization strategy of L(Ui,Vi) while keeping all the others\nﬁxed. The detailed optimization strategy is presented as follows.\nUpdate U iwith Fixing Others. The partial derivation of ( 7) with respect\ntoUiis:\n∂L(Ui,Vi)\n∂Ui=−2XV i+2UiVT\niVi+Φi. (9)\nUsing the KKT condition φcducd= 0, we get the following equation:\n−(XV i)cducd+(UiVT\niVi)cducd= 0 (10)\nFormula ( 10) is further simpliﬁed to obtain the following updating rule of Ui:\nucd←ucd(XV i)cd\n(UiVT\niVi)cd(11)\nUpdate V iwith Fixing Others. According to formula ( 8), the partial deriva-\ntion of ( 7) with respect to variable Viis presented as follows:\n∂L(Ui,Vi)\n∂Vi=(−2XTUi+2ViUT\niUi)+( 2 λ1KV i)+( 2 λ2LV i)+Ψi.(12)\nUsing KKT condition ψldvld= 0, so we get the following equation:\n−(XTUi)ldvld+(ViUT\niUi)ldvld+λ1(KV i)ldvld+λ2(LV i)ldvld=0.(13)\nSince L=D−Wand K=K+−K−(in which K+=1\n2(|K|+K),K−=\n1\n2(|K|−K)), Formula ( 13) is further simpliﬁed to obtain the following updating\nrule of Vi:\nvld←vldXTUi+λ1K−Vi+λ2WV i)ld\n(ViUT\niUi+λ1K+Vi+λ2DV i)ld. (14)\nThe whole procedure of ANMMF is summarized in Algorithm 1.\n3.3 Algorithm Analysis\nConvergence Analysis. In the following, we will investigate the convergence\nof the updating rules of ( 11) and ( 14). And regarding these two updating rules,\nwe have the following convergence theorem:', '72 J. Wang et al.\nAlgorithm 1. Optimization Algorithm for ANMMF\nInput : Data matrix X∈RD×n, the number of reduced dimension m ( m≤D), the\nnumber of multiple attribute factorization V,and the parameter λ1,λ2.\nInitialize :V i∈Rn×mby standard NMF ( i=1,···,V),/epsilon1=1 0−5.\nOutput :{U i,V i}V\ni=1.\n1:while not converged do\n2: fori=1:V do\n3: Updating {U i,V i}when ﬁxing the other pair of factorization factors\n{U j,V j}j/negationslash=i, i.e.,\n4: Updating variable U iaccording to ( 11).\n5: Updating variable V ia c c o r d i n gt o( 14).\n6: end for\n7: Checking the convergence conditions:\n|O t+1−O t|</epsilon1.\n8:end while\nTheorem 1. ForX,Ui,Vi≥0, the objective function in Eq. (5)is non-\nincreasing under the updating rules of (11)and(14).\nPlease see the Appendix for the detailed proof for the convergence theorem.\nAnd, we know that the multiplicative updating rules of ( 11) and ( 14)a r es p e -\nciﬁc cases of gradient decent with an automatic step parameter selection. Thus,\nTheorem 1guarantees that the iterative multiplicative updating rules converge\nto a local optimum.\nComplexity Analysis. In this section, we discuss the additional computational\ncost of our proposed algorithm, which is usually denoted by the symbol O.\nThe complexity of algorithm 1 mainly comes from the following aspects. ForANMMF, if the number of iteration is T, when the non-negative matrix U\ni\nandViare updated based on the formula ( 11) and ( 14), the calculation cost is\nO(Tm(nm+Dn+D)) and O(T(Dmn +n2m+nm2+m2D+mn)). Because\nthe goal of our model is dimension reduction, considering that typically m/lessmuchD\nandm/lessmuchn, the total cost of ANMMF is O(T(Dmn +n2m)).\n4 Experiments\n4.1 Experiment Settings\nWe conducted experiments on several benchmark databases to evaluate the per-\nformance of proposed ANMMF, including four databases, namely ORL, CMUPIE [15], Yale and Extended YaleB (EYaleB) [ 8] databases. The basic informa-\ntion of the used databases is summarized in Table 1.', 'Attributed Non-negative Matrix Multi-factorization for Data Representation 73\nTable 1. The description of the databases.\nDatabase ORL PIE Yale EYaleB\nSize ( V) 4002856 1652414\nDim ( D)1024 1024 1024 1024\nClusters ( c)40 68 15 38\nWe compare the performances of proposed algorithm ANMMF against\nsix existing state-of-art clustering methods or related algorithms, including\nK-means, PCA [ 1], Ncut [ 14], NMF [ 12], GNMF [ 2], and MCNMF [ 17]. All\nexperiments use the K-means method in the ﬁnal clustering stage, and the regu-\nlarization parameters are set by searching the grid {10−4,10−3,···,103}.I nt h e\nexperiments, some algorithms perform K-means after dimension reduction or aresensitive to the initialization. For all the methods, we repeat each algorithm 10\ntimes with random initialization and then report their average results.\nThe proposed model learns the diﬀerent representations for clustering. For\nconvenience, the Vis set as 3 for all the data sets. The regularization param-\neters λ\n1andλ2are also set by searching the grid {10−4,10−3,···,103}.T o\ncompare the methods fairly, we select several parameter combinations for the\nexperiments and then choose the best results for comparison. ANMMF learns\nmultiple diﬀerent representations {Vi}V\ni=1. Then the multi-view K-Means clus-\ntering method [ 3] is adopted to integrate the low-dimensional representations\nfrom diﬀerent aspects and make full use of the information from all attributes.\nIn all experiments, we set the number of clusters and nearest neighbor k\nequal to the true number of classes cfor all the clustering algorithms, we use\nthe heat kernel weighting scheme for constructing the k-nearest neighbor graph.\nIn addition, the data are non-normalized. The three evaluation metrics are usedto evaluate all the methods, i.e., normalized Mutual Information (NMI), Accu-\nracy (ACC), and F-score. These evaluation metrics comprehensively measure\nthe quality of clustering algorithms from diﬀerent aspects. For all the evaluationindicators, the higher the value is, the better the clustering eﬀect will be.\n4.2 Experiment Results\nTable 2shows the clustering results of diﬀerent algorithms on the ORL, Yale,\nPIE, and EYaleB databases. We use multiple evaluation indicators to evaluate\nthe clustering results and mark the best results in bold. As can be seen fromTable 2, the ANMMF algorithm is superior to other clustering results in most\ncases, which proves the eﬀectiveness of exploring diverse information among dif-\nferent attributes, especially on ORL and PIE databases. This indicates thatour proposed method can learn better parts-based representations of data. We\nalso ﬁnd that GNMF model is superior to the method of K-means clustering\nin most cases and ANMMF model is superior to the method of MCNMF, indi-cating that the geometric structure can maintain local structure in the learning', '74 J. Wang et al.\nTable 2. Clustering results on diﬀerent databases.\nMethod Metrics ORL PIE Yale EYaleB\nK-means ACC 0.5050 0.1359 0.4485 0.0932\nNMI 0.7104 0.2897 0.5006 0.1044\nF-score 0.5152 0.1472 0.4583 0.1020\nPCA [ 1] ACC 0.6200 0.2052 0.4000 0.1118\nNMI 0.7745 0.4217 0.4217 0.1312\nF-score 0.6144 0.1813 0.4215 0.1017\nNcut [ 14] ACC 0.2525 0.4093 0.2364 0.0953\nNMI 0.4214 0.5061 0.2896 0.1265\nF-score 0.2182 0.3055 0.2231 0.0824\nNMF [ 12] ACC 0.5850 0.4205 0.4000 0.1810\nNMI 0.7305 0.6392 0.4531 0.2569\nF-score 0.5836 0.3852 0.3978 0.1613\nGNMF [ 2]ACC 0.6075 0.3628 0.4606 0.1301\nNMI 0.7632 0.5990 0.4848 0.1806\nF-score 0.5926 0.3288 0.4562 0.1260\nMCNMF [ 17]ACC 0.6275 0.6068 0.4545 0.2336\nNMI 0.7866 0.7532 0.4914 0.3353\nF-score 0.6310 0.5525 0.4315 0.2057\nANMMF ACC 0.6375 0.6544 0.4848 0.2386\nNMI 0.7954 0.7775 0.5073 0.3457\nF-score 0.6358 0.5957 0.4679 0.2068\nrepresentation data representation and improve the clustering performance. In\naddition, since the data we used are non-normalized data, it is more diﬃcult to\ncapture characteristic information. However, our method can still obtain overall\nbetter experimental results. The results show that the attributed non-negative\nmatrix multi-factorization can be achieved, the local structure is preserved by\ngraph regularization and the redundant features are reduced by HSIC regular-izer. And, ANMMF learns more comprehensive information of data by exploring\ndiverse information among multiple independent low-dimensional representation\nand integrates them together to improve the clustering performances.\nBelow, we use two examples to investigate the eﬀectiveness of any learned\nparts-based representations V\ni. We conduct the K-means clustering method on\nthe all representations Vi, respectively. The related results are shown in Table 3.\nThe GNMF is the special case of proposed ANMMF when V=1a n d λ1=0 .\nBy considering the Table 3, the any learned parts-based representation Viof\nANMMF outperforms the GNMF which is showed the eﬃcacy of ANMMF tolearn the more discriminative representation. This is mainly due to the fact', 'Attributed Non-negative Matrix Multi-factorization for Data Representation 75\nTable 3. Clustering results on diﬀerent databases.\nData Metrics GNMF ANMMF\nV1 V2 V3\nORL ACC 0.6075 0.6700 0.6525 0.6575\nNMI 0.7632 0.7894 0.7884 0.7987\nF-score 0.5926 0.6677 0.6492 0.6614\nPIE ACC 0.3628 0.6334 0.6247 0.6138\nNMI 0.5990 0.7574 0.7605 0.7585\nF-score 0.6329 0.5795 0.5663 0.5621\nthat ANMMF learns multiple attribute representations which contain the rich\ninformation from the original features.\n4.3 Parameter Analysis\nIn this subsection, we analyze the parameters inﬂuence for the proposed\nANMMF. The objective function of ANMMF model contains two balance param-\neters, λ1andλ2. Concretely, λ1controls the importance of co-regularizing term\nandλ2controls the geometrical manifold regularization term. The clustering\nperformance of ANMMF varies in terms of parameters is shown in the Fig. 1.A s\nwe can ﬁnd, ANMMF is robust to the parameters with wide range. Especially,ANMMF can consistently achieve stable and excellent clustering performance\neven for λ\n1varies from 10−4to 103when the λ2is ﬁxed.\nWe also test the eﬀect of the number of components V. Here we ﬁxed param-\neter in the best combinations for the experiments and varied Vf r o m1t o5\nwith an increment of 1. Seen from Fig. 2, The best performance of the num-\nber of multiple attributes always at V= 3 on most databases. Speciﬁcally, the\naccuracy increases sharply when Vis tuned from 1 to 3, which indicates the\neﬀectiveness of ANMMF by exploring multiple attributes. Then the accuracyﬂuctuates slightly when Vincreases from 3 to 5. The ﬂuctuation could be due\nto a compromise between the amount of features for each representation and the\ndiverse information among them. Thus, we empirically ﬁx V=3 .\n(a) PIE (b) ORL (c) Yale\n (d) Extend YaleB\nFig. 1. Clustering accuracy of ANMMF on four data sets with varying λ1andλ2.', '76 J. Wang et al.\n12345\nNumber of attributes00.10.20.30.40.50.60.7ACC\n(a) PIE12345\nNumber of attributes00.10.20.30.40.50.60.7ACC\n(b) ORL12345\nNumber of attributes0.050.10.150.20.250.30.350.40.450.5ACC\n(c) Yale12345\nNumber of attributes00.050.10.150.20.25ACC\n(d) Extend YaleB\nFig. 2. The eﬀect of the number of components V.\n4.4 Convergence Study\nThe updating rule of the minimized objective function of ANMMF is iterative\nin a natural way. In this section, we study the convergence behavior of theproposed algorithm from experimental aspect. The experimental convergence\ncurve is shown in the Fig. 3, where y-axis is the value of the objective function\nand x-axis is the number of iterations. From the Fig. 3, we can clearly see that the\noptimization strategy monotonically decreases objective function and achieves\na stable value in a few iterations, which indicates that the proposed ANMMF\noptimization method is very eﬀective in practice.\n0 20 40 60 80 100 120 140 160 180 200\nNo.of Iteration012345678Objective Function Value1010\n(a) PIE0 20 40 60 80 100 120 140 160 180 200\nNo.of Iteration00.511.522.5Objective Function Value1010\n(b) ORL0 2 04 06 08 0 1 0 0 1 2 0 1 4 0 1 6 0 1 8 0 2 0 0\nNo.of Iteration01234567Objective Function Value109\n(c) Yale0 2 04 06 08 0 1 0 0 1 2 0 1 4 0 1 6 0 1 8 0 2 0 0\nNo.of Iteration0123456Objective Function Value1010\n(d) Extend YaleB\nFig. 3. Convergence curves on the all data sets.\n5 Conclusions\nIn this paper, we propose the Attributed Non-negative Matrix Multi-\nFactorization for Data Representation (ANMMF) approach which explores\nmulti-attribute decomposition of data to understand data from various aspects.\nDiﬀerent from NMF approaches that seek for a single low-dimensional repre-sentation based on original data, ANMMF simultaneously learns multiple low-\ndimensional representations with each one corresponding to one attribute. More-\nover, Hilbert Independence Schmidt Criterion is used to increase the diversitybetween diﬀerent attribute representations and reduce the redundant features.\nThe superiority of the proposed method is shown in several real data sets.\nAcknowledgements. This research was supported by National Natural Science Foun-\ndation of China under Grant No. 61772048, U19B2039, U1811463 and 61876012. It also\nwas supported in part by Beijing Talents Project (2017A24).', 'Attributed Non-negative Matrix Multi-factorization for Data Representation 77\nReferences\n1. Abdi, H., Williams, L.J.: Principal component analysis. Wiley Interdisc. Rev. Com-\nput. Stats 2(4), 433–459 (2010)\n2. Cai, D., He, X., Han, J., Huang, T.S.: Graph regularized nonnegative matrix fac-\ntorization for data representation. IEEE Trans. Pattern Anal. Mach. Intell. 338,\n1548–1560 (2011)\n3. Cai, X., Nie, F., Huang, H.: Multi-view k-means clustering on big data. In: IJCAI\n(2013)\n4. Dai, X., Zhang, K., Jiang, X., Zhang, X., Tu, Z., Zhang, N.: Weighted non-negative\nmatrix factorization for image recovery and representation. In: 2020 10th ICIST,pp. 288–293. IEEE (2020)\n5. Ding, C.H.Q., Li, T., Peng, W., Park, H.: Orthogonal nonnegative matrix tri-\nfactorizations for clustering. In: KDD 2006, pp. 20–23 (2006)\n6. Eggert, J., Korner, E.: Sparse coding and NMF. In: IEEE IJCNN, vol. 4, pp.\n2529–2533 (2004)\n7. Ge, S., Luo, L., Li, H.: Orthogonal incremental non-negative matrix factorization\nalgorithm and its application in image classiﬁcation. Comput. Appl. Math. 39(2),\n1–16 (2020). https://doi.org/10.1007/s40314-020-1091-2\n8. Georghiades, A.S., Belhumeur, P., Kriegman, D.: From few to many: illumination\ncone models for face recognition under variable lighting and pose. IEEE Trans.\nPattern Anal. Mach. Intell. 23, 643–660 (2001)\n9. Gretton, A., Bousquet, O., Smola, A., Sch¨ olkopf, B.: Measuring statistical depen-\ndence with Hilbert-Schmidt norms. In: Jain, S., Simon, H.U., Tomita, E. (eds.) ALT\n2005. LNCS (LNAI), vol. 3734, pp. 63–77. Springer, Heidelberg (2005). https://\ndoi.org/10.1007/11564089\n7\n10. He, X., Niyogi, P.: Locality preserving projections. In: NIPS, vol. 16, no. 1, pp.\n186–197 (2003)\n11. Jia, Y., Kwong, S., Hou, J., Wu, W.: Semi-supervised non-negative matrix factor-\nization with dissimilarity and similarity regularization. IEEE Trans. Neural Netw.\nLearn. Syst. 31(7), 2510–2521 (2020)\n12. Lee, D.D., Seung, H.S.: Learning the parts of objects by nonnegative matrix fac-\ntorization. Nature 401(7), 788–791 (1999)\n13. Riﬀenburgh, R.H., Clunies-Ross, C.W.: Linear discriminant analysis. Pac. Sci. 3(6),\n27–33 (2013)\n14. Shi, J., Malik, J.: Normalized cuts and image segmentation. IEEE Trans. Pattern\nAnal. Mach. Intell. 22(8), 888–905 (2000)\n15. Sim, T., Baker, S., Bsat, M.: The CMU pose, illumination, and expression database.\nIEEE Trans. Pattern Anal. Mach. Intell. 25(12), 1615–1618 (2004)\n16. Sun, J., Wang, Z., Sun, F., Li, H.: Sparse dual graph-regularized NMF for image\nco-clustering. Neurocomputing 316(17), 156–165 (2018)\n17. Wang, J., Tian, F., Wang, X., Yu, H., Liu, C.H., Yang, L.: Multi-component non-\nnegative matrix factorization. In: IJCAI (2017)\n18. Yang, Z., Zhang, Y., Xiang, Y., Yan, W., Xie, S.: Non-negative matrix factorization\nwith dual constraints for image clustering. IEEE Trans. Syst. Man Cybern. Syst.50, 2524–2533 (2020)', 'Improved Categorical Cross-Entropy Loss\nfor Training Deep Neural Networks\nwith Noisy Labels\nPanle Li1, Xiaohui He2(B), Dingjun Song1, Zihao Ding3,M e n g j i aQ i a o1,\nXijie Cheng1, and Runchuan Li1\n1School of Information Engineering, Zhengzhou University,\nZhengzhou 450001, China\n2School of Geoscience and Technology, Zhengzhou University,\nZhengzhou 450001, China\nhexh@zzu.edu.cn\n3College of Communication Engineering, Xidian University, Xi’an 710000, China\nAbstract. Deep neural networks (DNNs) have achieved impressive suc-\ncess in a variety of classiﬁcation tasks. However, the presence of noisy\nlabels in training dataset adversely aﬀects the performance of DNNs.\nRecently, numerous noise-robust loss functions have been proposed tocombat the noisy label problem. However, we ﬁnd that these loss func-\ntions are either slow to learn the potential data pattern or not suﬃciently\nrobust against noisy labels. Here, we propose an improved categoricalcross entropy (ICCE) to deal with this challenge. The ICCE can auto-\nmatically adjust the weighting scheme based on the predicted probabil-\nity distribution of DNNs by an exponential item, which makes it gain\nstrong noise robustness and fast learning ability. A theoretical analysis\nof the ICCE is presented in the context of noisy labels. Experiments ondatasets indicate that the ICCE can better improve the performance of\nDNNs even under high-level noise.\nKeywords: DNNs\n·Noisy labels ·Noise robustness\n1 Introduction\nRecently, deep neural networks (DNNs) have exhibited very impressive perfor-\nmance in many classiﬁcation tasks [ 4,11,17]. However, in all such cases, very\nlarge and accurate training data are needed to achieve high-level performance.\nAlthough crowdsourcing platforms such as Amazon Mechanical Turk have made\nlarge-scale labeled data, noisy labels are often inevitable in the labeling pro-\ncess. The presence of noisy labels in the training dataset is known to impair the\nElectronic supplementary material The online version of this chapter ( https://\ndoi.org/10.1007/978-3-030-88013-2 7) contains supplementary material, which is avail-\nable to authorized users.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 78–89, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_7', 'Improved Categorical Cross-Entropy Loss for Noisy Labels 79\nperformance of DNNs [ 5,6,8,14]. Therefore, improving the robustness of DNNs\nwith regard to noisy labels is of great importance, and it has attracted signiﬁcantinterest in recent years.\nA promising method for handling noisy labels is to propose a noise-robust\nloss function [ 1,2,9,13,15]. [3] proposed a novel auxiliary image regularizer (AIR)\nfor improving the loss function noise robustness. AIR aims to exploit the nonlin-\near manifold structure underlying images and encourage DNNs to select reliable\nimages in the training process. [ 7] theoretically showed that the mean absolute\nerror (MAE) can be robust against noisy labels under certain assumptions, while\nthe categorical cross entropy (CCE) cannot. However, much researches show that\nthe CCE has stronger learning ability than MAE [ 12], which can make DNNs\nobtain higher performance in a shorter training time. Therefore, [ 18]p r o p o s e d\na more generalized cross-entropy loss (GCE) for training DNNs under a noisy\ndataset. The GCE is a noise-robust loss function that combines the advantages\nof MAE and CCE. Following the works by [ 7] and [ 18], [16] proposed an improved\nMAE (IMAE) loss function to improve the learning speed of MAE. They trans-formed the gradients of MAE to nonlinear with an exponential function to enlarge\nthe contribution of examples. However, as we demonstrate below, the IMAE is not\nsuﬃciently robust to noise since it places too much emphasis on noisy examples.\nTo deal with this challenge, we propose an improved categorical cross entropy\n(ICCE) for training DNNs under the noisy dataset. Based on the predicted prob-\nability distribution of DNNs, the ICCE can automatically adjust the weightingscheme of CCE by an exponential item. As a result, the ICCE gains strong noise\nrobustness since it places a small weight on noisy samples. Meanwhile, by giv-\ning more weight to clean samples, the ICCE has a fast learning ability to learnthe true label distribution hidden clean examples. The main contribution of this\npaper falls into two aspects. First, we develop a novel noise-robust loss function,\nICCE, and present a theoretical analysis of the proposed loss functions in thecontext of noisy labels. Second, we report a thorough empirical evaluation of the\nproposed loss function using CIFAR-10 and CIFAR-100, which demonstrates\nsigniﬁcant improvement in terms of classiﬁcation accuracy.\n2 Improved Categorical Cross-Entropy Loss for\nNoise-Robust Classiﬁcation\nIn this section, we start by analyzing the drawbacks of CCE and MAE as classiﬁ-\ncation loss functions for DNNs on noisy datasets. Then, an improved categorical\ncross-entropy (ICCE) loss function is proposed to improve the noise robustness\nof DNNs. Finally, some novel analytical results about ICCE are presented to\ngive a clear understanding.\n2.1 Robustness Analysis of CCE and MAE\nIt is well known that the deep neural network fcan be viewed as a black box,\nand the update of parameters θis based on the backpropagation of the gradient.\nθ(l)←θ(l)+τ/triangleθ(l)(1)', '80 P. Li et al.\nwhere τis the leaning rate. The /triangleθ(l)is the update magnitude for each training\nsample, which is obtained through the gradient of the loss function. Therefore,an example’s contribution can be measured by the gradient’s magnitude of the\nloss function. This can be regarded as an example weighting that is a naturally\nbuilt-in loss function. Let us look at the gradient of the CCE and MAE lossfunction respect to the output of the ﬁnal layer z\nj:\n∂LCCE(f(x),y)\n∂zj=/braceleftBigg\np(y|x)−1j=y\np(j|x) j/negationslash=y(2)\n∂LMAE(f(x),y)\n∂zj=/braceleftBigg\n2p(y|x)(1−p(y|x))j=y\n2p(y|x)p(j|x) j/negationslash=y(3)\nwhere yis the ground truth of example xandjis the predicted label. Let K is\nthe total number class, p(j|x) can be denoted\np(j|x)=exp(zj)/summationtextK\ni=1exp(zi),j∈[K] (4)\nFrom the Eq. 2and3, we can see that CCE places more weight on the sam-\nple than MAE when the predicted probability p(y|x) is smaller the 0.5. This is\nthe reason why the CCE has a faster learning speed than MAE. However, this\nweighting scheme of CCE is desirable for training with clean data. When noisy\nexamples are included in the training dataset, their predicted probability is usu-ally lower than 0.5 at the beginning of training process [ 18] (See the Fig. 1). In\nthis case, the CCE more easily overﬁts the noisy sample than MAE. Conversely,\nthe MAE places small weights on the low prediction probability samples thatare more likely to be noisy samples [ 16,18]. This trait makes it more robust to\nnoisy labels, but it also slows the learning speed on the clean training examples\nand lead to a signiﬁcantly longer training time before convergence. Furthermore,from the Eq. 2and3, the MAE tend to place more weight than CCE on examples\nin which the predicted probability is larger than 0.5. These samples are more\nlikely to be clean at the later training stage. In this case, the training loss willﬂuctuate, and the classiﬁcation accuracy can be aﬀected.\n2.2 Improved Categorical Cross Entropy\nTo exploit the beneﬁts of both the noise robustness provided by MAE and the\nimplicit weighting scheme of CCE, we propose an improved categorical cross-entropy (ICCE) loss function:\nL\nICCE(f(x),y)=K/summationdisplay\nj=1yj/integraldisplay1\np(j|x)eT(p(j|x)−0.5)\np(j|x)dp(j|x)\n=/integraldisplay1\np(y|x)eT(p(y|x)−0.5)\np(y|x)dp(y|x)(5)', 'Improved Categorical Cross-Entropy Loss for Noisy Labels 81\n0 0.5 102000400060008000epoch=0\n00 . 51050010001500200025003000epoch=20\n00 . 5105001000150020002500epoch=35\n(a) Noise sample\n0 0.5 1050001000015000epoch=0\n0 0.5 100.511.52×104epoch=20\n0 0.5 100.511.522.5×104epoch=35\n(b) Clean sample\nFig. 1. The predicted probability distribution of ResNet56 on CIFAR-10 for clean and\nnoisy samples ( η=0.2). The horizontal axis denotes the predicted probability and the\nvertical axis denotes the frequence.\nwhere yis the one-hot encoded label of the sample xand yjcorresponds to\nthejthelement of y.Tcontrols the exponential base. The ICCE loss combines\nthe weighting advantages of CCE and MAE, which weights the samples with\nnonlinearly:\n∂LCCE(f(x),y)\n∂zj=/braceleftBigg\neT(p(y|x)−0.5)(p(y|x)−1)j=y\neT(p(y|x)−0.5)p(j|x) j/negationslash=y(6)\nIt is obvious that the proposed loss function is equivalent to CCE for T=\n0. Hence, this loss is a generalization of CCE. The ICCE can achieve strong\nnoise robustness and while has faster learning speed than other loss function. To\nmake a clear understand, we provide a detail comparison with other popular loss\nfunctions for j=yas following. For j/negationslash=y, we can draw the same conclusion.\nThe Fig. 2shows the weight comparison of diﬀerent loss functions for j=y.\nIt can be shown that the weight of ICCE with T = 1 or 2 is less than the\nimproved MAE (IMAE) [ 16] and larger than the generalized cross-entropy loss\n(GCE) [ 18]a tp(j|x)<0.5. This means that the ICCE has more noise robustness\nthan IMAE and more learning speed than GCE. Before p(j|x)<0.5, the weight\nchange in MAE and CCE is similar to GCE and IMAE, respectively, which\nindicates that ICCE has an advantage in noise robustness over MAE and CCE.Moreover, the examples in which p(j|x) is larger than 0.5 are more likely to be\nclean. In this case, the ICCE with T<1 has a faster learning speed than GCE\nand CCE but a lower learning speed than IMAE and MAE. We can increaseTto promote the learning speed of ICCE. However, too much Tis more likely', '82 P. Li et al.\n00 . 5100.411.41.8CCE MAE IMAE GCE ICCE(T=1) ICCE(T=2)\nFig. 2. Sample weight along with sample probability being classiﬁed to its labeled class.\nto cause ﬂuctuations, such as IMAE. In this paper, we select the optimal Tby\nconducting several experiments.\n2.3 Theoretical Analysis of ICCE\nIn this section, we present some novel analytical results about ICCE from the\ntheory. First, the sum of ICCE with respect to all classes is proven to be bounded\nby Theorem 5 and Theorem 2. Then, Theorem 3and Theorem 4show the\nerror bounds of the optimal classiﬁer obtained by ICCE on uniform and class-\ndependent noise, respectively.\nTheorem 1. For any p(y|x)∈[/epsilon1,1]andT≥1, theLICCE loss is bounded by:\n0≤LICCE(f(x),y)≤B (7)\nwhere B=( 1−/epsilon1)max(eT(/epsilon1−0.5)\n/epsilon1,e0.5T)(The proof see Appendix).\nBased on the Theorem 1, we will show the LICCE loss with respect to all\nclasses is bounded under condition p(y|x)∈[/epsilon1,1] and T≥1.\nTheorem 2. For any p(y|x)∈[/epsilon1,1]andT≥1, the sum of LICCE loss with\nrespect to all classes is bounded by:\n0≤K/summationdisplay\ni=1LICCE(f(x),i)≤A (8)\nwhere A=KB\nProof. Based on the Theorem 1, we can obtain:\nK/summationdisplay\ni=10≤K/summationdisplay\ni=1LICCE (f(x),i)≤K/summationdisplay\ni=1B=⇒0≤K/summationdisplay\ni=1LICCE (f(x),i)≤KB (9)\n/intersectionsq/unionsq', 'Improved Categorical Cross-Entropy Loss for Noisy Labels 83\nRemark 1. Theorem 2shows that the sum of LICCE with all class is bound\nunder a weak condition, p(y|x)∈[/epsilon1,1] and T≥1. Since Tis a constant that\ncontrol the exponential base, it is easy to set T≥1. In addition, although\np(y|x)∈[0,1] in theory, but we observe that the p(y|x) is not equal 0 since the\noutput of softmax layer is large than 0. In a word, our restrictive condition isreasonable in real application.\nNext, we will show that the optimal classiﬁer obtained by L\nICCE through risk\nminimization has the error bound under the uniform noise and class dependentnoise.\nTheorem 3. Using the bound in Theorem 2and under uniform noise with η≤\n1−\n1\nK, we can show\n−ηKB\nK−1≤Rη\nLICCE(f∗\nη)−RLICCE(f∗)≤B (10)\nwhere f∗\nηis the global optimal classiﬁer on the Dη,a n d f∗is the global optimal\nclassiﬁer on D.RLICCE(·)is the risk function of LICCE .\nProof. Recall that for any classiﬁer f,t h er i s ko f fis deﬁned as\nRLICCE(f)= ED[LICCE (f(x),y)] = Ex,y[LICCE (f(x),y)]\nwhere Edenotes expectation. And since for uniform noise with noise rate η,w e\nhave\nRη\nLICCE(f)= EDη[LICCE (f(x),ˆy)] = Ex,ˆy[LICCE (f(x),ˆy)]\n= ExEy|xEˆy|y,x[LICCE (f(x),ˆy)]\n= ExEy|x[(1−η)LICCE (f(x),y)+η\nK−1/summationdisplay\ni/negationslash=yLICCE (f(x),i)]\n= ExEy|x[(1−η)LICCE (f(x),y)+η\nK−1(K/summationdisplay\ni=1LICCE (f(x),i)\n−LICCE (f(x),y))]\n=( 1−η)RLICCE(f)−η\nK−1RLICCE(f)+η\nK−1ExEy|x[K/summationdisplay\ni=1\nLICCE (f(x),i)]\n=( 1−ηK\nK−1)RLICCE(f)+η\nK−1ExEy|x[K/summationdisplay\ni=1LICCE (f(x),i)]\nNow, from Theorem 2,w eh a v e\n(1−ηK\nK−1)RLICCE(f)≤Rη\nLICCE(f)≤\n(1−ηK\nK−1)RLICCE(f)+η\nK−1A\nThus, for f∗\nηandf∗are the global optimal classiﬁer on the DηandD, respec-\ntively.\nRη\nLICCE(f∗\nη)−RLICCE(f∗)≤\n(1−ηK\nK−1)RLICCE(f∗\nη)+ηA\nK−1−RLICCE(f∗)\n≤(1−ηK\nK−1)(RLICCE(f∗\nη)−RLICCE(f∗)) +ηA\nK−1\n≤B', '84 P. Li et al.\nbecause η≤1−1\nKandRLICCE(f∗\nη)−RLICCE(f∗)<B. This proves Rη\nLICCE(f∗\nη)−\nRLICCE(f∗) has the up-bound.\nRη\nLICCE(f∗\nη)−RLICCE(ˆf)≥(1−ηK\nK−1)RLICCE(f∗\nη)−RLICCE(ˆf)\n=( 1−ηK\nK−1)(RLICCE(f∗\nη)−RLICCE(f∗))−ηK\nK−1RLICCE(f∗)\n≥−ηKB\nK−1\nbecause η≤1−1\nKandRLICCE(f∗\nη)<B. This proves Rη\nLICCE(f∗\nη)−RLICCE(f∗)\nhas the bottom-bound. This completes the proof. /intersectionsq/unionsq\nRemark 2. Theorem 3shows that the optimal classiﬁer obtained by LICCE has\nthe error bound when the uniform noise rate is small the 1 −1\nK.T h i sd o e sn o t\ndepend on the data distribution. This theorem generalizes the existing results\nTheorem 1 in [ 7] and [ 18].\nTheorem 4. Using the bound in Theorem 3and under class dependent noise\nwithηij≤1−ηi,∀j/negationslash=i,∀i,j∈[K],w h e r e ηij=p(ˆy=j|y=i),∀j/negationslash=i,a n d\nηi=/summationtext\nj/negationslash=iηij, then\n−AED(2−ηy)−B≤Rη\nLICCE(f∗\nη)−RLICCE(f∗)\n≤AED(1−ηy)+(K−1)B(11)\nwhere f∗\nηis the global optimal classiﬁer on the Dη,a n d f∗is the global optimal\nclassiﬁer on D.\nProof. From the Theorem 3,w eh a v e\n−K/summationdisplay\ni/negationslash=yLICCE (f(x),i)≤LICCE (f(x),y)≤A−K/summationdisplay\ni/negationslash=yLICCE (f(x),i)\nUnder class dependent noise, for any f\nRη\nLICCE(f)= ED[(1−ηy)LICCE (f(x),y)] + ED[K/summationdisplay\ni/negationslash=yηy,iL(f(x),i)]\n≤ ED[(1−ηy)(A−K/summationdisplay\ni/negationslash=yL(f(x),i))] + ED[K/summationdisplay\ni/negationslash=yηy,iL(f(x),i)]\n=AED(1−ηy)− ED[K/summationdisplay\ni/negationslash=y(1−ηy−ηy,i)L(f(x),i)]\nand\nRη\nLICCE(f)= ED[(1−ηy)LICCE (f(x),y)] + ED[K/summationdisplay\ni/negationslash=yηy,iL(f(x),i)]\n≥ ED[(1−ηy)(−K/summationdisplay\ni/negationslash=yL(f(x),i))] + ED[K/summationdisplay\ni/negationslash=yηy,iL(f(x),i)]\n=−ED[K/summationdisplay\ni/negationslash=y(1−ηy−ηy,i)L(f(x),i)]', 'Improved Categorical Cross-Entropy Loss for Noisy Labels 85\nThus, for f∗\nηandf∗are the global optimal classiﬁer on the DηandD, respec-\ntively.\nRη\nLICCE(f∗\nη)−R(f∗)≤\nAED(1−ηy)− ED[K/summationdisplay\ni/negationslash=y(1−ηy−ηy,i)L(f∗\nη(x),i)]− ED[LICCE (f∗(x),y)]\n≤AED(1−ηy)− ED[K/summationdisplay\ni/negationslash=y(1−ηy−ηy,i)LICCE (f∗\nη(x),i)\n+K/summationdisplay\ni/negationslash=yLICCE (f∗(x),i)]≤AED(1−ηy)\nbecause ηij≤1−ηiandLICCE(f)≥0. This proves Rη\nLICCE(f∗\nη)−RLICCE(f∗)\nhas the up-bound.\nRη\nLICCE(f∗\nη)−R(f∗)\n≥− ED[K/summationdisplay\ni/negationslash=y(1−ηy−ηy,i)LICCE (f∗\nη(x),i)− ED[LICCE (f∗(x),y)]\n≥−A− ED[K/summationdisplay\ni/negationslash=y(1−ηy−ηy,i)LICCE (f∗\nη(x),i)−K/summationdisplay\ni/negationslash=yLICCE (f∗(x),i)]\n≥−A− ED[K/summationdisplay\ni/negationslash=y(1−ηy−ηy,i)(LICCE (f∗\nη(x),i)−LICCE (f∗(x),i))]\n≥−A−BED[K/summationdisplay\ni/negationslash=y(1−ηy−ηy,i)]\n=−AED(2−ηy)−B\nbecause ηij≤1−ηiandLICCE(f∗\nη)−LICCE(f∗)<B. This proves Rη\nLICCE(f∗\nη)−\nRLICCE(f∗) has the bottom-bound. /intersectionsq/unionsq\nRemark 3. Theorem 4establishes an error bound of the optimal classiﬁer trained\non class dependent noise training dataset with LICCE. The condition only needs\nηij≤1−ηi.\n3 Experiment\n3.1 Dataset and Model Architectures\nThe CIFAR-10 and CIFAR-100 [ 10] datasets are used in our experiments to\nverify the performance of ICCE. CIFAR-10 and CIFAR-100 [ 10] are two image\nclassiﬁcation image datasets that contain 10 and 100 classes, respectively. Both\nof them have approximately 50k images for training and 10k images for testing.The image size is 32 ×32.\nIn this paper, we use two DNN models, ResNet44 and ResNet56, which are\ngiven by the Keras oﬃce. All hyperparameter settings are kept ﬁxed for allexperiments using these datasets. Following previous works, we used stochastic\ngradient descent (SGD) with Ada and an initial learning rate of 0.0001. The\nlearning rate divides 10 after 100 epochs for all datasets.', '86 P. Li et al.\n3.2 Label Noise\nWe synthesize noisy data from clean data by stochastically changing some of the\nlabels. For uniform noise, the label of each image is replaced by one of the otherclass labels with a probability of η. Following [ 13,18], class-dependent noise is\ngenerated by ﬂipping each class into the next circularly with probability η.\n3.3 Evaluation of the CIF AR Dataset with Synthetic Noise\nWe validate our method through a series of experiments conducted on the CIFAR-\n10 and CIFAR-100 datasets. Two other state-of-the-art approaches are reimple-\nmented in this paper. One is an improved mean absolute error (IMAE) [ 16], which\nexploits the potential performance of MAE. The other is the generalized cross-\nentropy loss (GCE) [ 18], which is an improvement of cross-entropy loss. We list\nall the experimental results in Table 1and Table 2. The quantitative results on\nTable 1. Average test accuracy on CIFAR-10. We report accuracies when the model\nachieves convergence. T= 4 is used for all experiments with ICCE loss. The best\naccuracy is boldfaced.\nNetwork Lossfunction η=0.0Uniform noise Class dependent noise\nη=0.2η=0.3η=0.4η=0.2η=0.3η=0.4\nResNet44 CCE 0.8635 0.6737 0.5823 0.5116 0.6835 0.6013 0.5068\nMAE 0.7604 0.7673 0.5963 0.5923 0.5587 0.5048 0.4320\nIMAE 0.8409 0.7615 0.6940 0.5791 0.7459 0.6084 0.5216\nGCE 0.8588 0.8066 0.7824 0.7366 0.7476 0.6381 0.5090\nICCE 0.8624 0.8213 0.7954 0.7543 0.7796 0.6634 0.5304\nResNet56 CCE 0.8551 0.6697 0.5900 0.5830 0.6896 0.6027 0.5151\nMAE 0.7619 0.6688 0.4099 0.2470 0.4827 0.3784 0.3916\nIMAE 0.8529 0.7766 0.6869 0.6464 0.7542 0.6113 0.5169\nGCE 0.8530 0.8131 0.7848 0.7512 0.7454 0.6465 0.5105\nICCE 0.8616 0.8210 0.7919 0.7662 0.7737 0.6690 0.5217\nCIFAR-10 are summarized in Table 1.T h e Tis set to 4 for ICCE in the fol-\nlowing experiments. We have used uniform noise and class-dependent noise withη=0.0,0.2,0.3,0.4. We observe that ICCE is superior to the state-of-the-art (i.e.,\nthe bold values in Table 1). When η=0.0, the ICCE only has a slight advantage\nover the other methods. When the noise rate increases from 0.0 to 0.4, the accu-racy of all methods drops with the noise rate. However, in all cases, the decrease\nin accuracy with ICCE is much less than other methods, especially for MAE. This\nis because the MAE has a very slow learning speed and easily falls into the localoptimal solution.', 'Improved Categorical Cross-Entropy Loss for Noisy Labels 87\nIn Fig. 3, we show the test accuracies of ResNet56 with the number of training\nepochs on the CIFAR-10 dataset. We show the results for uniform noise andclass-dependent noise under serious level. We can see that the ICCE loss is\nhighly robust to uniform and class-dependent noise and the accuracy achieved\nwith ICCE loss is higher than other methods. Speciﬁcally, as training goes,CCE always tries to ﬁt the noisy training dataset, which shows that CCE learns\nconsiderable error information when severe noise exists. MAE has a very low\nlearning speed, and the ﬁnal accuracy is not compared with ICCE. In regard tothe IMAE, we ﬁnd that it has a fast learning speed. However, it is also easy to\noverﬁt the noisy example, which leads to a negative eﬀect on model performance.\nThe GCE shows better performance than other methods, but the best accuracyis smaller than that of ICCE regardless of the neural network and noise rate.\nEspecially for η=0.4, the gap between the ICCE and GCE is signiﬁcantly\nlarger than other noise rates. This demonstrates that the ICCE has strong noise\nrobustness on the training dataset that suﬀered serious noise. Besides the higher\ntest accuracies, we can ﬁnd that ICCE is the earliest convergence compared withother methods in most cases.\n0 200 400 600 700Iterations(104)0.10.20.30.40.50.60.70.8Test AccuracyUniform Noise=0.4\nCCE\nMAE\nIMAE\nGCE\nICCE\n0 200 400 60000.20.40.60.81Test AccuracyClass Dependent Noise=0.2\nCCE\nMAE\nIMAE\nGCE\nICCE\n0 200 400 6000.10.20.30.40.50.60.70.8Test AccuracyClass Dependent Noise=0.3\nCCE\nMAE\nIMAE\nGCE\nICCE\nFig. 3. The test accuracy of ResNet56 against training iterations on CIFAR-10 with\nnoise.\nThe quantitative results on CIFAR-100 are summarized in Table 2.C I F A R -\n100 contains more class examples than CIFAR-10, and it is a more challengingdataset. Therefore, the overall test accuracies on CIFAR-100 are not compared\nwith CIFAR-10. It is clear that the ICCE shows the best accuracy compared\nto the other methods. Although the MAE has strong noise robustness in the-\nory, it shows worse performance in the challenge dataset. The IMAE and GCE\nindeed improve the noise robustness for DNNs. However, there are still somegaps between ICCE and IMAE and GCE, especially for larger noise level. These\nexperimental results demonstrate that ICCE has the ability to obtain excellent\nnoise robustness performance in the challenge classiﬁcation dataset.', '88 P. Li et al.\nTable 2. Average test accuracy on CIFAR-100. We report accuracies when model\nachieves convergence. T= 4 was used for all experiments with ICCE. The best accuracy\nis boldfaced.\nNetwork Loss\nfunctionη=0.0Uniform noise Class dependent noise\nη=0.2η=0.3η=0.4η=0.2η=0.3η=0.4\nResNet44 CCE 0.6891 0.6015 0.5433 0.4542 0.6226 0.5945 0.5529\nMAE 0.3086 0.2722 0.2521 0.2507 0.2446 0.2420 0.2417\nIMAE 0.6455 0.5876 0.5819 0.5337 0.6076 0.5427 0.4831\nGCE 0.6643 0.5731 0.5496 0.4349 0.5591 0.5552 0.5019\nICCE 0.6878 0.6177 0.6053 0.5540 0.6296 0.6093 0.5540\nResNet56 CCE 0.6880 0.5677 0.5191 0.5026 0.6254 0.5857 0.5309\nMAE 0.2945 0.2830 0.3053 0.1831 0.2842 0.2351 0.2117\nIMAE 0.6631 0.6076 0.5924 0.5563 0.6137 0.5694 0.5012\nGCE 0.6646 0.5295 0.4938 0.4757 0.5539 0.5217 0.4918\nICCE 0.6913 0.6116 0.5947 0.5569 0.6323 0.5951 0.5438\n4 Conclusion\nIn this work, we ﬁrst present a thorough robust analysis of MAE and CCE tech-\nnically and empirically. Then, we propose an eﬀective and simple noise-robust\nloss function, ICCE, to improve the robustness of CCE. The ICCE is useful\nbecause it can encourage the DNNs to learn the true label distribution evenwhen the training set is high-level noise. In addition, the ICCE only involves\nminimal intervention to existing DNNs without any change in the network archi-\ntecture. Furthermore, we present extensive theoretical analysis to illustrate therobustness of ICCE.\nReferences\n1. Algan, G., Ulusoy, I.: Image classiﬁcation with deep learning in the presence of\nnoisy labels: a survey. arXiv preprint arXiv:1912.05170 (2019)\n2. Arpit, D., et al.: A closer look at memorization in deep networks. arXiv: Machine\nLearning (2017)\n3. Azadi, S., Feng, J., Jegelka, S., Darrell, T.: Auxiliary image regularization for deep\nCNNs with noisy labels. arXiv: Computer Vision and Pattern Recognition (2015)\n4. Ding, G., Guo, Y., Chen, K., Chu, C., Han, J., Dai, Q.: DECODE: deep conﬁdence\nnetwork for robust image classiﬁcation. IEEE Trans. Image Process. 28(8), 3752–\n3765 (2019)\n5. Frenay, B., Verleysen, M.: Classiﬁcation in the presence of label noise: a survey.\nIEEE Trans. Neural Networks 25(5), 845–869 (2014)\n6. Gevaert, C.M., Persello, C., Elberink, S.O., Vosselman, G., Sliuzas, R.: Context-\nbased ﬁltering of noisy labels for automatic basemap updating from UAV data.\nIEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. 11(8), 2731–2741 (2017)', 'Improved Categorical Cross-Entropy Loss for Noisy Labels 89\n7. Ghosh, A., Kumar, H., Sastry, P.: Robust loss functions under label noise for deep\nneural networks. In: Thirty-First AAAI Conference on Artiﬁcial Intelligence, pp.\n1919–1925\n8. Jiang, L., Zhou, Z., Leung, T., Li, L., Fei-Fei, L.M.: Regularizing very deep neural\nnetworks on corrupted labels. ICML (2018)\n9. Karimi, D., Dou, H., Warﬁeld, S.K., Gholipour, A.: Deep learning with noisy labels:\nexploring techniques and remedies in medical image analysis. Med. Image Anal.\n65, 101759 (2020)\n10. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny\nimages (2009)\n11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classiﬁcation with deep con-\nvolutional neural networks, pp. 1097–1105 (2012)\n12. Manwani, N., Sastry, P.: Noise tolerance under risk minimization. IEEE Trans.\nCybernet. 43(3), 1146–1151 (2013)\n13. Patrini, G., Rozza, A., Krishna Menon, A., Nock, R., Qu, L.: Making deep neural\nnetworks robust to label noise: a loss correction approach. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pp. 1944–1952(2017)\n14. Pelletier, C., Valero, S., Inglada, J., Dedieu, G., Champion, N.: Filtering mislabeled\ndata for improving time series classiﬁcation. In: 2017 9th International Workshopon the Analysis of Multitemporal Remote Sensing Images (MultiTemp), pp. 1–4.\nIEEE (2017)\n15. Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., Rabinovich, A.: Train-\ning deep neural networks on noisy labels with bootstrapping. arXiv preprint\narXiv:1412.6596 (2014)\n16. Wang, X., Hua, Y., Kodirov, E., Robertson, N.M.: IMAE for noise-robust learning:\nmean absolute error does not treat examples equally and gradient magnitude’s\nvariance matters (2019)\n17. Yuan, J.: Learning building extraction in aerial scenes with convolutional networks.\nIEEE Trans. Pattern Anal. Mach. Intell. 40(11), 2793–2798 (2018)\n18. Zhang, Z., Sabuncu, M.: Generalized cross entropy loss for training deep neural\nnetworks with noisy labels. In: Bengio, S., Wallach, H., Larochelle, H., Grauman,K., Cesa-Bianchi, N., Garnett, R. (eds.) Advances in Neural Information Processing\nSystems, vol. 31, pp. 8778–8788. Curran Associates, Inc. (2018)', 'A Residual Correction Approach for\nSemi-supervised Semantic Segmentation\nHaoliang Li1,2,3and Huicheng Zheng1,2,3(B)\n1School of Computer Science and Engineering, Sun Yat-sen University,\nGuangzhou, China\nzhenghch@mail.sysu.edu.cn\n2Key Laboratory of Machine Intelligence and Advanced Computing,\nMinistry of Education, Guangzhou, China\n3Guangdong Key Laboratory of Information Security Technology,\nGuangzhou, China\nAbstract. Fully-supervised deep learning models have achieved a great\nsuccess in complex semantic segmentation tasks. However, the segmen-\ntation annotations are prohibitively expensive, which causes a grow-ing interest in the methods that require lower annotating cost but still\nachieve a competitive performance. This paper proposes a residual cor-\nrection approach based on self-training for semi-supervised semantic seg-mentation. We train a residual correction network built on top of the seg-\nmentation network with labeled data to predict a residual of the original\nsegmentation. For unlabeled data, the output of the residual correctionnetwork is combined with the original segmentation to form the pseudo\nlabel used to train the segmentation network. Extensive experimental\nresults on the PASCAL VOC 2012 and the Cityscapes datasets demon-strate the eﬀectiveness of the proposed approach.\nKeywords: Semantic segmentation\n·Semi-supervised learning ·\nSelf-training\n1 Introduction\nWith the emergence of deep learning, most computer vision researches have\nbeen turned towards Convolutional Neural Networks (CNNs) [ 13] from tradi-\ntional machine learning algorithms. Current state-of-the-art deep learning mod-\nels based on CNNs achieve a great success in the domains of computer vision\nwith the availability of huge amount of annotated data. However, the acqui-sition of annotated data is often expensive and time-consuming, especially for\nsegmentation asking for pixel-level annotation.\nIn the task of semantic segmentation, semi-supervised learning (SSL) and\nweakly-supervised learning (WSL) has been extensively explored to reduce the\ncost of labeling [ 14,17,22–24,28]. Compared with fully-supervised learning using\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 90–102, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_8', 'A Residual Correction Approach for Semi-supervised Semantic Segmentation 91\npixel-level annotation, semi-supervised semantic segmentation leverages unla-\nbeled data to enhance the performance, while weakly-supervised semantic seg-mentation exploits weakly-annotated samples. The leading progresses consist\nof consistency regularization [ 14,23,24], adversarial training [ 11,25,27], self-\ntraining [ 16,31], and so on. The objective of consistency regularization is to\nenforce the model’s predictions to be invariant when small perturbations are\napplied to the inputs. Methods based on adversarial training extend the gen-\nerator of the generic Generative Adversarial Networks (GANs) [ 9] framework\nto segmentation network and enforce the discriminator to discriminate ground\ntruths and predictions with an adversarial loss so as to adapt for SSL setting.\nThe principle of self-training is to generate pseudo pixel-level labels for unlabeledor weakly-labeled data, which are used to train model in a supervised man-\nner, together with the original strongly-labeled data. Self-training framework is\ngenerally more generic and simpler in contrast to hand-crafted perturbations\nof consistency regularization and the complex training process of adversarial\ntraining.\nWe propose a residual correction approach that trains semantic segmentation\nmodels in a self-training framework. The key to self-training schemes is how to\ngenerate pseudo labels. [ 11,22] picked pseudo labels according to the conﬁdence\nof discriminator classifying the given segmentation as either real or fake, while\ntheir diﬀerence is discriminating at image-level [ 22] or pixel-level [ 11]. Moreover,\n[21] introduced a correction network to transform the given image-segmentation\npair to a correction map of N+ 1 classes, including Noriginal semantic classes\nand one additional class indicating whether the input segmentation matches the\nground truth. Then the original segmentations of unlabeled data is corrected bythe correction map and serve as pseudo labels. In contrast, we extend the seg-\nmentation network with an additional residual correction network which fuses\nthe image and its segmentation more eﬃciently and outputs a residual insteadof matching regions in [ 21]. It leads to an adaptive weighting scheme to solve the\nproblem of class imbalances in [ 21]. Two networks of our approach work collab-\noratively to generate pseudo labels on unlabeled images. Main contributions of\nthis paper are summarized as follows:\n– We design a residual correction network, which fuses the information of the\ninput image and its original segmentation eﬃciently and capture complemen-\ntary knowledge in residual form.\n– We propose a training schedule for semi-supervised semantic segmentation,\nwhere a segmentation network and a residual correction network are trained\njointly to perform a self-training scheme.\n– We carry out extensive and detailed experiments to demonstrate the eﬀective-\nness of our approach, achieving state-of-the-art performance on the PASCALVOC 2012 and Cityscapes in semi-supervised settings.', '92 H. Li and H. Zheng\n2 Related Work\n2.1 Supervised Semantic Segmentation\nWithin the scope of fully supervised segmentation, many excellent works based\non CNNs emerged. Long et al. proposed the pioneering Fully Convolutional\nNetwork (FCN) [ 20] which has been the most widespread deep neural network\nfor semantic segmentation. To extract long-range information, many encoder-\ndecoder based models using diﬀerent information-passing mechanism have been\nproposed. SegNet [ 1] records max-pooling indices in encoder and reuses them in\nthe decoding process. U-Net [ 26] introduces skip-connections between encoder\nand decoder network. ReﬁneNet [ 18] is a variant of U-Net, which introduces\nmultipath reﬁnement in the decoder. With reference of spatial pyramid pooling[15] used to capture contextual information, PSPNet [ 30] uses multiple pooling\nkernels to extract features of various scales. To increase the receptive ﬁeld of\nthe network without the lossy down-sampling, atrous convolution [ 29] was put\nforward. The series of DeepLab must be mentioned for semantic segmentation.\nAmong which, DeepLabv2 [ 3] captures multi-scale information with atrous spa-\ntial pyramid pooling (ASPP), and DeepLabV3+ [ 4] extends the previous design\nwith an eﬀective decoder and applies the depthwise separable convolution to\nfurther improve the performance.\n2.2 Semi-supervised Semantic Segmentation\nSemi-supervised learning leverages unlabeled data to reduce the annotation cost\nbut keep a competitive performance. Recently semi-supervised learning has beenapplied to semantic segmentation successfully.\nMethods based on consistency regularization are aimed to enforce an invari-\nance of the output of the model over some perturbations applied to the inputs.The eﬀectiveness of consistency regularization depends on the cluster assump-\ntion that diﬀerent classes are separated by low density regions. Thus, the deci-\nsion boundary should lie in low-density regions to improve generalization per-formance. CutMix [ 8] enforce a consistency between the mixed outputs and the\npredictions over the mixed inputs. MixMatch [ 2], incorporating ideas from the\ndominant paradigms for semi-supervised learning, guesses low-entropy labels for\ndata-augmented unlabeled examples. Observing that the low-density regions sep-\narating the classes are more apparent within the outputs of encoder than withinthe inputs, CCT [ 24] enforced the consistency over diﬀerent forms of perturba-\ntions applied to the encoder’s output.\nFor semi-supervised learning, pseudo labeling is a simple and eﬀective method\napplied widely [ 16]. The paradigm of pseudo labeling is to use a small set of\nlabeled data to generate pseudo labels for a large quantity of unlabeled data, thus\nreducing human labeling eﬀort. Self-training proposed in [ 31] performed pseudo\nlabeling with a teacher-student framework where a teacher model is trained\nwith labeled samples and then generate pseudo labels for unlabeled images. As\nthe teacher-student framework is widely studied in the literature of distillation,', 'A Residual Correction Approach for Semi-supervised Semantic Segmentation 93\n[19] considered that dense prediction is a structured prediction problem and\ntransferred the structure information from large networks to compact ones fordense prediction tasks.\nAdversarial learning facilitates eﬀective semi-supervised learning on various\ntasks. The architecture of GANs consists of two sub-networks, a generator and adiscriminator. During training, the generator tries to trick discriminator, while\ndiscriminator is committed to distinguishing between the real and those gen-\nerated by generator. Thus, these two sub-networks play a min-max game inthe training process. Souly et al. [ 27] applied adversarial learning to semantic\nsegmentation by keeping a generator to produce artiﬁcial samples but using a\nsegmentation network as a discriminator which classiﬁes each pixel as either gen-erated or the true class it belongs to. Qi et al. [ 25] adopted a novel Knowledge\nEmbedded Generative Adversarial Networks (KE-GANs) to capture semantic\nconsistency with a knowledge graph. Diﬀerent from those generators trained to\ngenerate images, Hung et al. [ 11] used a segmentation network as the genera-\ntor and a fully convolutional discriminator to diﬀerentiate the predictions fromground truths.\nKalluri et al. [ 12] proposed a universal approach to learn from labeled and\nunlabeled data across domains, which extends the segmentation network withan entropy module using the unlabeled examples to perform alignment of pixel\nwise features from multiple domains. Mittal et al. [ 22] proposed a dual-branch\napproach to combine semi-supervised classiﬁcation with semi-supervised segmen-tation, which consistently reduces both the low-level and the high-level artifacts.\n3M e t h o d\nThe segmentation network trained on limited data suﬀers from several types\nof failure, such as wrong object shapes, incoherent boundaries, and inaccurate\nsemantic class, motivating our residual correction approach. Given an image-segmentation pair, some low-level information in the relation between the image\nand its coarse segmentation can be leveraged to make reﬁnement. Our residual\ncorrection network is designed to capture this type of information.\nThe overall architecture is shown in Fig. 1.L e t D\nl={(xl\n1,yl\n1),...,(xl\nn,yl\nn)}\ndenotes a small set of nlabeled training examples and Du={xu\n1,...,xu\nm}denotes\na large set of munlabeled training examples. Our method contains two networks,\na segmentation network Sand a residual correction network C. The segmentation\nnetwork is optimized with labeled data to generate the original segmentation\nS(x), which can be any standard segmentation network for generating per-pixel\nclass prediction given an input image. The residual correction network explores\nthe image in combination with its original segmentation to generate a residualC(x,S(x)). During inference, the corrected map is treated as the ﬁnal prediction.', '94 H. Li and H. Zheng\nFig. 1. Illustration of our residual correction approach. For the supervised training, the\nimages xlof labeled examples ( xl,yl) are passed through the segmentation network S\nto obtain S(xl). Then xlcombined with S(xl) are input to the residual correction\nnetwork Cto output the residual. We compute the cross-entropy Lcebetween yland\nS(xl), together with the cross-entropy Lcorbetween yland the corrected prediction.\nFor unlabeled examples xu, the segmentation network is supervised by the pseudo-label\nypgenerated in the residual correction process, which corresponds to Lsemi.\n3.1 Residual Correction Network\nThe objective of the correction training is to model the relationship between the\ninput image and its original segmentation and then infer the residual, which is\ntotally diﬀerent from that of the original training for segmentation.\nThe key to the correction network is how to fuse the information of the image\nand its corresponding segmentation. Existing approaches [ 21,22] mainly concate-\nnate them directly and feed to following convolutional layers. However, there aretwo issues with such concatenation. First, as the input image and its original\nsegmentation belong to diﬀerent semantic levels, it is harmful that the details of\nthe segmentation map are lost along with those of the input images during theprocess of feature extraction. Second, most of encoder-decoder-based segmenta-\ntion networks initialize their encoder from a classiﬁcation model pretrained on\nlarge datasets (e.g., ImageNet [ 6]) to transfer knowledge from those datasets,\nwhich is no longer feasible in this case. To overcome these shortcomings, we take\na diﬀerent fusion approach by encoding the image and its original segmentationseparately. The correction network is trained with the labeled data and then\ntreated as a teacher to generate pseudo labels for unlabeled images.\nA ss h o w ni nF i g . 1, we extend an existing encoder-decoder segmentation\nmodel with ancillary encoders that embed the original segmentation information\nat diﬀerent scales. The image is inputted to the backbone to extract features,\nand its corresponding segmentation is inputted into multiple ancillary encoderswhere the input tensor is resized to the target shape and passed through several\nconvolution layers to generate an attention map. The features extracted from the\nbackbone are element-wise multiplied by the attention map to generate weighted', 'A Residual Correction Approach for Semi-supervised Semantic Segmentation 95\nfeatures that are fed to the following decoder. The fusion process is shown in\nFig.1where the features at two scales are fused by such an element-wise multi-\nplication parallelly.\nThe residual correction network is aimed to analyze the matching of the\ngiven image-segmentation pair and generate a residual map. Intuitively, settingthe target of the correction network to the ground truth, identical to that of the\nsegmentation network, can also achieve the same goal. However, experiments\nshow that this would lead to no signiﬁcant improvement compared with the seg-mentation network. To deal with this issue, [ 21] introduced an additional class\nindicating the semantically matched regions but led to extremely imbalanced\nlabel distribution. Though a ﬁxed weighting scheme is adopted to balance regularsemantic classes with the additional class, it introduces extra hyper parameters,\nwhich is suboptimal. Motivated by the skip connection introduced in [ 10], we\nconsider this problem from another direction. We adopt a residual architecture\nthat the correction network learns to produce a compensatory residual of the\noriginal segmentation. In this case, it is equal to assign diﬀerent weights to thepixels according to the correctness of the original segmentation. Since the mis-\nclassiﬁed pixels accounting for high loss during training process, the correction\nnetwork is promoted to focus on the misclassiﬁed regions. The residual correctionnetwork is only trained on the labeled data.\n3.2 Supervised Training\nIn standard supervised semantic segmentation training, the cross-entropy loss\nbetween the prediction and the ground truth of the labeled data is minimized.\nFor the labeled training samples, our segmentation network is trained using thecross-entropy loss as usual:\nL\nce=1\nn/summationdisplay\n(xl,yl)∈DlH(S(xl),yl) (1)\nwhere H(.,.) is the cross-entropy function. Additionally, the output of the resid-\nual correction network is added with the original segmentation to ﬁt the groundtruth, which enables the residual learning of the residual correction network:\nL\ncor=1\nn/summationdisplay\n(xl,yl)∈DlH(ˆyl+C(xl,S(xl)),yl) (2)\nFor a labeled training sample, its original segmentation, detached from the seg-\nmentation network to stop backpropagation, is denoted as ˆ yl.\nIn contrast to the concept of adversarial learning where each network\nattempts to confuse the other, our segmentation network and residual correc-tion network are working in a collaborative way. The original segmentation is\nimproving during training and provides more accurate information for the resid-\nual correction network to focus on those misclassiﬁed. The corrected predictionsof unlabeled data can be used as pseudo labels to train segmentation network in\nturn. The original segmentation and the corrected results will converge to about\nthe same, while the latter is generally better.', '96 H. Li and H. Zheng\n3.3 Semi-supervised Training\nThe concept of residual correction learning is to capture the relation between the\ngiven image and its original segmentation and oﬀer a residual to reﬁne the detailsof the original segmentation. In semi-supervised step, the residual correction\nnetwork is considered as a teacher model whose outputs of unlabeled data are\nused as pseudo labels. The output of segmentation network S(x\nu) combined with\nits residual C(xu,S(xu)) is transformed to discrete label representation yp:\nyp\nhwc∗=/braceleftBigg\n1,ifc∗= arg maxc(S(xu)+C(xu,S(xu))) hwc\n0,otherwise(3)\nwhere h,w,c indicate the row, column and channel, respectively. That is to say,\nwe choose the class having maximum predicted probability as pseudo label for\nevery pixel of unlabeled samples. We encourage the segmentation network to\nlearn from the pseudo labels, taking the form of cross-entropy as a self-trainingloss:\nL\nsemi=1\nm/summationdisplay\nxu∈DuH(S(xu),yp) (4)\nThe ﬁnal training objective Lis composed of the supervised and semi-supervised\nterms.\nL=Lce+Lcor+λL semi (5)\nwhere λis a coeﬃcient weighting the self-training loss. During early training,\nthe network is initialized randomly, resulting in inaccurate pseudo labels on\nunlabeled data. [ 14,16,24] adopted ramp-up schedules with diﬀerent increasing\ncurves to overcome this issue. In order to avoid such a hyperparameter adjust-\nment, we adopt the following pretraining strategy instead: 1) Initial training\nof the segmentation model with labeled samples; 2) Initial training of residualcorrection network with labeled samples; 3) The collaborative ﬁne-tuning of the\nwhole framework with labeled and unlabeled samples.\n4 Experiments\n4.1 Network Architecture\nThe segmentation network in our experiments is a DeepLabv3+ [ 4] with a\nResNet50 backbone [ 10]. The residual correction network is also based on a\nResNet50-DeepLabv3+ forming the encoder and decoder of the architecture\ndescribed in Sec. 3.1. The ancillary encoder consists of three 3 ×3 convolu-\ntional layers and a sigmoid activation whose output acts as an attention map.The features of two scales that are fed to the decoder in the original DeepLabv3+\narchitecture, are fused with the output of an individual ancillary encoder. The\nonly diﬀerence of DeepLabv3+ used in segmentation network and residual cor-rection network is the introduction of ancillary encoders.', 'A Residual Correction Approach for Semi-supervised Semantic Segmentation 97\n4.2 Datasets and Evaluation Metrics\nPASCAL VOC 2012. The original PASCAL VOC 2012 [ 7] consists of 1464\ntraining samples, 1449 validation samples and 1456 test samples with 20 fore-\nground classes and a single background class in various resolutions. Following\nprevious work, we augment the original training set with extra 9118 annotated\nsamples from the Segmentation Boundaries Dataset (SBD) [ 9] and obtain a total\ntraining set of 10,582 samples.\nCityscapes. Cityscapes [ 5] is an autonomous driving dataset collected from\ncars driving in 50 cities during diﬀerent seasons. Cityscapes contains 5000 images\ndivided into 2975 training, 500 validation and 1525 testing samples with highquality annotations covering 19 foreground classes for the segmentation.\nEvaluation Metric. The performance of our approach is evaluated with the\ncommonly used mean Intersection-over-Union (mIoU) for all the experiments.\nFor PASCAL VOC 2012 and Cityscapes, we report the results obtained on the\nvalidation set.\n4.3 Experimental Settings\nThe backbones of segmentation network and residual correction network are\ninitialized to the pre-trained ImageNet model. Both networks are trained with\nStochastic Gradient Descent (SGD) with a learning rate of 1 ×10\n−3on PAS-\nCAL VOC 2012 and 2 .5×10−3on Cityscapes. Following [ 21,22,24], we employ\naPoly learning rate policy where the initial learning rate is multiplied by\n1−(iter\nmax iter)powwithpow=0.9. As for data augmentation, we adopt ran-\ndom spatial scaling, horizontal ﬂipping and color jittering during training. ThePASCAL VOC 2012 images are randomly cropped with a size of 321 ×321 with a\nbatch-size of 8. As with Cityscapes, we take crops of size 768 ×768 with a batch-\nsize of 4. We train the whole framework for 35K iterations as in [ 22], before which\nboth networks are initialized with a pretraining for 10K iterations. The weight-\ning coeﬃcient λis set to 1.0. Diﬀerent from DeepLabv3+ [ 4], multi-resolution\nand mirroring are not employed for validation.\n4.4 Results\nAblation Studies. We carry out an ablation study to reveal the contribution of\neach component to the overall performance. We set up a baseline of DeepLabv3+\ntrained with the only 1464 samples extracted from the original training set ofPASCAL VOC 2012, achieving 68.99 shown in Table 1. Further, training the\ntwo networks collaboratively improves the performance signiﬁcantly even with-\nout the semi-supervised objective, which implies that the residual correctionnetwork does capture the complementary information to reﬁne the original seg-\nmentation. The result of using the pretraining strategy described in Sec. 3.3out-\nperforms that of training from scratch while both of them surpass the baseline', '98 H. Li and H. Zheng\nsigniﬁcantly. The simple pretraining strategy avoids complex hyper parameter\nsearch on λby setting semi-supervised training to a reasonable starting point\ninstead of a noisy initialization. In the same architecture, our fusion way yields\nan improvement of 2.09% (from 73.04% to 75.13%) over that using concatena-\ntion, which demonstrates the eﬀectiveness of our fusion design. The result ofcomplete form of our semi-supervised method is very close to that of training\nDeepLabv3+ with all available labeled data. Qualitative results generated by the\nproposed method are shown in Fig. 2. It is observed that the failure cases of the\noriginal segmentation are corrected with correction training and semi-supervised\nobjective. The coherence of boundary and the correctness of classiﬁcation are\nimproved signiﬁcantly when compared to the baseline.\nTable 1. Ablation study of our method on PASCAL VOC 2012. ES: Our fusion way\nof encoding separately, while the default is concatenation as in [ 21,22]. PS: Training\nwith pretraining strategy, while the default is training from scratch.\nMethod ESPSLabeled examples Unlabeled examples mIoU\nDeepLabv3+ --1464 0 68.99\nOurs+ Lcor --1464 0 72.81\nOurs+ Lcor+Lsemi /check ×1464 9118 73.55\nOurs+ Lcor+Lsemi ×/check1464 9118 73.04\nOurs+ Lcor+Lsemi /check/check1464 9118 75.13\nDeepLabv3+ --10582 0 75.37\n(a) images (b) annotations (c) baseline (d) +Lcor\n (e) +Lcor +Lsemi\nFig. 2. Qualitative results of our approach on the PASCAL VOC 2012 dataset with\nthe original training set as labeled data and the SBD set as unlabeled data.', 'A Residual Correction Approach for Semi-supervised Semantic Segmentation 99\nComparison with Existing Methods. To set the upper bound for our semi-\nsupervised results, we evaluate our implementation of DeepLabv3+ in fully-supervised case where all available labeled data are used. As shown in Table 2,\nwe achieve 75.37 mIoU on PASCAL VOC 2012, which is lower than 78.85 of the\nResNet101-DeepLabv3+ reported in [ 4] for the reason of a weaker backbone and\nlower resolution. Similarly, we get 74.45 on Cityscapes while 78.79 is achieved\nin [4] with an Xception style network backbone. We adopt such a backbone\nand resolution to reduce computational cost and focus on our intention of semi-supervised learning.\nTable 2. Comparison with existing methods. The experimental results of our models\ntrained with diﬀerent proportions of training data as labeled samples and the remaining\nas unlabeled samples. Sup: Only use the labeled samples. Semi: Use labeled samples\nand the remaining unlabeled samples\nMethod PASCAL VOC 2012 ( N= 10582) Cityscapes ( N= 2975)\nN/8 N/4 1464 Full N/8 N/4 Full\nSup Semi Sup Semi Sup Semi Sup Sup Semi Sup Semi Sup\nSouly et al. [ 27]- - - - 59.5 64.1 - - - - - -\nHung et al. [ 11]66.0 69.5 68.3 72.1 66.3 68.4 73.6 55.5 58.8 59.9 62.3 66.4\nKE-GAN [ 25] - - - - - - - 62.6 66.1 65.5 69.2 72.6\nCCT [ 24] - - - - - 69.4 - - - - - -\nMittal et al. [ 22]63.5 70.4 - - - - 74.6 56.2 59.3 60.2 61.9 66.0\nECS [ 21] 65.20 70.22 69.78 72.60 - - 76.29 63.12 67.38 67.31 70.70 74.76\nOurs 65.69 71.76 69.54 74.48 68.99 75.13 75.37 62.88 67.40 67.38 71.92 74.45\nThe labeled samples were randomly sampled from the whole dataset as in\n[11,21]. Following [ 21], we adopt a ResNet50-DeepLabv3+ as the segmenta-\ntion network and a modiﬁed ResNet50-DeepLabv3+ as the residual correction\nnetwork introduced in Sec. 3.1. We achieve a result similar to [ 21] in fully-\nsupervised case on Cityscapes. However, we get a relatively lower fully-supervised\nresult on PASCAL VOC 2012 for the reason that we adopt a smaller resolution\n321 ×321 on PASCAL VOC 2012 in contrast to 512 ×512 in [ 21]. Compar-\ning with the published results of [ 21], we achieve a consistent improvement in\ndiﬀerent semi-supervised settings. Additionally, we also present the results of\ntraining on the original training set of PASCAL VOC 2012 (1,464 samples) withthe SBD as unlabeled data to compare with [ 11,24,27]. Though other existing\nsemi-supervised methods use diﬀerent segmentation networks and distinctive\ntraining schedules, the improvement relative to the supervised results indicates\nthe superiority of our method.', '100 H. Li and H. Zheng\n5 Conclusions\nIn this work, we propose a residual correction approach for semi-supervised\nsemantic segmentation, a simple and eﬃcient self-training method leveraging\nunlabeled data. Our framework consists of a segmentation network and a resid-ual correction network that are trained collaboratively. The residual correction\nnetwork is designed to generate a residual of the original segmentation and serves\nas a teacher model on unlabeled data. The eﬀectiveness of our approach is demon-strated in extensive experiments on PASCAL VOC 2012 and Cityscapes.\nAcknowledgments. This work was supported in part by the National Natural Sci-\nence Foundation of China under Grant 61976231, Grant U1611461, Grant 61573387,and Grant 61172141, in part by the Guangdong Basic and Applied Basic Research\nFoundation under Grant 2019A1515011869, and in part by the Science and Technol-\nogy Program of Guangzhou under Grant 201803030029.\nReferences\n1. Badrinarayanan, V., Kendall, A., Cipolla, R.: SEGNET: a deep convolutional\nencoder-decoder architecture for image segmentation. IEEE Trans. Pattern Anal.Mach. Intell. 39(12), 2481–2495 (2017)\n2. Berthelot, D., Carlini, N., Goodfellow, I.J., Papernot, N., Oliver, A., Raﬀel, C.:\nMixmatch: a holistic approach to semi-supervised learning. In: Advances in NeuralInformation Processing Systems, pp. 5050–5060 (2019)\n3. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab:\nSemantic image segmentation with deep convolutional nets, atrous convolution,and fully connected CRFs. IEEE Trans. Pattern Anal. Mach. Intell. 40(4), 834–\n848 (2017)\n4. Chen, L.C., Zhu, Y., Papandreou, G., Schroﬀ, F., Adam, H.: Encoder-decoder with\natrous separable convolution for semantic image segmentation. In: Proceedings of\nthe European Conference on Computer Vision, pp. 801–818 (2018)\n5. Cordts, M., et al.: The cityscapes dataset for semantic urban scene understand-\ning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 3213–3223 (2016)\n6. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: a large-scale\nhierarchical image database. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 248–255 (2009)\n7. Everingham, M., Eslami, S.A., Van Gool, L., Williams, C.K., Winn, J., Zisserman,\nA.: The PASCAL visual object classes challenge: a retrospective. Int. J. Comput.\nVision 111(1), 98–136 (2015)\n8. French, G., Laine, S., Aila, T., Mackiewicz, M., Finlayson, G.: Semi-supervised\nsemantic segmentation needs strong, varied perturbations. In: British Machine\nVision Conference, pp. 1–14 (2020)\n9. Goodfellow, I.J., et al.: Generative adversarial nets. In: Advances in Neural Infor-\nmation Processing Systems, pp. 2672–2680 (2014)\n10. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npp. 770–778 (2016)', 'A Residual Correction Approach for Semi-supervised Semantic Segmentation 101\n11. Hung, W., Tsai, Y., Liou, Y., Lin, Y., Yang, M.: Adversarial learning for semi-\nsupervised semantic segmentation. In: British Machine Vision Conference, pp. 1–14\n(2018)\n12. Kalluri, T., Varma, G., Chandraker, M., Jawahar, C.: Universal semi-supervised\nsemantic segmentation. In: Proceedings of the IEEE International Conference on\nComputer Vision, pp. 5259–5270 (2019)\n13. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-\nvolutional neural networks. In: Advances in Neural Information Processing Sys-\ntems, pp. 1106–1114 (2012)\n14. Laine, S., Aila, T.: Temporal ensembling for semi-supervised learning. In: Interna-\ntional Conference on Learning Representations, pp. 1–13 (2017)\n15. Lazebnik, S., Schmid, C., Ponce, J.: Beyond bags of features: spatial pyramid\nmatching for recognizing natural scene categories. In: Proceedings of the IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition, pp.2169–2178 (2006)\n16. Lee, D.H., et al.: Pseudo-label: the simple and eﬃcient semi-supervised learning\nmethod for deep neural networks. In: Workshop on Challenges in RepresentationLearning, pp. 1–6 (2013)\n17. Lee, J., Kim, E., Lee, S., Lee, J., Yoon, S.: Ficklenet: weakly and semi-supervised\nsemantic image segmentation using stochastic inference. In: Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition, pp. 5267–5276\n(2019)\n18. Lin, G., Milan, A., Shen, C., Reid, I.: Reﬁnenet: multi-path reﬁnement networks\nfor high-resolution semantic segmentation. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 1925–1934 (2017)\n19. Liu, Y., Shu, C., Wang, J., Shen, C.: Structured knowledge distillation for dense\nprediction. IEEE Trans. Pattern Anal. Mach. Intell. 2020, 1–15 (2020). https://\ndoi.org/10.1109/TPAMI.2020.3001940\n20. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 3431–3440 (2015)\n21. Mendel, R., de Souza, L.A., Rauber, D., Papa, J.P., Palm, C.: Semi-supervised seg-\nmentation based on error-correcting supervision. In: Proceedings of the European\nConference on Computer Vision, pp. 141–157 (2020)\n22. Mittal, S., Tatarchenko, M., Brox, T.: Semi-supervised semantic segmentation with\nhigh-and low-level consistency. IEEE Trans. Pattern Anal. Mach. Intell. 43(4),\n1369–1379 (2021)\n23. Miyato, T., Maeda, S.i., Koyama, M., Ishii, S.: Virtual adversarial training: a reg-\nularization method for supervised and semi-supervised learning. IEEE Trans. Pat-\ntern Anal. Mach. Intell. 41(8), 1979–1993 (2018)\n24. Ouali, Y., Hudelot, C., Tami, M.: Semi-supervised semantic segmentation with\ncross-consistency training. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 12674–12684 (2020)\n25. Qi, M., Wang, Y., Qin, J., Li, A.: KE-GAN: Knowledge embedded generative\nadversarial networks for semi-supervised scene parsing. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pp. 5237–5246 (2019)\n26. Ronneberger, O., Fischer, P., Brox, T.: U-net: convolutional networks for biomedi-\ncal image segmentation. In: Proceedings of the International Conference on Medical\nImage Computing and Computer-Assisted Intervention, pp. 234–241 (2015)', '102 H. Li and H. Zheng\n27. Souly, N., Spampinato, C., Shah, M.: Semi supervised semantic segmentation using\ngenerative adversarial network. In: Proceedings of the IEEE International Confer-\nence on Computer Vision, pp. 5688–5696 (2017)\n28. Wang, Y., Zhang, J., Kan, M., Shan, S., Chen, X.: Self-supervised equivariant\nattention mechanism for weakly supervised semantic segmentation. In: Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, pp. 12275–12284 (2020)\n29. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In:\nInternational Conference on Learning Representations, pp. 1–13 (2016)\n30. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pp. 2881–2890 (2017)\n31. Zhu, Y., et al.: Improving semantic segmentation via self-training (2020). arXiv\npreprint: arXiv:2004.14960', 'Hypergraph Convolutional Network with\nHybrid Higher-Order Neighbors\nJiahao Huang1,2, Fangyuan Lei1,2(B), Senhong Wang3, Song Wang4,\nand Qingyun Dai1,2\n1Guangdong Provincial Key Laboratory of Intellectual Property and Big Data,\nGuangdong Polytechnic Normal University, Guangzhou 510665, China\n2School of Electronic and Information, Guangdong Polytechnic Normal University,\nGuangzhou 510665, China\n3School of Information Engineering, Guangdong University of Technology,\nGuangzhou 510006, China\n4Department of Computer Science and Engineering, University of South Carolina,\nColumbia, SC 29208, USA\nAbstract. Hypergraph-based methods can learn non-pairwise associ-\nations more eﬃciently in many real-world datasets. However, existing\nhypergraph-based methods do not consider the relationship of the hybridneighborhood. To address this issue, we propose a hybrid higher-order\nneighborhood based hypergraph convolutional network (HybridHGCN).\nTechnically, feature embeddings are generated via k-hop hypergraph con-volution layers and mixed by the hybrid message operator. To evaluate\nthe proposed HybridHGCN, we conduct experiments on the citation net-\nwork datasets and the visual object datasets. The experimental resultsshow that HybridHGCN brings signiﬁcant improvements over state-of-\nthe-art hypergraph neural network baselines.\nKeywords: Hypergraph\n·Higher-order correlation ·Hypergraph\nconvolutional networks\n1 Introduction\nGraphs are widely used to model the pair-wise relationships in the real world,\nsuch as collaboration networks and co-authoring networks [ 5,18]. An example of\nsuch a graph is shown in Fig. 1(a), in which a1,..., to a7 are the nodes which rep-\nresent the authors, and p1,..., to p4 are the edges which represent the papers con-\nnecting the co-authors. Non-pair-wise and complex relationships among entities\ncan be more eﬀectively and ﬂexibly modeled by a hypergraph [ 6]. An example of\na hypergraph is shown in Fig. 1(b), in which p1,..., to p4 are the hyperedges that\nconnect the multiple co-authors. The number of nodes connected by a hyperedge\nis deﬁned as the degree of the hyperedge. A hypergraph is simpliﬁed to a graphwhen the degree of the hyperedge is set to 2.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 103–114, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_9', '104 J. Huang et al.\nGraph convolutional network (GCN) [ 15] has been applied to citation\nnetworks and knowledge graphs, with signiﬁcant improvement in the semi-supervised node classiﬁcation task. Although GCN can be used for classiﬁca-\ntion tasks in visual data and social networks, it is highly desired to extend it to\nhypergraph structure given its more ﬂexible modeling of complex relationshipsin many real-world networks compared with graphs. Recently, many researchers\nexplored the hypergraph application in GNNs [ 3,4,10,14,25]. Feng et al. [ 10]\nproposed a hypergraph neural network (HGNN) by using hypergraph Laplacianto learn the message of non-pair-wise relationships. Yadati et al. [ 25] simpli-\nﬁed the hypergraph to a graph and then applied the GCN. However, existing\nhypergraph-based neural networks do not consider the k-hop neighbor relations.\n(a) Simple graph (b) Hypergraph (c) K-hop graph\n (d) K-hop hyper-\ngraph\nFig. 1. Illustration of the graph/hypergraph structures, higher-order graph and higher-\norder hypergraph. (a) A simple graph represents the pair-wise relations among authors\nand papers. (b) A hypergraph with 4 hyperedges that represent the non-pair-wiserelations among authors and papers. (c) A k-hop graph where ovals of diﬀerent colors\nrepresent diﬀerent orders. (d) A k-hop hypergraph where hyperedges of diﬀerent colors\nrepresent diﬀerent orders (Color ﬁgure online).\nInspired by [ 1], to consider the k-hop neighbor in a hypergraph, we propose\na hybrid higher-order neighborhood based hypergraph convolutional network(HybridHGCN). To aggregate k-hop neighbor messages, HybridHGCN is com-\nposed of k-hop hypergraph convolutional layers and a hybrid message operator.\nAs the higher-order neighbors of a hypergraph can be described by the inci-\ndence matrix of higher-order power. An example of a k-hop graph is shown in\nFig.1(c), in which ovals of diﬀerent colors represent diﬀerent orders. As illus-\ntrated in Fig. 1(d), a k-hop hypergraph is represented by diﬀerent-order hyper-\nedges, shown in diﬀerent colors. In the incidence matrix of diﬀerent power, the\nreceptive ﬁeld of neighbors is broadened as the order increases. Updated byk-hop hypergraph convolutional layers, the generated features embedding rep-\nresent the k-hop neighbor messages. In the hybrid message operator, we use an\nelement-wise max pooling operator to mix k-hop neighbor messages. To evaluatethe proposed HybridHGCN, we construct three diﬀerent hop hypergraph convo-\nlutional layers and examine the node classiﬁcation performance on both citation\nnetwork datasets and visual object datasets.\nThe main contributions of this paper are summarized as follows.', 'Hypergraph Convolutional Network with Hybrid Higher-Order Neighbors 105\n– We propose HybridHGCN, a new method to capture higher-order and low-\norder neighbor relations and it enhance the representation capability of thehypergraph network.\n– We propose the hypergraph structuration with the higher-order incidence\nmatrix to broaden the receptive ﬁeld of the hypergraph network.\n– The experimental results show the proposed HybridHGCN achieve the new\nstate-of-the-art performance on citation networks and visual object datasets.\nFig. 2. Overview of the proposed hybrid higher-order neighborhood based hypergraph\nconvolutional network (HybridHGCN). The parameters 1,..., to kdenote the hyper-\ngraphs with diﬀerent hops. The two-layer denotes the adopted two hypergraph con-volutional layers. The output feature embedding is composeed of N-nodes with D-\ndimension features. Mixing by a hybrid message operator, the number of nodes and\ndimension of features remain unchanged.\n2 Related Works\n2.1 Graph Neural Networks\nDue to the excellent performance of deep neural networks on structured data\nfrom various tasks, Bronstein et al. [ 7] extended the neural network model to\nthe graph structure data drawn from non-Euclidean space. Kipf et al. [ 15]p r o -\nposed Graph Convolutional Network (GCN) by learning neighboring node rep-\nresentations through the convolution deﬁned by the graph Laplacian. To solvethe problem of GCN’s dependence on the global graph structure, Veliˇ ckovi´c\net al. [ 22] proposed Graph Attention Network (GAT) in which the attention\nmechanism assigns diﬀerent weights to diﬀerent nodes. Abu-El-Haija et al. [ 1]', '106 J. Huang et al.\nproposed the MixHop model to explore neighborhood mixing relationships rep-\nresented by repeatedly mixing feature representations. More details about thecurrent research of graph neural networks and graph representation can be found\nin excellent surveys [ 9,23,29].\n2.2 Learning on Hypergraph\nOn hypergraphs, clique expansion or star expansion [ 2,16,28] are widely used to\nmine the hypergraph structure. Su et al. [ 21] utilized a hypergraph structure to\nrepresent the correlation between diﬀerent objects in a multi-view 3D dataset\nand then proposed a vertex-weighted hypergraph classiﬁcation algorithm. Yu\net al. [ 26] proposed an alternating optimization method to optimize the label\nand hyperedge weights in the hypergraph learning process. Hayashi et al. [ 12]\nemployed random walks based on edge-related vertex weights to construct diﬀer-\nent hypergraph Laplacian matrices, and proposed a ﬂexible hypergraph structuredata clustering framework.\n2.3 Hypergraph Neural Network\nFeng et al. [ 10] proposed Hypergraph Neural Network (HGNN) to model the\nnon-pair-wise relations as the weighted hypergraph. The hypergraph neural net-\nworks was also applied to visual classiﬁcation [ 19]. Jiang et al. [ 13]p r o p o s e d\nthe dynamic hypergraph neural network by extending the dynamic hypergraphlearning [ 27]. Bandyopadhyay et al. [ 4] proposed the line hypergraph convolu-\ntional network by mapping the hypergraph to a weighted attribute line graph.\n3 Preliminary Knowledge\n3.1 Hypergraph\nIn this section, we brieﬂy introduce the preliminary knowledge of hypergraphs.\nAn edge in a simple graph connects two vertices. Compared with simple graph,a weighted hypergraph is denoted as G=(V,E,W), where V={v\n1,v2,...,v n}\nis a set of nvertices, E={e1,e2,...,e m}is a set of mhyperedges and\nW={(w(e1),w(e2),...,w(em)}is a set of hyperedge weight. The structure of\nhypergraph Gcan be represented by an incidence matrix H∈IR|V|×|E|with\nentries\nh(v,e)=/braceleftbigg0i fv/∈e,\n1i fv∈e.(1)\nIn practice, h(v,e) takes continuous values in the range of [0 ,1] which indicates\nthe importance of the vertex vfor hyperedge e. The degree of a vertex v∈Vis\ndeﬁned by\nd(v)=/summationdisplay\ne∈Ew(e)h(v,e), (2)', 'Hypergraph Convolutional Network with Hybrid Higher-Order Neighbors 107\nwhere edenotes a hyperedge in the hyperedge set E, and the degree of hyperedge\ne∈Eis deﬁned by\nδ(e)=/summationdisplay\nv∈Vh(v,e). (3)\nLet Dvbe the diagonal matrix of the vertex degrees, Debe the diagonal matrix of\nthe hyperdege degrees, and Wbe the diagonal matrix of the hyperedge weights,\ni.e., Dv=diag(d(v1),d(v2),...,d(vn)),De=diag(δ(e1),δ(e2),...,δ(em)), and\nW=diag(w(e1),w(e2),...,w(em)).\n3.2 Hypergraph Convolution Network\nFor a hypergraph G=(V,E,W) and hypergraph signal X, the hypergraph con-\nvolution layer f(X,W,Θ) based on the theory of spectral convolution on hyper-\ngraph is deﬁned as\nX(l+1)=σ(D−1\n2vHWD−1\neHTD−1\n2vX(l)Θ(l)), (4)\nwhere X(l)∈IRN×D(l)represents the Nvertex features in the l−thlayer, Θ(l)\nis a learnable parameter of ﬁlter in the l−thlayer and σ(·) denotes the nonlin-\near activation function. Hyperedge weight W∈IRN×Nreﬂects the importance\nof diﬀerent hyperedges in the network. A two-layer hypergraph convolutionalnetwork model can be deﬁned by\nY=softmax (ˆH(σ(ˆHX\n(1)Θ(1)))Θ(2))), (5)\nwhere ˆH=D−1\n2vHWD−1\neHTD−1\n2v,a n d Θ(1)andΘ(2)represent the trainable\nparameter matrix in ﬁrst layer and second layers respectively.\n4M e t h o d\nIn this section, we propose the HybridHGCN to capture k-hop neighbor rela-\ntions. As shown in Fig. 2, HybridHGCN consists of kbranches and a hybrid\nmessage operator that each branch takes the two-layer structure. We employincidence matrices of diﬀerent order of powers to represent the k-hop hyper-\ngraph. In the hypergraph convolutional layer, node representation is updated\nwith the k-hop neighbor message by the incidence matrix of k-order power. Torepresent diﬀerent-order neighbor relations, we propose a hybrid message oper-\nator to mix feature embeddings generated from each branch. Speciﬁcally, the\nhybrid message operator is implemented before the softmax layer. The Hybrid-\nHGCN consisting of kbranches and a hybrid message operator can be written as\nY=softmax (HM(σ(ˆH\n(1)X(l)Θ(l)),σ(ˆH(2)X(l)Θ(l)),...,σ(ˆH(k)X(l)Θ(l))),\n(6)', '108 J. Huang et al.\nwhere HM(·) is a hybrid message operator, ˆH=D−1\n2vHWD−1\neHTD−1\n2v,a n d\nσ(·) is the Relu activation function. The parameter kdenotes the power of the\nincidence matrix, and lrepresents the layer of the hypergraph convolution. In\nHM(·) layer, the input feature of ( l+1)-layer is the output of l-layer hypergraph\nconvolution. When parameter l= 1, the input feature of the ﬁrst hypergraph\nconvolutional layer is the initial feature X. Speciﬁcally, ˆH(k)represents the inci-\ndence matrix of k-th order of power. The diagonal matrices of the edge degrees\nD(k)\neand vertex degrees D(k)\nvcorrespond to the k-th order of hypergraph Lapla-\ncianˆH(k), respectively. In the process of simultaneous high order and low order\nmessage propagation, we propose a hybrid message operator HM(·) as follows:\nOutHM=HM(FtsH(1),FtsH(2),...,FtsH(k)), (7)\nwhere OutHMrepresents the mixed feature. The features FtsH(1),FtsH(2)and\nFtsH(k)represent the output features of hypergraph convolution layers associ-\nated with the 1 st,2ndandk−thorders of power incidence matrices respectively.\nHM(·) performs an element-wise operation on eigenvector matrices of diﬀerent\norders, which selects the max elements. In this paper, we set the parameter l=2\nwhich means the HybridHGCN has two layers and we have\nY=softmax (HM(ˆH(1)σ(ˆH(1)X(l1)Θ(l1))Θ(l2),ˆH(2)σ(ˆH(2)X(l1)Θ(l1))Θ(l2),\n...,ˆH(k)σ(ˆH(k)X(l1)Θ(l1))Θ(l2))),\n(8)\nwhere ˆHis the normalized hypergraph Laplancian, Θ(l1)∈IRd0×d1andΘ(l2)∈\nIRd1×d2are the learnable weight parameters. For classiﬁcation with qclasses, we\nadopt cross entropy as the loss function of HybridHGCN:\nL=−/summationdisplay\ni∈V Lq/summationdisplay\nj=1ˆYijlnYij, (9)\nwhere VLdenotes the set of labeled examples and qrepresents the number of\nclasses. ˆYijdenotes the actual label of a node in VLandYijis the predicted label\ncomputed by Eq. ( 8).\n5 Experiments\n5.1 Datasets and Baseline\nIn our experiments, we use classiﬁcation accuracy as the evaluation criteria. To\nevaluate the HybridHGCN, we employ ﬁve datasets including citation networks\nand visual objects. Cora, Citeseer and Pumbed [ 17] are publicly used citation\nnetwork datasets, which contain the graph structures of citations with the bags-of-words feature vector of documents. In Cora, Citeseer, and Pumbed, nodes\ncorrespond to documents and edges represent the relation between each pair of\nnodes. Speciﬁcs of Cora, Citeseer, and Pumbed are presented in Table 1.', 'Hypergraph Convolutional Network with Hybrid Higher-Order Neighbors 109\nTable 1. Summary of citation network datasets.\nDataset Cora Citeseer Pumbed\nNumber of nodes 2,708 3,327 19,717\nNumber of edges 5,278 4,552 44,324\nLength of features 1,433 3,703 500\nNumber of classes 7 6 3\nThe Priceton ModelNet40 dataset [ 24] contains 12,311 visual objects of\n40 categories including airplane, bathtub, car, dresser, guitar, and so on. The\nNational Taiwan University 3-D model (NTU) dataset [ 8] contains 2,012 visual\nobjects of 67 categories, including chair, clock, door, frame and so on. The sum-\nmary of visual object, categories, and length of MVCNN and GVCNN extracted\nfeatures are given in Table 2.\nTable 2. Summary of visual object datasets.\nDataset Mo d e l Ne t 40 NTU\nNumber of object 12,311 2,012\nLength of MVCNN features 4,096 4,096\nLength of GVCNN features 2,048 2,048\nNumber of classes 40 67\nWe compare the following methods in our experiments. We denote HGNN as\nHGNN-s when HGNN adopt the same graph structure as HyperGCN [ 25]a n d\nLHCN [ 4].\n– HyperGCN [ 25]: The method approximates the hypergraph Laplacian to\ngraph and then performs graph convolution. 1-HyperGCN and FastHyper-GCN are two variants of HyperGCN. The former approximates the hyperedge\nby a pair of edges and adds mediators to enhance the performance. The latter\nreduces computation time where the hypergraph Laplacian is computed onlyonce before training.\n– Line Hypergraph Convolution Network (LHCN) [ 4]: The hypergraph struc-\nture is mapped to an attributed and weighted line graph which adapts in\ngraph convolution.\n– Hypergraph Neural Network (HGNN) [ 10]: The method adopts the normal-\nized hypergraph Laplacian to perform graph convolution in weighted cliqueexpansion hypergraph.\n5.2 Experimental Setting\nFor citation networks and visual object datasets, we construct the hypergraph\nfrom the original graph structure and Euclidean distance of visual objects,', '110 J. Huang et al.\nrespectively. For each node in the graph of citation networks, we take it as a\ncentral node and set a hyperedge with this central node as the centroid. Theother nodes that connect to the centroid are added into the hyperedge, and\nthe entries of hypergraph structure are formulated as Eq. ( 1). HGNN [ 10]a n d\nHybridHGCN adopt the same hypergraph structure. The graph structures ofHyperGCN have 1,579, 1,079 and 7,963 edges for Cora, Citeseer and Pubmed\nrespectively.\nFor visual object datasets, we select Mutil-view CNN [ 20] and Group-view\nCNN [ 11] to extract features, which have shown excellent performance in the\nrepresentation of 3D shape. Considering the feature of each visual object, we\ncalculate its Euclidean distance with other objects, and then select the knearest\nnodes to connect. In our experiments, we set the hyper-parameter kto be 10.\nThe entries of the constructed hypergraph structure is as follows.\nh(v,e)=/braceleftBigg0i fv/∈e\nexp/parenleftBig\n−\n2D2\nij\nΔ/parenrightBig\nifv∈e,(10)\nwhere Dijdenotes the Euclidean distance between node iand node j,a n d\nΔdenotes the average pairwise distance of all nodes.\nFor Cora, Citeseer, and Pubmed datasets, the dimensions of the hidden units\nof HybridHGCN are 32, 16, and 64com, respectively, and the orders of the inci-\ndence matrices are 1, 2, and 3 respectively.\nFor ModelNet40 and NTU datasets, the dimensions of the hidden units of\nHybridHGCN are 256, and the orders of the incidence matrices are 1, 2, and 3\nrespectively.\n5.3 Experimental Results and Discussion\nFor citation networks datasets, experiment results are shown in Table 3, where\nperformances of comparison methods are directly taken from [ 4,25]. The pro-\nposed HybridHGCN achieves the best testing accuracy of 81.88%, 70.96%, and\n78.31% on Cora, Citeseer, and Pubmed respectively. We adopt the datasets splitby using 5.2%, 4.1%, and 0.3% of data for training respectively and the rest is\nfor testing. The train-test split is the same as HyperGCN [ 25] which samples the\nnodes of the same size from each class.\nWe also evaluate the speciﬁc eﬀects of diﬀerent label rates on HGNN and\nHybridHGCN for the citation datasets, by trying to use 5%, 10%, 20%, 30%,and 40% of data for training, respectively and the rest is for validating and test-\ning. As shown in Table 4, compared with HGNN, the accuracy of HybridHGCN\nincreases by 0.1% to 1.79% for diﬀerent data splits on Cora, with the moreobvious improvement on the low label rate. For Citeseer, as shown in Table 5,\nHybridHGCN shows an increase in accuracy of 0.3% to 2.0% for diﬀerent data\nsplits compared to HGCN. It’s worth noting that HybridHGCN improves per-formance even more when the label rates are high in Citeseer. For Pubmed, from\nTable 6we can see that HybridHGCN achieves 0.2% to 0.3% increase in accu-\nracy for diﬀerent data splits, which are slight performance improvements over', 'Hypergraph Convolutional Network with Hybrid Higher-Order Neighbors 111\nTable 3. Test accuracy (%) on citation networks classiﬁcation. ±represents the stan-\ndard deviation.\nMethod Cora Citeseer Pubmed\nHGNN-s [ 25] 67.59 ±1.8 62.60 ±1.6 70.59 ±1.5\n1-HyperGCN [ 25]65.55 ±2.1 61.13 ±1.9 69.92 ±1.5\nFastHyperGCN [ 25]67.57 ±1.8 62.58 ±1.7 70.52 ±1.6\nHyperGCN [ 25] 67.63 ±1.7 62.65 ±1.6 74.44 ±1.6\nLHCN [ 4] 73.34 ±1.7 63.19 ±2.2 70.76 ±2.4\nHGNN [ 10] 80.99 ±0.8 69.72 ±1.0 78.14 ±1.7\nHybridHGCN 81.88 ±0.2 70.96 ±0.8 78.31 ±1.6\nTable 4. Test accuracy (%) on Cora dataset classiﬁcation. ±represents the standard\ndeviation. 5% to 40% represent the percentage of data used for training.\nMethod Cora\n5% 10% 20% 30% 40%\nHGNN [ 10] 79.86 ±1.1 81.72 ±1.6 84.60 ±0.9 86.36 ±1.4 88.02 ±1.4\nHybridHGCN 81.04 ±0.5 83.28 ±0.8 85.20 ±1.1 87.50 ±0.9 89.81 ±0.4\nHGNN. The success of HybridHGCN lies in the addition of the k-hop neighbor\nmessage to improve node representation capabilities.\nFor the experiment on ModelNet40 and NTU datasets, we follow the same\n80%–20% dataset split for training and testing respectively as in [ 24]. For visual\nobject datasets, from Table 7we can observe that HybridHGCN performs better\nthan the rest under most CNNs features. On the ModelNet40 dataset, comparedwith the MVCNN features and GVCNN features, the performance of Hybrid-\nHGCN is close to HGNN. On the NTU dataset, HybridHGCN brings the gain of\n2.24% and 2.93% on MVCNN features and GVCNN features respectively. The\nresults indicate that HybridHGCN is more eﬀective in node classiﬁcation tasks\nthan HGNN.\nTable 5. Test accuracy (%) on Citeseer dataset classiﬁcation. ±represents the stan-\ndard deviation. 5% to 40% represent the percentage of data used for training.\nMethod Citeseer\n5% 10% 20% 30% 40%\nHGNN [ 10] 70.22 ±1.2 71.70 ±0.6 73.82 ±1.0 74.98 ±0.5 75.04 ±1.3\nHybridHGCN 70.58 ±1.1 72.32 ±0.6 73.60 ±0.5 76.08 ±1.4 77.02 ±1.4', '112 J. Huang et al.\nTable 6. Test accuracy (%) on Pubmed dataset classiﬁcation. ±represents the stan-\ndard deviation. 5% to 40% represent the percentage of data used for training.\nMethod Pubmed\n5% 10% 20% 30% 40%\nHGNN [ 10] 83.16 ±0.2 83.80 ±0.8 84.54 ±0.5 84.60 ±0.3 84.66 ±0.5\nHybridHGCN 83.38 ±0.3 84.12 ±0.3 84.22 ±0.1 84.96 ±0.3 84.90 ±0.6\nTable 7. Test accuracy (%) on ModelNet40 and NTU datasets classiﬁcation. ±repre-\nsents the standard deviation. MVCNN and GVCNN represent the features extracted\nfrom Multi-view CNN [ 20] and Group-view CNN [ 11] respectively.\nMethod ModelNet40 NTU\nMVCNN GVCNN MVCNN GVCNN\nHGNN [ 10] 91.00 92.60 75.60 82.50\nHybridHGCN 90.62 ±0.0 92.76 ±0.1 77.84 ±0.1 85.43 ±0.1\n6 Conclusions\nThis paper proposed a hybrid higher-order neighborhood based hypergraph con-\nvolutional network (HybridHGCN), which explores the k-hop neighbor message.\nWe conducted extensive experiments on hypergraphs construct on citation net-\nworks and visual object datasets, and the results showed that HybridHGCNperforms better than the state-of-the-art methods. For the future work, we plan\nto further reduce the feature redundancy in mixing the k-hop neighbor messages.\nAcknowledgements. This work was supported in part by the National Natural Sci-\nence Foundation of China under Grant U1701266, in part by Guangdong Provincial\nKey Laboratory of Intellectual Property and Big Data under Grant 2018B030322016,in part by Special Projects for Key Fields in Higher Education of Guangdong under\nGrant 2020ZDZX3077 and Grant 2021ZDZX1042, and in part by Qingyuan Science and\nTechnology Plan Project under Grant 170809111721249 and Grant 170802171710591.\nReferences\n1. Abu-El-Haija, S., et al.: Mixhop: higher-order graph convolutional architectures via\nsparsiﬁed neighborhood mixing. In: Proceedings of the International Conference onMachine Learning, pp. 21–29 (2019)\n2. Agarwal, S., Branson, K., Belongie, S.: Higher order learning with graphs. In:\nProceedings of the International Conference on Machine Learning, pp. 17–24 (2006)\n3. Bai, S., Zhang, F., Torr, P.H.S.: Hypergraph convolution and hypergraph attention.\nPattern Recogn. 110, 107637 (2021)\n4. Bandyopadhyay, S., Das, K., Narasimha Murty, M.: Line hypergraph convolu-\ntion network: Applying graph convolution for hypergraphs (2020). arXiv preprint:\narXiv:2002.03392\n5. Benson, A.R., Gleich, D.F., Leskovec, J.: Higher-order organization of complex\nnetworks. Science 353(6295), 163–166(2016)', 'Hypergraph Convolutional Network with Hybrid Higher-Order Neighbors 113\n6. Bretto, A.: Hypergraph Theory. An Introduction. Mathematical Engineering.\nSpringer, Cham (2013)\n7. Bronstein, M.M., Bruna, J., LeCun, Y., Szlam, A., Vandergheynst, P.: Geometric\ndeep learning: going beyond Euclidean data. IEEE Signal Process. Mag. 34(4),\n18–42 (2017)\n8. Chen, D., Tian, X., Shen, Y., Ouhyoung, M.: On visual similarity based 3d model\nretrieval. Comput. Graph. Forum 22, 223–232 (2003)\n9. Chen, F., Wang, Y., Wang, B., Jay Kuo, C.-C.: Graph representation learning: a\nsurvey. APSIPA Trans. Signal Inf. Process. 9 (2020)\n10. Feng, Y., You, H., Zhang, Z., Ji, R., Gao, Y.: Hypergraph neural networks. In:\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, Vol. 33, pp. 3558–3565 (2019)\n11. Feng, Y., Zhang, Z., Zhao, X., Ji, R., Gao, Y.: GVCNN: group-view convolutional\nneural networks for 3d shape recognition. In: Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pp. 264–272 (2018)\n12. Hayashi, K., Aksoy, S.G., Park, C.H., Park, H.: Hypergraph random walks, lapla-\ncians, and clustering. In: Proceedings of the 29th ACM International Conferenceon Information & Knowledge Management, pp. 495–504 (2020)\n13. Jiang, J., Wei, Y., Feng, Y., Cao, J., Gao, Y.: Dynamic hypergraph neural networks.\nIn: Proceedings of the International Joint Conference on Artiﬁcial Intelligence, pp.2635–2641 (2019)\n14. Kim, E.-S., Kang, W.Y., On, K.-W., Heo, Y.-J., Zhang, B.-T.: Hypergraph atten-\ntion networks for multimodal learning. In: Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, pp. 14581–14590 (2020)\n15. Kipf, T.N., Welling, M.: Semi-supervised classiﬁcation with graph convolutional\nnetworks (2016). arXiv preprint: arXiv:1609.02907\n16. Li, P., Faltings, B.: Hypergraph learning with hyperedge expansion. In: Flach, P.A.,\nDe Bie, T., Cristianini, N., (eds.) Machine Learning and Knowledge Discovery in\nDatabases. ECML PKDD 2012. LNCS, vol. 7523, pp. 410–425. Springer, Berlin(2021). https://doi.org/10.1007/978-3-642-33460-3\n32\n17. Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., Eliassi-Rad, T.: Collec-\ntive classiﬁcation in network data. AI Mag. 29(3), 93 (2008)\n18. Shchur, O., Mumme, M., Bojchevski, A., G¨ unnemann, S.: Pitfalls of graph neural\nnetwork evaluation (2018). arXiv preprint: arXiv:1811.05868\n19. Shi, H., et al.: Hypergraph-induced convolutional networks for visual classiﬁcation.\nIEEE Trans. Neural Netw. Learn. Syst. 30(10), 2963–2972 (2018)\n20. Hang, S., Subhransu, M., Evangelos, K., Learned-Miller, E.: Multi-view convo-\nlutional neural networks for 3d shape recognition. In: Proceedings of the IEEEInternational Conference On Computer Vision, pp. 945–953 (2015)\n21. Su, L., Gao, Y., Zhao, X., Wan, H., Gu, M., Sun, J.: Vertex-weighted hypergraph\nlearning for multi-view object classiﬁcation. In: Proceedings of the International\nJoint Conference on Artiﬁcial Intelligence, pp. 2779–2785 (2017)\n22. Veliˇ ckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y.: Graph\nattention networks (2017). arXiv preprint: arXiv:1710.10903\n23. Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., Philip, S.Y.: A comprehensive\nsurvey on graph neural networks. IEEE Trans. Neural Netw. Learn. Syst. 32, 4–24\n(2020)\n24. Wu, Z., et al.: 3d shapenets: a deep representation for volumetric shapes. In: Pro-\nceedings of the IEEE Conference On Computer Vision And Pattern Recognition,pp. 1912–1920 (2015)', '114 J. Huang et al.\n25. Yadati, N., et al.: HyperGCN: a new method of training graph convolutional net-\nworks on hypergraphs (2018). arXiv preprint: arXiv:1809.02589\n26. Jun Yu, Dacheng Tao, and Meng Wang.: Adaptive hypergraph learning and its\napplication in image classiﬁcation. IEEE Transactions on Image Processing, vol.\n21, no. 7, pp. 3262–3272(2012)\n27. Zhang, Z., Lin, H., Gao, Y.: KLISS BNRist.: dynamic hypergraph structure learn-\ning. In: Proceedings of the International Joint Conference on Artiﬁcial Intelligence,\npp. 3162–3169 (2018)\n28. Zhou, D., Huang, J., Sch¨ olkopf, B.: Learning with hypergraphs: clustering, classi-\nﬁcation, and embedding. Adv. Neural Inf. Process. Syst. 19, 1601–1608 (2006)\n29. Zhou, J., et al.: Graph neural networks: a review of methods and applications\n(2018). arXiv preprint: arXiv:1812.08434', 'Text-Aware Single Image Specular\nHighlight Removal\nShiyu Hou1,2, Chaoqun Wang1,2, Weize Quan1,2, Jingen Jiang1,2,\nand Dong-Ming Yan1,2(B)\n1National Laboratory of Pattern Recognition (NLPR), Institute of Automation,\nChinese Academy of Sciences, Beijing 100190, China\n{weize.quan,dongming.yan }@nlpr.ia.ac.cn\n2School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences, Beijing\n100049, China\nAbstract. Removing undesirable specular highlight from a single input\nimage is of crucial importance to many computer vision and graphics\ntasks. Existing methods typically remove specular highlight for medi-\ncal images and speciﬁc-object images, however, they cannot handle theimages with text. In addition, the impact of specular highlight on text\nrecognition is rarely studied by text detection and recognition commu-\nnity. Therefore, in this paper, we ﬁrst raise and study the text-awaresingle image specular highlight removal problem. The core goal is to\nimprove the accuracy of text detection and recognition by removing the\nhighlight from text images. To tackle this challenging problem, we ﬁrst\ncollect three high-quality datasets with ﬁne-grained annotations, which\nwill be appropriately released to facilitate the relevant research. Then, wedesign a novel two-stage network, which contains a highlight detection\nnetwork and a highlight removal network. The output of highlight detec-\ntion network provides additional information about highlight regions toguide the subsequent highlight removal network. Moreover, we suggest a\nmeasurement set including the end-to-end text detection and recognition\nevaluation and auxiliary visual quality evaluation. Extensive experimentson our collected datasets demonstrate the superior performance of the\nproposed method.\nKeywords: Specular highlight removal\n·Text-awareness ·Datasets ·\nNeural network\n1 Introduction\nSpecular highlights often exist in real-world images due to the material property\nof objects and the capturing environments. It is always desired to reduce or elim-\ninate these specular highlights to improve the visual quality and to facilitate thevision and graphics tasks, such as stereo matching [ 10,12], text recognition [ 16],\nimage segmentation [ 3,6] and photo-consistency [ 30,31]. See Fig. 1for examples,\nthe performance of the end-to-end text detection and recognition drops due to\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 115–127, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_10', '116 S. Hou et al.\nHighlight Image OCR Result Mask Ours OCR Result\nFig. 1. Selected single image specular highlight removal results of our method. “Mask”\nand “Ours” separately are the outputs of our ﬁrst-stage and second-stage networks.\nthe existence of highlight in the images, while our method is designed to detect\nand remove the highlight so as to improve the subsequent OCR performance.\nIn the last decades, many approaches have been proposed to address this\nchallenging specular highlight removal problem. These existing works can be\nroughly classiﬁed into three categories: dichromatic reﬂection model-based meth-\nods [7,23,26,28,32], inpainting-based methods [ 4,18,21,27], and deep learning-\nbased methods [ 8,15,20]. The dichromatic reﬂection model [ 24] linearly com-\nbines the diﬀuse and specular reﬂections, and subsequently many methods are\nproposed based on this model. These methods usually require some simplify-ing assumptions. In addition, they often need to carry out the pre-processing\noperations, e.g., segmentation, when encountering images with diverse colors\nand complex textures, which results in low eﬃciency and weak practicability.Inpainting-based methods mainly recover the original image contents behind\nthe highlight borrowing the techniques from the image inpainting community.\nThis kind of methods have limited performance for the large highlight contam-\nination. Considering the complexity of single image specular highlight removal,\nsome recent works [ 8,15,20] are proposed based on the deep neural networks,\ne.g., convolutional neural network (CNN) and generative adversarial network\n(GAN). With the aid of the powerful learning capacity of deep models, these deep\nlearning-based methods usually have better performance compared with tradi-tional optimization-based methods. However, these deep learning-based methods\nrequire the large-scale training data, especially paired real-world images with\nnecessary annotations, which are time-consuming even diﬃcult to collect.\nExisting specular highlight removal methods mainly process the medical\nimages, natural images, and speciﬁc-object images, however, there is no work\nto focus on the text images. On the other hand, for the end-to-end text detec-tion and recognition, many approaches are proposed to handle texts with arbi-\ntrary shapes and various orientations. To our knowledge, the case of text images\nwith specular highlight contamination is rarely studied. Therefore, in this paper,', 'Text-Aware Single Image Specular Highlight Removal 117\nwe conduct an extensive study on the text-aware single image specular high-\nlight removal problem including dataset collection, network architecture, train-ing losses, and evaluation metrics. The main contributions of our work are as\nfollows:\n– We ﬁrst raise and study in the literature text-aware single image specular\nhighlight removal problem. To study this challenging problem, we collect three\nhigh-quality datasets with ﬁne-grained annotations.\n– We propose a novel two-stage framework of highlight regions detection and\nremoval implemented with two sub-networks. The highlight detection network\nprovides the useful location information to facilitate the subsequent highlightremoval network. For the training objectives, we jointly exploit detection\nloss, reconstruction loss, GAN loss, and text-related loss to achieve the good\nperformance.\n– For the result comparison, we suggest a comprehensive measurement set,\nwhich contains the end-to-end text detection and recognition performance\nand auxiliary visual quality evaluation.\n2 Related Work\n2.1 Dichromatic Reﬂection Model-Based Methods\nThe dichromatic reﬂection model [ 24] assumes that the image intensity can be\nrepresented by a linear combination of diﬀuse and specular reﬂections. This\nmodel have been widely used for specular highlight removal. Based on the dis-tribution of diﬀuse and specular points in the maximum chromaticity-intensity\nspace, Tan et al. [28] separated the reﬂection components by identifying the dif-\nfuse maximum chromaticity and then applying a specular-to-diﬀuse mechanism.Inspired by the observation that diﬀuse maximum chromaticity in a local patch\nof color images changes smoothly, Yang et al.[32] enhanced the real-time perfor-\nmance and robustness of the chromaticity estimation by applying the bilateralﬁltering. To exploit the global information of color images for specular reﬂection\nseparation, Ren et al. [23] proposed a global color-lines constraint based on the\ndichromatic reﬂection model. Fu et al. [7] reformulated estimating the diﬀuse\nand specular images as an energy minimization with sparse constraints, which\ncan be approximately solved. Recently, Son et al. [26] proposed a convex opti-\nmization framework to eﬀectively remove the specular highlight from chromatic\nand achromatic regions of natural images. These dichromatic reﬂection model-\nbased approaches often have limited performance for processing the images withdiverse colors and complex textures.\n2.2 Inpainting-Based Methods\nInpainting is to complete the missing regions of images by propagating informa-\ntion from the known regions, and this technique can be used to restore dam-\naged paintings or remove speciﬁc objects [ 5]. Tan et al. [27] ﬁrst proposed an', '118 S. Hou et al.\ninpainting-based method for highlight removal by incorporating the illumination-\nbased constraints. Ortiz and Torres [ 21] designed a connected vectorial ﬁlter inte-\ngrating into the inpainting process to eliminate the specular reﬂectance. Park\nand Lee [ 22] introduced a highlight inpainting method based on the color line\nprojection, however, this method needs two images taken with diﬀerent exposuretimes. Inpainting-based highlight removal methods were also proposed to handle\nthe medical images, such as endoscopic images [ 4] and colposcopic images [ 18].\nHowever, these inpainting-based methods are only eﬀective for images with smallareas of highlight contamination.\n2.3 Deep Learning-Based Methods\nDiﬀerent from the aforementioned two kinds of methods, the deep learning-\nbased methods do not require the specular highlight model assumption, and\nthus have the potential to handle various scenarios. Lee et al. [14]p r o p o s e da\nperceptron artiﬁcial neural network to detect the specular reﬂections of toothimages and then applied the smoothing spatial ﬁlter to recursively correct the\nspecular reﬂections. Due to the lack of paired training data, Funke et al. [8]\nadopted the cycle GAN framework [ 33] and introduced a self-regularization loss\naiming to reduce image modiﬁcation in non-specular regions. Similarly, Lin et\nal.[15] also adopted a GAN framework and proposed a multi-class discriminator,\nwhere classifying the generated diﬀuse images from real ones and original inputimages as well. Muhammad et al. [20] proposed two deep models (Spec-Net and\nSpec-CGAN) for specularity removal from faces. The former takes the intensity\nchannel as input while the latter takes the RGB image as input. These methods\nmainly proposed for the medical images, speciﬁc-object images or facial images,\nwhereas our work pays attention to the text images.\n3 Datasets\nIn the literature, there is no publicly available dataset for studying the text-aware single image specular highlight removal problem. Therefore, in this work,\nwe collect three high-quality datasets including a real dataset and two synthetic\ndatasets. The pipelines of datasets collection and an example of paired datasample are shown in Fig. 2.\n3.1 Real Dataset\nFigure 2(a) illustrates the pipeline of real dataset collection. For the real dataset,\nwe collect 2,025 image pairs: image with text-aware specular highlight, the cor-\nresponding highlight-free image and binary mask image indicating the location\nof highlight. The image contents include ID cards and driver’s licenses, whichcontain a lot of text information. We ﬁrst put the transparent plastic ﬁlm on\nthe picture and turn on the light. Then, the camera shoots to obtain a high-\nlight image. Correspondingly, we obtain a highlight-free image by turning oﬀ', 'Text-Aware Single Image Specular Highlight Removal 119\n(a)\n(c)Ground Truth Highlight Image Mask(b)Raw data \ncollection\nStep1 Step2Data processing\nStep3\nFig. 2. The collection pipelines of real dataset (a) and synthetic dataset (b), and an\nexample of paired data sample (c).\nthe light. The shapes and intensities of the highlights are various by adjusting\nthe location of the plastic ﬁlm. Binary mask image is achieved from the imagewith specular highlight and highlight-free image through diﬀerence and multiple\nthreshold screening. We randomly split this dataset (named RD) into a training\nset (1,800 images) and a test set (225 images).\n3.2 Synthetic Datasets\nTo further enrich the diversity of our dataset, we construct two sets of synthetic\nimages using the 3D computer graphics software Blender. Figure 2(b) shows the\npipeline of synthetic dataset collection. We ﬁrst collect 3,679 images with texts\nfrom supermarkets and streets, and 2,025 images mentioned in Sec. 3.1. Then,\nwe use the Blender with Cycles engine to automatically generate 27,700 groups\nof text-aware specular highlight images, the corresponding highlight-free images\nand highlight mask images. In particular, the highlight shapes include circles,triangles, ellipses, and rings to simulate the lighting conditions in real-world\nscenes. The material roughness is randomly set in the range [0.1,0.3], and the\nillumination intensity is randomly chosen from the range [40,70]. To force thespecular highlight on the text areas of the image, we provide the Blender with', '120 S. Hou et al.\ntI\noutM\nConv+ReLU\noutI\nConv+ReLU\nLSTM\nLSTMConv+ReLU\nLSTM\nLSTMDetection Loss\nReconstruction Loss Text LossgtMgtI\nHighlight \nDetection Network\nHighlight \nRemoval NetworkText Feature \nExtraction NetworkText Feature \nExtraction Network\nAdv LossConv+ReLU\nFig. 3. The whole structure of our proposed specular highlight removal framework,\nwhich consists of a highlight detection network, a highlight removal network, and a\npatch-based discriminator. Symbol/circleplustextmeans the channel-wise concatenation.\nthe location information of the text areas obtained via the text detection model\nCTPN [ 29].\nBecause the product or street view category contains less texts per image,\nwhile the texts in ID cards and driver’s licenses are more dense. Under the sameillumination condition, the diﬃculty of restoring the text information interfered\nby the specular highlight in these two kinds of images is diﬀerent. Therefore,\nwe divide the above two types of images into two datasets, namely, SD1 andSD2. SD1 contains 12,000 training sets and 2,000 test sets. SD2 contains 12,000\ntraining sets and 1,700 test sets. Note that, the image contents of RD and SD2\nare same.\n4 Proposed Method\nIn this work, we propose a two-stage framework to detect and remove the spec-ular highlight from text images. The whole architecture is shown in Fig. 3.I n\nthe following, we describe the details of our network architecture and the loss\nfunctions.\n4.1 Network Architecture\nHighlight Detection Network. The highlight detection network Net\nDtakes\nthe text image Itwith specular highlight as input and outputs a mask M out\nindicating the highlight regions. Each element of M outis in [0,1], and a larger', 'Text-Aware Single Image Specular Highlight Removal 121\nvalue stands for a higher probability that the corresponding location of image It\nis covered by the specular highlight. Due to the same width and height of Itand\nM out, for this network, we adopt a fully convolutional architecture consisting of\nthree downsampling and upsampling layers. Each downsampling layer is followed\nby two convolutional layers, and each upsampling layer is followed by threeconvolutional layers.\nHighlight Removal Network. After achieving the highlight mask M\nout,t h e\nhighlight removal network Net Ris then applied to remove the specular highlight\nand recover the text information. As input, Net Raccepts an input text image\nItand detected highlight mask M out. The output of our highlight removal net-\nwork Net Ris a highlight-free image Iout. Through introducing M out, network\nNet Rcan pay more attention to the highlight regions and achieve more better\nremoval performance. For the network architecture of Net R,i nt h i sw o r k ,w e\nadopt an encoder-decoder network with skip connection. This network consists\nof two downsampling layers, four residual blocks, and two upsampling layers.\nTo further enhance the removal performance, we also apply a patch-based dis-criminator [ 19]. The discriminator Dincludes one convolutional layer and ﬁve\ndownsampling layers with kernel size of 5 and stride of 2. The spectral normal-\nization is utilized to stabilize the training of the discriminator.\n4.2 Loss Functions\nNext, we illustrate the loss functions used for training our network.\nHighlight Detection Loss. For the objective function of highlight detection\nnetwork, we use l\n1loss, i.e.,LNe t D=/bardblM out−M gt/bardbl1, where M gtis the ground\ntruth of highlight mask.\nReconstruction Loss. The reconstruction loss is to add constraints on the pixel\nand feature space. The pixel-aware loss consists of pixel-wise diﬀerence item andtotal varition (TV) item: L\nP=5∗/bardblIout−Igt/bardbl1+0.1∗(/bardblIout(i,j)−Igt(i−1,j)/bardbl1+\n/bardblIout(i,j)−Igt(i,j−1)/bardbl1). The feature-aware loss including perceptual loss [ 11]\nand style loss [ 9]:LF=0.05∗/bardblΦ(Iout)−Φ(Igt)/bardbl1+ 120 ∗/bardblΨ(Iout)−Ψ(Igt)/bardbl1,\nwhere Φis the feature maps of pre-trained VGG-16 [ 25]a n d Ψ(·)=Φ(·)Φ(·)T\nis the Gram matrix [ 9]. The feature-aware loss improves the visual quality of\nresults.\nGAN Loss. In the highlight removal network, we use a patch-based discrimi-\nnator Dto enhance the visual realism of results. For the GAN loss, we adopt the\nhinge loss. Therefore, the adversarial loss for Net RisLG=−E[D(Iout)]. The\nloss used for training the discriminator Dis formulated as LD=E[max(0,1−\nD(Igt))] + E[max(0,1+D(Iout))].\nText-Related Loss. In this work, our specular highlight removal is text-aware.\nThis means that the highlight removal network Net Rneeds to pay more attention\nto recover the texts hidden behind the highlights. To do this, we apply the\npre-trained text detection and recognition models to provide the supervision', '122 S. Hou et al.\non the text recovering. More speciﬁcally, we add the consistent constraints on\nthe feature maps of Ioutand Igtextracted from above two pre-trained models,\nand the text-related loss is formulated as LT=/summationtext3\nc=1/bardblφc(Iout)−φc(Igt)/bardbl1+\n/bardblφd(Iout)−φd(Igt)/bardbl1, where φcstands for the c-th layer feature map from the\npre-trained CTPN model [ 1]a n dφddenotes the d-th layer feature map from the\npre-trained DenseNet [ 1].\nTo this end, the total objective function of Net DandNet RisL=\nλNe t DLNe t D+LP+LF+λGLG+LT. In all experiments, we set λNe t D=1 0\nandλG=0.01.\n5 Experiments\n5.1 Implementation Settings\nOur network is implemented with TensorFlow 1.15. As GPU we use a TITAN\nRTX from NVIDIAR/circlecopyrt. The Adam optimizer [ 13] with a batch size of 4 is used to\ntrain our network, where β1=0.5a n d β2=0.9. The learning rate is initialized\nas 0.0001. In our experiments, all the images are of size of 512 ×512. Note that,\nthe text recognition model used for result evaluation is diﬀerent from the model\nused in text-related loss.\n5.2 Qualitative Evaluation\nWe qualitatively compare our method with two recent advanced specular high-\nlight removal methods: Multi [ 15] and SPEC (Spec-CGAN [ 20]) on our collected\nthree datasets. The results are shown in Fig. 4. Among these three methods, our\nmethod can better remove the highlight and achieve the superior end-to-end textdetection and recognition performance. For example, our method successfully\nrecovers the name, address, and id number in the third row. Multi has apparent\nhighlight remnants in the third and fourth rows due to its blind removal prop-erty, whereas our method can better perceive the highlight regions. Compared\nwith Multi, the results of SPEC have less highlights, however, the capability of\nrecovering the texts is limited for the cycleGAN framework as SPEC followed.\n5.3 Quantitative Evaluation\nIn addition, we quantitatively compare the above three methods in terms of the\nend-to-end text detection and recognition performance and visual quality. For\nthe end-to-end text detection and recognition evaluation, we adopt the common\nmetrics [ 17]: recall, precision, and f-measure. We choose the current advanced\ntext detection and recognition algorithm Paddle OCR [ 2] to calculate these three\nmetrics. For visual quality evaluation, we utilize the PSNR and SSIM.\nTable 1reports the numerical results of the three methods on our three\ndatasets. Due to the same image contents of RD and SD2, for real dataset (RD),\nwe ﬁne-tune the model trained on SD2 using the training set of RD for all three', 'Text-Aware Single Image Specular Highlight Removal 123\nGround Truth Highlight Image Multi SPEC Ours\nFig. 4. Qualitative comparisons of our method with Multi [ 15] and SPEC [ 20].\nmethods. From Table 1, we can ﬁnd that our method achieves the best perfor-\nmance for end-to-end text detection and recognition (see 3–5 columns). Take\nthe recall as an example, our method can improve the end-to-end text detectionand recognition performance by 6.89% (SD1), 3.07% (SD2), and 13.65% (RD),\nrespectively. This improvement indicates that our method can better recover the\noriginal texts hidden behind the specular highlight. In addition, the end-to-enddetection and recognition performance of Multi and SPEC sometimes is lower\nthan that of Light Image. The reason is that these two methods remove the high-\nlight and texts as well. For PSNR and SSIM, SPEC is worst, while our method', '124 S. Hou et al.\nand Multi are competitive for synthetic datasets, and our method is better than\nMulti for real datasets. PSNR and SSIM of our method sometimes are lowerthan that of Multi, however, these two metrics are not exactly the same as the\nvisual quality that the human eyes perceive. More importantly, we focus on the\nend-to-end text detection and recognition performance after highlight removal,and the visual quality is an auxiliary aspect.\n5.4 Ablation Study\nTo verify the eﬀectiveness of the text-related loss, we perform the ablation exper-\niments and report the corresponding results in Table 2. We observe that the\nend-to-end text detection and recognition performance of our method with text-related loss is consistently improved for three datasets. This indicates that the\ntext-related loss can enforce the highlight removal network to conduct the text-\naware restoration. In addition, we can ﬁnd that the end-to-end text detection andrecognition performance of our method is already better than that of Multi and\nSPEC (comparing the ﬁrst row of each dataset in Table 2with the corresponding\nrows in Table 1), even though there is no text-related loss.\nTable 1. Quantitative comparison of our method and two recent state-of-the-art meth-\nods: Multi [ 15] and SPEC [ 20]. All three methods are trained and tested on our collected\nthree datasets separately. Recall, precision, f-measure, and SSIM are in %.\nDatasets Methods Recall ↑Precision ↑F-measure ↑PSNR ↑SSIM ↑\nSD1Light Image 85.03 94.70 88.70 17.58 82.37\nMulti (2019) 86.28 94.76 89.30 26.29 89.86\nSPEC (2020) 82.39 93.12 86.31 15.61 68.82\nOurs 91.92 96.32 93.57 22.65 88.33\nSD2Light Image 80.50 95.89 87.10 11.79 66.42\nMulti (2019) 79.21 93.82 84.88 28.99 91.81\nSPEC (2020) 78.87 95.10 85.55 9.66 53.95\nOurs 83.57 95.00 88.42 29.21 90.67\nRDLight Image 64.85 90.60 73.49 17.05 65.04\nMulti (2019) 61.58 87.63 70.72 17.17 64.23\nSPEC (2020) 70.59 91.62 78.38 14.82 52.49\nOurs 78.50 91.34 83.34 21.62 77.19', 'Text-Aware Single Image Specular Highlight Removal 125\nTable 2. Performance of our method without and with text-related loss on our collected\nthree datasets.\nDatasets Methods Recall ↑Precision ↑F-measure ↑PSNR ↑SSIM ↑\nSD1w/o text loss 91.43 94.12 92.75 21.88 87.19\nOurs 91.92 96.32 93.57 22.65 88.33\nSD2w/o text loss 82.69 93.48 87.76 28.12 89.93\nOurs 83.57 95.00 88.42 29.21 90.67\nRDw/o text loss 77.04 89.66 82.87 21.38 76.11\nOurs 78.50 91.34 83.34 21.62 77.19\n6 Conclusion and Future Work\nIn this work, we studied and solved the challenging specular highlight removal\nproblem of single text image. To facilitate this study, we collected three high-\nquality datasets with ﬁne-grained annotations. We proposed a two-stage frame-\nwork including a highlight detection network and a highlight removal network.The output of highlight detection network is used as an auxiliary information,\nwhich guides the highlight removal network to pay more attention to the high-\nlight regions. In addition, text-related loss was introduced to improve the recov-ering of texts. Our source code and datasets are available at https://github.com/\nweizequan/TASHR .\nIn the future, we would like to construct lager and richer dataset to promote\nthe development of related research. We would also like to design more eﬀective\nnetworks and loss functions. Furthermore, an exciting research problem is tosuggest more complete and exact visual quality measurements.\nAcknowledgments. This work was supported by the National Key R&D Program\nof China (2019YFB2204104), and the National Natural Science Foundation of China(Nos. 6210071649, 62172415 and 61772523).\nReferences\n1. Chineseocr: Ctpn plus densenet plus ctc based chinese ocr. https://github.com/\nYCG09/chinese ocr. Accessed 30 Apr 2021\n2. Paddleocr: Awesome multilingual ocr toolkits based on paddlepaddle. https://\ngithub.com/PaddlePaddle/PaddleOCR. Accessed 30 Apr 2021\n3. Arbel´ aez, P., Maire, M., Fowlkes, C., Malik, J.: Contour detection and hierarchi-\ncal image segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 33(5), 898–916\n(2011)\n4. Arnold, M., Ghosh, A., Ameling, S., Lacey, G.: Automatic segmentation and\ninpainting of specular highlights for endoscopic imaging. J. Image Video Process.\n(2010)\n5. Bertalmio, M., Sapiro, G., Caselles, V., Ballester, C.: Image inpainting. In: ACM\nSIGGRAPH, pp. 417–424 (2000)', '126 S. Hou et al.\n6. Fleyeh, H.: Shadow and highlight invariant colour segmentation algorithm for traf-\nﬁc signs. In: IEEE Conference on Cybernetics and Intelligent Systems (2006)\n7. Fu, G., Zhang, Q., Song, C., Lin, Q., Xiao, C.: Specular highlight removal for\nreal-world images. Comput. Graph. Forum 38(7), 253–263 (2019)\n8. Funke, I., Bodenstedt, S., Riediger, C., Weitz, J., Speidel, S.: Generative adversarial\nnetworks for specular highlight removal in endoscopic images. In: Medical Imaging2018: Image-Guided Procedures, Robotic Interventions, and Modeling, vol. 10576,\npp. 8–16 (2018)\n9. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neu-\nral networks. In: IEEE Conference on Computer Vision and Pattern Recognition,\npp. 2414–2423 (2016)\n10. Guo, X., Chen, Z., Li, S., Yang, Y., Yu, J.: Deep eyes: binocular depth-from-focus\non focal stack pairs. In: Chinese Conference on Pattern Recognition and Computer\nVision, pp. 353–365 (2019)\n11. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and\nsuper-resolution. In: Proceedings of the European Conference on Computer Vision,\npp. 694–711 (2016)\n12. Khanian, M., Boroujerdi, A.S., Breuß, M.: Photometric stereo for strong specular\nhighlights. arXiv preprint arXiv:1709.01357 (2017)\n13. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. In: Interna-\ntional Conference on Learning Representations (2015)\n14. Lee, S.T., Yoon, T.H., Kim, K.S., Kim, K.D., Park, W.: Removal of specular reﬂec-\ntions in tooth color image by perceptron neural nets. In: International Conferenceon Signal Processing Systems, vol. 1, pp. V1–285-V1-289 (2010)\n15. Lin, J., El Amine Seddik, M., Tamaazousti, M., Tamaazousti, Y., Bartoli, A.: Deep\nmulti-class adversarial specularity removal. In: Image Analysis, pp. 3–15 (2019)\n16. Long, S., He, X., Yao, C.: Scene text detection and recognition: The deep learning\nera. Int. J. Comput. Vis. 129(1), 161–184 (2021)\n17. Lucas, S., Panaretos, A., Sosa, L., Tang, A., Wong, S., Young, R.: Icdar 2003\nrobust reading competitions. In: International Conference on Document Analysis\nand Recognition, pp. 682–687 (2003)\n18. Meslouhi, O.E., Kardouchi, M., Allali, H., Gadi, T., Benkaddour, Y.A.: Automatic\ndetection and inpainting of specular reﬂections for colposcopic images. Central Eur.\nJ. Comput. Sci. 1(2011)\n19. Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for\ngenerative adversarial networks. In: International Conference on Learning Repre-\nsentations (2018)\n20. Muhammad, S., Dailey, M.N., Farooq, M., Majeed, M.F., Ekpanyapong, M.: Spec-\nnet and spec-cgan: Deep learning models for specularity removal from faces. Image\nVis. Comput. 93, 103823 (2020)\n21. Ortiz, F., Torres, F.: A new inpainting method for highlights elimination by colour\nmorphology. In: International Conference on Pattern Recognition and Image Anal-\nysis, pp. 368–376 (2005)\n22. Park, J.W., Lee, K.H.: Inpainting highlights using color line projection. IEICE\nTrans. Inf. Syst. 90(1), 250–257 (2007)\n23. Ren, W., Tian, J., Tang, Y.: Specular reﬂection separation with color-lines con-\nstraint. IEEE Trans. Image Process. 26(5), 2327–2337 (2017)\n24. Shafer, S.A.: Using color to separate reﬂection components. Color. Res. Appl.\n10(4), 210–218 (1985)', 'Text-Aware Single Image Specular Highlight Removal 127\n25. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition. In: International Conference on Learning Representations\n(2015)\n26. Son, M., Lee, Y., Chang, H.S.: Toward specular removal from natural images based\non statistical reﬂection models. IEEE Trans. Image Process. (2020)\n27. Tan, P., Lin, S., Quan, L., Shum, H.Y.: Highlight removal by illumination-\nconstrained inpainting. In: IEEE International Conference on Computer Vision,\npp. 164–169 (2003)\n28. Tan, R.T., Nishino, K., Ikeuchi, K.: Separating reﬂection components based on\nchromaticity and noise analysis. IEEE Trans. Pattern Anal. Mach. Intell. 26(10),\n1373–1379 (2004)\n29. Tian, Z., Huang, W., He, T., He, P., Qiao, Y.: Detecting text in natural image\nwith connectionist text proposal network. In: European Conference on Computer\nVision, pp. 56–72 (2016)\n30. Wang, T.C., Efros, A.A., Ramamoorthi, R.: Occlusion-aware depth estimation\nusing light-ﬁeld cameras. In: IEEE International Conference on Computer Vision,\npp. 3487–3495 (2015)\n31. Wang, W., Deng, R., Li, L., Xu, X.: Image aesthetic assessment based on perception\nconsistency. In: Chinese Conference on Pattern Recognition and Computer Vision,\npp. 303–315 (2019)\n32. Yang, Q., Wang, S., Ahuja, N.: Real-time specular highlight removal using bilateral\nﬁltering. In: European Conference on Computer Vision, pp. 87–100 (2010)\n33. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In: IEEE International Conference on\nComputer Vision, pp. 2242–2251 (2017)', 'Minimizing Wasserstein-1 Distance by\nQuantile Regression for GANs Model\nYingying Chen1,2, Xinwen Hou1(B), and Yu Liu1\n1Institute of Automation, Chinese Academy of Sciences, Beijing, China\n{chenyingying2018,xinwen.hou,yu.liu }@ia.ac.cn\n2School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences,\nBeijing, China\nAbstract. In recent years, Generative Adversarial Nets (GANs) as a\nkind of deep generative model has become a research focus. As a rep-resentative work of GANs model, Wasserstein GAN involves earth mov-\ning distance which can be applied to un-overlapped probability distribu-\ntions, and promoted the stability of model training. However, the alterna-tive optimization of the discriminative network in the Wasserstein GAN\nmodel makes it impossible to accurately calculate the Wasserstein-1 dis-\ntance between the real data distribution and generated data distribution.It may result in unstable training sometimes. So we propose minimiz-\ning the Wasseerstain-1 distance by Quantile Regression algorithm which\nworks well on minimizing the Wasserstein-1 distance between the real datascore distribution and the generated data score distribution. We named\nour method QR-GAN. QR-GAN involves the information about data dis-\ntribution instead of distinguishing data between fake and real in originalGANs model. Meanwhile, QR-GAN model adds the real data information\nto the updating of generator network, which makes the parameter updat-\ning of generator more eﬀective. In order to verify the eﬀectiveness of ourmethod, we perform experiments on MNIST, CIFAR-10, STL-10, LSUN-\nTower and the results demonstrate superiority of our method.\nKeywords: GANs model\n·Wasserstein-1 distance ·Quantile\nregression\n1 Introduction\nIn the past decade, deep learning has triggered a research boom of artiﬁcial\nintelligence technology. Deep learning models can be divided into two categories:\ndeep generative model and deep discriminative model. Compare with the great\nsuccess of deep discriminative model in computer vision, natural language pro-\ncessing and other ﬁelds, deep generative model has a slightly inferior prestigedue to the high-dimensional probability model which is diﬃcult to calculate.\nHowever, since Goodﬂow et al. propose the GANs model [ 1] in 2014, the deep\ngenerative model has received more and more attention.\nY. Chen—Student.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 128–139, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_11', 'Minimizing Wasserstein-1 Distance by Quantile Regression for GANs Model 129\nThe GANs model avoids the diﬃculty of calculating the likelihood function in\nexplicit density-based method. But there are some inherent shortcomings of themodel. Training instability and mode collapsing are two basic problems of GANs\nmodel. Training instability involves the generator and discriminator oscillating\nrather than converging to a ﬁxed point in training. Mode collapsing means thatthe generator tends to produce only a single sample or a small family of very\nsimilar samples.\nGANs can be viewed as a minimax two-player game. The mechanism revealed\nby the game is to make the generated distribution approximate to the real dis-\ntribution. The original GANs model measures the gap between the two distri-\nbutions by Jensen-Shannon (JS) divergence. But JS divergence is a constantwhen the two distributions don’t intersect. It will cause the gradient vanish-\ning problem. In addition, the improvement to alleviate the gradient vanishing\nis equipped with an unreasonable objective function, which will lead to training\ninstability [ 2]. Arjovsky et al. propose Wasserstein GAN model [ 3]. They intro-\nduce Wasserstein-1 distance into GANs model which promoted the stability ofGANs model training. It makes Wasserstein GAN model become a representative\nimprovement baseline of GANs model.\nHowever Ostrovski et al. state that the approximate error caused by incom-\nplete solution may aggravate the instability of training [ 4]. Fortunately the Quan-\ntile Regression (QR) algorithm can minimize the Wasserstein-1 distance between\nthe distributions of two one-dimensional random variables. It serves well in distri-butional reinforcement learning in recent years [ 5]. However the data generated\nby the GANs model are mostly high-dimensional data. Therefore, we model\nthe discriminative network as a scalar distribution mapping which can reﬂectthe characteristics of data distribution. It can capture more data distribution\ncharacteristics than single valued mapping. Then the Quantile Regression algo-\nrithm is introduced into the GANs model to minimize the Wasserstein-1 distancebetween the generated data score distribution and the real data score distribu-\ntion. In short, we provide a new idea for minimizing Wasserstein-1 distance\nin GANs model. Comparative experiments on MNIST, CIFAR-10, STL-10 and\nLSUN-Tower datasets demonstrate the eﬀectiveness of our method and superi-\nority to the Wasserstein GAN model.\n2 Related Work\nIn the past few years, the GANs model has achieved great success in datageneration, image restoration, image segmentation, image super-resolution and\nother ﬁelds. It is due to the ﬂexibility and broad application prospect of GANs\nmodel. Improvement and extension works based on GANs model can be dividedinto three categories: improvement schemes based on model structure, improve-\nment schemes based on objective function, and schemes triggered by extended\napplication.\nResearchers have evolved many variations of the GANs model by improv-\ning the model structure. Mirza et al. propose Conditional-GAN model[ 6].', '130 Y. Chen et al.\nBy conditioning the model on additional information, it is possible to guide\nthe data generation process. Such conditioning could be based on class labels,on some part of data for inpainting, or even on data from diﬀerent modality.\nZhang et al. propose Self-Attention GAN [ 7] by introducing the attention mech-\nanism into the GANs model. It allows attention-driven, long-range dependencymodeling for image generation tasks, such as the asymmetrical distribution of\neyes in human face generation. Ledig et al. propose a generative adversarial net-\nwork for image super resolution (SR-GAN) [ 8]. It is the ﬁrst framework capable\nof inferring photo-realistic natural images for 4 ×upscaling factors. Zhu et al.\npropose Cycle-GAN [ 9] which combines dual structure and cycle-loss. It demon-\nstrates the superiority of Cycle-GAN model in collection style transfer, objecttransﬁguration, season transfer, photo enhancement.\nThe representative work of the improvement scheme based on objective func-\ntion is Wasserstein GAN model. Arjovsky et al. ﬁnd that the JS divergence corre-\nsponding to the objective function of the original GANs model is not an excellent\ndistance metric [ 2]. It can not provide good gradient to guide the parameters\nupdating at the initial stage of GANs model training. Moreover, the scheme\nfor improving the above problem with a unstable objective function, which will\nresult in training breakdown. In order to promote the stability of the GANsmodel training, they introduced Wasserstein-1 distance into the GANs model\nby dual transformation. Arjovsky et al. verify the eﬀectiveness of Wasserstein\nGAN in promoting training stability by directly displaying visual renderings ofthe generated images.\nWasserstein GAN approximately minimizes Wasserstein-1 distance, and\nintroduces Lipschitz constraint at the same time. Arjovsky et al. clip the dis-criminative network weight parameters to ensure the Lipschitz constraint. It is\ntoo simple to make the critic network satisfy Lipschitz constraint. Lipschitz con-\nstraint actually restricts the norm of discriminative network gradient to be lessthan a constant. Gulrajani et al. propose gradient penalty for Wasserstein GAN\n(WGAN-GP) [ 10]. It enforces a soft version of the constraint with a penalty on\nthe gradient norm for the random samples. In addition, Miyato et al. propose\nSpectrally Normalized GAN (SN-GAN) [ 11]. It controls the Lipschitz constant\nof the discriminator function by literally constraining the spectral norm of eachlayer. The improved scheme to optimize Lipschitz constraint does not explore\nthe implementation of Wasserstein-1 distance, so the performance improvement\nto Generation Adversarial Nets model is limited to the set range of the originalWGAN framework.\nIn recent years, Quantile Regression (QR) algorithm performs well in the\nﬁeld of distributional reinforcement learning. It can minimize the Wasserstein-1distance between two distributions. Dabney et al. put forward the concept of\ndistributional reinforcement learning [ 12]. It learns value distribution informa-\ntion and realizes value distribution sampling. QR algorithm is used to minimizethe Wasserstein-1 distance between the model distribution and the target dis-\ntribution. Some researchers have tried to applied Quantile Regression algorithm\nto GANs models. But there is some deviation in the understanding of QR algo-rithm, which ﬁxes the target distribution quantile to a certain number. In this', 'Minimizing Wasserstein-1 Distance by Quantile Regression for GANs Model 131\npaper, QR regression is used to minimize the Wasserstein-1 distance between\nthe score distribution of generated data and that of real data, completely basedon the principle of QR regression.\n3 Background\n3.1 Wasserstein GAN\nThe Original GANs objective function is:\nmin\nGmax\nDV(G,D)= Ex∼pr(x)[logD(x)]\n+Ex∼pg(x)[log(1 −D(x))](1)\nwhere pr(x) is the real data distribution and pg(x) is the generated data distri-\nbution with x=G(z);zis the input to generator obeying a given distribution\npz(z);D(x) is the output of discriminator, represents the probability that x\ncomes from the real data rather than fake data.\nGANs involves JS divergence to measure the distance between pr(x)a n d\npg(x). Arjovsky et al. introduce Wasserstein-1 distance into GANs model to\npromoted the stability of model training. The Wasserstein distance is a metric\ndeﬁned by the concept of optimal transport:\nWc[p(x),q(y)] = inf\nζ∈/producttext(p(x),q(y))E(x,y)∈ζ[d(x,y)] (2)\nwhere p(x) is the target distribution; q(y) is an approximate constructed dis-\ntribution;/producttext(p(x),q(y)) denotes the set of all joint distributions ζ(x,y), whose\nmarginals are respectively p(x)a n d q(y);d(x,y) is the cost of moving yintox.\nWhen d(x,y)=||x−y||1,(2) denotes Wasserstein-1 distance:\nW1[p(x),q(y)] = inf\nζ∈/producttext(p(x),q(y))E(x,y)∈ζ[||x−y||1] (3)\nThe inﬁmum in ( 3) is highly intractable. Arjovsky et al. use Kantorovich-\nRubinstein duality to convert the inﬁmum problem into the following form:\nW1[p(x),q(y)] = sup\n||f||L≤KEx∼p(x)[f(x)]−Ex∼q(x)[f(x)] (4)\nwhere the supremum is over all the K-Lipschitz functions f:X→ R.A n d\nthen maximize ( 4) by searching some win the parameterized family functions\n{fw}w∈Wto approximately solve the supremum:\nmax\nw∈WEx∼pr(x)[fw(x)]−Ez∼p(z)[fw(gθ(z))] (5)\nwhere fw(x) denotes the discriminator mapping function which satisﬁes K-\nLipschitz constraint for some K; pr(x) is the target real data distribution; gθ(z)\ndenotes the generator mapping function parameterized by θ;fw(x)\nWasserstein GAN model still faces limitations. Due to insuﬃcient training\nof the discriminator or limitations of the function approximator, the gradient\ndirection produced by Wasserstein GAN model can be arbitrarily bad.', '132 Y. Chen et al.\n3.2 Quantile Regression\nThe original deﬁnition of Wasserstein distance is highly intractable due to the\ninfmum. But in some special cases, Wasserstein distance can be very concise.\nBickel et al. show that the lower bound in ( 4) is attainable [ 13]; In addition,\nMajor shows that the Wasserstein-1 distance of one-dimensional random vari-\nables is [ 14]:\nW1[X,Y]=/integraldisplay1\n0|F−1\n1(t)−F−1\n2(t)|dt (6)\nwhere F−1\n1(t)a n d F−1\n2(t) are the inverse function of the cumulative distribution\nfunction F1(x)a n d F2(x).F(x)a n d F−1(t) satisﬁes Eq. ( 7), for which xis also\ncalled the tquantile of the random variable X.\nF(x)=p(X≤x)\nF−1(t)=i n f {x∈R:t≤F(x)}(7)\nWhen the construction distribution q(y) modeled by a mixed Dirac distribu-\ntion, the Wasserstein-1 distance between XandYis shown as:\nW1[X,Y z]=N/summationdisplay\ni=1/integraldisplayτi\nτi−1|F−1\n1(t)−zi|dt (8)\nwhere ziis the sample of a mixed Dirac distribution. The probability density\nfunction of mixed Dirac distribution is:\np(z)=1\nNN/summationdisplay\ni=1δ(zi) (9)\nThe values for {z1,z2,...,z N}that minimize W1(X,Y z) are given by zi=\nF−1\n1(ˆτi). ˆτiis the quantile midpoints can be denoted by ˆ τi=τi−1+τi\n2. Unfor-\ntunately, quantile parametrization leads to biased gradients. However, Quantile\nRegression is an unbiased stochastic approximation method for the quantile func-tion of distributions.\nFor random variable X, and a given probability τ, the value of the quantile\nfunction x\nτ=F−1\nX(τ) can be characterized as the minimizer of the Quantile\nRegression loss:\nxτ= arg min\nz⎧\n⎨\n⎩/summationdisplay\nxk≥zθτ|xk−zθ|+/summationdisplay\nxk<zθ(1−τ)|xk−zθ|⎫\n⎬\n⎭(10)\nwhere xkis the the target distribution sample; zθis the quantile regression\nmapping with adjustable parameter θ,a n dz∗=xτ. More generally, for obtaining\nall quantile values {z1\nθ,z2\nθ,...,zN\nθ}that minimize the Wasserstein-1 distance, the\nobjective function is:\nN/summationdisplay\ni=1Ex∈X[ρτi(x−zi\nθ)] (11)\nwhere ρτi(u)=u(τ−δ(u<0)).', 'Minimizing Wasserstein-1 Distance by Quantile Regression for GANs Model 133\n4O u r M e t h o d\nThe Quantile Regression algorithm provides an unbiased stochastic approxima-\ntion of quantile value. By setting the probability {ˆτi}N\ni=1, the solutions of QR loss\nwill be the quantile values that minimize the Wasserstein-1 distance. However\nit is only suitable for one-dimension random variable distribution approxima-\ntion. The data generated by the GANs model is usually high-dimension data.Therefore, we consider to map the high-dimension data xto one-dimension score\ndistribution through the feature mapping network f(x). It is a one-dimensional\nscalar distribution that can reveal the characteristics of high-dimension data.\nThe Quantile Regression algorithm can be used to minimize the Wasserstein-1\ndistance between real data score distribution and generated data score distribu-tion.\nN/summationdisplay\ni=1Exr∼pr[ρτi(f(xr,τj)−f(xg,τi))] (12)\nwhere pris the real data distribution; xris the sample of real data; xgis the\nsample of generated data with xg=Gθ(z),z∼p(z);f(xr,τj) is the score of\nxrindexed by τj;f(xg,τi) is the score of xgindex by τi; By optimizing ( 12),\nthe score mapping network f(x) outputs the real score distribution τiquantile\nindexed indexed by τj. In a more general case, the real score distribution indexed\nby{τj}M\nj=1forms M true score distributions. Then the objective function for\ngenerator is :\nmin\nθ∼ΘExg∼pgM/summationdisplay\nj=1N/summationdisplay\ni=1Exr∼pr[ρτi(f(xr,τj)−f(xg,τi))] (13)\nwhere pgis the generated data distribution. The parameters θof the generator\nGθ(z) is updated by the minimization ( 13) so that the score distribution of the\ngenerated data approaches the score distribution of the real data. In fact, it\nis the optimization of Quantile Regression loss that minimize the Wasserstein-\n1 distance between two score distributions. ( 13) shows that QR-GAN model\ninvolves real data information when updating of generator network, which makesthe parameter updating of generator more eﬀective.\nFor the updating score mapping network f\nw(xr,τj), we hope it can distin-\nguish the real data score distribution from the generated data score distribution.And in order to provide a relatively tight constraint, we train the score mapping\nnetwork f\nw(xr,τj) by maximizing the following formula:\nmax\nw∼WExg∼pgN/summationdisplay\ni=1Exr∼pr[ρτi(fw(xr,τi)−fw(xg,τi))] (14)\nThat is, under the same index τi, the larger the diﬀerence between correspond-\ning score distribution quantiles, the better. In order to make the score mappingnetwork obtain the distribution characteristics eﬀectively, we limit the parame-\nters of the score mapping network to [ −c, c]. It avoids ineﬃcient optimization\nby simply maximizing the output of f\nw(·,·).', '134 Y. Chen et al.\nIn the above optimization process, Quantile Regression algorithm is used to\nminimize the wasserstein-1 distance between the real score distribution and thegenerated score distribution. The parameters θof the generator are updated\nby the gradient comes from QR regression loss function. The parameters wof\nthe score mapping network f\nw(x,τ) are updated by maximizing the weighted\ndeviation between the two score distributions. The main purpose of fw(x,τ)i s\nto obtain characteristics of the data by mapping the high-dimension data to\na scalar distribution. Compare with the discriminator network in the originalGANs model, the score mapping network considers the information of data dis-\ntribution. Instead of true and false binary classiﬁcation.\nFor convenience, we call our method QR-GAN. The structure of QR-GAN\nmodel and Wasserstein GAN model is shown in the Fig. 1. The QR-GAN model\nmaintains the basic structure of GANs model. The main diﬀerence is the struc-\nture of score mapping network. ϕ:X→ R\ndis the convolutional layer mapping\nfor extracting visual features of the image. ψ:[ 0,1]→ Rdcan map the prob-\nability τto a d-dimensional representation as shown in Eq. ( 15) with T= 16.\nϕ(x)a n d ψ(τ) are associated by element-by-element product (Hadama product).\nThen all the features are combined through the full connection layer f:Rd→ R\nwithd= 4096. The output of the ﬁnal score mapping network can be expressed\nas Eq. ( 16).\nψj(τ): =ReLU (T−1/summationdisplay\ni=0cos(πiτ)wij+bj) (15)\nfw(x,τ)=f(ϕ(x)⊙ψ(τ)) (16)\n(a) The structure of Wasserstein GAN (b) The structure of QR-GAN\nFig. 1. The structure of QR-GAN model and Wasserstein GAN model', 'Minimizing Wasserstein-1 Distance by Quantile Regression for GANs Model 135\n5 Experiment\n5.1 Basic Setting\nIn order to verify the eﬀectiveness of the QR-GAN model, we conduct a series of\ncomparative experiments with Wasserstein GAN as the baseline model. In each\ntraining iteration, we update the score distribution mapping network ntimes\nﬁrst, and then update the generated network once. The speciﬁc operation isshown as the Algorithm 1. Where ηis the learning rate; [ −c,c] represents the\nclipping interval of network weight parameters of score distribution mapping; m\nis mini-batch size of training data. Except for special instructions, the experi-\nments use Adam optimizer, in which the super parameter β\n1=0.5,β2=0.999.\nAlgorithm 1. QR-GAN.\nη=0.00005, c=0.05,m= 64, n=5 ,N=M=1 6\n1:while Θhas not converged do\n2: fort=0, ..., n do\n3: Sample {xi\nr}m\ni=1∼pr(x) a batch of the real data.\n4: Sample {zi}m\ni=1∼p(z) a batch of prior samples, and {xi\ng}m\ni=1={G(zi)}m\ni=1.\n5: gw←∇ w/bracketleftBig\n1\nm/summationtextm\nk=1/bracketleftBig/summationtextN\ni=11\nm/summationtextm\nh=1/bracketleftbig\nρτi/parenleftbig\nf(xh\nr,τi)−f(xk\ng,τi)/parenrightbig/bracketrightbig/bracketrightBig/bracketrightBig\n6: w←w+η·Adam (w,g w)\n7: w←clip(w,−c, c)\n8: end for\n9: Sample {zi}m\ni=1∼p(z) a batch of prior samples, and {xi\ng}m\ni=1={G(zi)}m\ni=1.\n10: gθ←∇ θ/bracketleftBig\n1\nm/summationtextm\nk=1/bracketleftBig/summationtextM\nj=1/bracketleftBig/summationtextN\ni=11\nm/summationtextm\nh=1/bracketleftbig\nρτi/parenleftbig\nf(xh\nr,τj)−f(xk\ng,τi)/parenrightbig/bracketrightbig/bracketrightBig/bracketrightBig/bracketrightBig\n11: θ←θ−η·Adam (θ,gθ)\n12: end while\n5.2 IS and FID Criteria Experiment\nWe train model on a range of datasets including MNIST, CIFAR-10, STL-10,\nLSUN-Tower. Figure 2shows the generated data by QR-GAN model and Wasser-\nstein GAN model under MNIST dataset, with a resolution of 32 ×32. We calcu-\nlate Inception Score (IS) and Fr´ echet Inception Distance (FID) to quantitatively\nevaluate the generated images quality. The IS indicator measures the quality anddiversity of the generated image at the same time. The greater the value, the\nbetter. The FID indicator calculates the Fr´ echet Distance between the generated\ndata and the real data statistics. As a distance measure, the smaller the value,the better. In the following experiments, the sample number of IS calculated is\n100,000, and that of FID calculated is 50,000.\nFigure 3shows the IS and FID curves of QR-GAN model and Wasserstein\nGAN model on CIFAR-10, STL-10 and LSUN-Tower datasets. The ﬁnal IS and\nFID indicator values of the QR-GAN model are better than that of the Wasser-\nstein GAN baseline model on all three datasets. However, in the early stage of', '136 Y. Chen et al.\n(a) Images generated by Wasserstein GAN (b) Images generated by QR-GAN\nFig. 2. Handwritten digital images generated by QR-GAN and Wasserstein GAN\nmodels.\ntraining (the number of generator iteration is less than 10000), the performance\nof QR-GAN model is poor. We think the score mapping network needs more\ndata to learn the distribution characteristics, the score mapping network withpoor capability in the early stage of training. It could not provide accurate infor-\nmation for the updating of the generative network quickly. With the progress of\ntraining, the ability of score mapping network is gradually enhanced, and theadvantages of learning data distribution are highlighted. QR-GAN model shows\nmore superiority than Wasserstein GAN model with single-valued mapping.\n(a) IS on CIFAR-10 dataset (b) FID on CIFAR-10 dataset\nFig. 3. IS and FID indicators curves of QR-GAN model and Wasserstein GAN model\non CIFAR-10, STL-10 and LSUN-Tower datasets.\nThe maximum value of IS indicator and the minimum value of FID indicator\nin the above experiments are listed in the Table 1. Under diﬀerent hardware\nenvironment and parameter setting, IS and FID values are diﬀerent. We do not\npursue the absolute value of IS and FID, but mainly focus on the improvementof the two indicators on our model relative to the baseline model under the', 'Minimizing Wasserstein-1 Distance by Quantile Regression for GANs Model 137\nsame experimental environment. According to the table, the IS indicator of QR-\nGAN model improves by an average of 7% over the Wasserstein GAN model.The FID indicator of QR-GAN model improves by an average of 17% over the\nWasserstein GAN model. Based on the promotion rate, we believe that the\nquality and diversity of images generated by QR-GAN model are better thanbaseline Wasserstein GAN model.\nTable 1. IS and FID indicator of QR-GAN model and Wasserstein GAN model\nModel Dataset\nCIFAR-10 STL-10 LSUN-Tower\nIS Wasserstein GAN 4.15 4.04 3.57\nQR-GAN 4.42 4.40 3.77\nFID Wasserstein GAN 43.38 47.43 43.15\nQR-GAN 38.64 40.90 30.75\n5.3 Adam and Rmsprob Experiment\nIn this section, we use Adam and RMSProp algorithm to train the model on\nSTL-10 dataset, and calculate the IS and FID indicator of the corresponding\nmodel. We believe that an algorithm with good performance should have certain\nadaptability. The gap between model performance under diﬀerent optimizationalgorithms should be small. Figure 4is the IS and FID indicators curves under\ndiﬀerent optimization algorithms. It shows the performance of QR-GAN model\nis better than Wasserstein GAN model. And the performance curves diﬀerencebetween Adam optimizer and RMSProp optimizer of QR-GAN model is smaller\nthan that of Wasserstein GAN model.\n(a) IS on STL-10 dataset (b) FID on STL-10 dataset\nFig. 4. IS and FID indicators curves on STL-10 dataset with diﬀerent optimization\nalgorithms', '138 Y. Chen et al.\n5.4 Diﬀerent Quantile Initialization Experiment\nIn the experimental process, we ﬁnd that diﬀerent quantile setting would aﬀect\nthe performance of QR-GAN model. Figure 5shows the IS and FID indicator\ncurves of CIFAR-10 data generated by the model under two diﬀerent quantile\ninitialization. There are diﬀerences in the performance of the QR-GAN models\nunder two initializations, but both of them are better than the baseline model.Previously, when we demonstrate the eﬀectiveness of QR-GAN model, we do not\nselect the curve with the best performance in the experiment. To a certain extent,\nit avoids over ﬁtting phenomenon caused by overtuning parameters. Yang et al.use parameterized τto optimize the problems caused by τvalue initialization in\ndistributional reinforcement learning [ 15]. The improvement of QR-GAN model\nperformance by parameterized τwill be one of the contents for further research\nin the future.\n(a) IS on CIFAR-10 dataset (b) FID on CIFAR-10 dataset\nFig. 5. IS and FID indicators curves on CIFAR-10 dataset with diﬀerent quantile\nsetting\n6 Conclusion\nThis paper starts from the problem of training instability caused by incomplete\noptimization of Wasserstein-1 distance in Wasserstein GAN model. Then weﬁnd a new way to minimize the Wasserstein-1 distance in the GANs model by\nextending the Quantile Regression algorithm to the GANs model. Experiments\non MNIST,CIFAR-10,STL-10, and LSUN-Tower demonstrate superiority of ourmethod in generated data quality and multi-optimization algorithm stability. In\nthe future work, we will do some research on parameterized adjustment of τto\nimprove the QR-GAN model.\nAcknowledgments. We would like to acknowledgment the Strategic Priority\nResearch Program of Chinese Academy of Sciences (XDA27000000). This paper issupported by it.', 'Minimizing Wasserstein-1 Distance by Quantile Regression for GANs Model 139\nReferences\n1. Goodfellow, I.: Generative adversarial nets. In: Advances in Neural Information\nProcessing Systems 27 NIPS, pp. 2672–2680, Curran Associates Inc., (2014)\n2. Arjovsky, M., Bottou, L.: Towards principled methods for training generative\nadversarial networks. In: ICLR (2017)\n3. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein gan. ArXiv:1701.07875 (2017)\n4. Ostrovski, G., Dabney, W., Munos, R.: Autoregressive quantile networks for gener-\native modeling. In: International Conference on Machine Learning, pp. 3933–3942\n(2018)\n5. Dabney, W., Rowland, M., Bellemare, M.G., Munos, R.: Distributional reinforce-\nment learning with quantile regression. In: AAAI (2018)\n6. Mirza, M., Osindero, S.: Conditional generative adversarial nets. arXiv preprint\narXiv:1411.1784 (2014)\n7. Zhang, H., Goodfellow, I.J., Metaxas, D.N., Odena, A.: Self-attention generative\nadversarial networks. In: International Conference on Machine Learning, pp. 7354–7363 (2018)\n8. Ledig, C., et al.: Photo-realistic single image super-resolution using a generative\nadversarial network. In: Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 4681–4690 (2017)\n9. Zhu, J.-Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In: Proceedings of the IEEE Interna-tional Conference on Computer Vision, pp. 2223–2232 (2017)\n10. Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.: Improved\ntraining of wasserstein gans. In: NIPS 2017 Proceedings of the 31st InternationalConference on Neural Information Processing Systems, vol. 30, pp. 5769–5779\n(2017)\n11. Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for\ngenerative adversarial networks. arXiv preprint arXiv:1802.05957 (2018)\n12. Bellemare, M.G., Dabney, W., Munos, R.: A distributional perspective on rein-\nforcement learning. arXiv preprint arXiv:1707.06887 (2017)\n13. Bickel, P.J., Freedman, D.A.: Some asymptotic theory for the bootstrap. Ann.\nStatist. 9(6), 1196–1217 (1981)\n14. Major, P.: On the invariance principle for sums of independent identically dis-\ntributed random variables. J. Multivariate Anal. 8(4), 487–517 (1978)\n15. Yang, D., Zhao, L., Lin, Z., Qin, T., Bian, J., Liu, T.-Y.: Fully parameterized\nquantile function for distributional reinforcement learning. In: Advances in NeuralInformation Processing Systems, vol. 32, pp. 6193–6202 (2019)', 'A Competition of Shape and Texture Bias\nby Multi-view Image Representation\nLingwei Kong, Jianzong Wang(B), Zhangcheng Huang, and Jing Xiao\nPing An Technology (Shenzhen) Co., Ltd., Shenzhen, China\n{konglingwei630,wangjianzong347,huangzhangcheng624,\nxiaojing661 }@pingan.com.cn\nAbstract. There are mainly two views on the interpretation of high eﬃ-\nciency of Convolutional Neural Networks (CNNs) for the task of image\nclassiﬁcation: shape bias and texture bias. This is critical to the causal-\nity and reliability of CNN models in real applications. In this work, wetry to explore the power of CNNs and reconcile the hypothesis contradic-\ntion of CNNs from a multi-view image representation. Firstly, we assume\nan image is generated from object shape representation, object texturerepresentation, and background information. Secondly, we segment and\nrecombine the object shape, texture and image background through two\nlosses: image reconstructed loss and feature discrepancy loss. Finally,the classiﬁcation loss is combined by shape, texture and background\ncontributions weighted by multi-view features. Comprehensive experi-\nments conducted on real-world datasets show that, ﬁrst, CNNs generallydo not have texture or shape bias, which change with the internal bias\nof data; second, CNNs are learning knowledge in a lazy way, i.e., high\nlevel knowledge is learned only if low level knowledge does not satisfythe task requirements. Our ﬁndings might beneﬁt the interpretability of\nCNNs and provide insight of more robust design.\nKeywords: CNNs\n·Multi-view ·Explainability ·Image representation\n1 Introduction\nver the last two decades, Convolutional Neural Networks (CNNs) have shown\nsuccessful performance in diﬀerent domains such as visual object recognition [ 36]\nand semantic segmentation [ 12]. This success is often attributed to its powerful\nability of feature representation learned from original data to intricate high-\ndimensional features, which can be grouped into two categories: object shapes\nand object textures. However, which plays a more important role for CNNs objectrecognition? Which are shape feature or texture information in the complicated\nhigh-dimensional numerical features? The problems have attracted the attention\nof lots of researchers and related to the explainability of CNNs [ 1,29,34], which\nare important to help us understand the CNNs decisions and causality.\nThis paper is supported by National Key Research and Development Program of China\nunder grant No. 2018YFB0204403, No. 2017YFB1401202 and No. 2018YFB1003500.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 140–151, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_12', 'Multi-view Image Representation on Shape and Texture Bias 141\nFig. 1. Classiﬁcation by a standard VGG-16 on diﬀerent situations. (1.a) and (2.a)\nare the original images. (1.b) and (2.b) are generated by ﬁlling the special texture into\neach others’ object. (1.a) and (2.a) are ﬁlled with the special textures to generate (1.c),(2.c) and (1.d) and (2.d). The diﬀerence is that (1.d) and (2.d) contains background\ncontent. As is shown, neither Shape hypothesis orTexture hypothesis is rigorous.\nExisting explanations can be broadly categorized into two types. Shape\nhypothesis holds that CNNs combine low-level features to increasingly complex\nshapes until the object can be readily classiﬁed. This hypothesis indicates that\nshape dominates and is supported by a number of research ﬁndings [ 16,18,24,35].\nTexture hypothesis considers that local textures already provide suﬃcient infor-\nmation for object recognition and play a more important role than object shapes.\nThe ﬁndings [ 2,4,8,11] have provided strong evidence to support the texture\nhypothesis . These two hypotheses seem to be contradictory, because they all\nhave their own theoretical or experimental support.\nDespite a great quantity of related studies, few articulate precisely the rela-\ntionship between shape hypothesis and texture hypothesis . Existing discussions\non the bias of CNNs are either inadequate or not rigorous. For example, Geirhos\net al. [ 11] supported that object textures are more important than global object\nshapes for CNN object recognition. The method trained on ImageNet, but tested\non Stylized-ImageNet (generated by AdaIN style transfer [ 14]), which was con-\ntrary to the basic data hypothesis (independently and identically distributed). Itwas actually a domain transformation or domain adaptation problem. Incorpo-\nrating Stylized-ImageNet in the training data must improve object classiﬁcation\nperformance, which not only conformed to the basic data hypothesis, but alsoaugmented data. The results shown in Fig. 1also validates our point of view as\nan example. With the continuous increase of texture area, the category of an\nimage will belong to the category of texture. It’s not the category of texturefrom the beginning, which does not seem to imply that CNNs have partiality for\ntexture.\nThe importance of shape and texture information is also highly dependent on\nthe application scenarios, such as medical imaging analysis. CNNs have proved\nthe performance and accuracy in medical imaging [ 21], but in details, some\nlesions are easily detected and segmented given texture information while the\nothers might rely more on shape feature. Interpretability determines whether\npeople can trust the artiﬁcial intelligence to make a diagnosis or even auxiliarytreatment.', '142 L. Kong et al.\nIn this paper, we propose a new bias-measure method to reconcile the hypoth-\nesis contradiction of CNNs from a multi-view representation [ 30]. The structure\nand overview are shown in Fig. 2and Fig. 3. Firstly, we assume that an image is\ngenerated from object shape representations that are domain-invariant, object\ntexture representations that capture object-speciﬁc properties, and image back-ground information that is independent of shape and texture. And then we sep-\narate and recombine the object shape, texture and image background using end-\nto-end structurally constrained CNNs through two losses: image reconstructedloss that ensures the integrity of image information, and feature discrepancy loss\nthat enforce divergence of the feature of diﬀerent views. Finally, three classiﬁ-\ncation losses are joined with diﬀerent contribution weights based on multi-viewfeatures. The experimental datasets contain: MNIST and Fashion-MNIST that\nconsist of more shape feature and less texture, SVHN and Deep-Fashion that\ninclude a large amount of texture information. This enables us to observe the\nfeature of each category separately and quantify their bias for object classiﬁ-\ncation task. Comprehensive and careful experiments are implemented on threedatasets and the results show that CNNs don’t have texture bias or shape bias.\nThe bias of CNNs change with data internal bias. We believe this approach could\nbe a good start to explore the interpretability of deep neural networks.\nThe main contributions of this work are brieﬂy summarized as follows:\n1) Separating object (foreground) and background from the original image fur-\nther emphasizes the diﬀerences between texture and shape to reduce the\nwhole image shape inﬂuence.\n2) Whether in process training or testing, we do not violate the basic data\nassumptions or avoid the transformation of bias-measure.\n3) After getting the pre-trained weights and the bias of CNNs, we just need to\nadjust the contribution weights to adapt to the new data, which improves\nthe generalization and robustness of the model.\n2 Multi-view Image Representations\nMulti-View Image representations in our work are inspired by recent works on\ndisentangled representation learning [ 22,27,37], which focuses on disentangling\ncontent from style. Ditching the disordered deﬁnition of content and style, we\nrefer content as object shape that is the underling spatial structure and style\nas object texture that is the rendering of the structure. Because object andbackground features are ﬁrstly extracted, the remaining image only contains\nobject. The changes of their names are widely accepted in the image style transfer\nliterature [ 10]. And we assume that an image can be decomposed into shape,\ntexture and background. Shape is global and decentralized information of object,\nwhich is associated with each category ﬁtted human shape perception. Texture is\nlocal and regular image information and background is composed by information\nother than the target object.', 'Multi-view Image Representation on Shape and Texture Bias 143\nFig. 2. The structure of the proposed Image representation and feature learning.\nShape representations FS\ni, texture representations FT\niand background feature Biare\nextracted from the input data xi. These information can form CNNs’ output image ˆ yi,\nreconstructed images ˆ xiand total loss Ltotalloss of the process. Details are explained\nin Fig. 3.\nFig. 3. Overview of the proposed Feature Learning using CNNs from Multi-View Rep-\nresentation framework. It consists of Image Matting, pre-trained CNNs, shape andtexture feature extractor, decoder and three classiﬁers. Image Matting separates the\nforeground and background of an image. Pre-trained CNNs is regarded as object feature\nextractor, whose parameters are ﬁxed. Shape and texture extractor accurately extracttexture and shape information respectively from original image. Decoder restructures\nthe image from three views features to ensure the integrity of image. Three classiﬁers\nare fed with the deep feature maps to obtain three predicted results.\n2.1 Background Representations\nSeparating the foreground and background of an image, also named image mat-\nting, is a fundamental computer vision problem and has many applications\n[5,25,33]. Image matting divides an image into foreground and background, and\nreturns the probability that it belongs to foreground or background. Extracting\nthem separately is not the focus of our attention, so we use [ 33]’s method for\nreference which had two parts: a deep convolutional encoder-decoder networkthat takes an image and the corresponding Trimap as inputs and predict the\nalpha matte of the image, and a small convolutional network that reﬁnes the', '144 L. Kong et al.\nalpha matte predictions of the ﬁrst network to have more accurate alpha values\nand sharper edges. Let EIMbe the image matting encoder:\n(Fi,B i)=EIM(xi) (1)\nwhere FiandBiis foreground and background features of image xi.\n2.2 Shape Representations\nThere is a widely accepted intuition that CNNs combine low-level features to\nincreasingly complex shapes until the object can be readily classiﬁed [ 7]. When\nmatching the content feature on a higher layer of CNNs, detailed pixel informa-\ntion of the image is not as strongly constraint. Hence, an image is passed throughthe CNNs and the shape representation S\nl\niin the higher layer lis transmitted\nto the constrained shape encoder which contains several residual blocks [ 13]t o\nfurther process Sl\ni. The convolutional layers in the residual blocks are followed\nby Instance Normalization [ 28]. Let the residual blocks be RBand the shape\nfeature be FS\ni:\nFS\ni=RB(Sl\ni) (2)\n2.3 Texture Representations\nTo obtain a representation of the texture of an object, we use a feature space\ndesigned to capture texture information [ 9,10]. The texture space can be built on\ntop of the ﬁlter responses in any layer of the CNNs. It consists of the correlationsbetween the diﬀerent ﬁlter responses, where the expectation is taken over the\nspatial extent of the feature maps. Due to the diﬀerent shape of the diﬀerent\nlayers, the feature are resized by the reshape layer, followed by a global averagepooling layer and a fully connected (FC) layer. Unlike shape representations,\ninstance normalization is not used in texture representations, which removes the\noriginal feature mean and variance that represent important style information.Let the reshape layer be RL, the global average pooling layer be GAP, and the\nfully connected layer be FC:\nF\nT\ni=FC(GAP(/summationdisplay\nRL(Tl\ni))) (3)\nwhere Tl\niis the texture representation of image xiin layer l.\n3L o s s\nTo reconcile hypothesis contradictions of CNNs from a multi-view representa-\ntion, we jointly minimise the three lossfunctions, i.e., the classiﬁcation loss ,t h e\nreconstruction loss , and the feature discrepancy loss .reconstruction loss ensures', 'Multi-view Image Representation on Shape and Texture Bias 145\nFig. 4. Image representations of our architecture. The original images in 4.a are ran-\ndomly selected from MNIST and Fashion-MNIST. The original images in 4.b are ran-\ndomly selected from SVHN and DeepFashion. The input images of the four datasetsare not pre-processed to Image Matting. Black solid and line dotted lines represent\nshape information and texture information, respectively.\nthe integrity of image information, feature discrepancy loss makes feature distri-\nbution as diﬀerent as possible. Let ( xi,yi)∈Xbe an image and its label from\nthe style domain. The classiﬁcation loss Lclassif ication can be concretized as:\nLclassif ication =/summationdisplay\niCrossEntropy (yi,ˆyi) (4)\nwhere\nˆyi= arg max\ny∈Ywbpi\nb+wspi\ns+wtpi\nt\ns.t. w b+ws+wt=1(5)\nwhere wb,ws,a n dwtare the contribution weight of diﬀerent feature representa-\ntions for image classiﬁcation. pi\nb,pi\ns,a n dpi\ntare predicted probability distributions\nof categories on multi-view representations for the image xi.pi\nbis calculated from\nbackground feature representation Biby fully connected layer and softmax layer,\nwhich is similar to pi\nsandpi\nt.\nFor the second loss , given an image sampled from the data distribution, the\nimage should be reconstructed after multi-view representation operations anddecoding and reconstruction loss is\nL\nreconstruction =/summationdisplay\niExi∈X[||G(Bi,FS\ni,FT\ni)−xi||1] (6)\nTheGin Eq. 6is a specially designed decoder, which reconstructs the input\nimage from its background, object shape and texture. It processes the shaperepresentation by a set of residual blocks and ﬁnally produces the reconstructed\nimage by several upsampling and convolutional layers. Similar to the structure\nused in [ 15], and take advantages of [ 3] we equip the residual blocks with Adap-\ntive Instance Normalization (AdaIN) [ 14,17] layers whose parameters are dynam-\nically generated by a multi-layer perceptron (MLP) from the texture code. With\nthe representations of multi-views of background, shape and texture, we fuse\nthem into a common representation with global loss function L\ntotal.', '146 L. Kong et al.\nThird, the three feature representations should have possibly diverse values\ndistribution in order to provide diﬀerent views on an image. feature discrepancy\nguarantees that diﬀerent view features can be accurately separated from the\noriginal image. Speciﬁcally, we enforce divergence of the values of the represen-\ntations by minimizing their cosine similarity. Therefore, we have the followingdiscrepancy loss :\nL\ndiscrepancy =Bi·FS\ni\n/bardblBi/bardbl/bardblFS\ni/bardbl+Bi·FT\ni\n/bardblBi/bardbl/bardblFT\ni/bardbl+FT\ni·FS\ni\n/bardblFT\ni/bardbl/bardblFS\ni/bardbl(7)\nWith the above lossterms, we jointly train the representation and classiﬁca-\ntion to optimize the ﬁnal objective 2, which is a weighted sum of three sub-loss :\nLtotal=ηLclassif ication +αLreconstruction +βLdiscrepancy (8)\nwhere η,α,a n d βdenote the hyper parameters that control the relative impor-\ntance of the three losses.\n4 Experiments\n4.1 Datasets\nMNIST database [ 19] has a training set of 60,000 examples, and a test set of\n10,000 examples. And handwritten digits have been size-normalized and centered\nin a ﬁxed-size 28 ×28 image. SVHN [23] is the abbreviation of Street View\nHouse Numbers data set, which contains 100k images (32 ×32 color) of house\nnumbers collected by Google Street View. Fashion-MNIST [32]i saM N I S T -\nlike fashion product database that consists of 70k gray scale 28 ×28 images,\nassociated with a label from 10 classes. Fashion-MNIST preferably representsmodern CV tasks. DeepFashion [20] is a large-scale clothes dataset with com-\nprehensive annotations, which is adapted to various CV tasks. It contains over\n800,000 images, which are richly annotated with massive attributes, clothinglandmarks, and correspondence of images taken under diﬀerent scenarios.\n4.2 Implementation Details\nImage Matting separates the foreground from the background of the image,\nwhich is used to reduce the impact of the background for classiﬁcation. Deep\nImage Matting [ 33] is used in our framework, consists encoder-decoder and reﬁne-\nment stages.\nA standard VGG16 [26] contains 13 convolutional layers, where ﬁlters with\na receptive ﬁeld: 3 ×3. The convolution stride is ﬁxed to 1 pixel and the strategy\nof the spatial padding is the spatial resolution is preserved after convolution.\nMax-pooling is performed over a 2 ×2 pixel window, with stride 2. There are\nthree fully connected layers: the ﬁrst two have 4096 channels each, the thirdcontains diﬀerent channels for diﬀerent datasets. What’s more, each input of', 'Multi-view Image Representation on Shape and Texture Bias 147\nFig. 5. The experimental results of the single variable changing. The ﬁrst two ﬁgures\nshow the classiﬁcation accuracies obtained when MNIST, Fashion-MNIST, SVHN and\nDeepFashion change shape weight or texture weight while ﬁxing other weights. The\nlast ﬁgure shows the results when shape and texture weights are ﬁxed.\nVGG16 is processed by the batch normalization. The structure of pre-trained\nVGG16 would be slightly diﬀerent from the standard one [ 31].\nShape Encoder consists of four residual blocks followed by Instance Nor-\nmalization, whose input feature maps are selected from the conv4−2l a y e ro f\nVGG16. The shape feature on a higher layer of the network, detailed pixel infor-\nmation of the image is not as strongly constraint. Hence, the conv4−2l a y e ri s\nsuitable, which is suggested by many researches.\nTexture Encoder captures texture information from the top of the ﬁlter\nresponses in any layer of the network. The purpose of this operation is to extend\nthe spatial expression ability of feature maps. And then all feature maps are\nfed to a global average pooling layer to regularize the network and resize thefeature maps. When classifying based on texture feature, all feature maps would\nbe connected.\nDecoder reconstructs the input image object from its shape and texture\ncode. It processes the shape and background code by a set of residual blocks and\nﬁnally produces the reconstructed image by several upsampling and convolu-\ntional layers. Inspired by recent works that use aﬃne transformation parametersin normalization layers to represent styles [ 6,14], we equip the residual blocks\nwith Adaptive Instance Normalization [ 14] ayers whose parameters are dynam-\nically generated by a multilayer perceptron (MLP) from the texture code.\n4.3 Results and Analysis\nThe main motivation behind our work was to investigate the bias of CNNs\nfor image classiﬁcation. Firstly, we visualized intermediate convnet outputs to\nverify the validity of the network structure of Image representations. Then, we\nlisted the contribution weights of diﬀerent feature representations on the diﬀerentdatasets. Finally, we observed the trend of classiﬁcation accuracy changing with\nthree weights to ﬁgure out the weight parameters that provides the best accuracy\nperformance. More importantly, whether classiﬁcation performances of CNNsdepend on shape or texture information is tested among the listed datasets.', '148 L. Kong et al.\nImage Representations. In the ﬁrst experiment, we visualized the shape and\ntexture features generated by the architecture as in Fig. 3. The size of the tex-\nture feature is reshaped as the shape. Due to the limitation of paper length, we\nrandomly selected 3 images from each dataset to show the performance of the\nnetwork. In SVHN and DeepFashion, the input images are processed images byimage matting. Speciﬁc details can be found in Fig. 4.F r o mF i g . 4,w ec a ns e e\nthat: 1) Our elaborate framework can accurately extract speciﬁc object informa-\ntion, i.g. separate shape feature and texture feature, which ensures that our sub-sequent experiments can be carried out as expected. 2) Compared with shape fea-\nture, the diﬀerence of texture information extraction is stronger, because global\naverage pooling erases the diﬀerence of features.\nContribution Weights. In a second experiment, we evaluate the contribution\nweights of diﬀerent image representations, which contained two sections: stan-\ndard training and single variable changing. Standard training minimized the\nthree loss under the whole architecture to obtain the three adaptive weights.The purpose of standard training is to ﬁnd the weights suitable for the distribu-\ntion of original training dataset. Single variable changing ﬁxed the all variables\nexcept single and speciﬁc weight. And the starting point is the result gettingfrom standard training. Note that the weights only plays a role in the integra-\ntion of ﬁnal classiﬁcation results and the sum of three weights is not equal to 1\nin the single variable changing.\nTable 1. The experimental results of three weights on the four real-world datasets. ‘-’\nrepresented that image representations did not conclude the background representation.\nMNIST SVHN Fashion-MNIST DeepFashion\nwb- 0.0794 - 0.1487\nws0.813 0.6162 0.7968 0.5863\nwt0.187 0.3044 0.2032 0.265\nTable 1presents detailed results obtained by the standard training on four\nreal-world datasets. Based on these results, we can ﬁnd that: 1) As expected,\nthe values of shape weight wsin MNIST and Fashion-MNIST is larger than\ntexture weight wt. The result suggest that CNNs has to learn global and more\nshape information when it faces datasets with less texture information. Thisresults lead to explain that CNNs can eﬀectively learn and inference from the\nfeatures of the data 2) Compared to the SVHN, the result of DeepFashion shows\nthat texture feature play a more important role for classiﬁcation. It is easy tounderstand and accept. Although they are all real-world natural image datasets,\nDeepFashion is obviously more complex than SVHN, so more information needs\nto be considered and synthesized for image classiﬁcation. We can reason from', 'Multi-view Image Representation on Shape and Texture Bias 149\nthe phenomenon that CNNs can learn more information from images. 3) We\ncompare MNIST and SVHN, Fashion-MNIST and DeepFashion, respectively,because they have similar shapes and diﬀerent textures. When the information\nprovided by the local texture is not enough to obtain a satisfactory result, the\nglobal shape information must be involved. It shows that, for images with highlycomplicated texture information, CNNs tend to ignore and avoid the obstacles,\nand prefer to learn easier knowledge from the shapes of the data. Hence, CNNs\nseems be ‘lazy’, which ﬁrst learn easy features and then hard features.\nA ss h o w ni nF i g . 5, we can see that: 1) No matter it is MNIST, Fashion-\nMNIST, SVHN or DeepFashion, the classiﬁcation accuracy ﬂuctuates with the\nchange of the weight, which indicates that no classiﬁcation can completely relyon a certain type of features alone. To achieve good results, all the features\nshould be considered, but in diﬀerent proportions. 2) When the image has a\nlarge amount of texture information, the accuracy of classiﬁcation is sensitive to\nthe change of texture weight. 3) Because in most cases salient object occupies\nmost of the whole image and provides rich texture information. Backgroundfeature have little inﬂuence, they are sometimes important.\n5 Conclusion and Future Work\nThis study proposed a novel perspective to explore the competition of shape\nand texture bias based on multi-view image representations. Our method ﬁrst\nseparates the foreground and background of the image to reduce the distractionof background and then extracts shape and texture features from the foreground\nthrough a special network structure. Finally, the classiﬁcation loss, reconstruc-\ntion loss, and discrepancy loss are calculated to update parameters. The exten-sive experimental results on real-world datasets show that CNNs do not have\ntexture bias or shape bias. CNNs is lazy to learn global shape feature if the\naccuracy requirement is enough.\nFor simplicity, we naively ﬁxed the parameters of pre-trained VGG16 that\nshould update in the training process. We noticed that this setting is quite sim-\nple and rough, which may lead to the non-convergence of gradients. Therefore,alleviating this setting more or less to improve further the performance of the\ncurrent work and no increase training complexity is a main direction for our\nfuture work. We are at the era of advancing from data driven approaches toknowledge representation methods. This work could provide some insights of\nthe explainability of deep neural networks and further the research of inﬁlling\nknowledge to deep learning and neural networks.\nAcknowledgments. This paper is supported by National Key Research and Devel-\nopment Program of China under grant No. 2018YFB0204403, No. 2017YFB1401202and No. 2018YFB1003500.', '150 L. Kong et al.\nReferences\n1. Adadi, A., Berrada, M.: Peeking inside the black-box: a survey on explainable\nartiﬁcial intelligence (XAI). IEEE Access 6, 52138–52160 (2018)\n2. Ballester, P., Araujo, R.M.: On the performance of GoogLeNet and AlexNet\napplied to sketches. In: Thirtieth AAAI Conference on Artiﬁcial Intelligence (2016)\n3. Bhattacharjee, D., et al.: DUNIT: detection-based unsupervised image-to-image\ntranslation. In: Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (2020)\n4. Brendel, W., Bethge, M.: Approximating CNNs with bag-of-local-features models\nworks surprisingly well on ImageNet. arXiv preprint arXiv:1904.00760 (2019)\n5. Cho, D., Tai, Y.-W., Kweon, I.: Natural image matting using deep convolutional\nneural networks. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016.\nLNCS, vol. 9906, pp. 626–643. Springer, Cham (2016). https://doi.org/10.1007/\n978-3-319-46475-6\n39\n6. Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style.\narXiv preprint arXiv:1610.07629 (2016)\n7. Erhan, D., Bengio, Y., Courville, A., Vincent, P.: Visualizing higher-layer features\nof a deep network. Univ. Montreal 1341(3), 1 (2019)\n8. Funke, C.M., Gatys, L.A., Ecker, A.S., Bethge, M.: Synthesising dynamic textures\nusing convolutional neural networks. arXiv preprint arXiv:1702.07006 (2017)\n9. Gatys, L., Ecker, A.S., Bethge, M.: Texture sythesis using convolutional neural\nnetworks. In: Advances in Neural Information Processing Systems, pp. 262–270\n(2015)\n10. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional\nneural networks. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 2414–2423 (2016)\n11. Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich-mann, F.A., Brendel,\nW.: ImageNet-trained CNNs are biased towards texture; increasing shape bias\nimproves accuracy and robustness. arXiv preprint arXiv:1811.12231 (2018)\n12. Hao, S., Zhou, Y., Guo, Y.: A brief survey on semantic segmentation with deep\nlearning. Neurocomputing 406(2020)\n13. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pp. 770–778 (2016)\n14. Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance\nnormalization. In: Proceedings of the IEEE International Conference on ComputerVision, pp. 1501–1510 (2017)\n15. Huang, X., Liu, M.-Y., Belongie, S., Kautz, J.: Multimodal unsupervised image-\nto-image translation. In: Proceedings of the European Conference on ComputerVision, pp. 172–189 (2018)\n16. Kubilius, J., Bracci, S., de Beeck, H.P.O.: Deep neural networks as a computational\nmodel for human shape sensitivity. PLoS Comput. Biol. 12(4), e1004896 (2016)\n17. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative\nadversarial networks. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (2019)\n18. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436 (2015)\n19. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P., et al.: Gradient-based learning\napplied to document recognition. Proc. IEEE 86(11), 2278–2324 (1998)', 'Multi-view Image Representation on Shape and Texture Bias 151\n20. Liu, Z., Luo, P., Qiu, S., Wang, X., Tang, X.: DeepFashion: powering robust clothes\nrecognition and retrieval with rich annotations. In: Proceedings of IEEE Conference\non Computer Vision and Pattern Recognition (2016)\n21. Milletari, F., Navab, N., Ahmadi, S.-A.: V-Net: fully convolutional neural net-\nworks for volumetric medical image segmentation. In: 2016 Fourth International\nConference on 3D Vision (3DV), pp. 565–571. IEEE (2016)\n22. Narayanaswamy, S., et al.: Learning disentangled representations with semi-\nsupervised deep generative models. In: Advances in Neural Information Processing\nSystems, pp. 5925–5935 (2017)\n23. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A.Y.: Reading digits\nin natural images with unsupervised feature learning (2011)\n24. Ritter, S., Barrett, D.G., Santoro, A., Botvinick, M.M.: Cognitive psychology for\ndeep neural networks: a shape bias case study. In: Proceedings of the 34th Interna-\ntional Conference on Machine Learning, vol. 70, pp. 2940–2949 (2017). JMLR.org\n25. Shen, X., Tao, X., Gao, H., Zhou, C., Jia, J.: Deep automatic portrait matting. In:\nLeibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9905,\npp. 92–107. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46448-0 6\n26. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition. arXiv preprint arXiv:1409.1556 (2014)\n27. Tran, L., Yin, X., Liu, X.: Disentangled representation learning GAN for pose-\ninvariant face recognition. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 1415–1424 (2017)\n28. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Improved texture networks: maximizing\nquality and diversity in feed-forward stylization and texture synthesis. In: Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npp. 6924–6932 (2017)\n29. Wang, N., Chen, M., Subbalakshmi, K.P.: Explainable CNN-attention networks (C-\nattention network) for automated detection of Alzheimer’s disease. arXiv preprint\narXiv:2006.14135 (2020)\n30. Wang, Q., et al.: IBRNet: learning multi-view image-based rendering. In: Proceed-\nings of the IEEE Conference on Computer Vision and Pattern Recognition (2021)\n31. Wang, R., et al.: Multi-view bearing fault diagnosis method based on deep learning.\nJ. Phys. Conf. Ser. 1757(1) (2021)\n32. Xiao, H., Rasul, K., Vollgraf, R.: Fashion-MNIST: a novel image dataset for bench-\nmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 (2017)\n33. Xu, N., Price, B., Cohen, S., Huang, T.: Deep image matting. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 2970–2979\n(2017)\n34. Xu, F., Uszkoreit, H., Du, Y., Fan, W., Zhao, D., Zhu, J.: Explainable AI: a brief\nsurvey on history, research areas, approaches and challenges. In: Tang, J., Kan, M.-\nY., Zhao, D., Li, S., Zan, H. (eds.) NLPCC 2019. LNCS (LNAI), vol. 11839, pp.\n563–574. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-32236-6 51\n35. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.\nIn: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS,\nvol. 8689, pp. 818–833. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-\n10590-1 53\n36. Zhang, C., Wang, D.-H.: Exploring the prediction consistency of multiple views for\ntransductive visual recognition. IEEE Signal Process. Lett. 28, 668–672 (2021)\n37. Zhao, B., et al.: Multi-view image generation from a single-view. In: Proceedings\nof the 26th ACM International Conference on Multimedia (2018)', 'Learning Indistinguishable\nand Transferable Adversarial Examples\nWu Zhang1, Junhua Zou1, Yexin Duan1, Xingyu Zhou2, and Zhisong Pan1(B)\n1Command and Control Engineering College, Army Engineering University of PLA,\nNanjing, China\n2Communication Engineering College, Army Engineering University of PLA,\nNanjing, China\nAbstract. The fast gradient sign method series can attack deep neural\nnetworks (DNNs) with high black-box success rates but with low image\nﬁdelity. Although the Adam iterative fast gradient tanh method breaksthis limitation, its performance is not good enough. In this paper, we\npropose a Mixed-input Adam Iterative FastGradient Piecewise Linear\nMethod ( MAI-FGPLM ) to generate adversarial examples with more\nindistinguishability and transferability for image classiﬁcation task. Our\nmethod utilizes the piecewise linear function and the gradient regular-\nization term to reduce the perturbation size for better image ﬁdelity, andimproves the transferability of adversarial examples via the mixed-input\nstrategy for higher attack success rates. Extensive experiments on an\nImageNet-compatible dataset show that the adversarial examples gener-ated by our attack method have smaller perturbation size while oﬀering\nhigher attack success rates. Our best attack, NI-TI-DI-MAILM, evades\nsix black-box defenses with the average perturbation size decreased by1.11 and the average success rate increased by 2.1% compared with the\nstate-of-the-art gradient-based attacks.\nKeywords: Deep neural networks\n·Adversarial examples ·\nIndistinguishability ·Transferability\n1 Introduction\nDNNs have been used in various ﬁelds such as image classiﬁcation, natu-\nral language processing and malware detection. However, recent work has\nshown that DNNs are vulnerable to adversarial examples, e.g., inputs with\nhuman-imperceptible perturbations can make DNNs produce incorrect predic-\ntions [ 6,18]. More seriously, adversarial examples can generalize across network\nmodels [ 12], thus causing serious security threats to real-world applications such\nas automatic driving [ 5] and face recognition [ 15].\nFoolbox [ 14] roughly divides existing attack methods into three branches:\nthe gradient-based attacks [ 3,6,9], the score-based attacks [ 13] and the decision-\nbased attacks [ 1,2]. In particular, the fast gradient sign method series which\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 152–164, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_13', 'Learning Indistinguishable and Transferable Adversarial Examples 153\nbelong to the gradient-based attacks are widely used due to good transferabil-\nity. However, adversarial examples generated by these methods usually have lowimage ﬁdelity [ 22,23]. To generate more indistinguishable adversarial examples,\nZhang et al. [22] integrated the just noticeable diﬀerence coeﬃcients into the\nthe fast gradient sign method series. Although this strategy improves the indis-tinguishability, it has bad transferability. Besides, Zou et al. [23] attributes the\npoor image ﬁdelity to the basic sign structure which increases the perturba-\ntion size. Hence, they proposed the Adam iterative fast gradient tanh method(AI-FGTM) to achieve smaller average perturbation size and higher attack suc-\ncess rates. However, adversarial examples generated by NI-TI-DI-AITM which\nbelongs to AI-FGTM series have not good enough image ﬁdelity from Fig. 1.\nClean NI-TI-DIM NI-TI-DI-AITM NI-TI-DI-MAILM( Ours )\nFig. 1. The comparison of adversarial examples generated by NI-TI-DIM, NI-TI-DI-\nAITM and NI-TI-DI-MAILM ensemble attack method with the equal maximum per-\nturbation size.\nTherefore, in this paper, we conduct a case study on MI-FGSM to know why\nthe classic fast gradient sign attacks generate adversarial examples with poor\nimage ﬁdelity. From Fig. 2(a), we ﬁnd that the gradient processing steps such\nas the gradient normalization, the gradient accumulation and the sign function\nmainly increase the perturbation size of adversarial examples. In other words,\nthe perturbation size may be reduced by modifying these gradient processingsteps to improve image quality. For adversarial examples, it is also necessary to\nhave high attack success rates, especially the black-box attack. In this paper,\nwe ﬁnd that mixing the original input with Gaussian blurred input can furtherimprove the black-box attack success rates.\nBased on the above ﬁndings, MAI-FGPLM is proposed to make adversar-\nial examples more indistinguishable and transferable in this paper. Although\nwe take AI-FGTM as the starting point for our research in this paper, there are\nseveral diﬀerences between our proposed method and AI-FGTM: 1) Our methodreplaces the tanh function with the piecewise linear function to get smaller per-\nturbation size; 2) We employ mixed-input strategy to generate more transferable\nadversarial examples; 3) In the ﬁrst order and the second order momentum term,our method preserves the loss gradient as regularization term to reduce the per-\nturbation size while maintaining attack success rates.', '154 W. Zhang et al.\n0.60.70.80.911.11.21.31.4\n123456789 1 0Average  Perturbation Size\nNumber of Iterationstanh function\n0.60.70.80.911.11.21.31.4\n123456789 1 0Average  Perturbation Size\nNumber of Iterationspiecewise linear functionMethod Gradient Processing Pipelines\nMI-FGSM1:  normalize the loss gradient \n2:  accumulate the normal ized-gradient  \n3:  sign function deal with  the accumulated-gradient \nMethod21: get the loss gradient \n2:  accumulate the loss gradient  \n3:  sign function deal with the accumulated-gradient\nMethod31:normalize the loss gradient \n2:accumulate the norma lized-gradient  \n3:piecewise linear function deal with the accumulated- gradient\nMethod41:normalize the loss gradient \n2:accumulate the normalized-gradient and normalize\n3:piecewise linear function deal with the normalized \naccumulated -gradient \nMethod51:get the loss gradient \n2:accumulate the loss gradient and normalize\n3:piecewise linear function deal with the normalized \naccumulated -gradient 567891011Average Perturbation Size\n(a)\n(b)\nFig. 2. Overview of the our observations.\nIn summary, our main contributions are listed as follows:\n•We comprehensively investigate why the fast gradient sign attack series gen-\nerate adversarial examples with high perturbation size.\n•We propose a mixed-input Adam iterative fast gradient piecewise linear\nmethod to generate more indistinguishable and transferable adversarial exam-\nples.\n•Our proposed method can be integrated into any gradient-based attacks to\nget smaller perturbation size and higher attack success rates.\n•Our best attack achieves the average perturbation size decreased by 1.11 and\nthe average success rate increased by 2.1% compared with the state-of-the-artgradient-based attack.\n2 Related Work\n2.1 Adversarial Example Generation\nGiven a clean input xand its corresponding ground-truth label y.F o rap r e -\ntrained classiﬁer f(·), it should correctly classify xtoy. The attacker can con-\nstruct an adversarial example xadvby adding small perturbation δto fool the\npre-trained classiﬁer, as f(x+δ)/negationslash=y. The gradient-based attack methods use\nL∞norm to constrain the perturbation δby the threshold ε,e.g.,/bardblδ/bardbl∞≤ε.\nMoreover, in order to generate non-targeted adversarial example, it also needs', 'Learning Indistinguishable and Transferable Adversarial Examples 155\nto maximize the cross-entropy loss function of the classiﬁer. The cross-entropy\nloss function can be expressed as\nJ(x,y)=−1y.log(softmax( l(x))), (1)\nwhere 1 yis one-hot encoding of the ground-truth label yandl(x) is the logits\noutput.\n2.2 The Fast Gradient Sign Method Series\nFast Gradient Sign Method (FGSM) [6] is the basic gradient-based method\nwhich only requires one-step gradient update to generate adversarial examples.This method increases or decreases each pixel equally according to the gradient\ndirection of the loss function. It can be expressed as\nx\nadv=x+ε·sign(∇xJ(x,y)), (2)\nwhere sign( ·) is the sign function and ∇xJ(x,y) computes the loss gradient.\nIterative Fast Gradient Sign Method (I-FGSM) [9] is the iterative ver-\nsion of FGSM. In other words, the one-step update is decomposed into multiple-\nstep iterative update to get smaller perturbation size and higher white-box suc-\ncess rates. The iterative process can be expressed as\nxadv\nt+1= Clipε\nx{xadv\nt+α·sign(∇xadv\ntJ(xadv\nt,y))}, (3)\nwhere xadv\ntdenotes the adversarial example generated in the t-th iteration and\nαis the step size. Clipε\nx{x/prime}performs element-wise clipping of the image x/primeto\nrestrict x/primein the vicinity of x, that can be expressed as min {255,x+ε,max{0,x−\nε,x/prime}}.\nMomentum Iterative Fast Gradient Sign Method (MI-FGSM) [3]\naccumulates previous gradient information by integrating momentum term into\nI-FGSM, which stabilizes update directions and escapes from trapping into local\noptima during the iterations. It can be expressed as\ngt+1=μ·gt+∇xadv\ntJ(xadv\nt,y)/vextenddouble/vextenddouble/vextenddouble∇\nxadv\ntJ(xadv\nt,y)/vextenddouble/vextenddouble/vextenddouble\n1, (4)\nxadv\nt+1= Clipε\nx{xadv\nt+α·sign(gt+1)}, (5)\nwhere gtrepresents the t-th accumulated-gradient and μis the decay factor.\nNesterov Iterative Fast Gradient Sign Method (NI-FGSM) [11] com-\nbines MI-FGSM with the anticipatory update of Nesterov accelerated gradient,\nso as to eﬀectively improve the transferability of adversarial examples. It can be\nformalized as\nxnes\nt=xadv\nt+α·μ·gt, (6)\ngt+1=μ·gt+∇xnes\ntJ(xnes\nt,y)/vextenddouble/vextenddouble∇xnes\ntJ(xnes\nt,y)/vextenddouble/vextenddouble\n1, (7)', '156 W. Zhang et al.\nxadv\nt+1= Clipε\nx{xadv\nt+α·sign(gt+1)}, (8)\nwhere xnes\ntdenotes the Nesterov term of the t-th iteration. Please note that the\nNesterov term will not be used in updating xadv\nt+1.\nDiverse Inputs Iterative Fast Gradient Sign Method (DI2-FG\nSM) [21] can get higher attack success rate by randomly resizing and padding\nthe input image with a given probability pat each iteration as\nD(xadv\nt;p)=/braceleftbigg\nD(xadv\nt) with probability p\nxadv\nt with probability 1 −p,(9)\nwhere D(·) is the transformation function. Moreover, DI2-FGSM can be com-\nbined with other methods to generate more transferable adversarial examples.\nTranslation-Invariant Iterative Fast Gradient Sign Method (TI2-\nFGSM) [4] makes adversarial examples less sensitive to the discriminative\nregions of the substitute model by convolving the gradient with the pre-deﬁned\nGaussian kernel W.T h a ti s ,T I2-FGSM achieves higher black-box attack success\nrates against the defense models by applying Gaussian blurred gradient strategy.\nAdam Iterative Fast Gradient Tanh Method (AI-FGTM) [23]\nchanges the basic sign structure and generates more transferable and indistin-\nguishable adversarial examples based on Adam [ 8] and the tanh function, which\nc a nb ee x p r e s sa s\nmt+1=mt+μ1·∇xadv\ntJ(xadv\nt,y), (10)\nvt+1=vt+μ2·(∇xadv\ntJ(xadv\nt,y))2, (11)\nxadv\nt+1= Clipε\nx{xadv\nt+αt·tanh(λmt+1√vt+1+r)}, (12)\nwhere mtandvtdenote the ﬁrst moment vector and the second moment vector\nrespectively. μ1andμ2are exponential decay rates. αtis the increasing step size,\nλis the scale factor and r=1 0−8.\nEnsemble Strategy Attack Method [12] uses an ensemble of multiple\nmodels as attack targets, so that the generated adversarial examples are less\nlikely to trap into the local optima of any speciﬁc model. Besides, Dong et al.[3]\nﬁnd that attacking multiple models with logit activations can get more trans-ferable adversarial examples. Therefore, to attack an ensemble of kmodels, the\nlogits can be fused as\nl(x)=/summationdisplay\nk\ni=1wili(x), (13)\nwhere li(x) is the logits output of the i-th model, wiis the ensemble weight with\nwi≥0a n d/summationtextk\ni=1wi=1 .\n3 Methodology\nIn this paper, we propose MAI-FGPLM to generate adversarial examples with\nmore indistinguishability and transferability, which modiﬁes the gradient pro-\ncessing steps for better image quality and utilizes mixed-input strategy for higher\ntransferability.', 'Learning Indistinguishable and Transferable Adversarial Examples 157\nAlgorithm 1. NI-TI-DI-MAILM ensemble attack method\nInput : A clean image xand its corresponding ground-truth label y;kclassiﬁers\nf1,f2, ..., f k; ensemble weights w1,w 2, ..., w k;\nHyper-Parameter : Perturbation threshold ε; iteration number T; transformation\nprobability p; pre-deﬁned kernel WandV;s l o p e ρ; regularization term factor γ1and\nγ2; moving average factor τ; exponential decay rates μ1andμ2; scale factor λ; increas-\ning step size αt.\nOutput : An adversarial example xadv.\n1:xadv\n0=x;\n2:fort=0t o T−1do\n3: Get xnes\ntby Eq.(6);\n4: Take xmix\nt=τ·V∗xnes\nt+( 1−τ)·xnes\ntas mixed-input;\n5: Input xmix\nt, apply the transformation function Eq.(9) and output the model\nlogits li(D(xmix\nt)) for i=1,2, ..., k;\n6: Fuse the logits as l(xmix\nt)=/summationtextk\ni=1wi(li(D(xmix\nt)));\n7: Get the cross-entropy loss J(xmix\nt,y) based on l(xmix\nt);\n8: Compute the loss gradient g=∇xmix\ntJ(xmix\nt,y);\n9: Update mt+1andvt+1by applying Gaussian blurred gradient strategy and the\ngradient regularization term as\nmt+1=mt+μ1·W∗g+γ1·g\nvt+1=vt+μ2·(W∗g)2+γ2·(g)2;\n10: Update xadv\nt+1by applying the piecewise linear function as\nxadv\nt+1= Clipε\nx{xadv\nt+αt·PL(λmt+1√vt+1+r)};\n11: end for\n12: return :xadv=xadv\nT.\n3.1 The Piecewise Linear Function\nCompared to the sign function, the tanh function can normalize the large gra-\ndient values as well as maintaining the small gradient values. Hence, AI-FGTM\nimproves the indistinguishability of adversarial examples by replacing the signfunction with the tanh function. However, in Fig. 2(b), the piecewise linear func-\ntion gets smaller perturbation size than the tanh function at each iteration for\nAI-FGTM by further making the small gradient values smaller. Hence, we pro-pose an Adam Iterative Fast Gradient Piecewise Linear Method (AI-FGPLM)\nto generate more indistinguishable adversarial examples. That is, the Eq. ( 12)\nshould be replaced with\nx\nadv\nt+1= Clipε\nx{xadv\nt+αt·PL(λmt+1√vt+1+r)}, (14)\nPL(z)=⎧\n⎨\n⎩ρ·zi,j if|zi,j|<3\n1 ifzi,j≥3\n−1 ifzi,j≤−3, (15)\nwhere PL(·) is the piecewise linear function and ρis the slope. zi,jdenotes the\nvalue of matrix zat position ( i,j).', '158 W. Zhang et al.\n3.2 The Gradient Regularization Term\nFurthermore, we ﬁnd that AI-FGPLM can further reduce the perturbation size\nby preserving the loss gradient in the ﬁrst order and the second order momentumterm when applying Gaussian blurred gradient. So as to take the loss gradient as\nregularization term into AI-FGPLM, the Eq. ( 10)a n dE q .( 11) should be changed\nwith\ng=∇\nxadv\ntJ(xadv\nt,y), (16)\nmt+1=mt+μ1·W∗g+γ1·g, (17)\nvt+1=vt+μ2·(W∗g)2+γ2·(g)2, (18)\nwhere γ1andγ2are regularization term factors, gis the loss gradient.\n3.3 The Mixed-Input Strategy\nOther than considering the indistinguishability of adversarial examples, achiev-\ning higher attack success rates is also equally important. In this paper, we ﬁndthat mixing original input with Gaussian blurred input can reach this goal. A\nmixed-input crafted by that method not only maintains original feature but\nalso has Gaussian blurred feature, so that the generated adversarial examplesare more transferable. Hence, we propose a mixed-input strategy, which can be\nexpressed as\nx\nmix=τ·V∗x+( 1−τ)·x, (19)\nwhere xmixis the mixed-input, τis the moving average factor and Vis pre-\ndeﬁned Gaussian kernel.\nIn order to make adversarial examples more indistinguishable and transfer-\nable, we naturally combine AI-FGPLM with mixed-input strategy to build a\nrobust attack, that is, MAI-FGPLM. MAI-FGPLM can be a basic structureto be integrated with other gradient-based attacks to get smaller perturbation\nsize and higher attack success rates. Particularly, NI-TI-DI-MAILM ensemble\nattack method is summarized in Algorithm 1which achieves the highest black-\nbox average attack success rate and the best indistinguishability in this paper. In\naddition, if we do not consider the Nesterov Accelerated Gradient in algorithm\n1, NI-TI-DI-MAILM will degrade to TI-DI-MAILM.\n4 Experiments\n4.1 Experimental Setting\nSee Table 1.\nDataset. We choose an ImageNet-compatible dataset which was used in the\nNIPS 2017 adversarial competition to perform experiments. This dataset is com-prised of 1000 images with size 299 ×299×3.', 'Learning Indistinguishable and Transferable Adversarial Examples 159\nTable 1. Abbreviations used in the paper.\nAbbreviation Deﬁnition\nTI-DIM The combination of MI-FGSM, TI2-FGSM and DI2-FGSM\nTI-DI-AITM The combination of AI-FGTM, TI2-FGSM and DI2-FGSM\nTI-DI-MAILM The combination of MAI-FGPLM, TI2-FGSM and DI2-FGSM\nNI-TI-DIM The combination of MI-FGSM, NI-FGSM, TI2-FGSM and DI2-FGSM\nNI-TI-DI-AITM The combination of AI-FGTM, NI-FGSM, TI2-FGSM and DI2-FGSM\nNI-TI-DI-MAILM The combination of MAI-FGPLM, NI-FGSM, TI2-FGSM and DI2-FGSM\nMetrics and Models. Two metrics, Attack Success Rate (ASR) and Average\nPerturbation Size (APS) of adversarial examples, are used to evaluate the attack\nperformance in this paper. The ASR refers to the percentage of adversarialexamples that can fool DNNs, while APS denotes image distortion. APS can be\ncomputed as /summationtextN\ni=1/summationtextM\nj=1|x/prime\nij−xij|\nN×M, where Ndenotes the total number of images,\nMis the dimensionality of clean image xand adversarial example x/prime,xijis the\npixel value of the j-th dimension of the i-th clean image, within range [0 ,255],\nandx/prime\nijis similar for adversarial example.\nWe consider four normally trained networks including Inception-v3 (Inc-\nv3) [17], Inception-v4 (Inc-v4), InceptionResnet-v2 (IncRes-v2) [ 16] and Resnet-\nv2-101 (Res-101) [ 7] as white-box models to generate adversarial examples. Then\nwe select six defense models as black-box models to evaluate the transferability\nof generated adversarial example. Three of the six defense models are ensem-\nble adverasarially trained models such as Inc-v3 ens3, Inc-v3 ens4and IncRes-\nv2ens[19]. The other are high-level representation guided denoiser (HGD) [ 10],\ninput transformation through random resizing and padding (R&P) [ 20], and\nNIPS-r3 in the NIPS 2017 adversarial competition.\nImplementation Details. In our experiment, we set perturbation ε= 16, the\nnumber of iteration T= 10 and the momentum decay factor μ=1.0a si n[ 3].\nAlso, we set exponential decay rates μ1=1.5,μ2=1.9, the transformation\nprobability p=1.0 and scale factor λ=1.3a si n[ 23]. For TI-DIM and NI-\nTI-DIM, the size of kernel Wis setted to 15 ×15 as in [ 4]. For TI-DI-AITM\nand NI-TI-DI-AITM, the size of kernel Wis setted to 9 ×9a si n[ 23]. For our\nmethods, we set ρ=0.5,γ1=0.12,γ2=0.12,τ=0.5a n d V=9×9.\n4.2 Ablation Study\nIn order to study the eﬀects of the piecewise linear function, the mixed-input\nstrategy and the gradient regularization term, we conduct an ablation experi-ment here. The adversarial examples are crafted for the ensemble of Inc-v3, Inc-\nv4, IncRes-v2 and Res-101 using the baseline method NI-TI-DI-AITM under the\ncorresponding settings in Table 2. In other words, the baseline method NI-TI-DI-\nAITM will evolve into our method NI-TI-DI-MAILM after gradually applying\nthe piecewise linear function, the gradient regularization term and the mixed-\ninput strategy. From Table 2, we can ﬁnd that the piecewise linear function and', '160 W. Zhang et al.\nthe gradient regularization term play important roles in reducing the APS of\nadversarial examples, e.g., the APS decreases from 8.11 to 7.31 by the piece-\nwise linear function and decreases from 7.41 to 7.00 by the gradient regulariza-\ntion term. We also observe that the mixed-input strategy mainly contribute to\nincreasing the ASR though slightly making the APS get higher (Table 2).\nTable 2. We show ASR and APS against six defense models. Note that PL, MIX and\nGR denote piecewise linear function, mixed-input strategy and gradient regularizationterm respectively.\nPLMIX GRInc-v3 ens3Inc-v3 ens4IncRes-v2 ensHGD R&P NIPS-r3 Average ASR APS\n×\n/check×\n×\n/check\n/check×\n×\n/check\n×\n/check\n/check×\n××\n/check\n×\n/check90.5\n90.792.3\n90.7\n92.4\n92.088.8\n89.291.2\n89.7\n91.2\n91.185.5\n86.088.2\n85.7\n88.4\n88.389.8\n89.791.5\n90.5\n91.3\n91.588.6\n88.990.9\n88.8\n91.0\n91.089.0\n88.990.9\n89.1\n91.1\n90.788.7\n88.990.8\n89.1\n90.9\n90.88.11\n7.318.21\n7.91\n7.41\n7.00\n4.3 The Results of Single-Model Attacks\nIn this section, we report the results of attacks against defense models under\nsingle-model setting. We ﬁrst conduct adversarial attacks on Inc-v3, Incv4,IncRes-v2 and Res-101 respectively using our methods and the baseline meth-\nods. Then we test the generated adversarial examples on six black-box defense\nmodels. The results are present in Table 3and Table 5.\nFrom Table 3and Table 5, we observe that our methods outperform all other\nbaseline methods on the average ASR and the APS. It demonstrates that our\nmethods can generate more transferable and indistinguishable adversarial exam-ples. Moreover, our methods combined with the Nesterov Accelerated Gradient\nmake the APS of adversarial examples decrease by a larger margin in Table 5,\nwhich indicates that adversarial examples can get better indistinguishabilitywhen our methods are combined with the Nesterov Accelerated Gradient.\nTable 3. The ASR (%) and APS of adversarial examples crafted by AI-FGTM and\nMAI-FGPLM against six defense models under single-model setting.\nModel Attack Inc-v3 ens3Inc-v3 ens4IncRes-v2 ensHGD R&P NIPS-r3 Average ASR APS\nInc-v3AI-FGTM\nMAI-FGPLM (Ours)20.0\n23.119.6\n24.19.4\n11.78.6\n12.08.8\n11.311.6\n13.413.0\n15.97.65\n6.98\nInc-v4AI-FGTM\nMAI-FGPLM (Ours)22.1\n26.722.8\n28.212.3\n16.013.7\n19.512.6\n16.114.7\n19.116.4\n20.97.99\n7.28\nIncRes-v2AI-FGTMMAI-FGPLM (Ours) 30.0\n37.727.9\n35.920.8\n29.222.8\n29.819.8\n26.621.1\n28.523.7\n31.38.12\n7.49\nRes-101AI-FGTM\nMAI-FGPLM (Ours)25.8\n27.526.4\n29.616.4\n18.719.6\n21.915.5\n18.517.5\n19.720.2\n22.77.58\n7.09', 'Learning Indistinguishable and Transferable Adversarial Examples 161\nTable 4. The ASR (%) and APS of adversarial examples crafted by NI-TI-DIM, NI-TI-\nDI-AITM and NI-TI-DI-MAILM against six defense models under single-model setting.\nModel Attack Inc-v3 ens3Inc-v3 ens4IncRes-v2 ensHGD R&P NIPS-r3 Average ASR APS\nInc-v3TI-DIM\nTI-DI-AITM\nTI-DI-MAILM (Ours)44.8\n43.3\n51.845.5\n52.2\n55.334.3\n38.6\n40.036.4\n44.0\n44.137.0\n42.3\n43.039.2\n46.4\n47.339.5\n46.3\n47.510.3\n8.22\n7.06\nInc-v4TI-DIM\nTI-DI-AITM\nTI-DI-MAILM (Ours)47.4\n53.9\n56.547.0\n51.8\n55.837.7\n42.4\n45.539.8\n48.1\n50.040.0\n46.2\n50.141.9\n49.0\n52.242.3\n48.6\n51.710.3\n8.29\n7.22\nIncRes-v2TI-DIMTI-DI-AITMTI-DI-MAILM (Ours)60.2\n65.8\n68.859.662.1\n66.758.661.4\n65.558.563.2\n64.562.464.2\n68.260.764.7\n68.760.063.6\n67.110.58.23\n7.03\nRes-101TI-DIMTI-DI-AITM\nTI-DI-MAILM (Ours)59.5\n63.4\n65.058.7\n60.5\n62.451.8\n53.7\n53.854.2\n58.9\n57.153.9\n56.7\n56.055.3\n59.9\n59.655.6\n58.9\n59.010.4\n7.98\n6.93\nTable 5. The ASR (%) and APS of adversarial examples crafted by NI-TI-DIM, NI-TI-\nDI-AITM and NI-TI-DI-MAILM against six defense models under single-model setting.\nModel Attack Inc-v3 ens3Inc-v3 ens4IncRes-v2 ensHGD R&P NIPS-r3 Average ASR APS\nInc-v3NI-TI-DIM\nNI-TI-DI-AITM\nNI-TI-DI-MAILM (Ours)41.3\n52.4\n52.541.7\n50.1\n53.430.5\n37.1\n37.232.2\n43.0\n41.632.4\n40.0\n40.535.8\n45.1\n45.635.7\n44.6\n45.110.3\n8.16\n6.85\nInc-v4NI-TI-DIM\nNI-TI-DI-AITM\nNI-TI-DI-MAILM (Ours)45.6\n54.5\n55.544.5\n51.2\n53.634.9\n42.3\n43.337.5\n48.5\n49.936.9\n46.5\n47.839.4\n49.0\n51.339.8\n48.7\n50.210.3\n8.34\n7.06\nIncRes-v2NI-TI-DIMNI-TI-DI-AITMNI-TI-DI-MAILM (Ours)57.0\n64.9\n68.254.862.2\n65.853.461.4\n64.853.864.0\n64.557.564.4\n67.857.365.5\n68.555.663.7\n66.610.38.33\n6.93\nRes-101NI-TI-DIMNI-TI-DI-AITM\nNI-TI-DI-MAILM (Ours)57.4\n62.4\n65.557.3\n61.0\n63.949.3\n54.2\n53.953.6\n60.1\n60.051.9\n57.1\n56.554.4\n60.2\n59.754.0\n59.2\n59.910.4\n7.88\n6.72\n4.4 The Results of Multi-model Attacks\nIn this section, we show the results of our methods when attacking multiple mod-\nels simultaneously. We adopt the ensemble strategy [ 12] to attack an ensemble\nof four normally trained models with equal ensemble weights using TI-DIM, TI-\nDI-AITM, TI-DI-MAILM, NI-TI-DIM, NI-TI-DI-AITM and NI-TI-DI-MAILMrespectively. The results are present in Table 6.\nTable 6. The ASR (%) and APS against six defense models under multi-model setting.\nThe adversarial examples are crafted for the ensemble of Inc-v3, Inc-v4, IncRes-v2 andRes-101 using the baseline methods and our methods.\nAttack Inc-v3 ens3Inc-v3 ens4IncRes-v2 ensHGD R&P NIPS-r3 Average ASR APS\nTI-DIM\nTI-DI-AITM\nTI-DI-MAILM (Ours)79.5\n89.3\n91.079.0\n87.8\n89.875.0\n84.3\n87.377.8\n87.9\n89.878.0\n87.2\n89.579.2\n87.5\n89.478.1\n87.3\n89.510.4\n8.14\n7.24\nNI-TI-DIM\nNI-TI-DI-AITM\nNI-TI-DI-MAILM (Ours)80.5\n90.5\n92.080.1\n88.8\n91.175.2\n85.5\n88.379.5\n89.8\n91.579.3\n88.6\n91.080.5\n89.0\n90.779.2\n88.7\n90.810.3\n8.11\n7.00', '162 W. Zhang et al.\nClean Image NI-TI-DIM NI-TI-DI-AITM NI-TI-DI-MAILM( Ours )\nFig. 3. The comparison of adversarial examples generated by NI-TI-DIM, NI-TI-DI-\nAITM and NI-TI-DI-MAILM ensemble attack method.\nWe can see that our methods can get higher success rates and smaller pertur-\nbation size than the baseline methods in Table 6. For example, our best attack\nmethod, NI-TI-DI-MAILM can fool six black-box defense models with an APS\nof 7.00 and an average ASR of 90.8%, while the state-of-the-art baseline methodNI-TI-DI-AITM has an APS of 8.11 and an average ASR of 88.7%. In other\nwords, our method achieves the average perturbation size decreased by 1.11 and\nthe average success rate increased by 2.1% compared with NI-TI-DI-AITM. Fur-thermore, we show some adversarial examples generated by the baseline methods\nand NI-TI-DI-MAILM under multi-model setting in Fig. 3. The ﬁrst column of\neach ﬁgure denotes the clean images, the second column and the third column\ndenote the adversarial examples generated by the baseline methods, and the\nlast column denotes the adversarial examples generated by NI-TI-DI-MAILM.As shown in Fig. 3, the adversarial example generated by our method are more\nsimilar to clean image than the baseline methods.\n5 Conclusion\nIn this paper, we comprehensively analyze why the fast gradient sign attack\nseries generate adversarial examples with higher perturbation size, and ﬁnd thatthe gradient processing steps lead to the perturbation size increase. Base on', 'Learning Indistinguishable and Transferable Adversarial Examples 163\nthis analysis, we propose a mixed-input Adam iterative fast gradient piecewise\nlinear method to generate more indistinguishable and transferable adversarialexamples. In addition, our method can be integrated into any gradient-based\nattack method to form stronger attack methods. The results on an ImageNet-\ncompatible dataset show that our method can get smaller perturbation size andhigher attack success rates under single-model setting and multi-model setting.\nOur best attack, NI-TI-DI-MAILM, can fool six classic black-box defenses with\nthe average perturbation size of 7.0 and the average success rate of 90.8%.\nReferences\n1. Brendel, W., Rauber, J., Bethge, M.: Decision-based adversarial attacks: reliable\nattacks against black-box machine learning models. In: 6th International Confer-ence on Learning Representations, ICLR (2018)\n2. Chen, J., Jordan, M.I.: Boundary attack++: query-eﬃcient decision-based adver-\nsarial attack\n3. Dong, Y., et al.: Boosting adversarial attacks with momentum. In: 2018 IEEE\nConference on Computer Vision and Pattern Recognition, CVPR, pp. 9185–9193\n(2018)\n4. Dong, Y., Pang, T., Su, H., Zhu, J.: Evading defenses to transferable adversar-\nial examples by translation-invariant attacks. In: IEEE Conference on Computer\nVision and Pattern Recognition, CVPR, pp. 4312–4321 (2019)\n5. Evtimov, I., et al.: Robust physical-world attacks on machine learning models.\narXiv preprint arXiv:1707.08945 (2017)\n6. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial\nexamples. In: 3rd International Conference on Learning Representations, ICLR\n(2015)\n7. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks. In:\nLeibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9908, pp.\n630–645. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46493-0\n38\n8. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. In: 3rd Inter-\nnational Conference on Learning Representations, ICLR (2015)\n9. Kurakin, A., Goodfellow, I.J., Bengio, S.: Adversarial examples in the physical\nworld. arXiv preprint arXiv:1607.02533 (2016)\n10. Liao, F., Liang, M., Dong, Y., Pang, T., Hu, X., Zhu, J.: Defense against adversarial\nattacks using high-level representation guided denoiser. In: 2018 IEEE Conferenceon Computer Vision and Pattern Recognition, CVPR, pp. 1778–1787 (2018)\n11. Lin, J., Song, C., He, K., Wang, L., Hopcroft, J.E.: Nesterov accelerated gradient\nand scale invariance for adversarial attacks. In: 8th International Conference onLearning Representations, ICLR (2020)\n12. Liu, Y., Chen, X., Liu, C., Song, D.: Delving into transferable adversarial examples\nand black-box attacks. In: 5th International Conference on Learning Representa-tions, ICLR (2017)\n13. Narodytska, N., Kasiviswanathan, S.P.: Simple black-box adversarial perturbations\nfor deep networks. arXiv preprint arXiv:1612.06299 (2016)\n14. Rauber, J., Brendel, W., Bethge, M.: Foolbox v0.8.0: a python toolbox to bench-\nmark the robustness of machine learning models. arXiv preprint arXiv:1707.04131\n(2017)', '164 W. Zhang et al.\n15. Sharif, M., Bhagavatula, S., Bauer, L., Reiter, M.K.: Accessorize to a crime: real\nand stealthy attacks on state-of-the-art face recognition. In: Proceedings of the\n2016 ACM SIGSAC Conference on Computer and Communications Security, pp.1528–1540 (2016)\n16. Szegedy, C., Ioﬀe, S., Vanhoucke, V., Alemi, A.A.: Inception-v4, inception-ResNet\nand the impact of residual connections on learning. In: Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence, pp. 4278–4284 (2017)\n17. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the incep-\ntion architecture for computer vision. In: 2016 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR, pp. 2818–2826 (2016)\n18. Szegedy, C., et al.: Intriguing properties of neural networks. In: 2nd International\nConference on Learning Representations, ICLR (2014)\n19. Tram` er, F., Kurakin, A., Papernot, N., Boneh, D., McDaniel, P.D.: Ensemble\nadversarial training: attacks and defenses. arXiv preprint arXiv:1705.07204 (2017)\n20. Xie, C., Wang, J., Zhang, Z., Ren, Z., Yuille, A.L.: Mitigating adversarial eﬀects\nthrough randomization. arXiv preprint arXiv:1711.01991 (2017)\n21. Xie, C., et al.: Improving transferability of adversarial examples with input diver-\nsity. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR,\npp. 2730–2739 (2019)\n22. Zhang, Z., Qiao, K., Jiang, L., Wang, L., Yan, B.: AdvJND: generating adversarial\nexamples with just noticeable diﬀerence. arXiv preprint arXiv:2002.00179 (2020)\n23. Zou, J., Pan, Z., Qiu, J., Duan, Y., Liu, X., Pan, Y.: Making adversarial examples\nmore transferable and indistinguishable. arXiv preprint arXiv:2007.03838 (2020)', 'Efﬁcient Object Detection and Classiﬁcation\nof Ground Objects from Thermal Infrared\nRemote Sensing Image Based on Deep Learning\nFalin Wu1(B), Guopeng Zhou1, Jiaqi He1, Haolun Li1, Yushuang Liu2,\nand Gongliu Yang1\n1SNARS Laboratory, School of Instrumentation and Optoelectronic Engineering, Beihang\nUniversity, Beijing 100191, China\nfalin.wu@buaa.edu.cn\n2Beijing System Design Institute of Electro-Mechanic Engineering, Beijing 100854, China\nAbstract. Wild searching and nature reserve monitoring are formidable tasks. In\norder to relieve the current pressure of general manpower observation, drone aerialsurveillance using visible and thermal infrared (TIR) cameras is increasingly being\nadopted. Automatic data acquisition has become easier with advances in unmanned\naerial vehicles (UA Vs) and sensors like TIR cameras, which enables executivesto search and detect ground objects at night. However, it’s still a challenge to\naccurately and quickly process the large amount of TIR data generated from this.\nIn response to the above problems, this paper designs an enhanced ground objectdetection network (UA V-TIR Retinanet) for the UA V thermal imaging system. The\nnetwork uses the Retinanet as infrastructure, extracts shallow features according to\nthe characteristics of thermal infrared remote sensing images, introduces an atten-tion mechanism and adaptive receptive ﬁeld mechanism. The method achieves the\nbest speed-accuracy trade-off on the dataset, reporting 74.47% AP at 23.48 FPS.\nKeywords: Ground object detection ·Thermal infrared remote sensing image ·\nDeep learning ·Attention mechanism ·Adaptive multiscale receptive ﬁeld\n1 Introduction\nWith the rapid development of economy and society, mankind has put forward higher\nrequirements for wild searching and nature reserve monitoring. Thermal infrared imag-\ning technology can transform the thermal radiation captured by sensors into thermal\ninfrared images, and reﬂect the thermal radiation characteristics in the observation rangethrough imaging. At the same time, thermal infrared imaging technology is not sensitive\nto light changes, which can effectively image in weak light, and can maintain good pen-\netration in fog, rain, snow weather [ 1]. With the rapid development of UA V technology,\nits related applications have not only achieved remarkable results in the military ﬁeld,\nbut also showed strong applicability in the civil ﬁeld. Especially with the development\nof light UA V platform, there are a large number of light UA V remote sensing loads, suchas optical, infrared spectrum, lidar, synthetic aperture radar and so on [ 2,3]. Combined\n© Springer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 165–175, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_14', '166 F. Wu et al.\nwith the ﬂexibility of UA V and the advantages of thermal infrared imaging technology,\nit can provide powerful information support for search and rescue, animal monitoringand other ﬁelds.\nThe ground image obtained by UA V thermal infrared remote sensing has the char-\nacteristics of too few object pixels, too large difference of object types observed fromdifferent angles, and difﬁcult to distinguish objects in complex background, which also\nconstitute the difﬁculty of this kind of object detection task. The use of traditional object\ndetection algorithm, which often requires noise reduction, image enhancement, manualselection of design features, extraction of features, and binarization to obtain the object,\nhas a high false detection rate, poor robustness, and long processing time [ 4]. Thanks to\nthe algorithm of deep convolutional neural network (CNN): A series of detection algo-rithms based on R-CNN have emerged in academic circles, such as SPP-Net (Spatial\nPyramid Potential Net) [ 5], Fast-RCNN [ 6], Faster-RCNN [ 7] and so on, the detection\naccuracy has been greatly improved. At the same time, there are end-to-end detection\nalgorithms which are different from the R-CNN series of two-step detection methods,\nsuch as SSD (Single Shot Multi Box Detector) [ 8], RetinaNet [ 9] and Yolo (You Only\nLook Once) [ 10], the detection speed has been greatly improved.\nOn balance, deep learning allows the computer to ﬁnd the best solution of the task\nas much as possible. It has strong feature learning ability, and can achieve the relativebalance between detection accuracy and detection speed through reasonable design.\nTherefore, it has considerable potential and broad development space to study the ground\nobject detection algorithm of thermal infrared remote sensing image based on deeplearning.\n2 Related Work\nIn the task of computer vision, the existing object detection models are widely used\nin the object detection of natural images [ 11]. The thermal infrared remote sensing\nimage is obviously different from the ordinary natural imaging in the aspects of imagingmechanism, imaging conditions, shooting equipment, shooting angle, etc.: the thermal\ninfrared image has only one channel and lacks color features, and the texture of the\nthermal infrared image is relatively weak compared with the visible image, rich tex-ture information cannot be obtained. At the same time, compared with natural images,\nthermal infrared images have the characteristics of low resolution and more noise. In\naddition, some high temperature points can also lead to defects such as sharp changes inthe brightness of thermal infrared data. These problems bring difﬁculties to the object\ndetection task of thermal infrared image, so there is a certain difference between the\nobject detection task of ordinary image and the ground object detection task of thermal\ninfrared remote sensing image. What’s more, most of the large-scale image data sets are\nfocused on the visible light ﬁeld. With the popularization of UA V technology, there aresome visible light data sets of ground vehicles and pedestrians from UA V perspective,\nbut a large number of thermal infrared data are still difﬁcult to obtain [ 12].', 'Efﬁcient Object Detection and Classiﬁcation of Ground Objects 167\n3 Methodology\nAccording to the application requirements of UA V thermal infrared remote sensing\nin the future, we designed an enhanced ground object detection network (UA V-TIR\nRetinanet) for the UA V thermal imaging system. The network uses the Retinanet asinfrastructure. According to the low pixel characteristics of infrared small object, the\nshallow convolution layer features are extracted, and the shallow convolution feature\noutput layer is constructed to improve the object detection rate. The attention mechanismis introduced to enhance the attention to the effective features of ground objects. The\nadaptive receptive ﬁeld mechanism is designed to realize the network receptive ﬁeld\nadaptation. Figure 1shows algorithm ﬂowchart.\nFig. 1. Algorithm ﬂowchart.\n3.1 Backbone Network\nThe residual module in Resnet [ 13] makes the gradient dispersion disappear, which can\nmake the network design as deep as possible, thus obtaining stronger representation\nability. Resnet has two basic blocks. One is Identity Block, the input and output data\ndimensions are the same, so multiple blocks can be concatenated; Another basic block\nis Conv Block. The input and output data dimensions are different, so they cannot beconnected in series. Its function is to change the dimension of feature vector. The ﬁnal\nResnet backbone is made up of the above two blocks. This paper selects Resnet50 as\nthe backbone network.\nAn efﬁcient channel attention module ECAM [ 14] is designed, as shown in Fig. 2.T h e\ninput features are ﬁrst processed by global average pooling (GAP) channel by channel,\nand cross channel interactive learning is carried out without reducing the number ofchannels, so as to effectively improve the representation of features at the cost of little\ncomputation. The number of cross channels K is 3. We add the feature graph weighting\nmodule of ECAM to the residual module of Resnet backbone. It is worth noting that', '168 F. Wu et al.\nthe designed ECA module not only improves the detection performance, but also hardly\naffects the detection speed.\nFig. 2. ECA module in resnet.\n3.2 Feature Pyramid Networks\nThe ground object in the UA V remote sensing image, with the increase of ﬂight altitude,\nthe object occupies less and less pixels. The small object is sent to the network, and\nmay gradually disappear in the process of several down sampling, because the object\npixel is too small is a common feature in the UA V remote sensing image, so we should\npay attention to the feature representation of small objects when construct the feature\npyramid. Using the inherent multi-scale and multi-level pyramid structure of deep con-volution neural network to construct feature pyramid network, and fusing high-level\nfeatures and low-level features, this pyramid network structure formed by constructing\nhigh-level semantic feature map at all scales is feature pyramid networks (FPN) [ 15].\nDue to the large size of the feature map, the shallow features can better reﬂect the\nposition of the feature corresponding to the original image, so the positioning is more\naccurate than the high-level features. Deep features are extracted by multi-layer con-volution, and their semantic information is higher than that of the lower layer, but they\ncannot be mapped back to the original image domain accurately, resulting in the weak\npositioning ability. Therefore, we should combine the advantages of shallow featuresand deep features to build a feature pyramid with excellent classiﬁcation judgment and\npositioning ability, this process can be realized by sampling on the feature map, com-\npressing or amplifying the feature channel. Taking Resnet-50 backbone as an exampleto construct the feature pyramid, as shown in Fig. 3.\nAlthough the top-down FPN designed above can fuse the shallow features and high-\nlevel features from the feature extraction network, it is also limited by the inherent\none-way information ﬂow. In order to improve the efﬁciency of the model, refer-ring\nto the idea of Bi-FPN [ 16], this paper designs a cross scale connection optimization\nmethod, Fig. 4shows example of building a weighted bi-directional feature pyramid.\nFirst, delete those nodes with only one input side. If a node has only one input edge\nand no feature fusion, it has less contribution to the feature network aiming at fusingdifferent features, which can greatly simplify the bidirectional network; Secondly, if they\nare at the same level, we add additional edges from the original input to output nodes,\nsimilar to the idea of shortcut channel, so as to fuse more features without increasingtoo much cost; Thirdly, each bidirectional (top-down and bottom-up) path is regarded\nas a feature network layer, and repeated many times in the same layer to achieve higher\nlevel feature fusion. When fusing features of different resolutions, the top-down FPN', 'Efﬁcient Object Detection and Classiﬁcation of Ground Objects 169\nFig. 3. Example of building feature pyramid.\ndesigned above ﬁrst adjusts them to the same resolution, and then summarizes them.\nBecause different input features have different resolutions, their contributions to output\nfeatures are usually unequal. In order to solve this problem, we consider adding an extraweight to each input to let the net- work learn the importance of each input feature.\nFig. 4. Example of building a weighted bi-directional feature pyramid.\n3.3 Adaptive Multiscale Receptive Field\nReceptive ﬁeld is the area size of the pixels on the output feature map of each layer\nof convolutional neural network mapped on the input image, which is similar to the\nhuman visual system [ 17]. Liu s imitated the structure of receptive ﬁeld in human visual\nsystem, and discussed the discrimination and robustness of receptive ﬁeld with different', '170 F. Wu et al.\nsizes and eccentricities [ 18]. Experiments show that the fusion of receptive ﬁelds with\ndifferent sizes and eccentricities can effectively improve the object detection accuracy,and can be embedded into the mainstream object detection model.\nIn the feature pyramid constructed above, the receptive ﬁeld size of each detection\nlayer feature is ﬁxed with the network structure, but the prior anchor frame scale isvariable. For small objects, there may be a mismatch between the hierarchical receptive\nﬁeld and the anchor frame. At the same time, although a small receptive ﬁeld is conducive\nto small object detection, compared with the object scale, the receptive ﬁeld smaller thanor close to the small object scale cannot play the best effect for small object detection.\nIt is necessary to increase the receptive ﬁeld appropriately and use the effective context\ninformation around the detected object to improve the detection effect of small objects.\nInspired by the structure of perception [ 19], this paper designs a adaptive multi-\nscale receptive ﬁeld module, which mainly includes two parts: multi-scale receptive\nﬁeld fusion module and adaptive selection module. It integrates different receptive ﬁeld\nfeatures and makes full use of multi-scale information. The purpose is to enhance the\nfeature representation ability of the network for small objects while minimizing theincrease of network parameters, The network structure is shown in Fig. 5.\nIn the multi-scale receptive ﬁeld fusion module, the receptive ﬁelds of different scales\nare expanded and fused through multiple shared convolutions. The adaptive selectionmodule uses the attention mechanism to make effective adaptive selection for the multi-\nscale features of different sizes of receptive ﬁeld, which makes the receptive ﬁeld more\nrealistic and closer to the biological visual system.\nFig. 5. Adaptive multiscale receptive ﬁeld module.\n3.4 Loss Function\nThe loss function is composed of two parts: one is the conﬁdence branch loss function\nof the category prediction, the other is the branch loss function of the boundary frame\nnetwork which is responsible for the shape and position prediction of the boundaryframe.\nFor the object detection task of UA V thermal infrared remote sensing data, although\nthe thermal radiation information can be more efﬁcient and accurate to capture theinformation of ground object in the extreme environment such as weak light, night,\nrain and snow weather, it is difﬁcult to detect the ground object when the back- ground\nenvironment is complex and the ambient temperature is high. When the conditions arelimited, it’s supposed to improve the performance of thermal infrared object detection in\nthe midday environment by improving the algorithm as much as possible. This requires\nsome hard mining methods. Therefore, we add the category loss in the form of focal', 'Efﬁcient Object Detection and Classiﬁcation of Ground Objects 171\nloss in the design of loss function [ 9]. On the one hand, we can mine the difﬁcult cases,\non the other hand, we can adjust the imbalance of positive and negative samples in thesingle stage object detection task.\n(1)\nwhere y is the true value, and a value of 1 indicates that the boundary box contains\nobjects. This value can be judged by predicting whether the intersection ratio between\nthe border and the real Border exceeds the threshold value, pis the conﬁdence prediction\nvalue. The addition of parameter γcan reduce the loss of simple samples and increase\nthe loss of difﬁcult samples. The addition of parameter αcan balance the positive and\nnegative samples in model training. When building the model, γtakes 2.5, αtakes 0.25.\nWhat’s more, the loss function of the branch of the bounding box network is the Smooth\nL1 loss [ 7,8] between the coordinates of the center point, the size of boundary box and\nthe true value, which will not be repeated here.\n4 Experiments and Results\n4.1 Dataset\nThe data set selected in this paper comes from a long wave thermal infrared (TIR)\ndataset released by Harvard University in 2020, which contains night images of animalsand humans in southern South Africa [ 12]. After the data set is ﬁltered, eliminated\nand enhanced, a total of 15850 available images were screened, with the resolution\nconcentrated in 640 ×480. The category information in the tag includes human and\nanimals (elephant, giraffe, lion, etc.). Some examples of data augmentation are shown\nin Fig. 6.\nFig. 6. Some examples of data augmentation.\nAt the same time, in order to better set the anchor frame information of the network,\nthe k-means++ [ 20] method is used to cluster the length and width information of the\nobjects in the data set, as shown in Fig. 7.\n4.2 Results\nIn object detection task, mean average precision (mAP) is usually used to evaluate the\nperformance of a detection model. In addition, considering the application background', '172 F. Wu et al.\nFig. 7. The width and height distribution of objects.\nof search and monitoring in the future, the number of images detected per second (FPS)\nis used to evaluate the detection speed [ 21]. In the test phase, the detection accuracy\nand speed of the proposed method and several general object detection algorithms are\ncompared. The results are shown in Table 1and Table 2.\nTable 1. Test result of UA V-TIR retinanet.\nBackbone Feature fusion AMRF mAP FPS\nResnet FPN 67.94% 24.14\nECA-resnet FPN 73.02% 23.73\nECA-resnet Bi-FPN 74.38% 18.50\nECA-resnet FPN√74.47% 23.48\nECA-resnet Bi-FPN√74.93% 18.37\nTable 2. Test result of general object detection model.\nModel Backbone mAP FPS\nFaster R-CNN Resnet 43.64% 12.00\nSSD Resnet 31.11% 33.10\nYOLO Resnet 28.20% 38.32\nCompared with several popular general detection models, the accuracy of the model\ndesigned in this paper is improved by about 30%, and has a certain detection speed.\nPart of the detection results are shown in Fig. 8, all objects are detected and correctly\nidentiﬁed.', 'Efﬁcient Object Detection and Classiﬁcation of Ground Objects 173\n(a) \n(b) \n(c) \n(d) \nFig. 8. Test result: the left side is the original image, and the right side is the model output.', '174 F. Wu et al.\n5 Conclusions\nThe main purpose of this paper is to design an object detection method of UA V thermal\ninfrared scene during the night based on deep learning theory. On the basis of balancing\ndetection accuracy and detection speed, it can realize the function of automatic detec-tion of ground objects such as human and animals, and further explore the application\npotential of ground object detection methods, which based on deep learning in thermal\ninfrared remote sensing images, in wild searching, disaster prevention, and animal mon-itoring. The UA V-TIR Retinanet designed for the UA V thermal imaging system, uses the\nRetinanet as infrastructure, extracts shallow features according to the characteristics of\nthermal infrared remote sensing images, introduces an attention mechanism and adap-tive receptive ﬁeld mechanism. The method achieves the best speed-accuracy trade-off\non the dataset, reporting 74.47% AP at 23.48 FPS.\nThe development of object detection algorithm is changing with each passing day.\nSometimes, speciﬁc remote sensing tasks need to customize the algorithm in depth in\norder to achieve the established goal. Taking this paper as an example, the subsequentUA V thermal infrared remote sensing ground object detection model can be improved\nin the direction of lighter weight, but the detection performance remains unchanged or\neven improved, so as to achieve the purpose of UA V real-time data processing. It can alsobe explored in Anchor Free direction to liberate the limitation of detector in multi-scale\nobject detection.\nAcknowledgements. We would like to express gratitude to the efforts of Bondi, Elizabeth and\nher team members for creating and making publicly available scientiﬁc data. We would also like\nto thank all the reviewers on their time and insightful comments which improved our manuscript.\nReferences\n1. He, Y ., et al.: Infrared machine vision and infrared thermography with deep learning: a review.\nInfrared Phys. Technol. 2021 , 103754 (2021)\n2. Yao, H., Qin, R., Chen, X.: Unmanned aerial vehicle for remote sensing applications—a\nreview. Remote Sens. 11(12), 1443 (2019)\n3. Feng, L., et al.: A comprehensive review on recent applications of unmanned aerial vehi-\ncle remote sensing with various sensors for high-throughput plant phenotyping. Comput.\nElectron. Agricult. 182, 106033 (2021)\n4. Rawat, S.S., Verma, S.K., Kumar, Y .: Review on recent development in infrared small target\ndetection algorithms. Procedia Comput. Sci. 167, 2496–2505 (2020)\n5. He, K., et al.: Spatial pyramid pooling in deep convolutional networks for visual recognition.\nIEEE Trans. Pattern Anal. Mach. Intell. 37(9), 1904–1916 (2015)\n6. Girshick, R.: Fast R-CNN. In: Proceedings of the IEEE International Conference on Computer\nVision (2015)\n7. Ren, S., et al.: Faster R-CNN: towards real-time object detection with region proposal\nnetworks. Adv. Neural Inf. Process. Syst. 28, 91–99 (2015)\n8. Liu, W., et al.: SSD: single shot multibox detector. In: Leibe, B., Matas, J., Sebe, N., Welling,\nM. (eds.) Computer Vision – ECCV 2016. ECCV 2016. LNCS, vol 9905, pp. 21–37. Springer,\nCham (2016). https://doi.org/10.1007/978-3-319-46448-0_2', 'Efﬁcient Object Detection and Classiﬁcation of Ground Objects 175\n9. Lin, T.-Y ., et al.: Focal loss for dense object detection. In: Proceedings of the IEEE\nInternational Conference on Computer Vision (2017)\n10. Redmon, J., et al.: You only look once: uniﬁed, real-time object detection. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition (2016)\n11. Kundid Vasi´ c, M., Papi´ c, V .: Multimodel deep learning for person detection in aerial images.\nElectronics 9(9), 1459 (2020)\n12. Bondi, E., et al.: BIRDSAI: a dataset for detection and tracking in aerial thermal infrared\nvideos. In: The IEEE Winter Conference on Applications of Computer Vision (2020)\n13. Wu, Z., Shen, C., Van Den Hengel, A.: Wider or deeper: revisiting the resnet model for visual\nrecognition. Pattern Recognit. 90, 119–133 (2019)\n14. Wang, Q., et al.: ECA-Net: efﬁcient channel attention for deep convolutional neural networks.\nIn: 2020 IEEE in CVF Conference on Computer Vision and Pattern Recognition (CVPR).\nIEEE (2020)\n15. Lin, T.-Y ., et al.: Feature pyramid networks for object detection. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (2017)\n16. Tan, M., Pang, R., Le, Q.V .: Efﬁcientdet: Scalable and efﬁcient object detection. In:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(2020)\n17. Luo, W., et al.: Understanding the effective receptive ﬁeld in deep convolutional neural net-\nworks. In: Proceedings of the 30th International Conference on Neural Information ProcessingSystems (2016)\n18. Liu, S., Huang, D., Wang, Y .: Receptive Field Block Net for Accurate and Fast Object Detec-\ntion. In: Ferrari, V ., Hebert, M., Sminchisescu, C., Weiss, Y . (eds.) ECCV 2018. LNCS, vol.11215, pp. 404–419. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01252-6_24\n19. Szegedy, C., et al.: Inception-v4, inception-resnet and the impact of residual connections on\nlearning. In: Thirty-ﬁrst AAAI Conference on Artiﬁcial Intelligence (2017)\n20. Liu, J., et al.: High precision detection algorithm based on improved RetinaNet for defect\nrecognition of transmission lines. Energy Rep. 6, 2430–2440 (2020)\n21. Cartucho, J., Ventura, R., Veloso, M.: Robust object recognition through symbiotic deep\nlearning in mobile robots. In: 2018 IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS). IEEE (2018)', 'MEMA-NAS: Memory-Eﬃcient\nMulti-Agent Neural Architecture Search\nQi Kong1,X i nX u2(B), and Liangliang Zhang1\n1Autonomous Driving Division, JD.com American Technologies Corporation,\nMountain View, CA 94043, USA\n{qi.kong,liangliang.zhang }@jd.com\n2Autonomous Driving Division, JD.com, Beijing, China\nxuxin178@jd.com\nAbstract. Object detection is a core computer vision task that aims to\nlocalize and classify categories for various objects in an image. With thedevelopment of convolutional neural networks, deep learning methods\nhave been widely used in the object detection task, achieving promis-\ning performance compared to traditional methods. However, design-ing a well-performing detection network is ineﬃcient. It consumes too\nmuch hardware resources and time to trial, and it also heavily relies\non expert knowledge. To eﬃciently design the neural network architec-ture, there has been a growing interest in automatically designing neu-\nral network architecture by Neural Architecture Search (NAS). In this\npaper, we propose a Memory-Eﬃcient Multi-Agent Neural ArchitectureSearch (MEMA-NAS) framework in end-to-end object detection neural\nnetwork. Speciﬁcally, we introduce the multi-agent learning to search\nholistic architecture of the detection network. In this way, a lot of GPUmemory is saved, allowing us to search each module’s architecture of the\ndetection network simultaneously. To ﬁnd a better tradeoﬀ between the\nprecision and computational costs, we add the resource constraint in ourmethod. Search experiments on multiple datasets show that MEMA-NAS\nachieves state-of-the-art results in search eﬃciency and precision.\n1 Introduction\nDeep learning has achieved promising results on a variety of tasks, such as image\nclassiﬁcation and object detection. However, most current neural networks are\ndesigned manually, which is time-consuming. Because of this, there has been a\ngrowing interest in automatically designing the neural network architecture byNeural Architecture Search (NAS).\nZoph et al. [ 20] proposed the ﬁrst NAS framework for the image classiﬁcation\ntask, which frames neural network search as a reinforcement learning problem.The key idea of their work is that they speciﬁed the architecture and connectivity\nof a neural network by using a conﬁguration string. To speed up the search, Zoph\nQ. Kong and X. Xu—Equal contribution.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 176–187, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_15', 'MEMA-NAS: Memory-Eﬃcient Multi-Agent Neural Architecture Search 177\net al. [ 21] then put forward a new search framework, named NAS-Net. Unlike\nprevious work searches on the global search space, NAS-Net only searches forthe architecture of the cell and then stacks searched cells together to generate a\ndeep neural network. Then Liu et al. [ 11] proposed PNASNet to further improve\nthe search eﬃciency. Besides these RL-based methods, some researchers attemptto adopt the evolutionary algorithm to search the neural network architecture.\nReal E et al. [ 14] proposed a novel search framework based on the evolutionary\nalgorithm, named AmoebaNet. AmobebaNet uses the same search space as theNAS-Net, searching for the architectures of the normal cell and the reduction\ncell. To ensure search eﬃciency, Chen et al. [ 4] introduce the reinforcement learn-\ning into the search framework based on evolutionary algorithm. They design amutation controller based on the reinforcement learning to learn the eﬀects of\nslight modiﬁcations and produce mutation actions.\nDespite the success of RL-based methods and EA-based methods, the huge\nsearch cost is still not acceptable. To further improve the search eﬃciency and\nreduce search costs, some NAS works try to use gradient optimization meth-ods to search optimal architectures. DARTS [ 12] is the ﬁrst gradient-based NAS\napproach, which relaxes the search space to be continuous so that the architec-\nture can be optimized with respect to its validation set performance by gradientdescent. Then some variants of DARTS, such as P-DARTS [ 3] and PC-DARTS\n[17], are proposed to further optimize remaining problems of DARTS.\nAlthough NAS works for the image classiﬁcation task have achieved great\nsuccess and searched models outperform human exports in classiﬁcation accu-\nracy, NAS for object detection is still poorly explored. The backbone is respon-\nsible for extracting semantic features of an image, which is an important modulein a detection network. Most detectors directly take the network designed for\nimage classiﬁcation tasks such as ResNet [ 7]o rR e s N e X t[ 15] as the backbone\nnetwork. There is a gap between the classiﬁcation task and the object detectiontask. Therefore, directly using the network designed for image classiﬁcation tasks\nmight be sub-optimal. To address the above issue, Chen et al. [ 19] proposed Det-\nNAS that applies NAS to design better backbone architecture for object detec-\ntion. They pre-trained a super-net which contains all candidate architectures on\nthe ImageNet dataset. Then they ﬁne-tuned the pre-trained super-net on theobject detection dataset and conducted architecture search on trained super-net\nwith an evolutionary algorithm (EA). However, in general, Batch Normaliza-\ntion (BN) cannot work well in the small batch size. To address this issue, Det-NAS replaces the batch normalization with Synchronized Batch Normalization\n(SyncBN) [ 2] when training the super-net.\nHigh-level features have strong semantic information and low resolution, and\nlow-level features are opposite. In general, strong semantic information is beneﬁ-\ncial to classiﬁcation and high-resolution is beneﬁcial to localization. Thus, FPN\nis proposed to fuse high-level features and low-level features to generate the fea-ture representations with strong semantic information and high-resolution. Some\nresearch works [ 8,9,13] proposed various cross-scale connections to fuse features\nto generate better feature representations. However, the FPN architecture of', '178 Q. Kong et al.\nthese works might not be optimal. Thus, Golnaz et al. [ 6]p r o p o s e da nF P N\nsearch framework named NAS-FPN. NAS-FPN designs the search space thatcan cover all possible cross-scale connections to generate better feature fusion\nrepresentations. Xu et al. [ 16] proposed an architecture search framework named\nAuto-FPN. Auto-FPN consists of two auto search modules for object detec-tion: Auto-fusion module aims at fusing the multi-level features and Auto-head\nmodule aims at searching for an eﬃcient RCNN head architecture for classiﬁ-\ncation and regression. However, Auto-FPN adopts the gradient-based algorithmto search for the network architecture, which is memory-ineﬃcient for object\ndetection. Since Auto-FPN consumes a large number of GPU memory, it only\nsearches for architectures of FPN and RCNN head one by one.\nCurrent NAS techniques face two challenges when applied to object detection:\n(1) Searching a detection network consumes too much GPU memory and time,\nwhich fails to enable an end-to-end search for the whole detection network; (2)\nThe ImageNet pre-training requirement makes it hard to directly search for the\narchitecture of the backbone. To address above problems, we propose a novelneural architecture search framework named Memory-Eﬃcient Multi-Agent NAS\n(MEMA-NAS). Speciﬁcally, during the search stage, we model the search process\nas a multi-agent learning problem. Multiple agents are established to supervisea set of candidate operations and all agents work together to ﬁnd the optimal\narchitecture. In this way, the GPU memory usage can be reduced to at least half\nof the gradient-based NAS method, ensuring that we have enough GPU memoryto search the architecture simultaneously.\nIn summary, the contributions of this paper are as follows:\n– We propose a novel end-to-end neural network architecture search framework\nfor object detection, named MEMA-NAS, which is more computationally eﬃ-\ncient compared to prior works. We introduce the multi-agent learning to solve\nthe typical problem of the memory explosion in gradient-based NAS frame-\nworks. Our method can complete a search of the whole detection network\nwithin 10 GPU days on the large-scale COCO dataset.\n– We prove the eﬀectiveness of MEMA-NAS with extensive experiments. It\nworks well on diﬀerent backbones (ResNet and ResNeXt) and various datasets\n(COCO and BDD dataset). Besides, our method demonstrates a strong gen-eralization ability when we transfer the searched models to other datasets\n(VOC dataset).\n2 MEMA-NAS Framework\nIn this section, we mainly introduce components of our end-to-end object detec-\ntion neural network architecture search framework named Memory-Eﬃcient\nMulti-Agent NAS (MEMA-NAS), including search algorithm, and the resource\nconstraint we used.\nDespite the success in the image classiﬁcation task, the gradient-based\nmethod (DARTS) is memory-ineﬃcient for object detection. The search space\nof the whole detection network-composed is much larger than the classiﬁcation', 'MEMA-NAS: Memory-Eﬃcient Multi-Agent Neural Architecture Search 179\nnetwork. Naively including all the candidate operations into a hyper-net and\nallowing all candidate operations to participate in the computation in per iter-ation will easily lead to GPU memory explosion. We attempt to address the\nabove issue by converting the NAS problem into a multi-agent learning prob-\nlem. The proposed method obviously reduces GPU memory usage comparedto DARTS, ensuring that architectures of the whole detection network can be\nsearched simultaneously.\n2.1 Multi-Agent Search Algorithm\nTo alleviate the problem of the huge computational complexity in DARTS,\ninspired by previous NAS work [ 1] for image classiﬁcation, we frame the NAS\nproblem as a multi-agent learning problem. Each agent is responsible for control-\nling a set of candidate operations and sequentially samples diﬀerent candidate\noperations based on the learned sample strategy. Agents interact with an envi-ronment, receiving feedback (In our work, the loss is used as the feedback) to\nupdate their sampling strategy over associated operations and all agents work\ntogether to ﬁnd the optimal architecture. Theoretically, since each agent sam-ples one candidate operation in per iteration, it saves\nK−1\nK(Kis the number of\ncandidate operations) GPU memory compared to DARTS [ 1].\nNeural Architecture Search as Multi-Agent Learning. As shown in\nFig.1, in the multi-agent search algorithm, the search space is represented as\na DAG with Nnodes and Vedges. We assign an agent for each edge ( i,j)\nto select an operation according to its sample strategy (action distribution)\na(Ai)\nt∼π(Ai)\nt/parenleftBig\na(Ai)\nt/parenrightBig\nin iteration t, where i∈{1,...,V }anda(Ai)\nt∈{1,...,K }.\nThus, we obtain a decision sequence at=/braceleftBig\na(A1)\nt,...,a(AV)\nt/bracerightBig\nand use its loss\nLt=(at,w t) to update the sample strategy (action distribution).\nSample Strategy. We denote the contribution of the operation kbelonging to\nthe agent AiascAi\nt[k] (similar to the architecture parameters αin DARTS) [ 1].\nEach agent Aisamples an operation from a softmax distribution in per iteration,\nwhich is formulated as:\ncAi\nt[k]=exp/parenleftBig\ncAi\nt[k]/parenrightBig\n/summationtextK\nj=1exp/parenleftBig\ncAi\nt[j]/parenrightBig, (1)\nwhere k/epsilon1{1,...K }andcAi\nt/epsilon1R|K|.\nUpdate Strategy. In our method, each agent Aidirectly associates the quality\nof its taken action with the loss Lt/parenleftBig\na(A1)\nt,...,a(AV)\nt/parenrightBig\nand the update strategy\nis given by:\nc(Ai)\nt[k]=/braceleftBigg\nc(Ai)\nt−1[k],a(Ai)\nt/negationslash=k,\n(1−μ)c(Ai)\nt−1[k]−μLt(at),a(Ai)\nt=k,(2)', '180 Q. Kong et al.\nFig. 1. An illustration of the multi-agent search algorithm, a directed acyclic graph\n(DAG) consists of Nnodes, each edge ( m, n) is assigned an agent Aito explore and\nsupervise all candidate operations, each agent Airecommends one in iteration taccord-\ning the current sample policy πAi\nt, the loss is used as the feedback to update its policy,\nand repeat the above process until the model converges.\nwhere at=/parenleftBig\na(A1)\nt,...,a(AV)\nt/parenrightBig\nandμis a momentum coeﬃcient. After search\nstage, each agent selects the optimal operation: o∗(i)= arg max\nkcAi[k], where\nk/epsilon1{1,...K }, to generate the ﬁnal architecture.\nOptimization. The parameters of our method can be divided into two\ntypes of parameters: Model parameters wand architecture parameters α=\n{αB,α N,α H}, where αB,α N,α Hdenote the architecture parameters for Back-\nbone, FPN neck and RCNN head, respectively. We split the training dataset into\ntraining data to optimize wby∇wLtrain(w,α) and validation data to optimize α\nusing Eq. 2. Following DARTS, we iteratively optimize wandαuntil the search\nmodel converges.\n2.2 Resource Constraint\nConsidering that the eﬃciency is a crucial factor for a detection system, we\nfurther add a resource constraint in our method. Speciﬁcally, we consider three\nmetrics for the resource constraint C(αB,α N,α H) : parameter size; ﬂoating-\npoint operations (FLOPs); and memory access cost (MAC), and the resource\nconstraint is calculated:\nC(αB,α N,α H)=/summationdisplay\niC/parenleftbig\nO/parenleftbig\nαi\nB/parenrightbig/parenrightbig\n+/summationdisplay\njC/parenleftBigg\nO/parenleftBig\nαj\nN/parenrightBig\n+/summationdisplay\nkC/parenleftbig\nO/parenleftbig\nαk\nH/parenrightbig/parenrightbig/parenrightBigg\n,(3)', 'MEMA-NAS: Memory-Eﬃcient Multi-Agent Neural Architecture Search 181\nwhere C/parenleftbig\nO/parenleftbig\nαi\nB/parenrightbig/parenrightbig\n,C/parenleftBig\nO/parenleftBig\nαj\nN/parenrightBig/parenrightBig\nandC/parenleftbig\nO/parenleftbig\nαk\nH/parenrightbig/parenrightbig\nare computational costs of\noperations sampled from agent αi\nB,αj\nN,a n d αk\nHrespectively, and the computa-\ntional cost is a sum of normalized parameter size, FLOPs, and MAC. In thiswork, we add C(α\nB,α N,α H) as a regularization term into the loss function to\ncontrol the computational complexity of the searched model:\nL(w,α)=Lmodel(w,α)+λC(αB,α N,α H), (4)\nwhere λis a hyper-parameter to control the importance of C(αB,α N,α H).\n(a) Searched backbone\n(b) Searched FPN (c) Searched RCNN head\nFig. 2. The illustration of the searched MEMA-NAS-R50 Marchitecture.', '182 Q. Kong et al.\nTable 1. Individual module search experimental results on COCO mini-val dataset.\nMEMA-NAS (Backbone) and MEMA-NAS (FPN) denote the separate search resultsof backbone and FPN respectively. * refers to the corresponding searched backbones.\nModel Backbone Parameters AP\nBackbone Neck Head Total\nFPN-R50 ResNet-50 23.51 3.3414.3141.7637.4\nMEMA-NAS (Backbone) ResNet-50* 34.65 3.3414.3152.90 39.1\nMEMA-NAS (FPN) ResNet-50 23.51 3.1314.3141.5438.5\nFPN-X50 ResNeXt-50 22.98 3.3414.3141.2338.2\nMEMA-NAS (Backbone) ResNeXt-50* 26.09 3.3414.3144.34 39.6\nMEMA-NAS (FPN) ResNeXt-50 22.98 3.7614.3141.6438.9\nFPN-R101 ResNet-101 42.50 3.3414.3160.7439.4\nMEMA-NAS (Backbone) ResNet-101* 73.56 3.3414.3191.81 41.0\nMEMA-NAS (FPN) ResNet-101 42.50 3.0214.3160.4340.7\n3 Experiments\nIn this section, we evaluate our method from multiple aspects on diﬀerent object\ndetection datasets.\n3.1 Datasets and Evaluations\nWe use multiple detection datasets to verify the eﬀectiveness of our proposed\nmethod: (1) MSCOCO [ 10]: a well-known large-scale dataset contains 118K\nimages with 80 classes; (2) PASCAL VOC [ 5]: the training data contains 10 K\nimage, which is a combination of VOC2007 trainval and VOC2012 trainval, and\ndataset VOC2007 test (4.9K images) is used for model evaluation; (3) BerkeleyDeep Drive (BDD) [ 18]: an autonomous drive dataset with 10 classes contains\n70K images for training and 10K images for evaluation.\nFor COCO dataset and BDD dataset, we adopt mean Average Precision\n(mAP) across IoU thresholds from 0 .5t o0.95 as our evaluation metric. For VOC\ndataset, we report mAP scores using IoU at 0 .5 for comparison with existing\nmethods.\n3.2 Illustration of Searched Architecture and Analysis\nFigure 2shows an example of our searched MEMA-NAS-R50\nMarchitecture.\nWe can see that the model tends to use convolutions with large kernel sizes\nin backbone, and experimental results show that models searched by MEMA-\nNAS achieve signiﬁcant improvements in large scale object detection. Besides,the model tends to use dilated convolutions in the 2nd layer of MEMA-NAS\nFPN module, demonstrating that the large receptive ﬁeld is very important for\nfeature fusing.', 'MEMA-NAS: Memory-Eﬃcient Multi-Agent Neural Architecture Search 183\nTable 2. Evaluation results on BDD dataset. MEMA-NAS are our searched models.\n* refers to the corresponding searched backbones.\nModel Backbone Parameters FLOPs AP\nBackbone Neck Head Total (G)\nFPN-R50 R50 23.51 3.3413.9541.39197.434.0\nMEMA-NAS-R50 R50* 52.25 3.1732.9688.96330.2 35.6\nFPN-X50 X50 22.98 3.3413.9540.87200.034.3\nMEMA-NAS-X50 X50* 27.08 3.5037.6868.85250.5 35.6\nFPN-R101 R101 42.50 3.3413.9560.38269.335.4\nMEMA-NAS-R101 R101∗112.56 3.5533.42150.12501.3 37.4\nFPN-X101 X101 86.74 3.3413.95104.63439.236.2\nMEMA-NAS-X101 X101* 111.65 3.8438.20154.28574.9 37.3\n3.3 Ablation Experiments for Individual Module Search\nTo verify the eﬀectiveness of our method for individual module search and inves-\ntigate the contribution of each searched component in MEMA-NAS, we conduct\nmore search experiments. We search for backbone and FPN module individuallybased on multiple prototype backbones. The results are shown in Table 1.W e\nﬁnd that both searched backbones and FPNs bring AP gains compared to the\nbaseline model FPN. Searching backbones can bring more performance improve-\nments, and searching FPN is able to boost the AP while slightly cuts down the\nmodel size.\n3.4 Generalization Performance Evaluation\nSearch on More Datasets. We conduct more searching experiments under\nthe similar setting on BDD dataset to evaluate the generalization performanceof our proposed method. Table 2shows the evaluation results using λ=0.02.We\ncan observe that the searched models achieve signiﬁcant improvements compared\nwith baseline models (at least 1% boost in AP), demonstrating that our proposedmethod can work well on other detection datasets.\nArchitecture Transfer. In order to further evaluate the generalization perfor-\nmance of our proposed method, we transfer models searched on COCO dataset to\nVOC dataset, and evaluation results are shown in Table 3. All transferred models\nshow noticeable performance improvements on VOC dataset, which proves the\ngreat generalization ability of searched models.', '184 Q. Kong et al.\nTable 3. Transfer results on VOC dataset. * denotes the corresponding searched back-\nbones.\nModel Backbone Params (M) AP\nFPN-R50 ResNet-50 41.44 79.7\nMEMA-NAS-R50 ResNet-50* 96.60 80.4\nFPN-X50 ResNeXt-50 40.92 79.5\nMEMA-NAS-X50 ResNeXt-50* 62.82 80.2\nFPN-R101 ResNet-101 60.43 81.6\nMEMA-NAS-R101 ResNet-101* 135.42 83.6\nFPN-X101 ResNeXt-101 104.68 82.6\nMEMA-NAS-X101 ResNeXt-101* 150.06 83.7\nTable 4. Comparison between MEMA-NAS and other NAS-based detection meth-\nods. Our eﬃcient search method takes only 10 GPU days to ﬁnish a search of the\nwhole detection network, outperforming all other NAS-based detection works in search\neﬃciency.\nModel Search part Backbone Search cost AP Search method\nDet-NAS Backbone DetNASNet 43.4 41.8Evolutionary\nNATS Backbone NATS-X101 20 41.4Gradient-based\nNAS-FPN FPN ResNeXt-101 100 44.8RL\nMEMA-NAS-X101 Whole ResNeXt-101* 10 43.5Multi-agent\n3.5 Comparison with NAS-Based Detection Works\nWe compare more NAS-based detection methods and show more details in\nTable 4. Our method is the ﬁrst NAS framework that is capable of searching\nfor a whole detection network. Our method achieves a 43 .5% AP on COCO\ndataset with a search cost of only 10 GPU days, which is eﬃcient enough to\nout perform all existing counterparts. The searched model surpasses Det-NASand NATS by 1 .7% and 2 .1%, respectively, and reaches a comparable AP with\nNAS-FPN, whose search cost is 10 ×than ours.', 'MEMA-NAS: Memory-Eﬃcient Multi-Agent Neural Architecture Search 185\nTable 5. The GPU memory consumption comparison between MEMA-NAS and\nDATRS. We use the same search space for both DARTS and MEMA-NAS. All exper-iments are conducted with batch size 1 and input size 1333 ×800.\nSearch part Method Memory usage (M) Δ\n– FPN-R50 3627 –\nBackbone DARTS 5797 2148\nMEMA-NAS 4709 1082−50%\nFPN DARTS 31771 28144\nMEMA-NAS 8779 5152−82%\nRCNN head DARTS 5447 1820\nMEMA-NAS 4491 864−53%\nOverall DARTS Exploded (more than 32G) More than 32G\nMEMA-NAS 10191 6564−76%\n3.6 GPU Memory Consumption Evaluation\nIn order to further evaluate the GPU memory usage eﬃciency of our method,\nwe compare the average GPU memory consumption for the search of each com-\nponent between our method and the gradient-based method (DARTS) with thesame search space. The results are shown in Table 5. Compared to DARTS,\nour method can save 50% ,82% and 53% GPU memory usage for the search of\nbackbone, FPN, and RCNN head, respectively. When conducting the search forthe whole detection network, DARTS results in a memory explosion while our\nmethod is able to complete the joint search with a considerable low resource\nrequirement.\n4 Conclusion\nIn this paper, we put forward an end-to-end framework, named MEMA-NAS,which is memory-eﬃcient and has the capability to search the holistic archi-\ntecture of the detection network. We use a more eﬃcient search algorithm to\nsearch network architecture. To ﬁnd a better tradeoﬀ between the precision andcomputational costs, we add a resource constraint in our method. We compare\nour MEMA-NAS with diﬀerent baselines and show its superiority under various\nconditions both on diﬀerent datasets.\nReferences\n1. Carlucci, F.M., et al.: MANAS: multi-agent neural architecture search. arXiv\npreprint arXiv:1909.01051 (2019)\n2. Chao, P., Xiao, T., Li, Z., Jiang, Y., Jian, S.: MegDet: a large mini-batch object\ndetector. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (2018)', '186 Q. Kong et al.\n3. Chen, X., Xie, L., Wu, J., Tian, Q.: Progressive diﬀerentiable architecture search:\nbridging the depth gap between search and evaluation. In: Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 1294–1303 (2019)\n4. Chen, Y., et al.: Reinforced evolutionary neural architecture search. arXiv preprint\narXiv:1808.00193 (2018)\n5. Everingham, M., Gool, L.V., Williams, C., Winn, J., Zisserman, A.: The pascal\nvisual object classes (VOC) challenge. Int. J. Comput. Vision 88(2), 303–338 (2010)\n6. Ghiasi, G., Lin, T.Y., Le, Q.V.: NAS-FPN: learning scalable feature pyramid archi-\ntecture for object detection. In: 2019 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) (2019)\n7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npp. 770–778 (2016)\n8. Kim, S.-W., Kook, H.-K., Sun, J.-Y., Kang, M.-C., Ko, S.-J.: Parallel feature pyra-\nmid network for object detection. In: Ferrari, V., Hebert, M., Sminchisescu, C.,\nWeiss, Y. (eds.) ECCV 2018. LNCS, vol. 11209, pp. 239–256. Springer, Cham\n(2018). https://doi.org/10.1007/978-3-030-01228-1 15\n9. Kong, T., Sun, F., Huang, W., Liu, H.: Deep feature pyramid reconﬁguration for\nobject detection. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.)\nECCV 2018. LNCS, vol. 11209, pp. 172–188. Springer, Cham (2018). https://doi.\norg/10.1007/978-3-030-01228-1 11\n10. Lin, T.-Y., et al.: Microsoft COCO: common objects in context. In: Fleet, D.,\nPajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp.740–755. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10602-1\n48\n11. Liu, C., et al.: Progressive neural architecture search. In: Ferrari, V., Hebert, M.,\nSminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11205, pp. 19–35.Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01246-5\n2\n12. Liu, H., Simonyan, K., Yang, Y.: DARTS: diﬀerentiable architecture search. arXiv\npreprint arXiv:1806.09055 (2018)\n13. Liu, S., Qi, L., Qin, H., Shi, J., Jia, J.: Path aggregation network for instance\nsegmentation. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 8759–8768 (2018)\n14. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-\nsiﬁer architecture search. In: Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 33 (2018)\n15. Xie, S., Girshick, R., Doll´ ar, P., Tu, Z., He, K.: Aggregated residual transformations\nfor deep neural networks. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 1492–1500 (2017)\n16. Xu, H., Yao, L., Zhang, W., Liang, X., Li, Z.: Auto-FPN: automatic network\narchitecture adaptation for object detection beyond classiﬁcation. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pp. 6649–6658\n(2019)\n17. Xu, Y., et al.: PC-DARTS: partial channel connections for memory-eﬃcient archi-\ntecture search. arXiv preprint arXiv:1907.05737 (2019)\n18. Yu, F., et al.: BDD100K: a diverse driving video database with scalable annotation\ntooling. arXiv preprint arXiv:1805.04687 2(5), 6 (2018)', 'MEMA-NAS: Memory-Eﬃcient Multi-Agent Neural Architecture Search 187\n19. Zhao, Z.Q., Zheng, P., Xu, S.T., Wu, X.: Object detection with deep learning: a\nreview. IEEE Trans. Neural Netw. Learn. Syst. 30(11), 3212–3232 (2019)\n20. Zoph, B., Le, Q.V.: Neural architecture search with reinforcement learning. arXiv\npreprint arXiv:1611.01578 (2016)\n21. Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning transferable architectures\nfor scalable image recognition. In: Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition, pp. 8697–8710 (2018)', 'Adversarial Decoupling for Weakly\nSupervised Semantic Segmentation\nGuoying Sun1, Meng Yang1,2(B), and Wenfeng Luo1\n1School of Computer Science and Engineering, Sun Yat-sen University,\nGuangzhou, China\nyangm6@mail.sysu.edu.cn\n2Key Laboratory of Machine Intelligence and Advanced Computing (SYSU),\nMinistry of Education, Guangzhou, China\nAbstract. Image semantic segmentation has been widely used in med-\nical image analysis, autonomous driving and other ﬁelds. However, the\nfully-supervised semantic segmentation network requires a lot of laborcost to label pixel-level training data, so weakly supervised semantic\nsegmentation (WSSS), which requires much easily available supervision,\nhas become a new research hotspot. This paper focuses on tackling thesemantic segmentation problem under weak supervision of image-level\nlabels. To estimate more accurate pseudo masks, this paper proposes\nto jointly explore sub-category clustering, context decoupling augmenta-tion and adversarial climbing to mine more object-related regions. With\nsub-categories of k-means clustering, model can learn better feature rep-\nresentations, which breaks the dependency of object on the context bydecoupling augmentation. The image is perturbed away from the classi-\nﬁcation boundary to further increase the classiﬁcation score with adver-\nsarial climbing method. In order to verify the eﬀectiveness of the methodin this paper, we conduct a large number of experiments on the PASCAL\nVOC 2012 dataset obtained an excellent performance of 69.8% mIoU on\nthe veriﬁcation set and 69.5% mIoU on the test set, which surpassedmany advanced models of the same level supervision.\nKeywords: Weakly supervised semantic segmentation\n·Context\ndecoupling ·Adversarial climbing ·K-means clustering\n1 Introduction\nImage semantic segmentation aims at assigning a class label to each pixel, oﬀer-\ning ﬁne-grained object compositions in the image. Due to its soaring successrecently, it has found practical application in many ﬁelds, including medical\nThis work is partially supported by National Natural Science Foundation of China\n(Grants no. 61772568), Guangdong Basic and Applied Basic Research Foundation\n(Grant no. 2019A1515012029), and Youth science and technology innovation talent\nof Guangdong Special Support Program.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 188–200, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_16', 'Adversarial Decoupling for Weakly Supervised Semantic Segmentation 189\ndiagnosis and autonomous driving. However, these segmentation models require\nhuge amount of pixel-level annotations, whose acquisition is label-intensive andtime-consuming. Therefore, researchers attempt to explore weak supervisions,\nsuch as image-level information, to tackle semantic segmentation task.\nCompared to pixel-level masks whose annotation takes 4 to 90 min [ 1,2],\nit only requires around 20 s [ 1] to provide image-level labels. Thus this paper\nfocuses on the weakly supervised semantic segmentation (WSSS) under image-\nlevel label, which is the most easily available annotation among all weak super-visions. Existing WSSS methods via image-level labels usually adopt three indi-\nvidual stages: 1) train a normal classiﬁer to produce class activation maps to\ncoarsely locate the target objects; 2) propose an expansion strategy to minemore object-related regions before generating the ﬁnal pseudo labels; 3) learn a\nsegmentation network based on the pseudo labels. This paper mainly makes its\ncontributions in the ﬁrst and second stages.\nAlthough there are various object expansion methods, they are all based on\nthe class activation map (CAM) [ 3]. There are mainly two aspects worth con-\nsidering for CAM. Firstly, the CAM trained by classiﬁcation network usually\nonly high-lights the distinguishing area. Secondly, since only image-level labels\nare provided, when the objects in the image are tightly coupled with the back-ground or co-occurrence category, such as “boat” and “water”, “aircraft” and\n“sky”, “train” and “track”, “horse” and “people”, CAM will mistakenly identify\nbackground and foreground objects due to context interference [ 4].\nMost methods use CAM of the classiﬁcation network to generate initial pre-\ndictions for each class [ 3]. However, the CAM generated by the weighted feature\nmap of the classiﬁcation network usually only focuses on the part of the tar-get object. The classiﬁcation network can distinguish diﬀerent categories better\nthrough the most distinguishing features, so the overall object is hardly located.\nIn order to solve the seed sparseness problem, many excellent methods havebeen proposed [ 5–7], which can eﬀectively obtain initial predictions with better\ncoverage. However, those methods do not consider the context, ignoring the inﬂu-\nence of the co-occurrence category and background. If most images containing\nhorses with people riding, the classiﬁcation model will mistakenly believe that\nmost horses appear with people, so the boundary between people and horses isambiguous. The pairs of train and track, boat and water, aircraft and sky also\nhave the same problems that need to be resolved.\nRegarding the two above-mentioned aspects of CAM, we ﬁrst adopt sub-\ncategory clustering [ 8] to mine more object-related region, which maps the orig-\ninal label space to a higher-dimensional label space. This essentially adds K\ncategory maps for each category with k-means clustering, which can solve theproblem of seed sparseness.\nAs traditional data augmentation methods do not consider the association of\nobject and context, we adopt the context decoupling augmentation method thatrandomly pastes the selected foreground objects into the other image to obtain\nthe enhanced image, which breaks the dependency of the object on the con-\ntext. Through the perturbation methods of sub-category clustering and contextdecoupling augmentation, the classiﬁcation network can ﬁnally cover the target', '190 G. Sun et al.\nobject more completely with more features in each category, and decouple the\ntarget object from the context.\nIn order to further decrease the wrong bias, we additionally adopt the adver-\nsarial climbing method as post-analysis and optimization of the classiﬁer to\ndecouple the object with other co-occurrence category objects by keeping theimage away from the decision boundary, in which the image is perturbed along\nthe pixel gradient to improve the classiﬁcation score of the target class [ 9].\nWe ultimately improve the performance of the initial class activation map andexpand the seed region to generate better pseudo labels by reﬁnement network\nof IRNet [ 10] for the downstream segmentation task. The conceptual description\nof our method in image processing is shown in Fig. 1.\nFig. 1. Conceptual description of image processing method for CAM.\nThe main contributions of this paper are three-fold:\n– We propose the SDA-CAM method to explore Sub-category clustering, con-\ntext Decoupling augmentation, Adversarial climbing to reﬁne the initial\nCAM , which improves the feature representation and identiﬁes more regions\nof the object.\n– The method in this paper eﬀectively alleviates the impact of co-occurrence\nproblems on image-level weakly supervised semantic segmentation.\n– Our method produces signiﬁcantly better performance than existing methods\nin weakly supervised semantic segmentation. In particular, it achieves 69.8%and 69.5% mIoU scores on val and test set of PASCAL VOC 2012 dataset.\n2 Related Work\n2.1 Initial Prediction for WSSS\nThe initial prediction can provide reliable prior information and approximate posi-\ntion for generating the pseudo labels. Many methods usually use CAM [ 3] as an ini-\ntial prediction. Since the CAM model is trained by the classiﬁcation task, it tends\nto activate small and distinct parts of the object, resulting in an incomplete initial\nmask. There are currently several directions to alleviate this problem.', 'Adversarial Decoupling for Weakly Supervised Semantic Segmentation 191\nOne way is to erase [ 11,12], which removes the discernible area of the object,\nforcing the model to look for more diﬀerent parts. However, the erasure methodsrequire modiﬁcation of the network structure, repeated execution of the training\nmodel and response aggregation steps. There are also proposed methods that\nimprove the original erasure method [ 13,14], which extend the initial response\nthrough end-to-end training against erasure strategies. The SeeNet method [ 15]\nadopts a self-erasing strategy to encourage the network to use both the object\nand the background cues, which prevents the attention from containing back-ground areas. The erasing method has certain eﬀects, but it is diﬃcult to pro-\nvide reliable termination conditions for iterative erasing. Besides, eliminating the\nidentiﬁcation area of the image will cause the network to misclassify the image.\nThere is also a way which suppresses the most distinctive areas [ 16], so that\nthe classiﬁcation network can cover the target more comprehensively. Our work\nadditionally uses a certain degree of suppression. There is another way to per-\nturb. The SC-CAM method [ 8] obtains the sub-categories under each parent\ncategory through unsupervised clustering, and learns to obtain more compre-hensive features. The AdvCAM method [ 4] uses adversarial climbing method to\nperturb the image along the pixel gradient to increase the classiﬁcation score.\n2.2 Response Reﬁnement for WSSS\nMany methods have been proposed to reﬁne the initial clues by expanding the ini-\ntial seed. The SEC method [ 17] proposes a loss function that constrained global\nweighted rank pooling and low-level boundaries to expand localized mapping. TheMCOF method [ 18] alternately expands the target area and optimizes the segmen-\ntation network by the iterative bottom-up and top-down framework. The MDC\nmethod [ 19] expands the seed by using multiple branches of the convolutional layer\nwith diﬀerent dilation rates. The DSRG method [ 20] reﬁnes the initial prediction\nwhen training the segmentation network with the seed region growth method.\nOther methods are developed through aﬃnity learning. AﬃnityNet [ 21]c o n s i d -\ners pixel-level aﬃnity and propagates local response to nearby areas. The IRNet\nmethod [ 10] reﬁnes the class activation map to obtain the pseudo labels with aﬃn-\nity learning through the class boundary map and the displacement ﬁeld.\nHowever, the initial seed still base on the CAM method. If these seeds only\ncome from the distinguishing part of the object, it is diﬃcult to extend the areato the non-distinctive part. In addition, if the initial prediction produces an incor-\nrect attention area, applying the reﬁnement step will cover the more inaccurate\narea. In this paper, we focus on improving the initial prediction to make the targetpositioning more accurate, which is conducive to the reﬁnement steps.\n3M e t h o d\nIn this section, we elaborate on the technical details of the proposed SDA-CAM\nmethod, which consists of three individual modules: sub-category clustering,\ncontext decoupling augmentation and adversarial climbing. The joint training', '192 G. Sun et al.\nstrategy takes into account the seed sparseness and co-occurrence issues, which\nsynergistically improves the segmentation performance.\nThe Overview of the proposed framework is shown in Fig. 2. Firstly, we adopt\nunsupervised clustering to explore sub-category for increasing the diﬃculty of the\nclassiﬁcation network, so that it can automatically learn more features in eachcategory. Then we decouple the class object from the image context, which can\nbreak the dependency between the object and the context. Finally, we disturb the\nimage to a certain extent via the iterative adversarial climbing method to obtainthe ﬁnal SDA-CAM, in which the image is moved away from the classiﬁc ation\nboundary to increase the classiﬁcation score.\nWe present a technical description of our method. We denote the space of\nimages and image-level labels by ˆXandˆY. Any image X ∈ˆXhas the corre-\nsponding image-level label Y ∈ˆY. The semantic labels belong to a set C\n/prime=C\n∪{cbg}, where C is a set of foreground labels and cbgis the background label.\nWe assume that the training data {(Xi,Y i)}N\ni=1, consists of N images, Xi∈ˆX,\nwhere each image is weakly labeled by Yi∈{0,1}Cof foreground labels.\nFig. 2. Overview of the proposed framework for generating the SDA-CAM.\n3.1 Sub-category Clustering\nWe ﬁrst train a normal classiﬁcation network using the available image-level\nlabels Yp∈{0,1}C.L e tF∈Rh×w×ddenote the output feature maps before the\nGlobal Averaging Pooling (GAP) layer. Then the initial CAM for class c could\nbe computed as the weighted sum of feature maps:\nCAM =ReLU/parenleftBigg/summationdisplay\nkFk×ωk\nc/parenrightBigg\n(1)\nwhere Fk∈Rh×wrepresents the k-th channel of F and ωc∈Rdare weights from\nthe classiﬁcation layer for class c. The initial CAM tends to focus on the most', 'Adversarial Decoupling for Weakly Supervised Semantic Segmentation 193\ndiscriminative regions, leading to incomplete object mask for the downstream\nsegmentation task. Motivated by [ 8], we propose to perform sub-category clus-\ntering to discover more object-related pixels.\nSub-category clustering maps the original class label space to a high dimen-\nsional one. The training images are ﬁrst divided into semantic subsets, each ofwhich contains images with the same class label. Speciﬁcally, we extract image\nrepresentation f∈R\ndfor each training image, which is obtained by performing\nGAP on the output feature maps:\nf=1\nhw/summationdisplay\ni/summationdisplay\njFi,j (2)\nThen k-means clustering is applied on the image representations/summationtextN\ni=1f∈RN×d\nto re-label the original training images with pseudo sub-categories Ys∈{0,1}KC,\nas visually presented in Fig. 3. Consequentially, the original parent class label\nYp∈{0,1}Cis mapped to Ys∈{0,1}KCwith K sub-categories.\nFig. 3. Sub-category clustering. Here we demonstrate that each parent class is mapped\nto K = 3 sub-categories.\nTo utilize the pseudo label Ys∈{0,1}KCwe add a parallel classiﬁcation layer\nwhich shares the backbone network with the existing one, as shown in Fig. 2.\nThe original classiﬁer Hpis trained by the parent class label Yp∈{0,1}Cwhile\nthe sub-category classiﬁer Hsis supervised by Ys∈{0,1}KC. So the overall\ntraining objective is deﬁned as:\nLcls=1\nNN/summationdisplay\niLCE(Hp(fi),Y p)+γL CE(Hs(fi),Y s) (3)\nwhere LCErepresents the cross entropy loss and γis a hyper-parameter control-\nling the relative importance of two terms.\nWith the attached sub-category classiﬁer, we essentially introduce extra K\nCAMs for each class, signiﬁcantly diversifying the response regions. Since thesetwo parallel classiﬁers share the same input features, so the responsive regions\nare also reﬂected in the parent classiﬁer. As a result, we are able to estimate\nmore complete object mask from M\n1.\n3.2 Context Decoupling Augmentation\nTraditional WSSS data augmentation methods usually adopt geometric transfor-\nmation, random cropping and horizontal ﬂip. However, simply adding the same', '194 G. Sun et al.\ncontext data bring less beneﬁt to the network for distinguishing objects. The\nimage-level classiﬁcation may not only be owing to the recognition of the objectitself, but also related to its co-occurrence context, which will cause the model to\npay less attention to the features of the object and lead to blurring pseudo-label\nboundaries. Motivated by [ 4], we propose to perform the context decoupling\naugmentation method by randomly pasting objects into the new image, which\nincreases the diversity of objects in the image and reduces the dependence of the\nsame object in the inherent scene.\nWhen two images with the same background have diﬀerent objects, they tend\nto leave a deep impression. By jointly training the original image and the context-\ndecoupled augmentation image, the network can further identify the target, andalleviate the impact of the co-occurrence to a certain extent. As most objects\nare located in the middle or prominent position of the image, we paste the\ncontext-decoupled object to the eccentric position of the image for ensuring the\naugmentation quality. Although the phenomenon of occluding the original image\nobjects may occur, the augmentation method generally improves performance.Algorithm 1summarizes the procedure for context decoupling augmentation\nmethod.\nAlgorithm 1. Context Decoupling Augmentation\nInput: Images ˆX, image-level labels ˆYand CAMs M // M generated by the sub-\ncategory clustering module\nOutput: Classiﬁer H\n1:forii n{1,2,. ..,N }do// Step1: Inferring\n2: ifIssingle (Yi)then // only chose image Xi∈Rh1×w1×3with single category\nto get object\n3: Ma s k i← Ref ining (Xi,Y i,M i) // reﬁning by IRNet, Ma s k i∈\nRh1×w1,Y i∈{0,1}C\n4: Object Oidecoupled from context ←Ma s k i·Xi//Oi∈Rh2×w2×3\n5: end if\n6:end for\n7:forji n{1,2,.. .,N }do// Step 2: Online Augmentation\n8: i ←Choose Random (Yj) // random choose category not appear in the original\nimage\n9: XAug\nj ,YAug\nj← P aste (Xj,Y j,O i,i)/ /p a s t e Oiinto image Xjappropriately\nby Gaussian smoothing, randomly scaling and rotating\n10: H ←Classif yN et/parenleftBig\nXj,Y j,XAug\nj ,YAug\nj/parenrightBig\n// adopt ResNet-50 [22] as classiﬁca-\ntion network\n11: end for\n3.3 Adversarial Climbing\nBoth sub-category clustering and context decoupling augmentation modules act\non the classiﬁcation network to yield a better CAM output. In this section, we\ninstead explore adversarial climbing [ 4] as the post-processing of the classiﬁer H\nto perturb the input image X for generating better CAM. Adversarial climbing', 'Adversarial Decoupling for Weakly Supervised Semantic Segmentation 195\nwas an anti-attacking technique that modiﬁed the input X in a direction to\ndisturb the image away from the classiﬁc ation boundary. Let Xtbe the perturbed\nimage after t-th update. Then the iterative update procedure can be formulated\nas follows:\nXt=Xt−1+ε/triangleinvXt−1st−1\nc (4)\nwhere st−1\ncis the classiﬁcation score, εis the step size and /triangleinvXt−1st−1\ncis the\ngradient with respect to the image. This treatment encourages the network tobecome activated in the previously no-discriminative regions, thus capable of\nmining more foreground pixels.\nHowever, there are two issues concerning the aforementioned update proce-\ndure: 1) the updated version X\ntmight cause other non-existing classes to be\nrecognized; 2) it could simply reinforce those regions that already have a rel-\natively large activation value. To avoid such trivial update, we introduce twoextra regularization terms to constrain the update procedure:\nL=s\nt−1\nc−/summationdisplay\nk∈C\\cst−1\nk−δ/bardblQr⊙|CAM/parenleftbig\nXt−1/parenrightbig\n−CAM/parenleftbig\nX0/parenrightbig\n|/bardbl1(5)\nXt=Xt−1+ε/triangleinvXt−1L (6)\nThe hyper-parameter δrepresents the degree of suppression. The ﬁrst regular-\nization term/summationtext\nk∈C\\cst−1\nktakes into consideration all the classiﬁcation scores so\nclasses other than c will be suppressed during the updating process. Moreover,\nthe second term achieves the goal of partial enhancement through the introduc-tion of suppression mask Q\nr, which takes the form of:\nQr=CAM/parenleftbig\nXt−1/parenrightbig\nmax{CAM (Xt−1)}>β (7)\nwhere βis a threshold value. As a result, regions indicated by Qrtends to\nstay the same. Ultimately, the perturbed image Xtof last step is utilized to\ngenerate class activation map SDA-CAM with more comprehensive coverage for\nthe downstream segmentation task.\n4 Experiments\n4.1 Evaluated Dataset and Metric\nWe evaluated the proposed method on the PASCAL VOC 2012 [ 23] semantic\nsegmentation benchmark, which contains 20 foreground classes and one back-\nground class. Following the previous WSSS methods, we enhance the train set\n(1,464 images) to trainaug set with 10,582 images by [ 24]. For all experiments,\nwe use the mean Intersection-over-Union (mIoU) as the evaluation metric. The\nresults of the test set are obtained from the oﬃcial PASCAL VOC evaluation\nwebsite.', '196 G. Sun et al.\n4.2 Ablation Study and Analysis\nAs only evaluating the CAM and the pseudo-mask with the mIoU to compare\nthe performance of diﬀerent methods has certain limitations, we additionallyperform the ablation experiments compared with the ﬁnal segmentation mod-\nels performance. For fair comparison, we adopt the IRNet [ 10] as the reﬁned\nnetwork and the DeepLab-v2 network [ 25] of the ResNet-101 [ 22] backbone as\nsegmentation model for all comparative experiments in Table 1. Baseline is the\nmethod that does not use any optimization into the CAM. Through the ablation\nexperiment, it can be seen that the three modules we adopted have certain eﬀectson the promotion of the CAM. Our SDA-CAM combined the three methods has\nachieved the best eﬀect.\nTable 1. CAM ablation experiments.\nBaseline Self-\nsupervisionContext decouplingaugmentation Adversarialclimbing CAM Pseudo masks Val\n√48.3 65.6 66.2\n√ √49.6 67.1 66.7\n√ √49.9 66.5 67.9\n√ √55.6 69.9 68.1\n√ √ √50.6 67.7 68.8\n√ √ √53.7 67.5 69.0\n√ √ √56.4 69.6 69.5\n√ √ √ √56.6 70.0 69.8\n4.3 Semantic Segmentation Performance\nFor fair comparisons with other WSSS methods, we adopt DeepLab-v2 [ 25] with\nthe backbone of ResNet-101 [ 22] as the supervised semantic segmentation model.\nImprovement on Initial Response. Figure 4shows the initial response of\nour method and the SC-CAM method [ 8] proposed by Chang et al. Our SDA-\nCAM often generates the response map that covers larger region of the object\ncompared to SC-CAM. It is worth mentioning that our method also can activate\nless the background region, as shown in the last column. Because the proposedmethod better decouple from the context.', 'Adversarial Decoupling for Weakly Supervised Semantic Segmentation 197\nFig. 4. Sample results of initial responses.\nQuality of the Mask. Table 2compares the initial seeds (CAM) and the\npseudo labels obtained by our method and other recent methods. Both seeds and\nmasks were generated from training images of the PASCAL VOC12 dataset [ 23].\nIt can be seen that our proposed SDA-CAM can provide the more accurate initialseed and obtain the highest pseudo-mask performance.\nTable 2. mIoU (%) of the initial seed (CAM), and the pseudo labels on PASCAL VOC\n2012 train images.\nMethod Seed Pseudo labels\nPSACVPR/prime18[26] 48.0 61.0\nIRNetCVPR/prime19[10] 48.3 66.5\nMixup −CAMBMVC/prime20[5]50.1 61.9\nSC−CAMCVPR/prime20[8] 50.9 63.4\nSEAMCVPR/prime20[6] 55.4 63.6\nCONTACVPR/prime20[21] 48.8 67.9\nAdvCAMCVPR/prime21[9] 55.6 69.9\nCDA 21[4] 50.8 67.7\nSDA-CAM (Ours) 56.6 70.0\nWeakly Supervised Semantic Segmentation. With the reﬁnement of CRF\nand ResNet-101 [ 22] backbone, our method achieves the mIoU results of 69.8%\non val set and 69.5% on the test set. The result on test set is available on the web-\nsite (http://host.robots.ox.ac.uk:8080/anonymous/GSAW9D.html ). In Fig. 5we\nvisualize some examples of the ﬁnal semantic segmentation results, which can\nbe seen that our results are close to ground truth. Our WSSS method withimage-level labels can segment and recognize the target object well whether it\nis the single-category image or the complex multi-category image. However, the', '198 G. Sun et al.\nsegmentation results are not ﬁne enough, especially in the bicycle category. The\nreason is that we adopt the DeepLab-v2 [ 25] segmentation network with ResNet-\n101 [22] backbone for fair comparison with other WSSS methods, which has been\ndown sampled by eight times and lost certain pixel prediction information. If a\nmore advanced semantic segmentation network is adopted, performance boost isexpected [ 27].\nFig. 5. Qualitative results on the PASCAL VOC 2012 validation set. (a) Input images.\n(b) Ground truth. (c) Our results.\nWe compare our method with the recent WSSS methods adopted ResNet\nbackbone segmentation network in Table 3. Our method surpasses many current\nadvanced WSSS methods with the same level supervision. In addition, methods\nsuch as FickleNet [ 7]a n dI C D[ 28] extract the background cues from an external\nsaliency detector, which requires additional pixels for training. Our method does\nnot require the additional external saliency detector, and only uses image-level\nclassiﬁcation labels, which achieves better performance.\nTable 3. WSSS performance on PASCAL VOC 2012 val and test set.\nMethod Backbone Saliency Val Test\nDCSPBMVC/prime17[5] ResNet-101√60.8 61.8\nMCOFCVPR/prime18[18] ResNet-101√60.3 61.2\nDSRGCVPR/prime18[20] ResNet-101√61.4 63.2\nAﬃnityNetCVPR/prime18[21]ResNet-38 – 61.7 63.7\nFickleNetCVPR/prime19[7]ResNet-101√64.9 65.3\nIRNetCVPR/prime19[10] ResNet-50 – 63.5 64.8\nSC−CAMCVPR/prime20[8]ResNet-101 – 66.1 65.9\nSEAMCVPR/prime20[6] ResNet-38 – 64.5 65.7\nICDCVPR/prime20[28] ResNet-101√67.8 68.0\nCDA 21[4] ResNet-50 – 65.8 66.4\nAdvCAMCVPR/prime21[9]ResNet-101 – 68.1 68.0\nOurs ResNet-101 – 69.8 69.5', 'Adversarial Decoupling for Weakly Supervised Semantic Segmentation 199\n5 Conclusion\nIn this paper, we propose a method to improve the class activation map via\nsub-category clustering, context decoupling augmentation and adversarial climb-\ning tasks, which eﬀectively alleviates the impact of co-occurrence on image-level WSSS. The SDA-CAM we proposed learns better feature representations\nthrough unsupervised clustering of subcategories, reduces the confusion bias of\nWSSS under image-level labels via the context decoupling augmentation, andis further optimized by the method of anti-climbing. Due to the high cost of\nlabeling for fully-supervised semantic segmentation, WSSS has gradually devel-\noped and narrowed the gap with fully-supervised semantic segmentation, whichis expected to play a huge role in medical diagnosis and other ﬁelds in the future.\nReferences\n1. Bearman, A., Russakovsky, O., Ferrari, V., Fei-Fei, L.: What’s the point: semantic\nsegmentation with point supervision. In: Leibe, B., Matas, J., Sebe, N., Welling, M.\n(eds.) ECCV 2016. LNCS, vol. 9911, pp. 549–565. Springer, Cham (2016). https://\ndoi.org/10.1007/978-3-319-46478-7 34\n2. Cordts, M., et al.: The cityscapes dataset for semantic urban scene understanding.\nIn: CVPR (2016)\n3. Zhou, B., et al.: Learning deep features for discriminative localization. In: CVPR\n(2016)\n4. Su, Y., et al.: Context decoupling augmentation for weakly supervised semantic\nsegmentation. CoRR abs/2103.01795 (2021)\n5. Chaudhry, A., Dokania, P.K., Torr, P.: Discovering class-speciﬁc pixels for weakly-\nsupervised semantic segmentation. In: BMVC (2017)\n6. Wang, Y., et al.: Self-supervised equivariant attention mechanism for weakly super-\nvised semantic segmentation. In: CVPR, pp. 12272–12281 (2020)\n7. Lee, J., et al.: FickleNet: weakly and semi-supervised semantic image segmentation\nusing stochastic inference. In: CVPR (2019)\n8. Chang, Y.T., et al.: Weakly-supervised semantic segmentation via sub-category\nexploration. In: CVPR (2020)\n9. Lee, J., Kim, E., Yoon, S.: Anti-adversarially manipulated attributions for weakly\nand semi-supervised semantic segmentation. In: CVPR (2021)\n10. Ahn, J., Cho, S., Kwak, S.: Weakly supervised learning of instance segmentation\nwith inter-pixel relations. In: CVPR, pp. 2209–2218 (2019)\n11. Singh, K.K., Yong, J.L.: Hide-and-seek: forcing a network to be meticulous for\nweakly-supervised object and action localization. In: ICCV (2017)\n12. Wei, Y., et al.: Object region mining with adversarial erasing: a simple classiﬁcation\nto semantic segmentation approach. In: CVPR (2017)\n13. Zhang, X., et al.: Adversarial complementary learning for weakly supervised object\nlocalization. CoRR abs/1804.06962 (2018)\n14. Li, K., et al.: Tell me where to look: guided attention inference network. In: CVPR\n(2018)\n15. Hou, Q., et al.: Self-erasing network for integral object attention. CoRR\nabs/1810.09821 (2018)\n16. Kim, B., Kim, S.: Discriminative region suppression for weakly-supervised semantic\nsegmentation. CoRR abs/2103.07246 (2021)', '200 G. Sun et al.\n17. Kolesnikov, A., Lampert, C.H.: Seed, expand and constrain: three principles for\nweakly-supervised image segmentation. In: Leibe, B., Matas, J., Sebe, N., Welling,\nM. (eds.) ECCV 2016. LNCS, vol. 9908, pp. 695–711. Springer, Cham (2016).https://doi.org/10.1007/978-3-319-46493-0\n42\n18. Wang, X., et al.: Weakly-supervised semantic segmentation by iteratively mining\ncommon object features. In: CVPR (2018)\n19. Wei, Y., et al.: Revisiting dilated convolution: a simple approach for weakly- and\nsemi-supervised semantic segmentation. In: CVPR, pp. 7268–7277 (2018)\n20. Huang, Z., et al.: Weakly-supervised semantic segmentation network with deep\nseeded region growing. In: CVPR (2018)\n21. Ma, T., Zhang, A.: AﬃnityNet: semi-supervised few-shot learning for disease type\nprediction. CoRR abs/1805.08905 (2018)\n22. He, K., et al.: Deep residual learning for image recognition. In: CVPR, pp. 770–778\n(2016)\n23. Everingham, M., et al.: The pascal visual object classes challenge. Int. J. Comput.\nVis. 88(2), 303–338 (2010)\n24. Hariharan, B., et al.: Semantic contours from inverse detectors. In: ICCV (2011)25. Chen, L.C., et al.: DeepLab: semantic image segmentation with deep convolutional\nnets, atrous convolution, and fully connected CRFs. IEEE Trans. Pattern Anal.\nMach. Intell. 40(4), 834–848 (2018)\n26. Ahn, J., Kwak, S.: Learning pixel-level semantic aﬃnity with image-level supervi-\nsion for weakly supervised semantic segmentation. In: CVPR (2018)\n27. Li, Y., et al.: Learning dynamic routing for semantic segmentation. CoRR\nabs/2003.10401 (2020)\n28. Fan, J., et al.: Learning integral objects with intra-class discriminator for weakly-\nsupervised semantic segmentation. In: CVPR (2020)', 'Towards End-to-End Embroidery Style\nGeneration: A Paired Dataset\nand Benchmark\nJingwen Ye, Yixin Ji, Jie Song, Zunlei Feng, and Mingli Song(B)\nZhejiang University, Hangzhou, China\n{yejingwen,jiyixin,sjie,zunleifeng,brooksong }@zju.edu.cn\nAbstract. Despite the numerous developments in image to image trans-\nlation, further improvement and applications are limited by monotonous\ndatasets, especially the lack of the datasets with aligned image pairs.\nIn this paper, we present the ﬁrst dedicated paired embroidery dataset,which contains 9k sets of the input sketches and the corresponding output\nembroidery images. Specialized on this dataset, we propose a new image-\nbased generative approach as the benchmark for automatically generat-ing a preview of the image produced by an expert on an embroidery\nmachine. The key idea is to segment the input image and use the two-\nstage generation. The ﬁrst stage trains a sub-generator function withineach segmentation region, while the second stage ﬁne-tunes the output\nof the ﬁrst stage with a global generator function. Experimental results\ndemonstrate that our proposed benchmark handles this task best, interms of both qualitative and quantitative measures.\nKeywords: Image-to-image translation\n·GAN·Benchmark\n1 Introduction\nApparel customization is a new trend fashion industry that aims to eﬃciently\ndeliver unique garments to customers. One typical type of customization is the\nembroidery customization, which has seen increasing demand through retailers\nsuch as CapBeast. Providing the customers with a real-time preview of the ﬁn-ished product is an essential aspect of the customization experience. However,\nit takes tremendous eﬀort to simulate embroidery and produce an accurate pre-\nview and there does not currently exist such an automatic embroidery previewsystem. In this paper, we investigate the problem of automatic embroidery pre-\nview, where a customer gives a sketch image as input and receives as output the\nembroidery-style preview.\nElectronic supplementary material The online version of this chapter ( https://\ndoi.org/10.1007/978-3-030-88013-2 17) contains supplementary material, which is\navailable to authorized users.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 201–213, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_17', '202 J. Ye et al.\nTraditional embroidery generation methods [ 35,38,41] require expert knowl-\nedge and graphics technology. They present several issues: how to extract theimage content, how to apply various stitching styles, how to simulate artistic\neﬀects, and how to generate the stitch sequence used by the embroidery machine.\nThese issues make the process of embroidery preview time-consuming and user-unfriendly. For example, the work of [ 41] proposes a stitch neighborhood graph to\nmodel the interactions among neighboring stitches and adopts several reaction-\ndiﬀusion procedures with high-level layout parameters.\nThe appearance of style transfer [ 17] makes it possible to transfer an embroi-\ndery style to an input image. Gatys et al. [ 7] propose to model the content of\nan image as the feature responses of a pre-trained CNN, and to model the styleof an artwork as a set of summary feature statistics. However, applying style\ntransfer directly to generate embroidery fails to learn the elaborate selection of\ndiﬀerent stitching patterns to achieve aesthetic and robustness goals.\nWith the appearance of generative adversarial networks (GAN), image-to-\nimage translation has been applied to problems such as image colorizationand transferring segmentation maps. Conditional GAN-based image translation\n[2,15,26,42] has shown remarkable success at taking an abstract user input, such\nas an edge map or a semantic segmentation map, and translating it to a realimage. We propose a new GAN-based embroidery generation benchmark that\nworks in two stages. We subdivide the input image into regions. In the ﬁrst\nstage, we generate locally coherent embroidery preview in each region, and inthe second stage, we ﬁne-tune the samples generated in the ﬁrst stage.\n(a) PEmD-Chara (b) PEmD-Cartn (c) PEmD-Sketch (d) PEmD-Real (e) PEmD-Com\nFig. 1. Sample image pairs from the paired embroidery style dataset PEmD.\nTo summarize, this paper makes the following contributions. We propose a\nnew generative network framework for embroidery style image generation. Thetraining process iterates between the ﬁrst local generator for each region and then\nthe global generator for ﬁne-tuning. Our proposed approach outperforms other\nstate-of-the-art approaches in all evaluation metrics on an embroidery dataset', 'Towards End-to-End Embroidery Style Generation 203\n(PEmD), shown in Fig. 1, containing over 9000 unique image pairs (input image\nand corresponding embroidery output produced by a human expert).\n2 Related Work\n2.1 Traditional Stitching Generation\nFocusing on the physical medium simulation problem and based on the ﬁeld\nof the computer graphics, a signiﬁcant number of works aimed at simulating\nthe appearance of various kinds of textiles, including batik [ 34], knitwear [ 18],\nand woven materials [ 1]. Also, this problem can be treated as a kind of art\nrendering [ 5,9,12,23]. Based on this, the related works can be classiﬁed into two\ntypes: ﬁlter-based method and stroke-based method.\nThere are mainly two kinds of strategies in the traditional stitching gener-\nation: greedy and optimization-based. In greedy methods [ 10,21,22], at each\nstep the algorithm determines the current stroke according to certain goals\nor image features. The optimization-based methods [ 6,36] compute the entire\nstrokes together to achieve a global optimal energy or desired statistics.\n2.2 Style Transfer\nRecently, inspired by the power of Convolutional Neural Networks (CNNs),\nGatys et al. [ 7] ﬁrstly study how to use a CNN to reproduce famous paint-\ning styles on natural images. Style transfer has been applied into multiple ﬁelds,including text eﬀects transfer [ 31], data augmentation [ 40] and color sketch gen-\neration [ 39]. After the pioneer work, numerous researchers [ 13,20,32] are devoted\nto accelerate the speed of the style transfer process.\nCMP Aerial Data\nBW2ColorEdge2ShoesTexture\nGetchuPortrait\nFig. 2. The samples from the 7 related datasets.', '204 J. Ye et al.\nTable 1. A comparison of publicly available datasets for image-to-image translation.\nDataset Pair Scale Description\nCMP [ 28] /check 606 Architecture label maps to real facade images\nAerial Data /check 1096 Map to aerial, which are scraped from Google Maps\nEdge2Shoes [ 37]/check 50k Zappos50k dataset and edges are computed by HED\nBW2Color [ 25]/check 1.2M Data from ImageNet is used for image colorization\nTextures × 120/c The Oxford dataset [ 4] composed of various textures\nGetchu [ 16] × 3.1k The images contains anime character face\nPortrait [ 19] × 1.1k/6.5k CelebA dataset + painting images from Wikiart\n2.3 Image-to-Image Translation\nWith the popularity of generative adversarial networks (GANs) proposed by\nGoodfellow et al. [ 8], it has been applied into generating samples in various\nﬁelds. Especially, Isola et al. [ 14] investigate conditional adversarial networks\n(cGAN) as a general-purpose solution to image-to-image translation problems.\nFor synthesizing high-resolution photo-realistic images, Wang et al. [ 29]p r e s e n t\na novel adversarial loss, as well as new multi-scale generator and discriminator\narchitectures. In order to handle more than two domains, Choi et al. [ 3]p r o p o s e\nStarGAN to simultaneously train multiple datasets with diﬀerent domains withina single network. Note that the above methods study the general image-to-image\ntranslation, the uniqueness of embroidery image is not considered.\n2.4 Related Datasets\nHere we review some common datasets in image-to-image translation. The\ndetailed information are concluded in Table 1and Fig. 2.\n3 Paired Embroidery Dataset Overview\nThere is no publicly available dataset of pairs of input image and correspond-\ning embroidery output. In this work, we used a proprietary structured datasetconsisting of 9k image pairs from the apparel customization ﬁrm CapBeast.\nPEmD contains 9k unique design-embroidery pairs. More than half of the\nimages’ resolution is 2000 ×2000. In practice, this resolution is enough to guar-\nantee that the stitches in each embroidery region are legible. The average sizes\nof the input and output image are about 76.9 and 239.5 KB, respectively (a total\nof over 2.8 GB for the entire dataset).\nAccording to the types of the input images, we split the whole dataset into\nﬁve subsets, named as PEmD-Chara, PEmD-Cartn, PEmD-Real, PEmD-Sketchand PEmD-Com, the details of which are displayed in Table 2. The ﬁve input\nimage categories can be ranked according to the diﬃculty of preview generation:\nPEmD-Real >PEmD-Com >PEmD-Cartn >PEmD-Sketch >PEmD-Chara.', 'Towards End-to-End Embroidery Style Generation 205\nTable 2. Detailed descriptions of diﬀerent sub-datasets in PEmD.\nSubset Prop. Description\nPEmD-Chara 38% The pairs with characters as input, including numbers, alphabets et al.\nPEmD-Cartn 3% The pairs with the cartoon images as input\nPEmD-Real 1% The pairs with real images as input, usually are the human portraits\nPEmD-Sketch 21% The pairs with sketch input, including simple color blocks and strokes\nPEmD-Com 37% The pairs with the mixture input of the above 4 types\n4 Automatic Embroidery Generation\nIn order to automatically synthesize reliable embroidery-style images, we build\nthe generative network specialized for embroidery translation based on the con-\nditional GAN, as shown in Fig. 3. The main idea of our work is to decompose\nthe whole generation process into 2 stages. In the ﬁrst local texture generation\nprocess, regular and clear stitches are generated by the local generator (Sub G)within each non-overlapping sub areas, which are then grouped into the primary\nembroidery I\nloc. Then in the second global ﬁne-tuning stage, the whole image is\nfed into the global generator (Global G) to ﬁne-tune Ilocto be the ﬁnal Iout.\n4.1 Local Texture Generation\nAs can be seen in Fig. 1, the input images Iinare composed of several color\nblocks, which often turn out to be of diﬀerent stitch types in the corresponding\nembroidery Igt. When using the dataset to directly train the vanilla GAN (e.g.\ncondition GAN), the regular and clear stitches are hard to learn within eachunique regions. With this consideration, in the local texture generation, we use\na set of sub generators to synthesize the sub embroideries for each regions.\nGiven an input image I\ninwith the resolution of H×W, several independent\nregions are segmented ﬁrstly. Note that in most cases, the pixel values of the\ninput images are concrete, it comes out with the satisfying results with the\nunsupervised segmentation algorithms [ 24]. After segmentation, the input image\nIinis separated into Snon-overlapping areas with the masks {B1,B2,...,BS},\nwhere the s-th region is donated as Is\nin=Bs·Iinfor all s∈[1,S]. Then in\nthe local generation stage, the sub generator Gsubis constructed to transfer the\nsegmented region Is\nininto the regular embroidery block Is\nloc:\nIs\nloc←Gsub(Is\nin,z,glob (Iin))s∈{1,2,...,S}, (1)\nwhere zis the random noise. In addition to the random noise and Is\nloc,w eu s e\na global encoder module ( glob(·)) to add the global information as a part of\njoint input to Gsub, which is consisted of several convolutional layers and global\npooling layers. The detailed architecture of the proposed global encoder moduleis in the supplementary.\nNote that for a total of Sregions, the sub generator G\nsubprocesses the input\nStimes to get all the Sgenerated sub embroideries Is\nloc. And for each generated', '206 J. Ye et al.\nInputSegmentation Sub Generated Image\nSub G Sub Input\nSub D\n Global G\nReal \nImageGlobal D L1 loss Sub Real Image\nL1 loss0/1\n0/1+Generated \nImage\n Edge \nInformation Concat\n+Element-wise \nMultiplicationStage-1: Local\nStage-2: Global Fine-tune\nFig. 3. The two-stage generation of embroidery style image. At the ﬁrst stage, we train\nthe sub generators (Sub G) within each local segmentation regions. At the second stage,ﬁne-tune the output of the ﬁrst stage with global generator (Global G).\nIs\nloc, a sub discriminator Dsubis applied to distinguish them from the real ones\nBsIgt,s∈{1,2,...,S}. Then the adversarial loss L1\nGANin the ﬁrst local texture\ngeneration stage can be concluded as:\nL1\nGAN(Gsub,Dsub)=1\nS/summationdisplay\nsEz∼pz(z)[log(1 −Dsub(BsIs\nloc))]\n+1\nS/summationdisplay\nsEIgt∼pdata (Igt)[logDsub(BsIgt)], (2)\nwhere the discriminator Dsubtries to maximize the output conﬁdence score from\nreal samples and minimize the output conﬁdence score from fake samples gen-\nerated by Gsub. On contrast, the aim of Gsubis to maximize the Dsubevaluated\nscore for its outputs, which can be viewed as deceiving the discriminator.\nIn addition to the adversarial loss, the L1 norm loss is added to ensure that\nthe generated samples are not only able to fool the discriminator, but also app-roach the corresponding ground truth I\ngt. The loss is formulated as:\nLL1(Gsub)= E[||Igt−/summationdisplay\nsBsIs\nloc||1]. (3)\nThus, the ﬁnal objective for the ﬁrst stage generation is:\nLs1= arg min\nGsubmax\nDsubL1\nGAN+LL1. (4)\nFinally at the ﬁrst local texture generation stage, the complete generated\nembroidery Is1is regrouped as Is1=/summationtextS\ns=1Bs·Is\nloc.', 'Towards End-to-End Embroidery Style Generation 207\n4.2 Global Fine-Tuning\nIn the local texture generation stage, the aim is to synthesize the elegant stitches\nwithin each unique segmentation areas, where the global information is not care-fully considered (although we have added the global information as input). With\nthe consideration of revising the primary output I\ns1, we build a global generator\nGglobto ﬁne-tune the output of the ﬁrst stage, especially the connection areas\nbetween each sub embroideries Is\nloc.\nIn order to acquire the conﬁdence of the generated Is1, we train an eﬃcient\npatch discriminator to distinguish Is1from Igt. To be speciﬁc, we utilize a\nconvolutional “PatchGAN” classiﬁer, the L×Qpatch discriminator Dλμ(1≤\nλ≤Land 1 ≤μ≤M), followed the work of [ 15]. Then the discriminator can\nbe optimized with the objective below:\nLp=1\nLQ/summationdisplay\nλ,μE[log(1 −Dλμ(Is1))] +1\nLQ/summationdisplay\nλ,μE[logDλμ(Igt)], (5)\nwhere the optimal discriminator D∗\nλμis obtained by updating the parameters to\nmaximize Lp.\nAfter this step, the reliability of the generated Is1can be measured by ﬁrst\nfeeding Is1toD∗\nλμand then resizing the output D∗\nλμ(Is1) to the size of Is1,\nwhich is donated as βij(1≤i≤Hand 1 ≤i≤W). Finally the ﬁltering mask\nMwhich marks down the unreliable pixels of the generated sample Is1from the\nﬁrst stage can be calculated as:\nM(i,j)=/braceleftBigg\n1βij≤ρ\n0βij>ρ, (6)\nwhere we set ρ=0.3 in the experiments by the cross validation. Thus, the\nunreliable areas MIs1are pointed out with mask Mand need to be re-processed\nin the global generator Gglob, with the left part of MIs1unchanged. So the ﬁnal\noutput embroidery Is2is the combination of: Is2=( 1−M)Is1+MIglob,where\nIglobis the generated target in the global ﬁne-tuning stage (the lower part of\nFig.3) with the global generator Gglob:\nIglob←Gglob(MIin,z,glob (Iin),C), (7)\nwhere the input of Gglobis consisted of 4 parts, the random noise z, the global\ninformation tensor glob(Iin), the masked input MIinand the edge map C.T h e\nrandom noise zand the global tensor glob(Iin) are in the same deﬁnition as\nEq.1. Speciﬁcally, in order to concentrate more on the unknown pixels, not the\nwhole input image Iinis fed into the generator. Instead, the ﬁltering mask M\nmasks out the key areas that needs to be focused on by Gglob, which shows up\nasMIin. Recall that the global generator is leveraged mainly to ﬁne-tune Is1,\nespecially boundary and connection regions, we add the edge information by\nattaching the edge detection map Cto intensify it.', '208 J. Ye et al.\nThen the adversarial loss for the global stage is rewritten as:\nL2\nGAN(Gglob,Dglob)= E[log(1 −Dglob(MIglob))] + E[logDglob(MIgt)].(8)\nThen the optimal generator Gglobcan be learned by the objective function:\nLs2= arg min\nGglobmax\nDglobL2\nGAN+LL1(Gglob), (9)\nwhere LL1(Gglob)= E[M· ||Igt−Iglob||1], which is the L1 norm loss deﬁned in\nEq.3. The loss of the second global ﬁne-tuning stage Ls2is calculated based on\nthe unknown pixels left in the ﬁrst stage.\nAfter obtaining the optimal generator in the ﬁrst and the second stage ( G∗\nsub\nandG∗\nglob), the ﬁnal output is produced by these two generators as:\nIout=Is2=( 1−M)G∗\nsub+MG∗\nglob, (10)\nby which, the generated embroidery not only has local consistent stitches but\nalso clear boundaries. The whole algorithm of the two stage embroidery style\ngeneration will be given in the supplementary.\n5 Experiments\nIn this section, we ﬁrst describe our experimental settings and then show the\nresults with the ablation study and the comparisons with other methods. Andmore details and experimental results are in the supplementary.\n5.1 Experimental Setting\nAll the experiments are done on the proposed PEmD dataset, which is split into\nthe training set with 8k pairs and the testing set with 1k pairs. The images inPEmD are not in the same resolution, and we resize them to 256 ×256 in the\nimplementation. We build the two-stage generation architecture modiﬁed from\ncondition GAN. The entire network is trained on the TensorFlow platform withan NVIDIA M6000 GPU of 24G memory. All models are optimized using Adam\noptimizer. During the training process, we use a batch size of 16. The learning\nrate is initialized to 0.0002 and exponentially decease after 50k iterations oftraining for both the two stages of generation. The numbers of training iterations\nof the ﬁrst local texture generation stage and the second global ﬁne-tuning stage\nare set to be 50k and 80k, respectively.\n5.2 Evaluation Metrics\nPSNR. (higher is better) Peak Signal-to-Noise Ratio (PSNR) is used to measure\nthe similarity between the generated image Iand the corresponding ground-\ntruth I\ngt.SSIM. (higher is better) The Structural similarity metric (SSIM) [ 33]\nis used to measure the similarity between generated image Iand ground truth\nIgt.FID Evaluation. (lower is better) To evaluate the quality of the generated\nimages, the FID [ 11] metric is utilized, the main idea of which is to measure the\ndistance between the generated distribution and the real one through features\nextracted by Inception Network [ 27].', 'Towards End-to-End Embroidery Style Generation 209\n5.3 Experimental Results\nCompare with SOTA. We compare the performance of the proposed bench-\nmark with three popular image translation methods, which are: Pix2pix [15],\nthe original conditional adversarial networks; CycleGAN [42], the network that\nlearns unpaired image-to-image translation; Pix2pixHD [30], the network for\nsynthesizing high-resolution photo-realistic images.\nTable 3. Quantitative comparisons with state-of-the-art methods evaluated on using\nthree diﬀerent criterions. From the top to bottom the results are evaluated on PEmD-\nChara, PEmD-Cartn, PEmD-Real, PEmD-Sketch, PEmD-Com and the whole PEmD.\nPix2pix CycleGAN Pix2pixHD Ours\nPSNR(db) ↑ 20.73 19.46 22.27 24.34\nSSIM ↑ 0.7345 0.7537 0.8234 0.8902\nFID↓ 16.29 15.48 12.34 11.07\nPSNR(db) ↑ 15.59 12.68 18.30 19.08\nSSIM ↑ 0.6734 0.3248 0.6923 0.7155\nFID↓ 21.33 20.90 18.71 18.85\nPSNR(db) ↑ 12.39 10.58 14.90 14.59\nSSIM ↑ 0.5456 0.4385 0.6738 0.6766\nFID↓ 22.35 21.48 20.56 20.88\nPSNR(db) ↑ 20.67 19.01 21.98 25.30\nSSIM ↑ 0.6989 0.6885 0.7537 0.8605\nFID↓ 18.43 19.99 15.36 11.52\nPSNR(db) ↑ 18.43 17.86 20.94 22.20\nSSIM ↑ 0.6883 0.6325 0.7210 0.7359\nFID↓ 20.08 21.29 19.11 15.75\nPSNR(db) ↑ 19.63 18.48 21.52 23.49\nSSIM ↑ 0.7062 0.6791 0.7654 0.8194\nFID↓ 18.35 18.77 15.75 13.22\nTable 4. Ablation study on the proposed two-stage generation model. And we show\nthe eﬀectiveness of the components in the benchmark.\nBaseline o/Edge o/Glob Benchmark\nPSNR(db) ↑20.42 23.42 20.80 23.49\nSSIM↑ 0.7338 0.8138 0.7563 0.8149\nFID↓ 17.27 13.81 18.92 13.22\nThe quantitative results comparing with the above three methods are shown\nin Table 3, where all the methods for comparison are performed in the same\nsetting. As shown in the table, we evaluate the quality of the generated imagesﬁrst on the 5 sub datasets separately and then the whole PEmD dataset. And\nthe proposed method outperforms other methods on the whole PEmD dataset\nwith the 3 criterions by a large margin. Speciﬁcally, on the PEmD-Sketch, thesuperiority of our method is remarkable. The benchmark proposed in this paper\ndoes well in all the sub datasets except for the most challenging PEmD-Real,\nthe performance of which can be further improved as one future direction.', '210 J. Ye et al.\nThe visualization results are depicted in Fig. 4, and our method outperforms\nthe other methods. Speciﬁcally, for the samples on the second row, our methodshows the extraordinary performance on keeping the shape of the centering char-\nacters. And for the input images without the pure white background (samples\nin the third row), the output embroideries are failed with ‘Pix2pix’, ‘CycleGAN’and ‘Pix2pixHD’, while our benchmark ignores the unless information of the\nbackground and produces satisfying results. Also, we zoom in the samples in the\nlast row, which demonstrate the eﬀectiveness further.Ablation Study. Also, we have done the ablation study to analyze the com-\nponents of the proposed method on the whole PEmD dataset. The results are\nshown in Table 4, where the experiments are conducted with the following set-\ntings: ‘Baseline’ is conducted with the vanilla one-stage condition GAN; ‘o/Edge’\ndenotes the model that does not include the edge information at the second stage;\n‘o/Glob’ denotes the model does not include the global information at the ﬁrst\nand the second stage; ‘Benchmark’ denotes the proposed method in the paper\nwith the full setting.\nA ss h o w ni nT a b l e 4, the proposed method (‘Benchmark’) improves the\nbaseline method a lot in all the 3 criterions, including PSNR(db), SSIM and\nFID. Another observation is that the add of edge information does not show\nInput GT Pix2pix CycleGAN Pix2pixHD Ours\nZoom In\nFig. 4. The qualitative comparison. We show visualization results of ‘Pix2pix’, ‘Cycle-\nGAN’, ‘Pix2pixHD’ and ‘Ours’. Each row represents one testing sample.', 'Towards End-to-End Embroidery Style Generation 211\nsigniﬁcant improvement in this quantitative comparison (‘o/Edge’ vs ‘Bench-\nmark’), but it plays an vital role in the visualization results. And comparingthe results of ‘o/Glob’ and ‘Benchmark’, the proposed global encoder module is\nprove to be of vital importance.\n6 Conclusion\nIn this paper, we present the ﬁrst paired embroidery dataset (PEmD) consisted\nof 9k sets of the input design and the corresponding output embroidery, which\ngives a new challenge in the ﬁeld of image-to-image translation. Based on this\ndataset, we propose a benchmark embroidery preview method that works in twostages. In the ﬁrst stage, we train the sub-generators within each segmentation\nregions; in the second stage, we ﬁne-tune the output of the ﬁrst stage with a\nglobal generator. Experimental results show that our benchmark method solves\nthe embroidery problem much better than others—by measuring PSNR, SSIM,\nand FID scores on a dataset containing 9k pairs input image and expert-producedembroidery.\nAcknowledgement. This work is supported by National Key Research and Devel-\nopment Program (2016YFB1200203), National Natural Science Foundation of China(61572428, U1509206), Key Research and Development Program of Zhejiang Province\n(2018 C01004), and the Startup Funding of Stevens Institute of Technology and the\nFundamental Research Funds for the Central Universities (2021FZZX001-23).\nReferences\n1. Adabala, N., Magnenat-Thalmann, N., Fei, G.: Real-time rendering of woven\nclothes. In: VRST (2003)\n2. Beg, M.A., Yu, J.Y.: Generating embroidery patterns using image-to-image trans-\nlation. In: SAC (2020)\n3. Choi, Y., Choi, M.J., Kim, M., Ha, J.W., Kim, S., Choo, J.: StarGAN: uniﬁed\ngenerative adversarial networks for multi-domain image-to-image translation. In:\nCVPR, pp. 8789–8797 (2017)\n4. Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., Vedaldi, A.: Describing textures\nin the wild, pp. 3606–3613 (2014)\n5. Deussen, O., Hiller, S., van Overveld, C.W.A.M., Strothotte, T.: Floating points:\na method for computing stipple drawings. CGF 19, 41–50 (2000)\n6. Gansner, E.R., Hu, Y., North, S.C.: A maxent-stress model for graph layout. TVCG\n19, 927–940 (2012)\n7. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional\nneural networks. In: CVPR, pp. 2414–2423 (2016)\n8. Goodfellow, I.J., et al.: Generative adversarial nets. In: NIPS (2014)\n9. Hausner, A.: Simulating decorative mosaics. In: SIGGRAPH (2001)\n10. Hays, J., Essa, I.: Image and video based painterly animation. In: NPAR, pp.\n113–120 (2004)\n11. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: GANs\ntrained by a two time-scale update rule converge to a local nash equilibrium. In:\nNIPS (2017)', '212 J. Ye et al.\n12. Huang, H., Fu, T., Li, C.F.: Painterly rendering with content-dependent natural\npaint strokes. TVC 27, 861–871 (2011)\n13. Huang, X., Belongie, S.J.: Arbitrary style transfer in real-time with adaptive\ninstance normalization. In: ICCV, pp. 1510–1519 (2017)\n14. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-\ntional adversarial networks. In: CVPR, pp. 5967–5976 (2016)\n15. Isola, P., Zhu, J., Zhou, T., Efros, A.A.: Image-to-image translation with condi-\ntional adversarial networks. In: CVPR, pp. 5967–5976 (2017)\n16. Jin, Y., Zhang, J., Li, M., Tian, Y., Zhu, H., Fang, Z.: Towards the automatic\nanime characters creation with generative adversarial networks. In: NIPS (2017)\n17. Jing, Y., Yang, Y., Feng, Z., Ye, J., Song, M.: Neural style transfer: a review. In:\nTVCG (2017)\n18. Kaldor, J.M., James, D.L., Marschner, S.: Simulating knitted cloth at the yarn\nlevel. In: SIGGRAPH (2008)\n19. Lee, H.-Y., Tseng, H.-Y., Huang, J.-B., Singh, M., Yang, M.-H.: Diverse image-to-\nimage translation via disentangled representations. In: Ferrari, V., Hebert, M.,\nSminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11205, pp. 36–52.Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01246-5\n3\n20. Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer\nvia feature transforms. In: NIPS (2017)\n21. Litwinowicz, P.: Processing images and video for an impressionist eﬀect. In: SIG-\nGRAPH (1997)\n22. Lu, J., Sander, P.V., Finkelstein, A.: Interactive painterly stylization of images,\nvideos and 3d animations. In: I3D (2010)\n23. Mart´ ın, D., Arroyo, G., Luz´ on, M.V., Isenberg, T.: Scale-dependent and example-\nbased grayscale stippling. Comput. Graph. 35, 160–174 (2011)\n24. Prewitt, J.M., Mendelsohn, M.L.: The analysis of cell images. ANN NY ACAD\n128(3), 1035–1053 (1966)\n25. Russakovsky, O., et al.: ImageNet large scale visual recognition challenge. IJCV\n115(3), 211–252 (2015)\n26. Sangkloy, P., Lu, J., Chen, F., Yu, F., Hays, J.: Scribbler: controlling deep image\nsynthesis with sketch and color. In: CVPR (2017)\n27. Szegedy, C., et al.: Going deeper with convolutions. In: CVPR, pp. 1–9 (2015)\n28. Tylecek, R., S´ ara, R.: Spatial pattern templates for recognition of objects with\nregular structure. In: GCPR (2013)\n29. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-\nresolution image synthesis and semantic manipulation with conditional GANs. In:\nCVPR, pp. 8798–8807 (2017)\n30. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-\nresolution image synthesis and semantic manipulation with conditional GANs. In:\nCVPR (2018)\n31. Wang, W., Liu, J., Yang, S., Guo, Z.: Typography with decor: intelligent text style\ntransfer. In: CVPR, pp. 5882–5890 (2019)\n32. Wang, X., Oxholm, G., Zhang, D., fang Wang, Y.: Multimodal transfer: a hierar-\nchical deep convolutional neural network for fast artistic style transfer. In: CVPR,\npp. 7178–7186 (2016)\n33. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:\nfrom error visibility to structural similarity. TIP 13(4), 600–612 (2004)\n34. Wyvill, B., van Overveld, C.W.A.M., Carpendale, M.S.T.: Rendering cracks in\nbatik. In: NPAR (2004)', 'Towards End-to-End Embroidery Style Generation 213\n35. Xu, X., Zhang, L., Wong, T.T.: Structure-based ASCII art. TOG 29(4), 1 (2010)\n36. Yang, Y., Wang, J., Vouga, E., Wonka, P.: Urban pattern: layout design by hier-\narchical domain splitting. TOG 32, 181:1–181:12 (2013)\n37. Yu, A., Grauman, K.: Fine-grained visual comparisons with local learning. In:\nCVPR, pp. 192–199 (2014)\n38. Zeng, K., Zhao, M., Xiong, C., Zhu, S.C.: From image parsing to painterly render-\ning. TOG 29(1), 1–11 (2009)\n39. Zhang, W., Li, G., Ma, H., Yu, Y.: Automatic color sketch generation using deep\nstyle transfer. CG&A 39, 26–37 (2019)\n40. Zheng, X., Chalasani, T., Ghosal, K., Lutz, S., Smolic, A.: STADA: style transfer\nas data augmentation (2019)\n41. Zhou, J., Sun, Z., Yang, K.: A controllable stitch layout strategy for random needle\nembroidery. J. Zhejiang Univ. Sci. C 15(9), 729–743 (2014). https://doi.org/10.\n1631/jzus.C1400099\n42. Zhu, J., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using\ncycle-consistent adversarial networks. In: ICCV, pp. 2242–2251 (2017)', 'Eﬃcient and Real-Time Particle\nDetection via Encoder-Decoder Network\nYuanyuan Wang, Ling Ma(B), Lihua Jian, and Huiqin Jiang\nSchool of Information Engineering, Zhengzhou University, Zhengzhou, China\n{ielma,ielhjian,iehqjiang }@zzu.edu.cn\nAbstract. Particle detection aims to locate and count valid particles\nin pad images accurately. However, existing methods fail to achieveboth high detection accuracy and inference eﬃciency in real applica-\ntions. In order to keep a good trade-oﬀ between inference eﬃciency and\naccuracy, we propose a computation-eﬃcient particle detection network(PAD-Net) with an encoder-decoder architecture. For the encoder part,\nMobileNetV3 is tailored to greatly reduce parameters at a little cost of\naccuracy drop. And the decoder part is designed based on the light-weight ReﬁneNet, which further boosts particle detection performance.\nBesides, the proposed network is equipped with the adaptive attention\nloss (termed AAL), which improves the detection accuracy with a neg-ligible increase in computation cost. Finally, we employ a knowledge\ndistillation strategy to further boost the ﬁnal detection performance of\nPAD-Net without increasing its parameters and ﬂoating-point operations(FLOPs). Experimental results on the real datasets demonstrate that our\nmethods can achieve high-accuracy and real-time detection performance\non valid particles compared with the state-of-the-art methods.\nKeywords: Particle detection\n·Knowledge distillation ·Light-weight\nneural networks ·PAD-Net\n1 Introduction\nConductive particle detection is the key step in the circuit inspection of liq-\nuid crystal modules and its main task is to accurately locate and count valid\nparticles in pad images, then determine whether a good conductive circuit is\nformed between IC and glass substrate [ 5,8,14,20,21]. In addition, in real indus-\ntrial inspection, it is almost equivalently important to achieve an acceptable\naccuracy and to obtain a fast inference speed. Many light-weight networks have\nbeen designed for various application ﬁelds [ 3,7,13,16,18,22]. However, the light-\nweight particle detection network has been rarely studied, despite its importance\nin industrial inspection.\nThis work is supported by the National Natural Science Foundation of China (No.\nU1604262 and U1904211).\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 214–226, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_18', 'Eﬃcient and Real-Time Particle Detection 215\nIn our work, a light-weight and eﬀective network is proposed for parti-\ncle detection. This network adopts the encoder-decoder architecture, achievinga good trade-oﬀ between detection accuracy and speed. The encoder part is\ndesigned based on MobileNetV3-Small [ 3] by using few convolution layers (6\nbnecks instead of 11 bnecks) and few feature channels. And the decoder partis designed based on the light-weight ReﬁneNet [ 13]. Additionally, we use AAL\nand knowledge distillation to improve the ﬁnal detection accuracy of PAD-Net.\nExperiment results show that our network dramatically reduces parameters andFLOPs and can achieve comparable performance with state-of-the-arts. The\nmain contributions are summarized as follows:\n\x81An eﬃcient network (PAD-Net) is speciﬁcally designed for real-time particle\ndetecting. The MobileNetV3-based encoder and light-weight ReﬁneNet-baseddecoder are tailored to achieve a good balance between inference speed and\ndetection accuracy.\n\x81A target adaptive attention loss function (AAL) is proposed to enforce the\nnetwork to learn semantic features of valid particles.\n\x81Knowledge distillation strategy is employed to transfers a teacher model’s\ninformation to our PAD-Net. By using this strategy, it can further improvethe ﬁnal performance of PAD-Net without increasing parameters and FLOPs.\n2 Related Work\nIn this section, we will introduce the related works from four aspects: (1) Particle\ndetection methods; (2) Decoder architectures; (3) Light-weight model designs;\n(4) Knowledge distillation (KD).\n2.1 Particle Detection Methods\nAccording to the forms of extracting features, particle detection methods can be\ndivided into two categories: (1) the handcrafted feature-based methods [ 5,14,21];\n(2) the convolutional neural networks (CNNs)-based methods [ 8,19,20]. The\nhandcrafted feature-based methods attempt to design appropriate descriptorsfor valid particles. Through the handcrafted features, semantic information\nextracted is simple and can’t well describe detection targets. Moreover, many\nparameters in handcrafted rules need to be adjusted manually, resulting in incon-venience in industrial inspection [ 8,20]. The mainstream methods are based on\nCNNs, which can oﬀer more discriminative features compared with the hand-\ncrafted features. For instance, Liu et al. [ 8] propose the U-ResNet network to\ndetect particles, which obtains better results than traditional methods. Simi-\nlarly, Ma et al. [ 19] propose a compact U-shape structure to accurately esti-\nmate valid particles, utilizing semantic features at multi-scales. Based on the\nwork [ 19], GAN [ 20] is introduced in the detection model, in which the discrim-\ninator is used to correct high-order inconsistencies between predicted particlesand ground truth, further improving the detection results.', '216 Y. Wang et al.\nCompared with the existing CNNs for particle detection, our encoder-decoder\nnetwork is speciﬁcally designed for high-accuracy real-time particle detection onindustrial devices with limited computation resources. In Sect. 4, the proposed\nnetwork is evaluated in detail.\n2.2 Decoder Architectures\nThe decoder aims to recover high-resolution spatial information while the\nencoder models low-resolution category information, and it has been widely used\nfor image segmentation. For instance, fully convolutional network (FCN) [ 11]\nforms a decoder structure through simple skip connections, making CNNs tran-\nsition from classiﬁcation networks to segmentation networks. Later, FCN net-works with diﬀerent decoders appear one after another, such as U-Net [ 15],\nReﬁneNet [ 6], etc. In U-Net [ 15], symmetric decoder (SD) is introduced to\nintegrate shallow-layer and deep-layer information, achieving ﬁne segmentationaccuracy. The decoder of ReﬁneNet [ 6] improves the segmentation accuracy via a\nmulti-path reﬁnement module. In order to achieve real-time segmentation speed,\nLight-Weight ReﬁneNet decoder [ 13] is proposed based on ReﬁneNet [ 6]. In addi-\ntion, researchers also designed other decoder architectures based on neural archi-\ntecture search (NAS), such as [ 12]. As a kind of pixel-wise task, CNNs-based\nparticle detection network also needs a decoder to generate high-quality maps.In our work, PAD-Net uses a light-weight decoder based on the work [ 13]a n d\ngreatly improves the detection performance.\n2.3 Light-Weight Networks\nLight-weight networks mainly focus on how to tune deep neural architectures\nto make a good trade-oﬀ between accuracy and performance. Many eﬀectivelight-weight networks are based on eﬃcient CNNs architectures designing. For\ninstance, MobileNetV1 [ 4], using depthwise separable convolutions to design a\nsmall network, achieves high classiﬁcation accuracy. Comparing with [ 4], Shuf-\nﬂeNet [ 22] achieves higher eﬃciency by reducing computation cost based on\nchannel shuﬄe and group convolution. Based on the inverted residual structure\nwith liner bottleneck, MobileNetV2 [ 16] builds light-weight deep networks and\nobtains great performance. After [ 16], MobileNetV3-Small [ 3] further utilizes\nAutoML technology achieving better classiﬁcation performance. Beneﬁting from\nprevious eﬀective CNNs architectures designing, our PAD-Net adopts a fast andeﬀective encoder based on MobileNetV3-Small [ 3].\n2.4 Knowledge Distillation\nKnowledge distillation (KD) aims to transfer knowledge from a complex network\nto improve the performance of a light-weight network [ 1,2,9,10]. For instance,\nHinton et al. [ 2] transfer knowledge of a large network to a small network. In\n[1], the authors transfer the knowledge embedded in layer-wise features of the', 'Eﬃcient and Real-Time Particle Detection 217\ncomplex network to optimize the segmentation accuracy of the small network.\nIn [9], the authors transfer the distinguishing features from the large network\nto improve the counting performance of the compact network. In our work,\nwe introduce KD into the particle detection task. The structured knowledge\nextracted from the complex networks is utilized to optimize the PAD-Net, whichfurther boosts the detection performance of PAD-Net without increasing its\nparameters and FLOPs.\n3 The Proposed Method\nIn this section, we will describe the proposed PAD-Net, including network frame-\nwork, loss function, and our KD strategy.\nFig. 1. Overall framework of PAD-Net. (a) PAD-Net consists of encoder and decoder.\n(b) MaxPool-DSConv-Residual chain (MDR). (c) FUSION module for integrating low-\nres features and high-res features. (b) and (c) together constitute the decoder in (a).\n3.1 Network Framework\nThe PAD-Net adopts the light-weight encoder-decoder framework, as shown in\nFig.1. The encoder MobileNetV3-Small [ 3] is speciﬁcally designed for real-time\nclassiﬁcation, with signiﬁcantly reduced memory footprint and improved classiﬁ-cation accuracy. To reduce both parameters and FLOPs to satisfy the demands\nof real industrial detection, we choose MobileNetV3-Small [ 3] as the encoder\nand adjust it speciﬁcally for particle detection. Through experiments, we ﬁnd', '218 Y. Wang et al.\nthat using 6 bnecks instead of 11 can obtain a better trade-oﬀ between accu-\nracy and parameters. Speciﬁcally, compared with MobileNetV3-Small-11bnecks(Mv3-11BN), MobileNetV3-Small-6bnecks (Mv3-6BN) reduces 50% parameters\nand over 55% FLOPs with a trivial drop of accuracy. In addition, we further\ncompress Mv3-6BN by reducing the number of channels to ﬁnd an optimal bal-ance between inference performance and accuracy. Comparing with the encoder\nMv3-6BN, Our encoder in PAD-Net reduces over 95% parameters and over 90%\nFLOPs with a tolerable drop of accuracy. The detail of the encoder in PAD-Netis shown in Fig. 2.\nFig. 2. The architecture of our encoder in PAD-Net (Fig. 1(a)). Detailed conﬁgurations\nof each bneck can be referred to Table 1.\nTable 1. Diﬀerent settings of our proposed encoders. The bneck is residual bottleneck\nproposed in MobileNetV3 [ 3]. SE denotes whether there is a Squeeze-And-Excite in\nthat block. NL is the type of nonlinearity used. Here, HS is h-swish and RE is ReLU.In, Exp, Out denote the number of channels in the input, expansion, and output layers\nrespectively in that block.\nOperator OutputSize Stride SE NL PAD-Net PAD-Net ×2 PAD-Net ×4\nIn Exp Out In Exp Out In Exp Out\nImage 1/1 – – – 1 – – 1 – – 1 – –\nconv2d, 3 ×31/1 1 – HS 1 – 2 1 – 4 1 – 8\nbneck, 3 ×3 1/1 1 /check RE 2 8 4 4 16 8 8 32 16\nbneck, 3 ×3 1/1 1 – RE 4 8 4 8 16 8 16 32 16\nbneck, 3 ×3 1/2 2 /check HS 4 16 8 8 32 16 16 64 32\nbneck, 3 ×3 1/2 1 /check HS 8 16 8 16 32 16 32 64 32\nbneck, 3 ×3 1/4 2 /check HS 8 32 16 16 64 32 32 128 64\nbneck, 3 ×3 1/4 1 /check HS 16 32 16 32 64 32 64 128 64\nIn our decoder, we incorporate multi-scale semantic features to distinguish\nvalid particles. The high-layer semantic features, middle-layer features, and low-layer spatial details from the encoder are integrated into the decoder for improv-\ning the detection performance. Inspired by [ 13], our decoder consists of three\nMDR and two FUSION modules, as shown in Fig. 1(a). The decoding process\nof PAD-Net starts with the last output of the encoder. High-layer semantic fea-\ntures are fed into MDR through conv1 ×1 operation (Fig. 1). The subsequent\nresults are sent into a FUSION module along with middle-layer features. InsideFUSION module (Fig. 1(c)), each path is convolved with conv1 ×1. Then, the\nsemantic features with low-resolution are up-sampled to the high-resolution and', 'Eﬃcient and Real-Time Particle Detection 219\nmerge with spatial details with high-resolution by Concat operation. Two paths\nare then convolved with conv3 ×3, and analogously further propagated through\nsubsequent MDR and FUSION modules until the desired resolution is reached.\nCompared with the light-weight ReﬁneNet decoder, our decoder can obtain bet-\nter detection performance with lower FLOPs and fewer parameters. In addition,we also try to use diﬀerent decoder sizes by changing the number of feature\nchannels to ﬁnd a better balance between eﬃciency and accuracy. The number\nof channels in our decoder is all set to 4. And the number of channels in ourdecoder+ is 4 times that of our decoder.\nFig. 3. The visualization of the adaptive coeﬃcient matrix M. (a) Input image. (b)\nThe global value distribution of M, corresponding to the distribution of particles in (a).\n(c) The local magniﬁcation of M: the attention at the particle center is the strongest.\n3.2 Loss Function\nTo solve the unbalanced number of pixels between particles and background, the\nAAL is proposed to make the model put more focus on the particle regions. AAL\nloss is deﬁned as: L=α1·LCE+α2·LIoU, where α1,α2are weight coeﬃcients.\nThe ﬁrst item in AAL loss, is cross-entropy loss, which is deﬁned as:\nLCE=−1\nwhw−1/summationtext\nx=0h−1/summationtext\ny=0(p(x,y) log (ˆp(x,y))+(1 −p(x,y)) log (1 −ˆp(x,y)))(1)\nwhere h,wdenote the size of image, ˆ p(x,y)a n d p(x,y) are the output at a\nspeciﬁc location ( x,y) of the prediction and ground truth, respectively.\nThe second item in AAL loss, is pixel-level IoU loss with the target adaptive\nattention coeﬃcient matrix M.LIoUis deﬁned as:\nLIoU=1\nwhw−1/summationtext\nx=0h−1/summationtext\ny=0Mx,y/parenleftBig\n1−p(x,y)·ˆp(x,y)+ε\np(x,y)+ˆp(x,y)−p(x,y)·ˆp(x,y)+ε/parenrightBig\n(2)\nandMis deﬁned as: M=ones matrix +AvgPool (P).Mx,yis the coeﬃcient value\nat (x,y)o fM;AvgPool is an average pooling operation with size of 5 ×5a n d\nstep size of 1; Pis ground truth, in which each pixel is either 0 or 1; ones matrix\nhas the same size as P, and its all values are 1.\nIn the training, the coeﬃcients in Mare adjusted adaptively according to two\naspects: (1) the global distribution of particles in P; (2) the local distribution of\nparticle center. At the global level, as shown in Fig. 3(b), the value distribution in\nMis self-adaptive to the spatial distribution of valid particles in the input image,', '220 Y. Wang et al.\nthat is, the coeﬃcients of particle region are larger than that of background,\nwhich alleviates the imbalance pixels between the particles and background. Atthe local level, as shown in Fig. 3(c), the value distribution in Mis also self-\nadaptive to the location distribution of particle center, that is, the closer to the\nparticle center, the greater the coeﬃcient. The coeﬃcient at the particle centeris the largest, which can strengthen the penalty for the location deviation of\nthe predicted particle. In addition, we also use other classical loss functions for\ncomparison. Compared with these classical loss functions, our AAL achieves thebest particle detection performance. The quantitative comparison in Sect. 4.\nFig. 4. Comparison of the features before and after KD. High-layer (ﬁrst row), Middle-\nlayer (second row), and Low-layer (last row) feature maps are from three MDR units of\nPAD-Net’s decoder, respectively. The left feature maps are obtained before KD, whilethe right feature maps are extracted after KD. We can observe that the features after\ndistillation contain clearer and more eﬀective information. Best viewed in color. (Color\nﬁgure online)\n3.3 Structured Knowledge Distillation\nConsidering the particularity of particle detection, we need a method to transfer\nthe structured knowledge of discriminative semantics and spatial details of par-\nticles. Since the intra-layer and cross-layer knowledge distillation [ 9] can more\neﬀectively transfer the structured knowledge of semantics and spatial details, wedirectly use this special knowledge distillation strategy for optimizing our PAD-\nNet. First, we build a complex teacher network, in which the encoder employs the\noriginal MobileNetV3-Small [ 3] without the last classiﬁcation structure. And the\ndecoder uses the original light-weight ReﬁneNet decoder [ 13], in which the SUM\noperation of FUSION module is replaced with Concat and Conv3 ×3 operations\nfor fully integrating low-res and high-res features.\nIn KD training, the teacher model is ﬁrst trained to generate accurate feature\nlabels. Then we obtain diﬀerent level feature maps after each MDR unit ofPAD-Net, as shown in Fig. 4, which are used to calculate intra-layer and cross-\nlayer knowledge losses with the feature labels generated by the teacher model\nrespectively. The total loss is the sum of the two types of losses and used foroptimizing the PAD-Net. In the test phase, compared with the PAD-Net before\nKD, PAD-Net after KD does not increase the parameters and FLOPs, but the\ndetection performance is further improved. In order to clearly show the goodeﬀect of KD on particle detection performance, the visualization comparison of\nfeature maps is shown in Fig. 4. In addition, a detailed quantitative comparison\nis given in Sect. 4.3.', 'Eﬃcient and Real-Time Particle Detection 221\n4 Experiments\nIn this section, we ﬁrstly introduce the dataset, implementation details, and\nevaluation metrics. Then, we evaluate the diﬀerent encoders, diﬀerent decoders,\ndiﬀerent loss functions, and the knowledge distillation strategy. Finally, we com-pare the three settings of our models with other detection methods.\n4.1 Datasets and Implementation Details\nAs we know, there are no public particle datasets for real detection tasks. We\ncollect pad images containing eﬀective particles from real assembly lines. Pad\nimages are taken by a line scan camera and the size of pad image is 112 ×48. Our\ndataset includes 2952 pad images with a total of 73819 valid particles, in which\ntraining data has 1952 images and testing data has 1000 images. In addition,\ninput images need to be normalized before being fed into the network.\nThe training is performed on PyTorch framework and a workstation with\nIntel Xeon E5-2630 v4 CPU,128G RAM NVIDIA, Tesla K80 ×8G P U .I no u r\nexperiment, we use Adam optimization with a batch size of 8 and a weightdecay rate of 1 ×10\n−3. The number or training epoch is set to 100. The initial\nlearning rate is set to 1 ×10−4,β1is set to 0.9, and β2is set to 0.99.\n4.2 Evaluation Metrics\nMAE and RMSE: For particle detection tasks, mean absolute error (MAE) and\nroot mean square error (RMSE) are used to evaluate our model, more details\nrefer to former works [ 9,17].\nPrecision and Recal l: In order to more accurately evaluate the detection perfor-\nmance, Precision andRecall are also adopted to evaluate our models, which is\nthe same as former works [ 8,20].\n4.3 Results and Analysis\nComparison of Diﬀerent Encoders. Under a ﬁxed decoder, we evalu-\nate the encoder of PAD-Net against four diﬀerent encoders, including the\nMobileNetV2 [ 16] with 7 bottlenecks (denoted as Mv2-7BN) and 4 bottlenecks\n(denoted as Mv2-4BN), the MobileNetV3-Small [ 3] with 11 bnecks (denoted as\nMv3-11BN) and 6 bnecks (denoted as Mv3-6BN). Besides, our encoder is also\nevaluated with diﬀerent channels (i.e. PAD-Net, PAD-Net ×2, PAD-Net ×4, as\ns h o w ni nT a b l e 1). From Table 2, we can observe that PAD-Net can obtain a\nbetter trade-oﬀ between detection accuracy and the number of parameters and\nFLOPs. Speciﬁcally, compared with Mv3-11BN, PAD-Net reduces 98% parame-\nters and over 97% FLOPs with a roughly 7.2% reduction of Precision and 4.0%\nreduction of Recall. In Table 2, compared with other encoders, the encoder of\nPAD-Net is more suitable for high-speed particle detection.', '222 Y. Wang et al.\nComparison of Diﬀerent Decoders. Under a ﬁxed encoder, we compare\ndiﬀerent decoders for fusing multi-scale features, including SD, the proposeddecoder (our decoder), and another version with more feature channels (our\ndecoder+). Besides, light-weight Reﬁne decoder [ 13] with the same number of\nchannels as our decoder+ is also involved in the comparison (denoted as LWRD).From Table 3, we can ﬁnd that the proposed decoders perform better than other\ndecoders in terms of the MAE, RMSE, Precision, and Recall. Speciﬁcally, the\ncomputation complexity of SD is more than two times larger than our decoder,but its detection performance is slightly worse. Moreover, compared with LWRD\nin Table 3, our decoder achieves better particle detection performance with fewer\nparameters and fewer FLOPs. In addition, compared with our decoder+, ourdecoder reduces the parameters and computational complexity, but its perfor-\nmance is still close to our decoder+. Hence, our decoder achieves a better trade-\noﬀ between computational eﬃciency and accuracy.\nComparison of Diﬀerent Loss Functions We make a comparison of AAL\nand other loss functions. From Table 4, we can observe that AAL performs better\nthan other loss functions in terms of MAE, RMSE, Precision, and Recall. Among\nthese loss functions, AAL achieves the best detection performance, which veriﬁesthe eﬀectiveness of AAL.\nComparison Before and After Knowledge Distillation. We choose a com-\nplex network with high detection accuracy as the teacher network (the details in\nSect.3.3). The structured knowledge of the discriminative semantics and spatial\ndetails from the teacher model is transferred into the PAD-Net. The performance\ncomparison of PAD-Net before and after KD is shown in Table 5.F r o mT a b l e 5,\nwe can see the performance of PAD-Net after KD (denoted as PAD-Net(KD)) isbetter than the one before KD. Particularly, PAD-Net(KD) decreases the MAE\nby over 0.29, the RMSE by over 0.34, and increases the Precision by 4.93%\nagainst the PAD-Net. Experimental results in Table 5show that knowledge dis-\ntillation can further improve the detection performance of PAD-Net.\nComparison with State-of-the-Arts. We compare state-of-the-art detec-\ntion approaches with our proposed models (i.e. PAD-Net, PAD-Net ×2, and\nPAD-Net ×4), as shown in Table 6.F r o mT a b l e 6,i tc a nb es h o w nt h a tP A D -\nNet achieves an optimized trade-oﬀ between detection accuracy and compu-\ntation speed. Speciﬁcally, the total number of parameters in PAD-Net is only\n0.0057M while the Precision and Recall of PAD-Net can reach 0.8858 and 0.9210respectively. PAD-Net’s detection performance is still competitive. What’s more,\nafter knowledge distillation, the ﬁnal performance of PAD-Net has been further\nimproved, which reaches the Precision of 0.9351 and the Recall of 0.9244, without\nthe increase of parameters and FLOPs. Visualization comparison of detection\nresults of diﬀerent models is shown in Fig. 5, which indicates that our networks\ncan obtain high-quality detection results, especially PAD-Net(KD), as shown', 'Eﬃcient and Real-Time Particle Detection 223\nin Fig. 5(m). In addition, compared with the models with higher accuracy, our\nmodel achieves a better trade-oﬀ between detection accuracy and inference eﬃ-ciency. As shown in Table 7, we compare the forward inference time of models,\nfor input images (112 ×48), on three types of platforms (GTX 1060, Tesla K80,\nand GTX 2080), and with two batch sizes (128 and 256). From Table 7,o u r\nPAD-Net(KD) is signiﬁcantly faster than the compared models. It arrives at\n0.082ms on GTX 2080 with batch size equals 256.\nTable 2. Comparison of diﬀerent encoders under a ﬁxed decoder, in terms of\nParams( ↓), MFLOPs( ↓), MAE( ↓), RMSE( ↓), Precision( ↑), and Recall( ↑).\nEncoder Params (MB) MFLOPs MAE RMSE Precision Recall\nMv2-7BN 1.82 1484.5 1.1025 1.5489 0.9536 0.9586\nMv2-4BN 0.25 228.8 1.5494 2.1021 0.9185 0.9445\nMv3-11BN 0.44 319.6 1.0833 1.5341 0.9545 0.9594\nMv3-6BN 0.22 142.4 1.5230 2.0746 0.9212 0.9490\nPAD-Net 0.0057 8.6 1.9380 2.5938 0.8858 0.9210\nPAD-Net ×20.016 18.5 1.8978 2.4970 0.8965 0.9397\nPAD-Net ×40.052 52.6 1.7930 2.2790 0.9098 0.9403\nTable 3. Comparison of diﬀerent decoders using the same encoder (PAD-Net ×4), in\nterms of Params( ↓), MFLOPs( ↓), MAE( ↓), RMSE( ↓), Precision( ↑), and Recall( ↑).\nEncoder Params (MB) MFLOPs MAE RMSE Precision Recall\nSD 0.11 173.6 1.9361 2.5812 0.8896 0.9243\nLWRD 0.057 61.7 1.8851 2.3693 0.9002 0.9204\nOur decoder 0.052 52.6 1.7932 2.2792 0.9098 0.9403\nOur decoder+ 0.065 92.0 1.7441 2.2221 0.9137 0.9428\nTable 4. Comparison of AAL and other loss functions in terms of MAE( ↓), RMSE( ↓),\nPrecision( ↑), and Recall( ↑). We use the same detection model (PAD-Net).\nLoss functions MAE RMSE Precision Recall\nIoU-loss 2.4783 3.4178 0.8350 0.8976\nDice-loss 2.0006 2.6561 0.8818 0.9130\nMSE-loss 2.0247 2.7171 0.8844 0.9203\nBCE-loss 1.9468 2.5989 0.8814 0.8841\nAAL (ours) 1.9380 2.5938 0.8858 0.9210', '224 Y. Wang et al.\nTable 5. Comparison of PAD-Net model before and after knowledge distillation.\nModels Params (MB) MFLOPs MAE RMSE Precision Recall\nTeacher Net 0.4697 422.5 0.8539 1.2884 0.9673 0.9797\nPAD-Net 0.0057 8.6 1.9380 2.5938 0.8858 0.9210\nPAD-Net (KD) 0.0057 8.6 1.6453 2.2480 0.9351 0.9244\nFig. 5. Examples of particle detection results from diﬀerent models. (a) Input image;\n(b) Particle center in Ground truth; (c) U-Net; (d) U-ResNet; (e) U-MultiNet; (f) MV2-7BN; (g) MV2-4BN; (h) MV3-11BN; (i) MV3-6BN; (j) PAD-Net ×4; (k) PAD-Net ×2;\n(l) PAD-Net; (m) PAD-Net(KD). Best viewed in color. (Color ﬁgure online)\nTable 6. Comparison of diﬀerent detection methods in terms of Params( ↓),\nMFLOPs( ↓), MAE( ↓), RMSE( ↓), Precision( ↑), and Recall( ↑).\nModels Params (MB) MFLOPs MAE RMSE Precision Recall\nU-Net 0.0086 4.4 2.4935 3.2949 0.8502 0.8913\nU-ResNet [ 8] 0.0125 7.3 2.1873 2.9012 0.8685 0.8929\nU-MultiNet [ 20]0.0297 48.4 1.9358 2.5780 0.8874 0.9333\nMv2-7BN 1.82 1484.5 1.1025 1.5489 0.9536 0.9586\nMv2-4BN 0.25 228.8 1.5494 2.1021 0.9185 0.9445\nMv3-11BN 0.44 319.6 1.0833 1.5341 0.9545 0.9594\nMv3-6BN 0.22 142.4 1.5230 2.0746 0.9212 0.9490\nPAD-Net 0.0057 8.6 1.9380 2.5938 0.8858 0.9210\nPAD-Net ×2 0.016 18.5 1.8978 2.4970 0.8965 0.9397\nPAD-Net ×4 0.052 52.6 1.7930 2.2790 0.9098 0.9403\nPAD-Net (KD) 0.0057 8.6 1.6453 2.2480 0.9351 0.9244\nTable 7. Comparison of the forward inference times of diﬀerent models on three types\nof platforms (GTX 1060, Tesla K80, and GTX 2080).\nModels GTX1060 (ms) Tesla K80 (ms) GTX2080 (ms)\nbatchsize128 batchsize256 batchsize128 batchsize256 batchsize128 batchsize256\nMv2-7BN 5.270 5.336 7.269 7.228 1.755 1.758\nMv2-4BN 1.516 1.514 1.970 1.976 0.496 0.494\nMv3-11BN 2.674 2.671 3.505 3.502 0.903 0.901\nMv3-6BN 1.202 1.199 1.444 1.440 0.424 0.422\nPAD-Net ×4 0.693 0.689 0.854 0.842 0.243 0.241\nPAD-Net ×2 0.404 0.400 0.496 0.487 0.137 0.135\nPAD-Net 0.264 0.259 0.332 0.324 0.086 0.082\nPAD-Net (KD) 0.264 0.259 0.332 0.324 0.086 0.082', 'Eﬃcient and Real-Time Particle Detection 225\n5 Conclusion\nIn this paper, we present a light-weight encoder-decoder network for eﬃcient\nand real-time particle detection, named PAD-Net. The encoder and decoder of\nPAD-Net are designed to strike an optimal balance between detection accuracyand inference speed. Besides, we propose a target Adaptive Attentional Loss\nfunction to train the PAD-Net, which makes the network concentrate on learn-\ning the features of valid particles. In addition, the knowledge distillation methodis introduced into the training model, which further boosts the ﬁnal detection\nperformance of PAD-Net without increasing parameters and FLOPs. Extensive\nexperiments demonstrate that our method achieves comparable particle detec-tion performance by using fewer computation resources.\nReferences\n1. He, T., Shen, C., Tian, Z., Gong, D., Sun, C., Yan, Y.: Knowledge adaptation\nfor eﬃcient semantic segmentation. In: IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2019, Long Beach, CA, USA, 16–20 June 2019, pp.578–587 (2019)\n2. Hinton, G.E., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.\nCoRR arXiv:abs/1503.02531 (2015)\n3. Howard, A., et al.: Searching for mobilenetv3. In: 2019 IEEE/CVF International\nConference on Computer Vision (ICCV) (2020)\n4. Howard, A.G., et al.: MobileNets: eﬃcient convolutional neural networks for mobile\nvision applications. CoRR arXiv:abs/1704.04861 (2017)\n5. Lin, C.S., Lu, K.H.H., Lin, T.C., Shei, H.J., Tien, C.L.: An automatic inspection\nmethod for the fracture conditions of anisotropic conductive ﬁlm in the TFT-LCDassembly process. Int. J. Optomechatronics 5(9), 286–298 (2011)\n6. Lin, G., Milan, A., Shen, C., Reid, I.D.: ReﬁneNet: multi-path reﬁnement networks\nfor high-resolution semantic segmentation. In: 2017 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, 21–26 July 2017,\npp. 5168–5177 (2017)\n7. Lin, P., Sun, P., Cheng, G., Xie, S., Li, X., Shi, J.: Graph-guided architecture\nsearch for real-time semantic segmentation. In: 2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, 13–19\nJune 2020, pp. 4202–4211 (2020)\n8. Liu, E., Chen, K., Xiang, Z., Zhang, J.: Conductive particle detection via deep\nlearning for ACF bonding in TFT-LCD manufacturing. J. Intell. Manuf. 31(4),\n1037–1049 (2020)\n9. Liu, L., Chen, J., Wu, H., Chen, T., Li, G., Lin, L.: Eﬃcient crowd counting\nvia structured knowledge transfer. In: Proceedings of the 28th ACM International\nConference on Multimedia, pp. 2645–2654 (2020)\n10. Liu, Y., Chen, K., Liu, C., Qin, Z., Luo, Z., Wang, J.: Structured knowledge dis-\ntillation for semantic segmentation. In: IEEE Conference on Computer Vision andPattern Recognition, CVPR 2019, Long Beach, CA, USA, 16–20 June 2019, pp.\n2604–2613 (2019)\n11. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 3431–3440 (2015)', '226 Y. Wang et al.\n12. Nekrasov, V., Chen, H., Shen, C., Reid, I.D.: Fast neural architecture search of\ncompact semantic segmentation models via auxiliary cells. In: IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA,16–20 June 2019, pp. 9126–9135 (2019)\n13. Nekrasov, V., Shen, C., Reid, I.: Light-weight reﬁneNet for real-time semantic\nsegmentation. In: British Machine Vision Conference 2018, BMVC 2018, Newcastle,UK, 3–6 September 2018, p. 125 (2018)\n14. Ni, G., Liu, L., Du, X., Zhang, J., Liu, J., Liu, Y.: Accurate AOI inspection of\nresistance in LCD anisotropic conductive ﬁlm bonding using diﬀerential interfer-\nence contrast. Optik 130, 786–796 (2017)\n15. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomed-\nical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.\n(eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015).\nhttps://doi.org/10.1007/978-3-319-24574-4\n28\n16. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: MobileNetV 2:\ninverted residuals and linear bottlenecks. In: 2018 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) (2018)\n17. Tian, Y., Lei, Y., Zhang, J., Wang, J.Z.: PaDNet: pan-density crowd counting.\nIEEE Trans. Image Process. 29, 2714–2727 (2020)\n18. Wang, Y., et al.: LedNet: a lightweight encoder-decoder network for real-time\nsemantic segmentation. In: 2019 IEEE International Conference on Image Pro-\ncessing, ICIP 2019, Taipei, Taiwan, 22–25 September 2019, pp. 1860–1864 (2019)\n19. Wang, Y., Ma, L., Jiang, H.: Detecting conductive particles in TFT-LCD with U-\nmultinet. In: 2019 8th International Symposium on Next Generation Electronics\n(ISNE), pp. 1–3. IEEE (2019)\n20. Wang, Y., Ma, L., Jiu, M., Jiang, H.: Detection of conductive particles in TFT-\nLCD circuit using generative adversarial networks. IEEE Access PP(99), 1 (2020)\n21. Yu-ye, C., Ke, X., Zhen-xiong, G., Jun-jie, H., Chang, L., Song-yan, C.: Detection\nof conducting particles bonding in the circuit of liquid crystal display. Chinese J.Liq. Cryst. Disp. 32(7), 553–559 (2017)\n22. Zhang, X., Zhou, X., Lin, M., Sun, J.: ShuﬄeNet: an extremely eﬃcient convolu-\ntional neural network for mobile devices. In: 2018 IEEE Conference on ComputerVision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, 18–22\nJune 2018, pp. 6848–6856 (2018)', 'Flexible Projection Search Using Optimal\nRe-weighted Adjacency for Unsupervised\nManifold Learning\nY uting Tao1(B), Haifeng Zhao1,2, and Yan Zhang1\n1School of Software Engineering, Jinling Institute of Technology, Nanjing 211169, China\ntao_yuting@jit.edu.cn\n2Jiangsu HopeRun Software Co. Ltd., Nanjing 210012, China\nAbstract. Graph-based manifold learning plays an important role in clustering\nand classiﬁcation tasks. Regarding the unsupervised case, the local structure of\neach sample is vital to the quality of clustering results. Many state-of-the-art meth-ods seek the low-dimensional projection matrix for graph embedding, ignoring\nthe contortion of original local structure in the projected space, which impairs the\nquality of clustering. To remedy this shortcoming, we propose an iterative leaningapproach in this paper, aiming at preserving the original locality in each itera-\ntion. During iterative steps, adjacency weights of each currently projected sample\nare optimally deﬁned by quadratic programming with equality constraints, andin return the upcoming projection matrix that attempts to keep the original local-\nity is ﬂexibly determined based on the current weights. Such iteration proceeds\nuntil the objective function value converges. In particular, the proposed approachrequires very few parameters, leading to simple operation in experiments. Further,\nlocal distances of projected samples could be directly obtained from their inner\nproducts, with no explicit calculation of projection matrix in each iteration. Werepeat k-means clustering many times w.r.t these samples after convergence, and\nexperimental results reveal the obvious better clustering quality of the proposed\napproach on average, compared to the state-of the-art ones.\nKeywords: Unsupervised manifold leaning ·Quadratic programming ·Flexible\nprojection ·Re-weighted adjacency ·k-means clustering\n1 Introduction\nIn machine learning ﬁeld, unsupervised learning is the case with no prior label infor-\nmation to guild how to distinguish or build up relationships among samples.The typical\nway for this case is clustering [ 1], aiming at separating samples into groups based on\ntheir similarities, i.e. intra-group similar and inter-group dissimilar. Samples’ relationsinterpreted by graph are constructed under the assumption that data lies in the manifold\nwhere local ambience of each sample is as ﬂat as Euclidean (linear) space, but global\nstructure is nonlinear. Original samples e.g. images, however, are generally of too highdimensions to tackle with, leading to the curse of dimensionality. In order to facilitate the\n© Springer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 227–239, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_19', '228 Y . Tao et al.\ntasks of computer vision and pattern recognition, dimensionality reduction technique is\nrequired, to pursue compact representation of samples and their relations’ preservation,i.e. graph embedding [ 2].\nIn terms of graph-based manifold learning, there are several state-of-the-art unsu-\npervised methods for dimensionality reduction and graph embedding, such as principlecomponent analysis (PCA) [ 3,4], laplacian eigenmap (LE) [ 5], locality preserving pro-\njection (LPP) [ 6], local linear embedding (LLE) [ 7], neighbor preserving embedding\n(NPE) [ 8] and etc. Yan et al. [ 2] pointed out that PCA belong to the special case of mani-\nfold learning, since it assumes the global data structure lie in the Euclidean space, while\nothers take structure as linear locally but nonlinear globally. LE seeks knearest neigh-\nbors of each sample and attempts to ﬁnd a low-dimensional space for the preservation ofthe local structure. LPP takes linear space to best depict the reduced space of LE. Rather\nthan LE which deﬁnes adjacency weights by Gaussian kernel as exp {−/bardbl x\ni−xj/bardbl2/2σ2},\nLLE determines the weights by assuming that each sample is approximated by the linear\ncombination of its knearest neighbors, and maps such locality into low dimensional\nspace. NPE is the linear approximation to the space derived by LLE, which thereforecan be generalized to the out-of-sample problems. Apart from these, spectral clustering\n(SC) [ 9] conducts the dimensional reduction using the technique of LE over the graph\nbuilt by local adjacency, and operates k-means clustering on it thereafter in the reduced\nspace.\nDifferent from the methods introduced above, some scholars in this area designed\nlinear regression model that approximate the nonlinear low-dimensional projected sam-ples. For example, Dornaika et al. [ 10,11] took such model, proposing joint learning\nframework for semi-supervised manifold learning. Also using this model, Nie et al. [ 12]\nproposed projective unsupervised ﬂexible embedding (PUFE). Wang et al. [ 13] pointed\nout that original data was likely contaminated by noise, and proposed projective unsuper-\nvised ﬂexible embedding models with optimal graph (PUFE-OG), by integrating optimal\nadjustment of adjacency weights and the update of linear low-dimensional projection.Compared to PUFE with no weight adjustment, PUFE-OG achieved better clustering\nresults.\nIn recent years, deep learning has achieved a great success in the tasks of recogni-\ntion. Especially, convolutional neural network (CNN) performs excellently for image\nclassiﬁcation, which originated from the LeNet-5 model proposed by LeCun et al. [ 14]\nin 1998. Inspired by this, Chen et al. [ 15] combined manifold learning with CNN, where\nmanifold that interprets data structure was embedded through convolutional layers for\ntraining process, enhancing CNN’s discriminative power for action recognition. Despiteof superiority of deep learning, it does not work well for unsupervised case.\nFrom the viewpoint of metric learning, Liu et al. stated in [ 16] that manifold learning\nare confronted with a problem in common, i.e. it is no longer the Euclidean distance w.r .t.\nlocal samples in the projected space, instead is the so-called Mahalanobis. It indicates\nthat some sample’s nearest neighbor in the original space will be possibly second or\nthird nearest in the projected space, since the projection matrix could scale the originaldistances (stretch or shrink), leading to the contortion of original local structure. Hence,\nLiu et al. [ 16] provided a joint learning of adjacency weights and distance metric (i.e.\nprojection matrix) for label propagation in semi-supervised learning.', 'Flexible Projection Search Using Optimal Re-weighted Adjacency 229\nIn this paper, we take account of the potential local structure contortion w.r .t. projected\ndata, and propose an innovative unsupervised manifold learning approach, i.e. ﬂexibleprojection search using optimal re-weighted adjacency (FPSORA). It adopts the joint\nlearning framework by integrating the updated projection matrix and the re-weighted\nlocal adjacency into iterative steps. To be exact, in each iteration, adjacency weightsof each sample in the current projected space are optimally updated by quadratic pro-\ngramming with equality constraints [ 17], and in return the upcoming projection matrix\nthat attempts to preserve the original locality is ﬂexibly determined based on the currentadjacency weights.\nThe contribution of FPSORA are as follows:\n(1) Instead of taking many parameters in some traditional manifold learning methods,\nthe objective function of FPSORA does not need many parameters, leading to much\nsimpler operation in experiment.\n(2) Orthogonality w.r .t. the vectors of Pis not imposed, i.e. such constraint as P\nTP=I\nis not required, where Pis the projection matrix.\n(3) The updated Pin each iteration requires no explicit calculation, since the local\ndistances among newly projected samples can be directly obtained from their inner\nproducts.\n(4) Pcalculated by many state-of-the-art methods are model-driven (i.e. eigen-\ndecomposition), but FPSORA seeks Piteratively using result-driven strategy,\nassuming that the upcoming Pis supposed to preserve the original locality without\nchanging the current objective function value.\nAfter convergence, we repeat k-means clustering many times w.r.t these samples\nin the eventually projected space, and take clustering accuracy (Acc) and normalized\nmutual information (NMI) [ 18] for evaluation, which reveal the obvious better clustering\nquality of FPSORA on average, compared to the state-of the-art unsupervised ones, likePCA, LPP , NPE, SC, PUFE and PUFE-OG.\nThe rest of the paper is arranged as follows: Sect. 2introduces the related knowledge\nof graph construction and metric learning. Section 3presents the details of the proposed\nFPSORA. Section 4conducts experiments and gives clustering results. Section 5draws\nthe ﬁnal conclusion.\n2 Graph Construction and Metric Learning\n2.1 Graph-Based Afﬁnity Matrix\nSuppose there are totally nsamples represented by X=[x1,…, xn], where X∈Rp×n,\nand pis the original samples’ dimension. Let us recall the construction of afﬁnity matrix\nW∈Rn×n, based on the local adjacency of each sample in the set X.\nwij=/braceleftBigg\nexp/parenleftBig\n−/bardblxi−xj/bardbl2\n2σ2/parenrightBig\nifxj∈Nk(xi)\n0 otherwise(1)', '230 Y . Tao et al.\nwhere Nk(xi)is the realm of knearest neighbor of xi, and wijis the i-th row and j-\nth column of W.Besides, it indicates from Eq. ( 1) that the smaller the local distance\n/bardblxi−xj/bardbl2is, the larger the weight wijwill be.\nLet D∈Rn×nbe the diagonal matrix, where Diiis the summation of the i-th row\nofW, i.e. Dii=/summationtext\njwij, standing for the summation of local adjacency weights of xi.\nTherefore, the following formula holds:\ntrace/parenleftBig\nXL XT/parenrightBig\n=1\n2/summationdisplay\ni/summationdisplay\njwij/bardblxi−xj/bardbl2(2)\nwhere L=D−Wis the so-called graph Laplacian.\n2.2 Graph Embedding\nIf we project Xinto the low-dimensional space P,then Y=PTXis the projected\nsample set where Y=[ y1,..., yn]∈ Rd×n. The state-of-the-art manifold learning\nmethods attempt to seek such space P, i.e.\nmin\nPtrace/parenleftBig\nPTXLXTP/parenrightBig\n=min1\n2/summationdisplay\ni/summationdisplay\njwij/vextenddouble/vextenddoubleyi−yj/vextenddouble/vextenddouble2(3)\nIn Eq. ( 3), the graph Laplacian Lrepresenting the local adjacency is supposed to be\npreserved as much as possible, and YLYTis called graph embedding [ 2].\n2.3 Distance Metric Learning\nLet M=XTPPTX, then it is easy to get that:\n/bardblyi−yj/bardbl2=Mii+Mjj−Mij−Mji (4)\nFrom the viewpoint of metric learning, /bardblyi−yj/bardbl2in Eq. ( 4) is no longer the square\nof Euclidian distance between xiand xj. Instead, it is the Mahalanobis distance[ 16],\nsince/bardblyi−yj/bardbl2=/bardbl xi−xj/bardbl2\nP. In fact, PPTinMis not the identity matrix I, indicating\nthat Pscale the original Euclidean distances between xiand xj(stretch or shrink).\nTherefore, the original distances among xiand its knearest neighbors, are possibly\nscaled to different degrees after projection. Such phenomenon leads to the contortion\nof original local structure, e.g. the nearest neighbor of xibecomes the second or third\nnearest after projection.\n3 Flexible Projection Search Using Optimal Re-weighted\nAdjacency (FPSORA)\nAs discussed in Sect. 2.2,2.3, the target of graph embedding is to seek the projection\nmatrix P, in which the local structure of original ambiance may be contorted. In this\nsection, we propose a new approach, i.e. FPSORA, trying to remedy this shortcoming.To be exact, we build up an iterative framework by integrating adjacency weights’\nupdate using quadratic programming and ﬂexible projection search for original locality\npreservation.', 'Flexible Projection Search Using Optimal Re-weighted Adjacency 231\n3.1 Quadratic Programming for Updating Adjacency Weights\nProjected in P, the adjacency weight of xiand xjis still wij,a ss h o w ni nE q .( 3), which\nis deﬁned based on the original Euclidean distance in Eq. ( 1). In the proposed approach,\nwijcan be optimally re-weighted in the given projected space P, to further minimize\nEq. ( 3). To begin with, we consider that Eq. ( 3) is equivalent to:\nmin1\n2\nW/summationdisplay\ni/summationdisplay\njwij/vextenddouble/vextenddoubleyi−yj/vextenddouble/vextenddouble2=/summationdisplay n\niminwij1\n2/summationdisplay\nxj∈Nk(xi)wij/vextenddouble/vextenddoubleyi−yj/vextenddouble/vextenddouble2(5)\nAs in Eq. ( 5), adjacency weights for knearest neighbors of xi(i=1,…, n) can be\nreformulated to the following form:\nminμij1\n2/summationdisplay\ni/summationdisplay\nxj∈Nk(xi)μ2\nijdij,s.t./summationdisplay\njμij=1( 6 )\nwhere dij=/bardbl yi−yj/bardbl2.\nLemma 1. The imposed linear constraint/summationtext\njμij=1i nE q .( 6) does not impair the\nlocal structure composed of the knearest neighbors of xi.\nProof. If/summationtext\nj√wij=C, then μij=√wij\nC,s oμ2\nij=wij\nC2and such constraint scales the\noriginal adjacency weights of xito the same extent, i.e. divide by C2./square\nTo be further, Eq. ( 6) can be optimized by quadratic programming with equality\nconstraint [ 17], i.e.\nfi=minμi1\n2trace(μT\ni/Lambda1iμi)+λ(μT\nie−1) (7)\nwhere /Lambda1i=⎛\n⎜⎝di1··· 0\n.........\n0··· dik⎞\n⎟⎠∈Rk×kis the diagonal matrix where the diagonal elements\ndenote the klocal distances of xiprojected in the given space P.μi=[μi1,...,μ ik]T\nand e∈Rkis the column vector with all the elements 1.\nSince fiin Eq. ( 7) is the convex function w.r .t.μi, its optimal solution could be got\nby letting the ﬁrst derivative be 0, i.e.\n∂fi\n∂μi=/Lambda1iμi+λe=0. (8)\nCombining Eq. ( 8) and/summationtext\njμij=1, we deﬁne.\n(9)\nIn Eq. ( 9),A∈R(k+1)×(k+1)and b∈Rk+1are known beforehand, where bis the\nvector with all 0 but the last element 1. x∈Rk+1is the vector consisting of k+1\nvariables to be determined, in which the optimal μialong with the parameter λwill be\nobtained by solving the linear equation Ax=b.', '232 Y . Tao et al.\n3.2 Flexible Projection Search for Preservation of Original Locality\nLet LAand Lodenote the graph Laplacian constructed by original weightswij\nC2(to ensure\nthat/summationtext\njμij=1 always hold) and the optimally updated ones, respectively. We know\nfrom Sect. 3.1that trace/parenleftbig\nPT\nt−1XL o_tXTPt−1/parenrightbig\n≤trace/parenleftbig\nPT\nt−1XL AXTPt−1/parenrightbig\nat the t-th itera-\ntion. Our target now is to seek the unknown space Pt, in which we attempt to preserve LA,\nmeanwhile to keep the value of trace/parenleftbig\nPT\nt−1XL o_tXTPt−1/parenrightbig\nby constructing the following\nequation:\nL1/2\nAXTPtPT\ntXL1/2\nA=L1/2\no_tXTPt−1PT\nt−1XL1/2\no_t (10)\nIn Eq. ( 10), taking singular value decomposition (SVD), i.e. LA=UA/Sigma12\nAUTAand Lo_t=\nU/Sigma12UT,we get L1/2\nA=UA/Sigma1AUT\nA,and likewise L1/2\no_t=U/Sigma1UT. Furthermore, Eq. ( 10)\ncan be deducted into the new metric as below:\nMt=XTPtPT\ntX=(L1/2\nA)+L1/2\no_tXTPt−1PT\nt−1XL1/2\no_t(L1/2\nA)+(11)\nwhere (L1/2\nA)+is the pseudo-inverse of L1/2\nA, because LAis of rank-deﬁciency.\nThe local distance between xiand xjprojected in Ptcan be calculated directly from\nMtin Eq. ( 11), ready for the weights’ update in the upcoming t+1-th iteration to\nconstruct Lot+1. Please note that the exact Ptrequires no computation, because Mtis\nknown from the rightmost side of Eq. ( 11).\n3.3 Complete Workﬂow of the Proposed FPSORA\nHere the whole procedure of the proposed FPSORA is detailed in Table 1.\nSummary. The proposed FPSORA ’s objective function i.e. trace/parenleftbig\nPTXLXTP/parenrightbig\nin Eq. ( 3),\ndeceases monotonously as the iteration proceeds. In fact, trace/parenleftbig\nPTXLXTP/parenrightbig\nshrinks after\nthe weights’ update scheme, and it is unchanged after Xare projected into the space to\nbe searched i.e. Ptin the current iteration t, i.e.\ntrace(PT\ntXL AXTPt)=trace(PT\nt−1XL o_tXTPt−1)≤trace(PT\nt−1XL AXTPt−1) (12)\nAfter convergence, the eventual projection matrix shown in Table 1is:\nP=(XT)+(L1/2\nA)+L1/2\no_ts...( L1/2\nA)+L1/2\no_1XTP0 (13)\nwhere tsis the total number of iterations before convergence. As a matter of fact, we\ndirectly take the projected data PTX,i.e. PT\n0XL1/2\no1(L1/2\nA)+...L1/2\no_ts(L1/2\nA)+for clustering\nquality evaluation after convergence. It has to explicitly calculate Ponly in the case of\nout-of-sample problem.', 'Flexible Projection Search Using Optimal Re-weighted Adjacency 233\nTable 1. Complete workﬂow of the proposed FPSORA\n4 Experiments and Analysis\n4.1 Evaluation Metrics\nFor the unsupervised feature extraction, we cluster the projected samples using k-means\nclustering by 100 times for each method. After that, we evaluate the average cluster-\ning quality using two metrics, i.e. Clustering Accuracy (Acc) and normalized mutual\ninformation (NMI) [ 18].\nClustering Accuracy (Acc). It measures the degree to which the ground truth classes\nare consistent to the clustering results. Let /Omega1={ω1,...,ω c}be the ground truth classes,\nand G={ g1,..., gc}be the clusters. Suppose siis the cluster label of the i-th sample\nand tiis its ground truth class label, then the accuracy is\nAcc(/Omega1, G)=1\nn/summationdisplay n\ni=1δ(si,ti) (14)\nwhere δ(si,ti)=1i f si=ti, andδ(si,ti)=0i f si/negationslash=ti.\nNormalized Mutual Information (NMI). It measures the similarity between ground\ntruth classes and the clustering results, from the viewpoint of joint probability. Let ni\nand n/logicalandtext\njdenote the number of samples contained in class ωi(1≤i≤c) and the number', '234 Y . Tao et al.\nof samples belonging to cluster gj(1≤j≤c), respectively. Given a clustering result,\nNMI is deﬁned by\nNMI=/summationtextc\ni=1/summationtextc\nj=1ni,jlog nni,j\nniˆnj/radicalbigg/parenleftbig/summationtextc\ni=1nilogni\nn/parenrightbig/parenleftBig/summationtextc\ni=1ˆnjlogˆnj\nn/parenrightBig(15)\nwhere ni,jrepresents the number of samples in the intersection between cluster gjand\nclassωi.\n4.2 Experimental Setup\nHere we introduce three datasets taken in the experiment, i.e. JAFFE Dataset,Yale Face\nDataset and ETH80 Object Category Dataset, as well as the parameter settings. for these\ndatasets.\nJAFFE Dataset. http://www.kasrl.org/jaffe.html It contains 10 Japanese female models,\nwith 7 posed facial expressions (6 basic facial expressions +1 neutral) for each model,\nas shown in Fig. 1. There are several images of each expression, therefore the whole\ndata set is composed of 213 images in total [ 19]. Each image’s resolution is of 256 ×\n256 pixels, and we resize to 26 ×26 in the experiment.\nFig. 1. The 7 posed facial expressions for one female model, i.e. neural, joy, surprise, anger, fear,\ndisgust and sadness from left to right.\nYale Face Dataset. It is constructed by the Yale Center for Computational Vision and\nControl http://cvc.yale.edu/projects/yalefaces/yalefaces.html . It contains 165 Gy scale\nimages of 15 individuals. The image demonstrates variations in lighting condition, facialexpression (normal, happy, sad, sleepy, surprised and wink). Each person has 11 images\nwith the size 32 ×32, as shown in Fig. 2.\nFig. 2. Image samples of one person from the Yale Face Dataset\nETH80 Object Category Dataset. It includes 8 categories (classes) of biological and\nartiﬁcial objects. For each category, there are 10 objects, with each represented by 41\nimages from viewpoints spaced equally over the upper viewing hemisphere (at distances\nof 22.5–26◦), as shown in Fig. 3. Therefore, there are totally 41 ×10=410 for each', 'Flexible Projection Search Using Optimal Re-weighted Adjacency 235\nFig. 3. The example of 8 categories of objects in the ETH80 Dataset\ncategory. More details can be found in [ 20,21]. All images of the size 128 ×128 are\nreduced to 64 ×64 in this experiment. For the computational efﬁciency, we only select\nthe ﬁrst 50 samples from each category, so there are 200 in total.\nParameter Settings\nReduced Dimension . The selected dimension dvaries within the range [10,20,…,210] for\nJAFFE dataset, [10,20,…,160] for Yale dataset and [10,20,…,200] for ETH80 dataset,all of which are at the interval of 10.\nManifold Parameters .A ss h o w ni nE q .( 1), the three datasets, i.e. JAFFE, Yale and\nETH80 take the basic parameters k=10σ=1,k=2σ=10, and k=5σ=10\nrespectively.\nBesides, the objective function of PUFE is:\nmin\nw,b,Ftr/parenleftBig\nFTLAF/parenrightBig\n+μ/bardblXTW+1bT−F/bardbl2\nF+γ/bardblW/bardbl2\nFs.t.FTF=I (16)\nAnd PUFE-OG takes the following objective form:\nmin\ns,w,b,F/bardblS−A/bardbl2\nF+λtr/parenleftBig\nFTLsF/parenrightBig\n+μ/bardblXTW+1bT−F/bardbl2\nF+γ/bardblW/bardbl2\nF (17)\nwhere FTF=Iand SI=I,S≥0.Lsis the graph Laplacian built by the weight matrix\nS, in which the summation of each row equals to 1.\nIn terms of all the three datasets, the parameters λ=1,μ=1 and γ=1a r et a k e n\nfor PUFE and PUFE-OG in the experiments.\n4.3 Experiments and Analysis\nAfter k-means clustering for 100 times, we compare Acc and NMI among PCA, LPP ,\nNPE, PUFE, PUFE-OG and FPSORA as the dimension varies, shown in Fig. 4.\nThe curves in Fig. 4tell that PCA is robust to the dimensional variation, and NPE,\nPUFE and PUFE-OG perform better as dimension reduces. In particular, PUFE-OGoutperforms PUFE at very small dimension for all the three dataset, but falls behind\nwhen dimension goes up. LPP achieves slightly better clustering quality with dimension\ngoing up for JAFFE and Yale, but it takes on the opposite trend for ETH80. FPSORA', '236 Y . Tao et al.\nyields the best results for all the three datasets, which surpasses PCA and LPP for\nJAFFE and Yale, respectively, as dimension increases. As of ETH80, FPSORA alwaysoverwhelms the others.\nThe best result (mean% ±std) of each method is listed in Table 2, with what in the\nparentheses being the dimension at which each best result is achieved. The bold fontdenotes the best clustering quality among these methods weighed by the same evaluation\nmetric for the same dataset. Please note that the dimension of spectral clustering (SC)\nis always ﬁxed, since it directly clusters the nrows of eigenvector set P=[ p\n1,..., pc]\ninto cgroups, where P∈Rn×c(cis the class number).\nRegarding FPSORA, the monotonous decrease of objective function values along\nwith the iterative steps are demonstrated in Fig. 5, consisting with the theoretical\nfoundation stated in Sect. 3.\nFig. 4. Clustering quality comparison along with the varying dimension among PCA, LPP , NPE,\nPUFE, PUFE_OG and FPSORA. Acc (left) and NMI (right) with JAFFE on the 1strow, Yale on\nthe 2ndrow and ETH80 on the 3rdrow.', 'Flexible Projection Search Using Optimal Re-weighted Adjacency 237\nTable 2. Comparison of best performance (mean clustering result % ±standard deviation) with\nthe corresponding dimension in the parentheses ( Note: the dimension of spectral clustering (SC)\nis always ﬁxed)\nMethod JAFFE dataset Yale dataset ETH80 dataset\nAcc NMI Acc NMI Acc NMI\nSC[9]76.41 ±3.15 84.87 ±1.55 44.65 ±1.70 51.42 ±1.48 59.99 ±0.80 62.65 ±0.31\nPCA[3,4]85.34 ±4.35\n(200)88.43 ±2.09\n(20)44.83 ±2.67\n(20)51.50 ±2.23\n(20)57.04 ±2.52\n(200)58.21 ±1.85\n(200)\nLPP[6]79.82 ±2.69\n(200)86.49 ±1.53\n(200)49.39 ±2.96\n(160)54.68 ±2.40\n(160)38.42 ±0.27\n(10)35.91 ±0.16\n(10)\nNPE[8]86.47 ±4.24\n(20)88.28 ±3.23\n(20)45.25 ±2.34\n(20)49.13 ±2.01\n(20)59.77 ±3.61\n(10)59.32 ±2.98\n(10)\nPUFE[12]85.15 ±1.86\n(10)90.54 ±1.33\n(10)46.52 ±1.32\n(10)51.29 ±1.26\n(10)58.35 ±2.23\n(10)58.98 ±2.93 (10)\nPUFE-OG[13]84.98 ±0.01\n(10)90.04 ±0.02\n(10)47.25 ±1.26\n(10)53.22 ±0.96\n(10)51.62 ±2.59\n(10)51.89 ±3.14\n(10)\nFPSORA 96.55 ±1.13\n(210)96.50 ±2.16\n(200)50.86 ±3.33\n(160)55.24 ±3.07\n(160)62.71 ±0.44\n(20)63.55 ±0.56\n(20)\nFig. 5. Objective function value of FPSORA monotonously decreases as iteration proceeds, i.e.\nJAFFE, Yale, ETH80 from left to right.\n5 Conclusion\nIn this paper, we propose a new unsupervised manifold learning approach named\nFPSORA, taking account of the potential contortion of original local structure after\nprojection in each iteration. In order to decrease the objective function value in the given\nprojected space, FPSORA updates the adjacency weights by quadratic programming withequality constraints for each sample. In return, FPSORA seeks new projected space that\nis supposed to preserve the original local structure without changing the current func-\ntion value. Such iterative steps proceed until it achieves convergence. In particular, thespace to be updated needs no exact calculation, since local distances among the upcom-\ning projected samples can be directly obtained from their inner products. Experimental\nresults show that FPSORA achieves better clustering quality than that of other traditional', '238 Y . Tao et al.\nmethods, since it attempts to shorten local distances in the reduced space while keep\noriginal local structure as much as possible. The most time-consuming part of FPSORAis weights’ update, as it is operated sample by sample. In the future work, we plan\nto adopt parallel computing for computational efﬁciency, since such update could be\nindependently operated among samples.\nAcknowledgements. This work was supported by the International Science and Technology\nCooperation Project of Jiangsu Province under grant BZ2020069, Research Foundation for\nAdvanced Talents and Incubation Foundation of Jingling Institute of Technology under grant\nJIT-B-201717, JIT-B-201617, JIT-FHXM-201808, and JIT-FHXM-201911.\nReferences\n1. Chong, Y ., Ding, Y ., Yan, Q., et al.: Graph-based semi-supervised learning: a review.\nNeurocomputing 408(30), 216–230 (2020)\n2. Yan, S., Xu, D., Zhang, B., et al.: Graph embedding and extensions: a general framework for\ndimensionality reduction. IEEE Trans. Pattern Anal. Mach. Intell. 29(1), 40–51 (2007)\n3. Jolliffe, I.T.: Principal Component Analysis. Springer-V erlag (2005)\n4. Turk, M.A., Pentland, A.P .: Face recognition using eigenfaces. In: IEEE Computer Society\nConference on Computer Vision and Pattern Recognition, pp. 586–591 (1991)\n5. Belkin, M., Niyogi, P .: Laplacian eigenmaps and spectral techniques for embedding and\nclustering. Adv. Neural Inf. Process. Syst. 14, 585–591 (2001)\n6. He, X., Yan, S., Hu, Y ., et al.: Face recognition using laplacian faces. IEEE Trans. Pattern\nAnal. Mach. Intell. 27(3), 328–340 (2005)\n7. Roweis, S., Saul, L.: Nonlinear dimensionality reduction by locally linear embedding. Science\n290(5500), 2323–2326 (2000)\n8. He, X., Deng, C., Yan, S., et al.: Neighborhood preserving embedding. In: Tenth IEEE\nInternational Conference on Computer Vision (2005)\n9. Ng, A.Y ., Jordan, M.I., Weiss, Y .: On spectral clustering: analysis and an algorithm. In:\nAdvances in Neural Information Processing System, p. 849–856 (2001)\n10. Dornaika, F., Traboulsi, Y .E.: Learning ﬂexible graph-based semi-supervised embedding.\nIEEE Trans. Cybernet. 46(1), 206–218 (2016)\n11. Dornaika, F., Traboulsi, Y .E., Assoum, A.: Inductive and ﬂexible feature extraction for semi-\nsupervised pattern categorization. Pattern Recogn. 60, 275–285 (2016)\n12. Nie, F., Xu, D., Tsang, W.H., et al.: Flexible manifold embedding: a framework for semi-\nsupervised and unsupervised dimension reduction. IEEE Trans. Image Process. 19(7), 1921–\n1932 (2010)\n13. Wang, W., Yan, Y ., Nie, F., et al.: Flexible manifold learning with optimal graph for image\nand video representation. IEEE Trans. Image Process. 27(6), 2664–2675 (2018)\n14. Lecun, Y ., Bottou, L., Haffner, P .: Gradient-based learning applied to document recognition.\nProc. IEEE 86(11), 2278–2324 (1998)\n15. Xin, C., Jian, W., Wei, L., et al.: Deep manifold learning combined with convolutional neural\nnetworks for action recognition. IEEE Trans. Neural Netw. Learn. Syst. 29(9), 3938–3952\n(2018)\n16. Liu, B., Meng, W., Hong, R., et al.: Joint learning of labels and distance metric. IEEE Trans.\nSyst. Man Cybernet. Part B Cybernet. 40(3), 973–978 (2010)\n17. Boyd, S., V andenberghe, L.: Convex Optimization. Cambridge University Press (2004)', 'Flexible Projection Search Using Optimal Re-weighted Adjacency 239\n18. Strehl, A., Ghosh, J.: Cluster ensembles – a knowledge reuse framework for combining\nmultiple partitions. J. Mach. Learn. Res. 3(3), 583–617 (2002)\n19. Lyons, M.J., Kamachi, M., Gyoba, J.: The Japanese Female Facial Expression (JAFFE)\nDatabase. In: IEEE International Conference on Face and Gesture Recognition, pp.14–16,\n(1998)\n20. Leibe, B., Schiele, B.: Analyzing appearance and contour based methods for object cat-\negorization. In: International Conference on Computer Vision and Pattern Recognition\n(2003)\n21. Leibe, B.: The ETH-80 Image Set. http://www.mis.informatik.tu-darmstadt.de/Research/Pro\njects/categorization/eth80-db.html', 'Fabric Defect Detection via Multi-scale\nFeature Fusion-Based Saliency\nZhoufeng Liu(B), Ning Huang, Chunlei Li, Zijing Guo, and Chengli Gao\nZhongyuan University of Technology, Zhengzhou, Henan, China\nlzf@zut.edu.cn\nAbstract. Automatic fabric defect detection plays a key role in control-\nling product quality. Salient object detection (SOD) based on convolu-\ntional neural network has been proven applicable in fabric defect detec-tion, but how to learn powerful features and feature fusion for saliency\nare still challenging tasks due to the complex texture of fabric image. In\nthis paper, a novel visual saliency based on multi-scale feature fusion isproposed for fabric defect detection. First, a Multi-scale Feature Learn-\ning Module (MFLM) by simulating the parallel processing mechanism of\nmulti-receptive is proposed to eﬃciently characterize the complex fab-\nric texture. In addition, Feedback Attention Reﬁnement Fusion Module\n(FARFM) is designed to selectively aggregate multi-level features forenhancing the feature fusion. In the end, fabric defect detection is local-\nized by segmenting the generated saliency map. Experimental results\ndemonstrate that the proposed method can localize the defect regionwith high accuracy, and outperform the 5 state-of-the-art methods.\nKeywords: Fabric defect detection\n·Salient object detection ·\nMulti-scale feature learning ·Feedback attention reﬁnement fusion\n1 Introduction\nMany fabric defects maybe occur during the production process of fabrics, such\nas broken weft, double weft, thin and dense paths, broken warps and jumping\nﬂowers, etc. These defects have negative impacts on the appearance and com-\nfort of the fabric. Therefore, fabric defect detection plays an important role in\nthe quality control of textile production. It is traditionally conducted throughvisual inspections by the skilled workers. The manual detection method can\nonly achieve a detection rate of 40%–60%, and is easily aﬀected by subjective\nfactors [ 1,2]. Consequently, to meet the high-quality requirements of fabric prod-\nucts, it is necessary to develop an automatic fabric defect detection method based\non machine vision and image processing technology.\nThe traditional computer vision-based fabric defect detection methods can\nbe categorized into structural methods [ 3], statistical methods [ 4], spectrum\nmethods [ 5], model-based methods [ 6]. Structure-based methods regards texture\nas a combination of texture primitives, the defect region can be localized by\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 240–251, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_20', 'Fabric Defect Detection via Multi-scale Feature Fusion-Based Saliency 241\nidentifying the irregular primitives. But this method can be only applied for the\nfabric with structural texture. Statistical methods usually adopt the statistical fea-tures, such as ﬁrst-order and second-order statistics to detect the fabric, but they\nfail to detect the tiny defects or the defects with low contrast. Spectral methods use\nspectral features to conduct the fabric defect detection in the spectrum domain.However, the computational complexity is high and the detection results depends\non the selected ﬁlters. The model-based methods model the fabric texture as a ran-\ndom process, and the defect detection is treated as a hypothesis testing problembased on the model. Nevertheless, these methods often have high computational\ncomplexity, and poor performance for the defects with small size.\nThe biological visual system can quickly search and locate signiﬁcant\nobjects [ 7], and thus the visual saliency has been widely applied into object\ndetection [ 8], image segmentation [ 9] and visual tracking [ 10], etc. Although\nthe fabric texture and defects are complex and diversity, the defective regions\nare outstanding in the complex background. Therefore, several saliency-based\nmethods for fabric defect detection have been proposed [ 11–13]. However, these\nmethods adopted the traditional pattern recognition technology to model the\nvisual saliency, the detection performance mainly depend on the extracted hand-\ncrafted features, and it lacks of self-adaptivity. Therefore, it is necessary to fur-ther improve the existing methods and build a robust detection model.\nFully convolutional network (FCNs) based on deep learning has successfully\nbroken the limitations of traditional hand-crafted features [ 14]. With the powerful\nfeature representation ability that can simultaneously characterize the semantic\ninformation and detail texture feature, FCNs has been proved to be eﬀective in\nsalient object detection. Based on these characteristics, some novel architectureshave been designed to further improve the performance of salient object detec-\ntion. Wang et al. [ 15] developed a weakly supervised learning method for saliency\ndetection using image-level tags only. Li et al. [ 16] proposed an end-to-end deep\ncontrast network for generating better boundary. Pang et al. [ 17] considered both\nencoder and decoder for multi-level contextual information integration. Although\nthese methods have made great progress at present, two issues in fabric defect\ndetection still need to be paid attention to how to extract more robust features\nfrom the data of scale variation and how to integrate them eﬀectively.\nTo address the above challenges, a novel visual saliency based on multi-scale\nfeature fusion is proposed for fabric defect detection. The proposed network con-\nsists of three components: Backbone used to extract features, Multi-scale fea-ture learning module (MFLM) and Feedback attention reﬁnement fusion mod-\nule (FARFM). First, four side outputs are generated by backbone network. And\nthe MFLM module which is proposed to extract multi-scale features by simulatingthe parallel processing mechanism of multi-receptive, is designed to solve the scale\nvariation problem. Furthermore, FARFM module is proposed to selectively aggre-\ngate multi-level features for enhancing the feature fusion at each stage. The mod-ule is based on the observations that: 1) deeper layers encode high-level knowledge\ncontain rich contextual information, which can help to locate objects; 2) shallower\nlayers capture more spatial details. Therefore, the proposed method can improvethe detection performance for the fabric defect.', '242 Z. Liu et al.\nTo demonstrate the performance of the proposed method, we report exper-\niment results and visualize some saliency maps. We conduct a series of abla-tion studies to evaluate the eﬀect of each module. Quantitative indicators and\nvisual results show that the proposed method can obtain signiﬁcantly better\nlocal details and improve saliency maps. Ours main contributions as follows:\n1. We propose the multi-scale feature learning module to eﬃciently characterize\nthe complex fabric texture by simulating the parallel processing mechanismof multi-receptive, which can solve the scale variation problem.\n2. We design a feedback attention reﬁnement fusion module to selectively aggre-\ngate multi-level features for enhancing the feature fusion, which can feed-\nback features of both high resolutions and high semantics to previous ones toimprove them for generating better saliency maps.\n3. We compare the proposed methods with 5 state-of-the-art methods on our\ndatasets in terms of seven metrics. The results demonstrate that the eﬀec-\ntiveness and eﬃciency of the proposed method.\n2 Proposed Method\nFig. 1. The overall framework of the proposed model.\nIn this section, a novel fabric defect detection method via multi-scale feature\nfusion-based saliency is proposed. ResNet-50 is used as the backbone encoder.\nMFLM is proposed to eﬃciently characterize the complex fabric texture by sim-\nulating the parallel processing mechanism. FARFM is designed to selectively', 'Fabric Defect Detection via Multi-scale Feature Fusion-Based Saliency 243\naggregate multi-level features for enhancing the feature fusion. Finally, ﬁve pre-\ndiction maps are obtained. The ﬁnal saliency map is generated by the sum ofthe ﬁve prediction maps after the joint loss, and the defect region is localized\nby threshold segmentation. The overall architecture of the proposed method is\nillustrated in Fig. 1. And the speciﬁc modules can be described as follows.\n2.1 Multi-scale Feature Learning Module\nThe fabric texture and defect are complex and diversity in scale, shape and posi-\ntion, the original CNN network composed of convolution kernels with the same\nsize cannot eﬃciently characterize the fabric texture feature. Inspired by the\nvisual receptive ﬁeld, a multi-scale feature learning module is proposed to solvethis issue by simulating the parallel processing mechanism of multi-receptive.\nFig. 2. Multi-scale feature learning module.\nReceptive ﬁeld is one of the important concepts of convolutional neural net-\nwork. It originated from human neural system and was introduced into the ﬁeldof computer vision. In CNN, receptive ﬁeld is used to represent the receptive\nrange of diﬀerent neurons in the network to the image, or the region size of\nthe pixels on the feature map output by each layer of the network mapped onthe image. The larger the receptive ﬁeld is, the larger the image range it can\ntouch, which means that it contains more global information; On the contrary,\nit contains more local and detailed features.', '244 Z. Liu et al.\nSpeciﬁcally, we remove the original global average pooling, fully connected\nand softmax layers and maintain the previous ﬁve convolutional blocks inResNet-50 network to eﬃciently characterize the fabric texture, and denote as\n{f\ni|i=1,...,5}with resolutions [H\n2i−1,W\n2i−1]. Since the lowest level features is\nvulnerable to the noises, and have adverse eﬀects on the ﬁnal detection result,the last four convolutional blocks are used to extract multi-level features as side\noutputs. For each side outputs, we proposed the multi-scale feature learning\nmodule to characterize the fabric texture. And the multi-scale feature learningmodule have ﬁve branches. A convolution with 1 ×1 kernel is applied on the\nﬁrst branch. Then we adopt the following two ways to assist each other on the\n2, 3 and 4 branches respectively. It aims to realize a convolution operation withkernel size of n×n: (i) A original convolution with kernel size of n×nis replaced\nby a combination of 1 ×nandn×1 kernels. (ii) Two successive convolutions with\nkernel size of n×nto obtain the same channel number and cascaded with the\nﬁrst way. Further, the ﬁrst four branches are concatenated, followed by a 3 ×3\nconvolution. Finally, the shortcut structure of ResNet is employed on the lastlayer. And the module can be shown in Fig. 2. The proposed multi-scale feature\nlearning module can capture the multi-scale feature to resist scale variation.\n2.2 Feedback Attention Reﬁnement Fusion Module\nAccording to the proposed MFLM module, we generate the Multi-scale features\nfor each side outputs. How to eﬀectively fuse multi-level features with large\nreceptive ﬁeld is still a challenge in fabric defect detection. Therefore, a feedbackattention reﬁnement fusion module (FARFM) is designed to integrate the four\nmulti-scale features to generate multi-level features, which includes feature fusion\nmodule (FFM) [ 18] and attention reﬁnement module (ARM) [ 19]. The proposed\nFARFM can better generate the saliency maps by feeding back the low-level\ntexture feature and high semantic feature to the previous ones.\nFeature Fusion Module. Due to the restriction of the receptive ﬁeld, low-level\nfeatures retain rich details as well as background noises. These features haveclear boundaries, which are important for generating accurate saliency maps. In\ncontrast, high-level features are coarse on boundaries because of multiple down-\nsampling, high-level features still have consistent semantics, and is important for\nlocalizing the defect region.\nFFM performs cross-fusion of high-level features and low-level features to\nreduce the discrepancy between features. Firstly, it extracts the common part\nbetween low-level features and high-level features by element-wise multiplication,\nand then combines them with the original low-level features and high-level fea-tures by element-wise addition, respectively. Compared with the direct addition\nor concatenation, FFM avoids the redundant information of low-level features\nand high-level features, thus can eﬃciently characterize the texture feature forgenerate the better saliency maps.\nSpeciﬁcally, FFM consists of two branches, one is high-level features and the\nother is low-level features, as shown in Fig. 3(a). First, we conduct convolution', 'Fabric Defect Detection via Multi-scale Feature Fusion-Based Saliency 245\nFig. 3. Two components of Feedback attention reﬁnement fusion module.\non the two layers of features. Then these features are transformed and fused by\nmultiplication. Finally, the fused features is appended into the original low-levelfeatures and high-level features as output, and it is shown in Eq. 1and Eq. 2.\nf\nl=fl+Gl(Gl(Gl(fl))∗Gh(Gh(fh))) (1)\nfh=fh+Gh(Gl(Gl(fl))∗Gh(Gh(fh))) (2)\nWhere flandfhare low-level features and high-level features, respectively. Gl(·),\nGh(·) are the combination of convolution, batch normalization and relu operator\nof low-level features and high-level features, respectively. After obtaining the\nfused features, a 3 ×3 convolution is applied to restore the original dimension.\nThe proposed module is a symmetrical structure, where flembeds its details\nintofh,a n d fhﬁlters the background noises of fl.\nAttention Reﬁnement Module. The layers with diﬀerent depth can rep-\nresent diﬀerent scale information. For the fabric image, we should selectively\nutilize the feature layer information for suppressing non-information branches\nand automatically promoting diﬀerentiated branches. Therefore, we propose anattention reﬁnement module (ARM) to reﬁne the features of each FFM, and it\nis shown in Fig. 3(b).\nARM employs global average pooling to capture global context information\nand computes an attention vector to guide the feature learning by a series of con-\nvolution, BN and sigmoid layers. This module can optimize the output features\nbetween FFMs. It easily integrates the global context information without anyup-sampling operation, and has a small computation. The calculation equation\ncan be described as follows,\nω=S(B(C(G(f)))) (3)', '246 Z. Liu et al.\nf=f⊗ω (4)\nWhere S,B,CandGrepresent sigmoid operator, batch normalization, convolu-\ntion operator and global average pooling, respectively. ⊗indicates element-wise\nmultiplication, fandωare replicated to the same shape before multiplication.\n2.3 The Joint Loss\nBinary cross entropy (BCE) is the widely used loss function. However, BCE loss\nhas the following disadvantages. First, it independently calculates the loss of eachpixel, thus ignores the global structure of the image. In addition, the feature of\nsmall defect object may be weakened. Moreover, BCE treats all pixels equally.\nIn fact, the pixels in the fabric defect region should be paid more attention. Toaddress these issues, a weighted binary cross entropy loss (wBCE) is proposed\nto highlight the defect objects in the paper, which can be written as,\nL\ns\nwBCE =−H/summationtext\ni=1W/summationtext\nj=1(1 +γα ij)1/summationtext\nl=01(gs\nij=l)log Pr( ps\nij=l|Ψ)\nH/summationtext\ni=1W/summationtext\nj=1γα ij(5)\nWhere 1( ·) is an indication function and γis a hyperparameter. l∈{0,1}is\nthe labels of defect and background. ps\nijandgs\nijare the prediction and ground\ntruth of pixels at position ( i,j) in the image. Ψindicates all the parameters of\nthe model and Pr( ps\ni,j=l|Ψ) represents the predicted probability.\nForLs\nwBCE, each pixel is assigned a weight α, which can be regarded as an\nindicator of the importance of pixels, which is calculated based on the diﬀerence\nbetween the center pixel and its surroundings, as shown in Eq. 6.\nαs\nij=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext\nm,n∈Aijgs\nmn\n/summationtext\nm,n∈Aij1−gs\nij/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(6)\nWhere A\nijrepresents the neighborhood of the pixel ( i,j). For all pixels, the larger\nαs\nij∈[0,1] means that the pixel at ( i,j) is quite diﬀerent from its surroundings,\notherwise, the pixel ( i,j) is prone to be in the background.\nIn addition, in order to eﬃciently characterize the global structure feature, a\nweighted IoU (wIoU) loss which has been widely used in image segmentation is\nadopted in our method. It is a complementation of wBCE. Unlike IoU loss, the\nwIoU loss assigns more weight to distinguished pixels, which could be denoted as:\nLs\nwIoU=1−H/summationtext\ni=1W/summationtext\nj=1(gs\nij∗ps\nij)∗(1 +γαs\nij)\nH/summationtext\ni=1W/summationtext\nj=1(gs\nij+ps\nij−gs\nij∗ps\nij)∗(1 +γαs\nij)(7)', 'Fabric Defect Detection via Multi-scale Feature Fusion-Based Saliency 247\nThe joint loss which is combined wBCE with wIoU can be described as Eq. 8.\nThe joint loss integrates the local texture and the global structure information,which can better highlight the defect region.\nL\ns\nJ=Ls\nwBCE +Ls\nwIoU (8)\n3 Experiments\nIn this section, we ﬁrstly present the experimental setup, including datasets, the\nevaluation criteria, and implementation details of the proposed method. Then,\nexperiments are conducted to evaluate and compare the proposed network with\nthe 5 state-of-the-art methods. Finally, we provide a rigorous analysis of theexperimental results.\n3.1 Dataset and Evaluation Metrics\nDataset. We evaluated the proposed model on our fabric image dataset, which\ncontains 1,600 images with scratches, stains, holes, and many other types of\ndefects. We divide the dataset into two parts: 1100 images are used for training,\nand the remaining 500 images are used for testing.\nEvaluation Metrics. In this paper, we evaluate the performance of the pro-\nposed network and existing state-of-the-art salient object detection methods by\nusing seven widely-used evaluation metrics, including the precision-recall (PR)curves, F-measure curves, Max F- measure (maxF), mean F-measure (meanF),\nmean absolute error (MAE), structural similarity measure (S\nm) and E-measure\n(Em).\n3.2 Implementation Details\nDuring the training stage, we apply horizontal ﬂip for the training data aug-\nmentation, which aims to avoid overﬁtting. To ensure model convergence, ournetwork is trained for 50 epochs with an NVIDIA GTX 1080 Ti GPU. Our\nmodel is implemented in PyTorch. The parameters of the backbone are initial-\nized by the ResNet-50 and the remaining convolution layers are initialized bythe default settings of PyTorch. The whole network adopts stochastic gradient\ndescent (SGD) optimizer with a weight decay of 5e −4, a maximum learning rate\nof 5e−3 and a momentum of 0.9 to train our model. Warm-up and linear decay\nstrategies are utilized to adjust the learning rate.\n3.3 Comparison with State-of-the-Arts\nWe compare the proposed algorithm with 5 state-of-the-art saliency detection\nmethods, including the NLDF [ 20], DSS [ 21], PiCANet [ 22], CPD [ 23], BAS-\nNet [24]. For fair comparisons, all saliency maps of these methods are computed\nby running their released codes under the default parameters.', '248 Z. Liu et al.\nFig. 4. Visual comparison of the proposed method and several state-of-the-art saliency\nobject detection methods.\nVisual Comparison. Figure 4shows the visual comparison of the proposed\nmethod and several state-of-the-art saliency object detection methods. The ﬁrstcolumn is the test image, the second column is GT, the third column is the\ndetection results using NLDF, the fourth column is the detection results using\nDSS, the ﬁfth column is the detection results using PiCA, the sixth column is thedetection results using CPD, the seventh column is the detection results using\nBAS and the last column is the detection results of our method. From this ﬁgure,\nwe can see that our method is able to accurately detect the fabric defects, and\nhighlight the defect objects with clear boundaries, such as some slender strip\ndefects (1st and 2nd rows), dot defects (4th, 5th, 6th, 7th and 8th rows), etc.\nQuantitative Evaluation. To further illustrate the performance of our method,\nwe give the quantitative evaluation, and the comparisons are shown in Table 1.', 'Fabric Defect Detection via Multi-scale Feature Fusion-Based Saliency 249\nTable 1. Quantitative evaluation. The best results on our dataset are highlighted in\nboldface.\nMethod maxF meanF MAE Sm Em\nNLDF 0.5632 0.4661 0.0041 0.6986 0.7838\nDSS 0.5923 0.3134 0.0045 0.6854 0.5949\nPiCANet 0.5985 0.1519 0.0069 0.6634 0.4016\nCPD 0.6064 0.2410 0.0052 0.6869 0.5028\nBASNet 0.6058 0.4195 0.0107 0.7078 0.7150\nOurs 0.6233 0.4886 0.0047 0.7241 0.7624\nFig. 5. Quantitative comparisons of the proposed method and several state-of-the-art\nsaliency object detection methods.\nThe evaluation metrics include Max F-measure (max F, larger is better), Mean\nF-measure (meanF, larger is better), Mean Absolute Error (MAE, smaller is bet-\nter), S-measure (S m, larger is better) and E-measure (E m, larger is better). As can\nbe seen from the Table 1, the overall performance of our proposed model is outper-\nform the existing methods. In addition, the PR curves and the F-measure curves\nare demonstrated in Fig. 5. From these curves, we can conclude that our model\noutperforms most other models at diﬀerent thresholds, which demonstrates that\nour method has better performance in fabric defect detection.\n3.4 Ablation Study\nIn this section, we conduct extensive ablation studies to validate the eﬀectiveness\nof the proposed components in our model. ResNet-50 is adopted as the backbone\nnetwork on our model.\nPerformance of MFLM. The MFLM model can eﬃciently characterize the\ncomplex fabric texture by simulating the parallel processing mechanism of multi-\nreceptive, which is able to improve the ability of solving scale variation problem.\nWe install the MFLM on the baseline network and evaluate its performance. Theresults are shown in Table 2. It can be seen that the module achieves signiﬁcant\nperformance improvement over the baseline.', '250 Z. Liu et al.\nTable 2. Ablation analysis for the main architecture.\nBaseline MFLM FARFM maxF meanF MAE Sm Em\n√0.6069 0.4571 0.0049 0.7253 0.7396\n√ √0.6136 0.4682 0.0053 0.7222 0.7456\n√ √0.6158 0.4756 0.0046 0.7293 0.7547\n√ √ √0.6233 0.4886 0.0047 0.7241 0.7624\nPerformance of F ARFM. FARFM is proposed to selectively aggregate multi-\nlevel features for enhancing the feature fusion. To demonstrate the eﬀectiveness\nof FARFM, only it will be introduced into the baseline network. It can be seen\nfrom the third row of Table 2that the ﬁve metrics have improved signiﬁcantly.\n4 Conclusion\nIn this paper, a novel visual saliency based on multi-scale feature fusion enhances\nthe performance of saliency detection, which is proposed for fabric defect detec-\ntion. First, considering the fabric texture and defects are complex and diversity,\na multi-scale feature learning module (MFLM) is employed to generate powerful\nfeatures, which is able to improve the ability of solving scale variation prob-\nlem. Besides, feedback attention reﬁnement fusion module (FARFM) is designedto generate 5 prediction maps by the selective fusion of multi-level features. In\naddition, we obtain the ﬁnal saliency map by the adopting joint loss (JT). We\ncomprehensively validate the eﬀectiveness of each component of our network inablation study. Subsequently, extensive experimental results demonstrate that\nthe proposed method outperforms 5 state-of-the-art methods under diﬀerent\nevaluation metrics.\nAcknowledgement. This work was supported by NSFC (No. 61772576, No.\n62072489, U1804157), Henan science and technology innovation team (CXTD2017091),\nIRTSTHN (21IRTSTHN013), Program for Interdisciplinary Direction Team in\nZhongyuan University of Technology, ZhongYuan Science and Technology InnovationLeading Talent Program (214200510013).\nReferences\n1. Srinivasan, K., Dastoor, P.H., Radhakrishnaiah, P., et al.: FDAS: a knowledge-\nbased framework for analysis of defects in woven textile structures. J. Text. Inst.\n83(3), 431–448 (1992)\n2. Zhang, Y.F., Bresee, R.R.: Fabric defect detection and classiﬁcation using image\nanalysis. Text. Res. J. 65(1), 1–9 (1995)\n3. Abouelela, A., Abbas, H.M., Eldeeb, H., et al.: Automated vision system for local-\nizing structural defects in textile fabrics. Pattern Recogn. Lett. 26(10), 1435–1443\n(2005)', 'Fabric Defect Detection via Multi-scale Feature Fusion-Based Saliency 251\n4. Behravan, M., Tajeripour, F., Azimifar, Z., et al.: Texton-based fabric defect detec-\ntion and recognition. Journal, 57–69 (2011)\n5. Chan, C., Pang, G.K.H.: Fabric defect detection by Fourier analysis. IEEE Trans.\nInd. Appl. 36(5), 1267–1276 (2000)\n6. Bu, H., Wang, J., Huang, X.: Fabric defect detection based on multiple fractal\nfeatures and support vector data description. Eng. Appl. Artif. Intell. 22(2), 224–\n235 (2009)\n7. Guan, S.: Fabric defect detection using an integrated model of bottom-up and\ntop-down visual attention. J. Text. Inst. 107(2), 215–224 (2016)\n8. He, H.: Saliency and depth-based unsupervised object segmentation. IET Image\nProc. 10(11), 893–899 (2016)\n9. Zhang, G., Yuan, Z., Zheng, N.: Key object discovery and tracking based on\ncontext-aware saliency. Int. J. Adv. Rob. Syst. 10(1), 15 (2013)\n10. Mahadevan, V., Vasconcelos, N.: Saliency-based discriminant tracking. In: IEEE\n(2009)\n11. Liu, Z., Li, C., Zhao, Q., et al.: A fabric defect detection algorithm via context-\nbased local texture saliency analysis. Int. J. Cloth. Sci. Technol. (2015)\n12. Li, C., Yang, R., Liu, Z., et al.: Fabric defect detection via learned dictionary-based\nvisual saliency. Int. J. Cloth. Sci. Technol. (2016)\n13. Zhang, H., Hu, J., He, Z.: Fabric defect detection based on visual saliency map\nand SVM. In: IEEE (2017)\n14. Lee, G., Tai, Y.W., Kim, J.: Deep saliency with encoded low level distance map\nand high level features. In: IEEE (2016)\n15. Wang, L., Lu, H., Wang, Y., et al.: Learning to detect salient objects with image-\nlevel supervision. In: IEEE (2017)\n16. Li, G., Yu, Y.: Deep contrast learning for salient object detection. In: IEEE (2016)17. Pang, Y., Zhao, X., Zhang, L., et al.: Multi-scale interactive network for salient\nobject detection. In: IEEE (2020)\n18. Wei, J., Wang, S., Huang, Q.: F\n3Net: fusion, feedback and focus for salient object\ndetection. In: AAAI (2020)\n19. Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N.: BiSeNet: bilateral segmen-\ntation network for real-time semantic segmentation. In: Ferrari, V., Hebert, M.,Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11217, pp. 334–349.\nSpringer, Cham (2018). https://doi.org/10.1007/978-3-030-01261-8\n20\n20. Luo, Z., Mishra, A., Achkar, A., et al.: Non-local deep features for salient object\ndetection. In: IEEE (2017)\n21. Hou, Q., Cheng, M.M., Hu, X., et al.: Deeply supervised salient object detection\nwith short connections. In: IEEE (2017)\n22. Liu, N., Han, J., Yang, M.H.: PicaNet: learning pixel-wise contextual attention for\nsaliency detection. In: IEEE (2018)\n23. Wu, Z., Su, L., Huang, Q.: Cascaded partial decoder for fast and accurate salient\nobject detection. In: IEEE/CVF (2019)\n24. Qin, X., Zhang, Z., Huang, C., et al.: BASNet: boundary-aware salient object\ndetection. In: IEEE/CVF (2019)', 'Improving Adversarial Robustness\nof Detector via Objectness Regularization\nJiayu Bao1,2, Jiansheng Chen1,2,3(B), Hongbing Ma1, Huimin Ma2, Cheng Yu1,\nand Yiqing Huang1\n1Department of Electronic Engineering, Tsinghua University, Beijing 100084, China\n2School of Computer and Communication Engineering, University of Science and\nTechnology Beijing, Beijing 100083, China\njschen@ustb.edu.cn\n3Beijing National Research Center for Information Science and Technology,\nTsinghua University, Beijing 100084, China\nAbstract. Great eﬀorts have been made by researchers for achieving\nrobustness against adversarial examples. However, most of them are con-ﬁned to image classiﬁers and only focus on the tiny global adversarial\nperturbation across the image. In this paper, we are the ﬁrst to study the\nrobustness of detectors against vanishing adversarial patch, a physicallyrealizable attack method that performs vanishing attacks on detectors.\nBased on the principle that vanishing patches destroy the objectness fea-\nture of attacked images, we propose objectness regularization (OR) todefend against them. By enhancing the objectness of the whole image\nas well as increasing the objectness discrepancy between the foreground\nobject and the background, our method dramatically improves the detec-tors’ robustness against vanishing adversarial patches. Compared with\nother defense strategies, our method is more eﬃcient but robust to adap-\ntive attacks. Another beneﬁt brought by our method is the improvementof recall on hard samples. Experimental results demonstrate that our\nmethod can generalize to adversarial patches of diﬀerent strengths. We\nreduce the vanishing rate (VR) on YOLOv3 and YOLOv4 under thevanishing attack by 49% and 41% respectively, which is state-of-the-art.\nKeywords: Adversarial defense\n·Vanishing patch ·Object detection ·\nObjectness regularization\n1 Introduction\nDeep neural networks (DNNs) have achieved remarkable success on object detec-\ntion [10,19,23] and fueled the development of many applications. However, DNNs\nare found to be easily fooled by adversarial examples [ 21]. In the computer vision\nﬁeld, adversarial examples are maliciously crafted images that aim at misleadingthe predictions of DNNs. Typically, for an input image xand a model F(.), the\ngoal of adversarial attacks is to ﬁnd the adversarial example x\n/primewhich satisﬁes\nEq. (1), where Δis a metric to measure the diﬀerence between xandx/prime. In most\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 252–262, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_21', 'Improving Adversarial Robustness of Detector via Objectness Regularization 253\nof studies [ 17,24,29],Δis the p-norm ( p=2o r ∞) metric, and the perturbation\nis restricted to be within the p-norm ball of radius /epsilon1, which means the adversarial\nperturbation is imperceptible to humans.\nF(x)/negationslash=F(x/prime)∧Δ(x,x/prime)</epsilon1 (1)\nObject detection is wildly used in many security critical areas like\nautonomous driving and medical diagnosis. Hence the robustness of deep detec-\ntors has attracted great attention. Many attack methods [ 20,22,30] have been\nproposed to study the robustness of detectors. DAG [ 26] misleads the classiﬁ-\ncation of all the bounding boxes, while RAP [ 14] also includes regression loss\nin attacks. TOG [ 8] designs diﬀerent objectness loss functions to conduct three\ntypes of adversarial attacks. However, detectors are wildly used in the real worldwhile attacks with global tiny perturbations are not realizable in the physical\nworld. Among those physically realizable attacks, adversarial patches [ 4,6,12,16]\nare the most common attacks which misleading networks with localized imagepatches. Like what shows in Fig. 1, localized adversarial patches can mislead the\nobject detector to detect nothing. Attackers can implement those patches in the\nreal world and poses real threats to detectors [ 6,13,30]. So we pay attention to\nthe physically realizable adversarial patch in this work.\nFig. 1. Illustration of our objectness regularization method. The objectness regular-\nization module is inserted into the detector. We regenerate adversarial patch for themodel with objectness regularization.', '254 J. Bao et al.\nDefenses against adversarial patches, such as LGS [ 18] and DW [ 11], are\ndesigned originally for classiﬁers. Besides, those strategies are proved to be easilycircumvented [ 7] by exploiting BPDA [ 1] to approximate gradients. By contrast,\nmuch less work has been done to improve the adversarial robustness of detectors.\nIt has proven to be an eﬀective way for classiﬁers to achieve robustness by adver-sarial training (AT) [ 17]. AT continuously generates adversarial examples and\noptimizes the model on these samples in the training phase. By such min-max\noptimizations, AT can achieve a classiﬁer with a very smooth decision boundary,thus it’s more diﬃcult for a benign example to cross the decision boundary with\nonly an imperceptible perturbation. Therefore, AT is extremely time-consuming\nand usually serves as a trade-oﬀ between clean accuracy and adversarial robust-ness [ 29]. However, when applying to object detectors, AT can even cause a\n25.8 mAP drop on clean images [ 28], which is far from satisfactory. AT methods\nare robust to adaptive attacks [ 1] but only eﬀective to adversarial perturbations\nof small l\n2orl∞norm, which is inappropriate for defending against physically\nrealizable perturbations (often have large l∞norm and small l0norm).\nThe vanishing attack [ 2,8] is the most frequently used adversarial attack on\ndetectors that allowing the object to evade detection. This type of attack is\nharmful for it is natural-style [ 25,27] but can be catastrophic to security-critical\nsystems like automatic driving [ 6,30]. And the adversarial patch that performing\nvanishing attacks poses security concerns for the employment of detectors in the\nreal world. Hence we shine a light on the robustness of detectors against vanishingadversarial patches in this paper.\nOur contributions are as follows:\n– To the best of our knowledge, we are the ﬁrst to bridge the gap of detec-\ntion robustness against adversarial patches. We develop vanishing adversarialpatch to judge the robustness.\n– We propose objectness regularization, a simple yet eﬀective method for\nachieving robustness against vanishing adversarial patches, with a propertrade-oﬀ between clean performance and adversarial robustness.\n– Our method is eﬃcient and robust to adaptive attacks. We reduce the van-\nishing rate (VR) on YOLOv3 [ 10] and YOLOv4 [ 3] under the adaptive attack\nby 49% and 41% respectively.\n2M e t h o d\nIn this section, we ﬁrst revisit the vanishing adversarial patch method in Sect. 2.1.\nThen we introduce our objectness regularization method in Sect. 2.2.\n2.1 Vanishing Adversarial Patch\nFor an input image x, the object detector outputs bounding box predictions\nb(x) and classiﬁcation predictions c(x). The objectness scores of bounding boxes\nare denoted as o(x), representing the conﬁdence of containing objects. In some', 'Improving Adversarial Robustness of Detector via Objectness Regularization 255\ndetectors [ 3,10], objectness is directly deﬁned. While in detectors of other struc-\ntures [ 19,23], objectness can be represented by the sum of all foreground object\nprobabilities. Objectness plays an import role in distinguishing the foreground\nobjects and the background. So it is often attacked by vanishing attacks [ 8].\nThe most import aspects of the vanishing patch are the positions of patches\nand the vanishing loss function. We choose the positions of patches in an empiri-\ncal way for attack eﬃciency. The center of objects are proven to be good choices\nfor vanishing attacks in TOG [ 8] and SAA [ 2], so we add vanishing patches at the\ncenters of corresponding objects. Speciﬁcally, for an input image x, we get the\ndetection result F(x) and choose patch locations using bounding box positions\ninF(x). Since it is diﬃcult to optimize the l0perturbation directly, we exploit\na predeﬁned location mask mwith bool values to restrict the adversarial per-\nturbation positions. The adversarial example x/primeis then denoted as Eq. ( 2). Here\nx/primedenotes the image with perturbation, ⊙is the element-wise multiplication\noperator and pis the vanishing patch we want to get.\nx/prime=x⊙(1−m)+p⊙m (2)\nWe optimize patch pusing stochastic gradient descent (SGD) with properly\ndesigned loss function and step length. The loss function we employ here is as\nwhat exactly used in SAA [ 2]a sE q .( 3). Here o(x/prime) is the objectness predictions\nofx/primeand the loss function erases all the predictions with a high objectness. The\nstep length of adversarial attack is ﬁxed in most of cases [ 5,9,17]. However, we\nﬁnd exponential decay step length will create more powerful adversarial examplesunder the same attack budgets (e.g. iterations, time). So we use both ﬁxed step\nlength and decay step length attacks to evaluate the adversarial robustness.\nloss=max(o(x\n/prime)) (3)\nLike many other works [ 17,24,29], we consider the attack budgets in evaluat-\ning the robustness of detectors. In this work, we use three indicators to indicate\nthe attack strength, the total pixels of patches ( l0norm), the maximum iteration\nnumbers (resource constraints), and the step length (optimization strategy).\n2.2 Objectness Regularization\nThe vanishing adversarial patch in the previous Sect. 2.1is an objectness gra-\ndient based attack. To erase objects in detections, vanishing attacks have to\nfool the objectness of the victim object. In a clean image, foreground objects\noften have much higher objectness than the background. However, the vanish-ing patch changes this situation. Compared to clean images, adversarial images\nwith vanishing patches often have much lower objectness. Like what shows in\nFig.2(b), the highest value of objectness feature map is far less than 0, and is far\nless than 0.1 even after the sigmoid activation. We conclude the impacts of the\nvanishing patch as follows. First, the vanishing patch reduces the whole object-ness of the image. Second, the vanishing patch compresses the objectness to a', '256 J. Bao et al.\nFig. 2. The objectness distribution of an adversarial image. (a) is an image with van-\nishing patches and (b) is the objectness distribution of the image. (c) is the sigmoidfunction. We mark the objectness of the object and the background with blue arrows\nin (c).\nsmall range, where the discrepancy between the foreground and the background\nbecomes smaller.\nTo defend against such vanishing attacks, we have to boost the objectness of\nthe image. However, there are two problems we have to solve. First, we should\nconsider the robustness to adaptive attacks. That is, when attackers know our\ndefense strategy and regenerate adversarial examples for attacks, our defensemethod should be still eﬀective. Second, since the discrepancy between the fore-\nground and the background is smaller, it is of vital importance to avoid too\nmany false predictions when boosting the objectness of the whole image. Tosolve these problems, we design an objectness boosting function with the post-\nprocess parameter of the detector included (to defend adaptive attacks). Before\nthe ﬁnal classiﬁcation and regression, we apply our objectness boosting func-tion to correct the objectness feature, like what shows in Fig. 1. We describe the\nfunction design in the following.\nThe basic idea of our method is to make it more diﬃcult for attackers to\nreduce the objectness of foreground objects. We observe that the foreground\nobject still remains a slightly higher objectness than the background due to\nthe feature discrimination, though the vanishing patch reduces the objectnessof the whole victim image to a large scale. So we boost the objectness of the\nwhole image as well as increase the discrepancy between the foreground and the\nbackground. We denote the objectness feature of an image xasf\nobj(x), and the\nobjectness feature after regularization can be formulated by Eq. ( 4).\nf/prime\nobj(x)=r∗S(fobj(x)) +b (4)\nHererserves as a range control, bis a necessary balancing bias and S(.)\ndenotes the boosting function which has the form of Eq. ( 5).\nS(t)=1\n1+e−t(5)', 'Improving Adversarial Robustness of Detector via Objectness Regularization 257\nWe choose the boosting function S(.)a st h e sigmoid function. The sigmoid\nfunction can map the objectness feature to the range of 0 and 1 while maintainingthe internal relative numerical size relationship in it. The object attacked by the\nvanishing patch has low objectness but is often slightly higher than that of the\nbackground. We choose the sigmoid function for it also increases the objectness\ndiscrepancy between the attacked object and the background when boosting the\nobjectness. As can be seen from Fig. 2, the objectness of the attacked object\nand that of the background are in diﬀerent segments of the sigmoid function\n(indicates with blue arrows). In our method, higher objectness results in a greater\ngain due to the attacked image typically has all values of objectness feature\nlower than 0. Therefore, we increase the objectness of the foreground objectand avoiding too high objectness of the background. That is, our method can\nenhance object detection under vanishing adversarial attacks without generating\ntoo many unreasonable bounding boxes.\nThe regularization parameters randbare closely related to the objectness\nthreshold τ.T h e τis used in the post process of the detector to erase redundant\nbounding boxes. We argue that a larger tangent slope in the sigmoid function\ncorresponding to τis supposed to have a smaller range r. So we deﬁne ras\nthe reciprocal of the sigmoid derivative. Due to the convenience of sigmoid\nderivative calculation, we can easily get ras Eq. ( 6). We introduce a constant\nproduct factor 1/4 into Eq. ( 6) to avoid too high objectness.\nr=1\n4τ(1−τ)(6)\nThebis an essential bias that control the lowest prediction objectness. We\nmust ensure that the lowest objectness after regularization is high enough forachieving robustness against adaptive adversarial attacks. However, considering\nthe time consumption of post-process, the lowest objectness after regularization\nshould not be higher than S\n−1(τ) (otherwise there will be too many redundant\nbounding boxes with prediction objectness higher than threshold τ). Therefore\nwe design the bas Eq. ( 7), where S−1(.) is the inverse function of the sigmoid\nfunction and /epsilon1is a small constant to ﬁlter redundant bounding boxes with rel-\native low objectness. We choose the value of /epsilon1quite empirically and will study\nthe eﬀect of it in Sect. 3.\nb=S−1(τ)−/epsilon1=l n (τ\n1−τ)−/epsilon1 (7)\n3 Experiment\nIn this section, we evaluate the proposed objectness regularization (OR) method.\nBoth standard object detection and adversarial object detection are investigated.\n3.1 Experimental Setup\nWe introduce our experimental setup in this section. For standard object detec-\ntion and adversarial object detection, we use diﬀerent settings accordingly.', '258 J. Bao et al.\nDatasets. For standard object detection, we evaluate the performance of detec-\ntors on COCO2014 [ 15] validation set. However, many tiny objects are contained\nin COCO dataset and will be covered by patches directly. For a fair judgment, we\ngenerate adversarial examples on 1000 images chosen from the COCO dataset.\nAll the objects in the chosen 1000 images are large enough to not be covered bypatches directly.\nModels. We evaluate our method on YOLOv3 [ 10] and YOLOv4 [ 3]t h a ta r e\nboth pre-trained on COCO2014 [ 15]. The input size of both detectors is 416 * 416\npixels in our experiments. The performances of detectors on clean images are\nevaluated on COCO validation set with the objectness threshold of 0.001 fornon-max suppression (NMS). While the objectness threshold τof detectors is\nset to be the most frequently used 0.5 in all our robustness evaluations.\nPatches. The method in Sect. 2.1is exploited to generate adversarial patches.\nWe evaluate our defense method under patches with diﬀerent strengths. The\nattacks are strong enough that the iteration number of patches is set to be atleast 100. We generate adversarial patches for every single image independently.\nAnd we evaluate our method under defense aware adversarial patches. That is, we\nregenerate vanishing patches for models equipped with objectness regularization.\nMetrics. The mAP is chosen to demonstrate the performance on clean images.\nFor convenience, we use mAP-50 in experiments. While the vanishing rate (VR)\nis introduced to demonstrate the performance under vanishing attacks. The lower\nthe VR, the better robustness of detectors against vanishing patches. The VR is\ncalculated as Eq. ( 8), where B(x)a n d B(x\n/prime) denote the prediction of the clean\nimage xand the adversarial image x/primeseverally. The IOU(.) is a function that\ncalculates the reasonable detections in B(x/prime) where B(x) serves as the ground\ntruth result.\nVR=1−IOU(B(x),B(x/prime))\nB(x)(8)\n3.2 Experimental Result\nThe performance of detectors on clean images and adversarial images is reported\nin this section.\nResilience to Adaptive Attacks. We investigate the VR of detectors under\nattacks with diﬀerent strengths. We exploit adversarial patches of various sizes\n(2500, 3000, and 3500 total pixels respectively) to attack detectors. For each\nsize of the patch, we design three iteration number budgets (100, 200, and 255\nrespectively) for robustness evaluation. A constant update step of 1/255 is usedin all attacks of 100 and 200 iterations. While a decaying update step with initial\nupdate step 8/255, decay rate 1/2 and decay point at 30, 90, 190 is employed in\nall attacks of 255 iterations.\nThe VR on YOLOv3 and YOLOv4 under attacks with diﬀerent budgets are\npresented in Table 1. The hyper-parameter /epsilon1in experiments of Table 1is 0.01\nfor YOLOv3 and 0.05 for YOLOv4. From Table 1, adversarial patches using', 'Improving Adversarial Robustness of Detector via Objectness Regularization 259\nTable 1. The VR on YOLOv3 and YOLOv4 (with and without OR defense) under\nattacks of diﬀerent strengths. Attacks to defense models are all adaptive attacks in this\ntable.\nPatch pixels Attack iters YOLOv3 YOLOv3 OR YOLOv4 YOLOv4 OR\n2500 100 34.5% 20.4% ( ↓14.1%) 28.3% 25.4% ( ↓2.9%)\n200 48.1% 22.1% ( ↓26.0%) 37.5% 27.6% ( ↓9.9%)\n255 79.7% 31.5% ( ↓48.2%) 59.5% 32.0% ( ↓27.5%)\n3000 100 41.3% 21.9% ( ↓19.4%) 34.4% 26.5% ( ↓7.9%)\n200 56.2% 23.9% ( ↓32.3%) 45.7% 29.0% ( ↓16.7%)\n255 85.2% 37.4% ( ↓47.8%) 70.1% 34.5% ( ↓35.6%)\n3500 100 48.8% 23.1% ( ↓25.7%) 38.3% 27.3% ( ↓11.0%)\n200 64.4% 26.0% ( ↓38.4%) 52.5% 30.0% ( ↓22.5%)\n255 91.0% 42.2% ( ↓48.8% )78.3% 37.0% ( ↓41.3% )\nstrategies of SAA greatly increase the VR on the two detectors, with even 91%\nand 78% of objects evade detection. Our method reduces the VR on YOLOv3 andYOLOv4 by 48.8% and 41.3% respectively under the strongest attack in Table 1.\nAs also can be seen from Table 1, the OR method is particularly eﬀective against\nstrong adversarial attacks, which is more representative of the real robustness ofmodels.\nFig. 3. Detection results on a clean image. left: detection results of YOLOv3, right:\ndetection results of YOLOv3 OR.\nEﬀects on Clean Images. Our method changes the original objectness for\nachieving detection robustness. However, the method only causes a decrease\nof 2.67 mAP on clean images in YOLOv3, as demonstrated in Table 2.T h e\nperformance of YOLOv4 on clean images only has a slight drop from 56.74 mAP', '260 J. Bao et al.\nto 56.45 mAP. Despite the comprehensive performance drop on clean images,\nour method improves the recall of weak-feature objects signiﬁcantly. It can beobserved from Table 2that the recall of tiny objects like spoon and baseball\nglove has an increase of over 5% when using OR. The recall of the refrigerator\neven reaches 96.3% with our method. As demonstrated in Fig. 3, our method is\nhelpful to detect small objects like donuts and hard samples like the dining-table\nand the cup.\nTable 2. The recall of some COCO dataset categories in YOLOv3 with and without\nOR.\nCategory YOLOv3 (54.30 mAP) YOLOv3 OR (51.63 mAP)\nStop sign 88.1% 90.5%( ↑2.4%)\nBaseball glove 67.6% 72.7%( ↑5.1%)\nSpoon 62.5% 71.1%( ↑8.6%)\nBanana 70.9% 76.6%( ↑5.7%)\nRefrigerator 90.7% 96.3% (↑5.6%)\nAblation Study. The eﬀects of the hyper-parameter /epsilon1are presented in Table 3.\nThe adversarial attack used in Table 3has a strength of pixel 3000 and iteration\nnumber 255. It’s obvious that a smaller /epsilon1typically results in a better performance\non adversarial images and a slightly worse performance on clean images, at theexpense of inference speed. The negative values of /epsilon1are abandoned by us in\nexperiments due to the explosion of inference time. We choose /epsilon1as 0.01 for\nbalance in most of the experiments.\nTable 3. The eﬀects of /epsilon1on clean mAP (mAP on clean images of COCO validation\nset), VR, and inference time per image (test on GTX 2080 Ti) in YOLOv3 OR.\n/epsilon1 Clean mAP VR (%) Time (ms)\n0.3 50.32 76.2 23.6\n0.1 51.69 58.3 23.9\n0.01 51.63 37.4 27.0\n0.001 51.63 28.9 41.8\n0.0001 51.63 27.4 96.0\n4 Conclusion\nIn this paper, we propose a defense method called objectness regularization (OR)\nagainst vanishing adversarial patch attacks. The sigmoid function is chosen to', 'Improving Adversarial Robustness of Detector via Objectness Regularization 261\nenhance the image objectness as well as increase the objectness discrepancy\nbetween the foreground object and the background. Our method is eﬃcient (com-pared to adversarial training methods) but eﬀective against adaptive adversarial\nattacks. The experimental results on YOLOv3 and YOLOv4 demonstrate that\nOR can generalize to attacks of diﬀerent strengths. This method signiﬁcantlyimproves the recall of hard samples on clean images with only little mAP degra-\ndation. We will further generalize our method to detectors of other structures\nand adjust it to resist diﬀerent types of adversarial attacks.\nAcknowledgement. This work is supported by the National Natural Science Foun-\ndation of China (No. 61673234, No. U20B2062), and Beijing Science and Technology\nPlanning Project (No. Z191100007419001).\nReferences\n1. Athalye, A., Carlini, N., Wagner, D.: Obfuscated gradients give a false sense of\nsecurity: circumventing defenses to adversarial examples. In: International Confer-\nence on Machine Learning, pp. 274–283 (2018)\n2. Bao, J.: Sparse adversarial attack to object detection. arXiv preprint\narXiv:2012.13692 (2020)\n3. Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M.: YOLOv4: optimal speed and accuracy\nof object detection. arXiv preprint arXiv:2004.10934 (2020)\n4. Brown, T.B., Man´ e, D., Roy, A., Abadi, M., Gilmer, J.: Adversarial patch. arXiv\npreprint arXiv:1712.09665 (2017)\n5. Carlini, N., Wagner, D.: Towards evaluating the robustness of neural networks. In:\n2017 IEEE Symposium on Security and Privacy (SP), pp. 39–57. IEEE (2017)\n6. Chen, S.-T., Cornelius, C., Martin, J., Chau, D.H.P.: ShapeShifter: robust physical\nadversarial attack on faster R-CNN object detector. In: Berlingerio, M., Bonchi,\nF., G¨artner, T., Hurley, N., Ifrim, G. (eds.) ECML PKDD 2018. LNCS (LNAI),\nvol. 11051, pp. 52–68. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-\n10925-7 4\n7. Chiang, P.y., Ni, R., Abdelkader, A., Zhu, C., Studor, C., Goldstein, T.: Certiﬁed\ndefenses for adversarial patches. In: International Conference on Learning Repre-\nsentations (2019)\n8. Chow, K.H., et al.: Adversarial objectness gradient attacks in real-time object\ndetection systems. In: 2020 Second IEEE International Conference on Trust, Pri-\nvacy and Security in Intelligent Systems and Applications (TPS-ISA), pp. 263–272.\nIEEE (2020)\n9. Dong, Y., et al.: Boosting adversarial attacks with momentum. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 9185–9193\n(2018)\n10. Farhadi, A., Redmon, J.: YOLOv3: an incremental improvement. Computer Vision\nand Pattern Recognition, cite as (2018)\n11. Hayes, J.: On visible adversarial perturbations & digital watermarking. In: Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition\nWorkshops, pp. 1597–1604 (2018)\n12. Karmon, D., Zoran, D., Goldberg, Y.: LaVAN: localized and visible adversarial\nnoise. In: International Conference on Machine Learning, pp. 2507–2515 (2018)', '262 J. Bao et al.\n13. Komkov, S., Petiushko, A.: AdvHat: real-world adversarial attack on ArcFace face\nID system. In: 2020 25th International Conference on Pattern Recognition (ICPR),\npp. 819–826. IEEE (2021)\n14. Li, Y., Tian, D., Bian, X., Lyu, S.: Robust adversarial perturbation on deep\nproposal-based models\n15. Lin, T.-Y., et al.: Microsoft COCO: common objects in context. In: Fleet, D.,\nPajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp.\n740–755. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10602-1 48\n16. Liu, X., Yang, H., Liu, Z., Song, L., Chen, Y., Li, H.: DPatch: an adversarial patch\nattack on object detectors. In: SafeAI@ AAAI (2019)\n17. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning\nmodels resistant to adversarial attacks. In: International Conference on Learning\nRepresentations (2018)\n18. Naseer, M., Khan, S., Porikli, F.: Local gradients smoothing: defense against local-\nized adversarial attacks. In: 2019 IEEE Winter Conference on Applications of Com-puter Vision (WACV), pp. 1300–1307 (2019)\n19. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object\ndetection with region proposal networks. In: Proceedings of the 28th International\nConference on Neural Information Processing Systems-Volume 1, pp. 91–99 (2015)\n20. Song, D., et al.: Physical adversarial examples for object detectors. In: 12th\nUSENIX Workshop on Oﬀensive Technologies (WOOT 2018) (2018)\n21. Szegedy, C., et al.: Intriguing properties of neural networks. arXiv preprint\narXiv:1312.6199 (2013)\n22. Thys, S., Van Ranst, W., Goedem´ e, T.: Fooling automated surveillance cameras:\nadversarial patches to attack person detection. In: Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition Workshops (2019)\n23. Tian, Z., Shen, C., Chen, H., He, T.: FCOS: fully convolutional one-stage object\ndetection. In: Proceedings of the IEEE/CVF International Conference on Com-\nputer Vision, pp. 9627–9636 (2019)\n24. Wu, D., Xia, S.T., Wang, Y.: Adversarial weight perturbation helps robust gener-\nalization. Advances in Neural Information Processing Systems 33 (2020)\n25. Wu, Z., Lim, S.-N., Davis, L.S., Goldstein, T.: Making an invisibility cloak: real\nworld adversarial attacks on object detectors. In: Vedaldi, A., Bischof, H., Brox,T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12349, pp. 1–17. Springer, Cham\n(2020). https://doi.org/10.1007/978-3-030-58548-8\n1\n26. Xie, C., Wang, J., Zhang, Z., Zhou, Y., Xie, L., Yuille, A.: Adversarial exam-\nples for semantic segmentation and object detection. In: Proceedings of the IEEE\nInternational Conference on Computer Vision, pp. 1369–1378 (2017)\n27. Xu, K., et al.: Adversarial T-shirt! Evading person detectors in a physical world.\nIn: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol.12350, pp. 665–681. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-\n58558-7\n39\n28. Zhang, H., Wang, J.: Towards adversarially robust object detection. In: Proceed-\nings of the IEEE International Conference on Computer Vision, pp. 421–430 (2019)\n29. Zhang, H., Yu, Y., Jiao, J., Xing, E., El Ghaoui, L., Jordan, M.: Theoretically\nprincipled trade-oﬀ between robustness and accuracy. In: International Conferenceon Machine Learning, pp. 7472–7482. PMLR (2019)\n30. Zhao, Y., Zhu, H., Liang, R., Shen, Q., Zhang, S., Chen, K.: Seeing isn’t believing:\ntowards more robust adversarial attack against real world object detectors. In: Pro-\nceedings of the 2019 ACM SIGSAC Conference on Computer and Communications\nSecurity, pp. 1989–2004 (2019)', 'IPE Transformer for Depth Completion\nwith Input-Aware Positional Embeddings\nBocen Li1, Guozhen Li1, Haiting Wang1, Lijun Wang1(B), Zhenfei Gong2,\nXiaohua Zhang1, and Huchuan Lu1\n1Dalian University of Technology, Dalian, China\nljwang@dlut.edu.cn\n2Guangdong OPPO Mobile Telecommunications Corp. Ltd., Dongguan City, China\nAbstract. In contrast to traditional transformer blocks using a set of pre-\ndeﬁned parameters as positional embeddings, we propose the input-aware\npositional embedding (IPE) which is dynamically generated according to\nthe input feature. We implement this idea by designing the IPE trans-former, which enjoys stronger generalization powers across arbitrary input\nsizes. To verify its eﬀectiveness, we integrate the newly-designed trans-\nformer into NLSPN and GuideNet, two remarkable depth completion net-works. The experimental result on a large scale outdoor depth completion\ndataset shows that the proposed transformer can eﬀectively model long-\nrange dependency with a manageable memory overhead.\nKeywords: Transformer\n·Positional embeddings ·Depth completion\n1 Introduction\nDepth information implies the geometric relationships, but is lost during the\nprojection from real word 3D scenes to 2D images. Depth estimation aims to\nrecover the 3D geometric relationships, providing richer perceptual cues of the3D environment, and thus plays an important role in downstream applications,\nlike 3D reconstruction, autonomous driving, etc.\nThe acquisition of depth values often requires speciﬁc sensors, e.g., Lidar and\nKinect. However, the depth maps produced by these hardware sensors are often\nextremely sparse, especially under outdoor scenarios. For instance, in the large-\nscale autonomous driving data set KITTI [ 1], the accurate depth values after\nscreening and veriﬁcation often only account for 35% of the total number of pix-\nels in the entire image, which is far from the demand of real-word applications.To address this issue, a number of depth completion algorithms have been pro-\nposed in the literature, which are able to reconstruct the dense depth map using\nonly 5% of the valid depth values delivered by hardware sensors. Though muchprogress has been achieved, it is currently still a challenging task and attracts\nprogressively more attention from the community.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 263–275, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_22', '264 B. Li et al.\nWith the development of deep learning techniques, fully convolutional net-\nworks (FCNs) [ 2] have achieved impressive performance in many pixel-level pre-\ndiction tasks, like semantic segmentation, saliency detection, etc. Its tremendous\nlearning capabilities have also been successfully transferred to the depth com-\npletion area by recent works [ 3–7] which surpass traditional methods with a\nsigniﬁcant margin. As one of the key advantages, FCNs can handle input images\nwith arbitrary sizes due to its fully convolutional architecture, serving as an\neﬃcient solution for pixel-level tasks.\nThere is also a recent surge of interests in exploring transformers [ 8–12]\nfor vision tasks. Compared to FCNs, transformers can better model long-range\ndependencies, which enables the network to render more accurate depth comple-tion results based on the relationships among pixels with/without valid depth\nvalues. Nonetheless, most of the traditional transformers [ 11–13] employ a set\nof pre-deﬁned parameters as the positional embeddings. As a consequence, they\nare only applicable to input images with a pre-ﬁxed size, and are shown to suﬀer\nfrom performance degeneration when input sizes at training and inference areinconsistent. In light of this issue, we propose to generate the positional embed-\nding adaptively based on the input feature. Based on this idea, we design a new\ntransformer with input-aware positional embeddings. As opposed to traditionaltransformers, ours is able to eﬀectively tackle input images with arbitrary sizes\nwithout performance drop. By integrating our newly designed transformer with\nFCN networks, we combine the best of both worlds, i.e., eﬀective long-rangedependency modeling and high ﬂexibility in handling arbitrary input sizes. To\nverify its eﬀectiveness, we apply the proposed transformer to the depth comple-\ntion task, yielding promising results.\nThe major contribution of this paper can be summarized as follows:\n(1) We propose a new method to generate positional embeddings, which enables\ntransformers to better generalize across arbitrary input sizes.\n(2) We implement the above idea by designing a new transformer with input-\naware positional embeddings, which can be easily plugged into FCN archi-\ntectures and shows superior performance in the depth completion task.\nTo the best of our knowledge, we are the ﬁrst to apply Transformer to\nthe depth completion task. Extensive experiments on a large scale benchmark\ndataset demonstrate the eﬀectiveness of our proposed method.\n2 Related Work\nMSG-CHN [ 7] addresses the need for global or semi-global information in depth\ncompletion by utilizing multi-scale Hourglass [ 14] structure to extract features,\nso that the feature map contains large-scale information and ﬁne-grained local\ninformation. SPN (Spatial Propagation Network) [ 15] proposed that image A\ncan be transformed by Laplace matrix to obtain image B, and this transfor-\nmation process is standard anisotropic diﬀusion process. Compared with image\nA, B has clearer object boundary, less noise, and stronger similarity within the', 'IPE Transformer for Depth Completion 265\nsame object, which is the same demand in dense depth map. The article [ 16]\nuses spatial propagation to further smooth the depth of the same target andhighlight the edges of diﬀerent targets. This propagation is achieved by spread-\ning information from pixels with known depth value to their surrounding regions\naccording to aﬃnity matrix between any pixel and its eight neighbors. Based onthat, CSPN++ [ 17] introduces multi-scale aﬃnity matrix to tackle the problems\nof small receptive ﬁeld of the aﬃnity matrix in CSPN [ 16], resulting in more\naccurate dense depth.\nNLSPN [ 18] believes that CSPN and CSPN++ calculate aﬃnity matrix\nwithin a ﬁxed neighbor region is not conducive for the network to learn more\nﬂexible features within a local region, and replace convolution with deformableconvolution [ 19,20] in spatial propagation process. Meanwhile it introduces a new\nlocal normalization method, which aims to further strengthen the relationship\nbetween features of a point and its neighbor region.\nTransformer was ﬁrst proposed by the Google team [ 21] and applied to nat-\nural language processing. Later it was gradually applied to computer vision.ViT [22] and DETR [ 23] use transformer blocks after a convolutional encoder\nto extract global features, which are applied to image classiﬁcation and object\ndetection tasks, respectively. In addition, self-attention is an important part ofthe transformer block. Prajit Ramachandran et al. [ 24] uses this attention mech-\nanism to construct a fully self-attention network and achieves better results than\nconvolutional networks in object detection. For pixel-level tasks, such as segmen-tation, SETR [ 25] follows ViT and cuts the input image into several patches,\nand these patches are then fed into the transformer blocks as tokens. Beyond\nthat, Swin [ 8] divides the input image into more patches, performs self-attention\nwithin each patch, and uses shifted patch partition to model the relationship\namong patches.\nPositional embedding, as a set of pre-deﬁned learnable parameter in recent\nvision transformers, plays an important role in modeling spatial information and\nacts as coordinate index. It has diﬀerent names such as relative index, positional\nencoding or relative position and so on. Diﬀerent transformers treat it as diﬀerent\ncomponent. Such as in Swin, position encoding is a pair-wise parameters for\neach token inside the window, and is directly added to relation map in Eq. 1.I n\nViT, positional embedding is a pair of patch-level parameters interpreting which\npatch is more likely to contain useful information. Besides, it is added to patch\ntokens directly, which is diﬀerent Swin. And in axial-attention [ 9], relation map\nis multiplied by positional encoding. In another word, positional embeddings in\naxial-attention act more like weight but in Swin and ViT more like bias.\nBut we should notice that a set of predeﬁned parameters will cause the\nfollowing drawbacks:\n(1) if shape of input images changes, common method to handle with the shape\nof positional encoding is resize [ 10,22], which will cause artifact.\n(2) A set of pre-deﬁned parameter can model distribution of spatial information\nin data set well, but is shared for all images which harms the ability to model\nspeciﬁc object distribution.', '266 B. Li et al.\nIn summary, taking these factors into account, we propose a new transformer\nwith input-aware positional embedding, named IPE transformer, which can featthe variation of input size and get better result. Besides, it can be embedded\ninto any other network.\n3M e t h o d\nIn this section, we ﬁrst introduce the idea of generating positional embedding\naccording to input feature, and then explain how to incorporate it into transform-ers, leading to transformers with Input-aware Positional Embeddings (called IPE\ntransformers). Finally, we apply the IPE transformers to the depth completion\ntask.\n3.1 Input-Aware Positional Embedding\nThe core component of transformer block is self-attention module. This module\naims to compute attention among every pixel in input feature map, which can\nalso be interpreted as relation or aﬃnity matrix among all pixels. The basic ﬂow\nof self-attention mechanism is to calculate relation map among pixels using K\nandQderiving from input feature map Xand then to perform matrix mul-\ntiplication between relation map and V. This procedure can be described as\nfollows:\nR(X,X)=φ(conv\nK(X),conv Q(X)) (1)\nφ(A,B)=softmax (ABT) (2)\nA(X)=R(X,X)conv V(X) (3)\nout=FFN(attention (X)) (4)\nm=conv m(X),m=Q,K,V (5)\nwhere conv maims to calculate Key, Query and Value from the input tensor X\nin convolution way repectively and conv mis implemented as 1 ×1 convolution.\nTheR(X,X) is the realtion map and can be regarded as similarity matrix or\naﬃnity matrix among pixels. For example, given two point iandjinX,t h e\ncorresponding point ( i,j)i nR(X,X) matrix means how close this point pair is.\nAs for A(X), it can be seen as the weighted Xusing the relation map calculated\nby Eq. 1.FFN(·) is a feed forward network to further fuse the result.\nPositional embedding can be learned from dataset during training process,\nand acts more like prior knowledge during inference and test process. Considering\nits drawbacks discussed above, we propose a new method to calculate positionalembedding dynamically according to the input feature.\nConvolution can keep spatial information in feature map according to pixel\nposition even in the deepest level. So we conjecture that it do not have to learn a\nset of parameters additionally, and we can get spatial information directly from\nfeature map. On the basis of this idea, given a feature map X∈R\nB×C×H×W,', 'IPE Transformer for Depth Completion 267\nwe can get spatial information directly after 1 ×1 convolution using Eq. ( 6),\nwhich can be seen as positional embedding.\n∗embed =Conv1×1∗embed(X),∗=Q,K,V (6)\nUsing Eq. ( 5)a n dE q .( 6), we can get Q,K,V and its corresponding positional\nembedding. Q,Q embed andK,K embed is with shape of RB×C×H×WbutV,V embed\nwith the shape of RB×2C×H×W.\nThe input tensor is with shape of RB×C×L, where L=H×W, which can\nbe regarded as number of tokens. In the other self-attention modules, their posi-\ntional embedding is with shape of RC×L×Lwhich can be added to relationship\nmap by using unsqueeze operation on batch dimention or added to token fea-ture directly. In our input-aware positional embedding, it has the same shape\nwith their counterpart ∈R\nB×C×L, which means diﬀerent images have their own\nspeciﬁc positional embedding and can model their unique spatial informationbetter.\n3.2 IPE Transformer\nTraditional self-attention module tends to ﬂatten the input tensor with shape\nR\nB×C×H×WintoRB×C×L, which will cost extra memory to ﬁnish this calcu-\nlation. One way to alleviate this notorious out-of-CUDA-memory problem is tocut an image into several patches [ 8,22,25], and the other way axial attention [ 9].\nIn axial attention, it does not calculate the relationship between one point\nand other points directly, but reaches this goal in two steps, calculating therelationship of one point among its corresponding row and column sequentially,\nnamely width step and height step respectively. As shown in Fig. 1, information\nwill propagate from A to C in two step, from A to B in column direction andthen from B to C in row direction.\nFig. 1. An example of how information propagates from one point to another.\nThe input of axial-attention block is diﬀerent from other self-attention mod-\nules. Assuming that X∈RB×C×H×Wis the input tensor and then ﬂattened into\nshape of X/prime∈RB×C×(H×W), where B,C,H,Wis batch size, number of channels,\nheight and width of the input feature map. In axial-attention, the input tensor\nis re-organized into shape of R(B×H)×C×WorR(B×W)×C×Hfor width step or', '268 B. Li et al.\nheight step respectively. In another word, in width step or height step, diﬀer-\nent rows or columns are treated as diﬀerent image and concatenated in batchdimension.\nDue to its eﬃciency and eﬀectiveness, we design our IPE transformer based\non the axial attention structure [ 9]. In order to produce more positional-sensitive\nrelation map in Eq. ( 1), we combine our input-aware positional embeddings\nwith axial-attention not by directly addition but matrix multiplication between\nQ,K,V and their corresponding positional embeddings which has the same\nshape of its counter part, according to Eq. 7.\nattention =softmax (θ(K,K\nembed)+θ(Q,Q embed)+θ(K,Q))(V+Vembed)\n(7)\nwhere θ(A,B) denotes matrix multiplication, which can be re-written as follows:\nθ(A,B)b,i,j=ABT\nb,i,j=/summationdisplay\ncAb,c,i·Bb,c,j (8)\nFor any point ( b,i,j)i nθ(A,B)∈R(B×W×W)orRB×H×Hfor width step or\nheight step respectively, we can calculate its value by multiplying corresponding\npoint in AandBimage by image. In this formulation, we can model pixel’s\nrelationship in arbitrary image shape and pay more attention to unique object\ndistribution in every diﬀerent image.\nIn order to feat the variation of input size, in our IPE transformer, positional\nencoding is generated from input feature map. As shown in Fig. 2,t h er e db o x\nrepresents our IPE transformer and the yellow box represents one step of this\ntwo step mechanism talked above. Every step has two input, one of which is\nall-embeddings that contains diﬀerent positional embedding for K,Q and V, and\nthe other is used to calculate K,Q and V that comes from the input tensor or laststep result. Besides, we use the same positional embedding for both width step\nand height step to keep that these two step share the same spatial information.\n3.3 Network Structure\nIn this section, we will show the eﬀectiveness of our attention module and com-\nbine it with NLSPN [ 18] network. The core component in NLSPN is spatial\npropagation module. It uses aﬃnity information which can be learned from RGBto propagate known information to the unknown.\nIn this procedure, aﬃnity matrix model local information within a small\nregion which results in losing long range generality. Our IPE transformer canhelp this procedure to capture long range information.\nThe network structure is showed in Fig. 3. RGB and sparse depth are concate-\nnated and then fed into encoder-decoder structure. Owing to limited memory\ncondition, we embed our IPE transformer into the middle of this structure to\nmodel global information among all pixels in one image and its sparse depth.After that, there are three part in the output of decoder, namely initial depth,\naﬃnity matrix and conﬁdence. In this network, spatial propagation takes aﬃnity', 'IPE Transformer for Depth Completion 269\nFig. 2. Structure of IPE transformer. ⊗denotes matrix multiplication and ⊕denotes\nelemental-wise summation. Firstly, the input tensor Xwith shape of RB×H×C×W\nis fed into 1 ×1 convolution and then re-organized into shape of R(B×H)×C×Wor\nR(B×W)×C×Hfor width step or height step respectively. Q, K and their corresponding\npositional embedding are in this shape, but V and its embedding has 2 Cchannels.\n(Color ﬁgure online)\nFig. 3. Structure of NLSPN combined with IPE transformer', '270 B. Li et al.\nmatrix and conﬁdence as input to reﬁne initial depth iteratively. At last, we will\nget a dense depth map.\nFurthermore, owing to our module’s convenience, it can be embedded into\nany other encoder-decoder structure.\n4 Experiment\nIn order to verify eﬀectiveness of our proposed transformer, we combine it with\nNLSPN [ 18] and GuideNet [ 5], apply them to the depth completion task. This\nsection will present the detailed experimental results.\n4.1 Setup\nDataset. We train our network on KITTI dataset, which contians 86 kimages\nwith 35% and 5% valid points in groundtruth and sparse depth respectively.\nThe image size is 375 ×1242, but top-cropped 96 pixels and random cropped\ninto 256 ×1216 during training. As for testing, image is of size 352 ×1216 and\ntop-cropped 96 pixels before fed into network.\nMetrics. Following prior works, we choose Root Mean Square Error (RMSE),\nMean Square Error (MAE), Root Mean Square Error of the inverse depth\n(iRMSE) and inverse Mean Square Error of the inverse depth (iMAE) as evalu-ation metrics to verify our module’s eﬀectiveness and ﬂexibility.\nImplementation Details. We train our NLSPN with IPE-transformer\n(NLSPN-IPE) on KITTI dataset for 40 epochs. Initial learning rate is 0.001,\nand it will drop to 4 e−4, 1.6e−4, 6.4e−5 at 15, 25 and 35 epoch. Following\nNLSPN, RMSE and MAE are choosen as our loss funciton and their weight is the\nsame as shown in Eq. ( 9). It should be noticed that, all of networks are trained\nwith top-crop but tested with (marked as -crop in the following experiment) andwithout top-crop.\nLoss=1.0×RMSE (pred,gt )+1.0×MAE (pred,gt ) (9)\nIn ablation study, we use learning rate with 0.001, and train those networks\nfor 10 epochs under the same learning rate and using the same loss function\ntalked above. It should be noticed that we train all of the networks with the\nsame setting for convenience, even if it is diﬀerent from the settings original inpaper.', 'IPE Transformer for Depth Completion 271\n4.2 Experiment Results\nIn order to enlarge receptive ﬁeld, traditional convolution neural network stacks\nmore convolution layers, which means long-range dependency is more likely\noverwhelmed in deeper structure by local information. Transformer uses self-\nattention to model global information directly, and can model large or longobject’s feature more easily and accurately.\nAs shown in Fig. 4, the ﬁrst row is input image with shape of 352 ×1216. The\nprediction is separated into two part. The input size in blue box is 352 ×1216,\nand 256 ×1216 in the red one which is top cropped for 96 pixels before fed into\nnetwork. In blue box, object depth in yellow box is more complete and reasonable\nwith our IPE transformer than the other network, which denotes that long-rangedependency can help networks to learn and infer object feature better.\nTable 1. Result of NLSPN and NLSPN-IPE on KITTI selective validiton set\nRMSE (mm) MAE (mm) iRMSE (1/km) iMAE (1/km)\nNLSPN 771.81 197.33 2.03 0.83\nGuideNet 1007.47 251.05 2.74 1.07\nNLSPN-IPE 769.47 199.88 2.05 0.85\nNLSPN-crop 771.25 197.24 2.03 0.83\nGuideNet-crop 777.11 221.29 2.33 0.99\nNLSPN-IPE-crop 768.88 199.45 2.04 0.84\nIn order to verify our module’s ability of handling variation of diﬀerent input\nsize, we conduct a comparison experiment of diﬀerent input size while testing,\n352×1216 and 256 ×1216 respectively, but with same size of 256 ×1216 while\ntraining. The result is shown in red box. At the boundary of this trunk, both\nNLSPN and GuideNet do not perform as well as our IPE transformer given larger\ninput size while testing. In another word, our IPE transformer can help networksto model relationship within longer distance, which will beneﬁt handling with\nthe problem of variation of input size. Quantitative result is shown in Table 1.\nWith help of our IPE transformer, it further improves the accuracy of modelingfeature.\n4.3 Ablation Study\nIn this section, we verify the eﬀectiveness and ﬂexibility of our IPE transformer.\nWe train four diﬀerent models, NLSPN, NLSPN with IPE transformer (NLSPN-\nIPE), GuideNet and GuideNet with IPE transformer (GuideNet-IPE). We sim-\nply embed our module into the middle of encoder-decoder structure of NLSPNand GuideNet to implement their IPE transformer counterpart. The training\nprocess use the setting talked above. Quantitative result is shown in Table 2.I t\nproves that our module is eﬀective and easy to embed into other network.', '272 B. Li et al.\nFig. 4. Experiment result. (a), (b) and (c) are GuideNet, NLSPN, NLSPN-IPE respec-\ntively (Color ﬁgure online)\nTable 2. Result of NLSPN and NLSPN-IPE on KITTI selective validiton set\nRMSE (mm) MAE (mm) iRMSE (1/km) iMAE (1/km)\nNLSPN 809.98 207.52 2.23 0.87\nNLSPN-IPE 808.18 207.31 2.25 0.87\nGuideNet 824.50 236.69 2.75 1.03\nGuideNet-IPE 819.35 235.80 2.72 1.03', 'IPE Transformer for Depth Completion 273\nTable 3. Result of diﬀerent transformers\nRMSE (mm) MAE (mm) iRMSE (1/km) iMAE (1/km)\nNLSPN 809.98 207.52 2.23 0.87\nNLSPN-Swin 814.62 209.25 2.30 0.88\nNLSPN-ViT 819.14 210.55 2.33 0.89\nNLSPN-AA 812.67 209.03 2.31 0.91\nNLSPN-IPE 808.18 207.31 2.25 0.87\nThen we combine diﬀerent transformers with NLSPN by embedding these\nmodules into the middle of encoder-decoder structure and experiment result\nis shown in Table 3. These transformers include axial-attention, Swin, ViT and\nmarked with AA, Swin and ViT respectively. Compared with other transformers,\nour IPE transformer shows superiority over the others. Swin needs to pad the\ninput feature to feat the change of input size but without resizing positionalembeddings. ViT and Axial-attention need to resize positional embedding to\nappropriate size. Our IPE transformer is input-aware and can generate positional\nembedding dynamically. We conjecture that this is the reason why our modelcan bypass the others.\n5 Conclusion\nIn this paper, we propose a new method to generate input-aware positional\nembeddings and design a new transformer with it, named IPE transformer. Then\nwe embed this transformer into NLSPN and GuideNet, remarkable networks indepth completion. Comparison experiment and ablation study show that our\nproposed IPE transformer can model long-range dependency better and is easy\nto transfer to other network by simply embedding it into the middle of encoder-decoder structure.\nAcknowledgement. We thank all editors and reviewers for their helpful sugges-\ntions. This work is supported by National Natural Science Foundation of China(No.61906031), and Fundamental Research Funds for Central Universities (No.\nDUT21RC(3)025).\nReferences\n1. Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: The KITTI\ndataset. Int. J. Robot. Res. 32(11), 1231–1237 (2013)\n2. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 3431–3440 (2015)', '274 B. Li et al.\n3. Hu, M., Wang, S., Li, B., Ning, S., Fan, L., Gong, X.: Penet: towards precise and\neﬃcient image guided depth completion. arXiv preprint arXiv:2103.00783 (2021)\n4. Liu, L., et al.: FCFR-net: feature fusion based coarse-to-ﬁne residual learning for\nmonocular depth completion. arXiv preprint arXiv:2012.08270 (2020)\n5. Tang, J., Tian, F.P., Feng, W., Li, J., Tan, P.: Learning guided convolutional\nnetwork for depth completion. IEEE Trans. Image Process. 30, 1116–1129 (2020)\n6. Zhao, S., Gong, M., Fu, H., Tao, D.: Adaptive context-aware multi-modal network\nfor depth completion. arXiv preprint arXiv:2008.10833 (2020)\n7. Li, A., Yuan, Z., Ling, Y., Chi, W., Zhang, C.: A multi-scale guided cascade hour-\nglass network for depth completion. In: The IEEE Winter Conference on Applica-\ntions of Computer Vision, pp. 32–40 (2020)\n8. Liu, Z., et al.: Swin transformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint arXiv:2103.14030 (2021)\n9. Wang, H., et al.: Axial-DeepLab: stand-alone axial-attention for panoptic segmen-\ntation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020.\nLNCS, vol. 12349, pp. 108–126. Springer, Cham (2020). https://doi.org/10.1007/\n978-3-030-58548-8 7\n10. Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction.\narXiv preprint arXiv:2103.13413 (2021)\n11. Li, S., Sui, X., Luo, X., Xu, X., Liu, Y., Goh, R.S.M.: Medical image segmentation\nusing squeeze-and-expansion transformers. arXiv preprint arXiv:2105.09511 (2021)\n12. Wang, Y., et al.: End-to-end video instance segmentation with transformers. arXiv\npreprint arXiv:2011.14503 (2020)\n13. Wu, B., et al.: Visual transformers: Token-based image representation and process-\ning for computer vision. arXiv preprint arXiv:2006.03677 (2020)\n14. Newell, A., Yang, K., Deng, J.: Stacked Hourglass Networks for Human Pose Esti-\nmation. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) Computer Vision,\nECCV 2016. ECCV 2016. LNCS, vol. 9912, pp. 483–499. Springer, Cham (2016).\nhttps://doi.org/10.1007/978-3-319-46484-8 29\n15. Liu, S., De Mello, S., Gu, J., Zhong, G., Yang, M.H., Kautz, J.: Learning aﬃnity\nvia spatial propagation networks. In: Advances in Neural Information Processing\nSystems, pp. 1520–1530 (2017)\n16. Cheng, X., Wang, P., Yang, R.: Learning depth with convolutional spatial propa-\ngation network. arXiv preprint arXiv:1810.02695 (2018)\n17. Cheng, X., Wang, P., Guan, C., Yang, R.: Cspn++: Learning context and resource\naware convolutional spatial propagation networks for depth completion. In: AAAI,\npp. 10615–10622 (2020)\n18. Park, J., Joo, K., Hu, Z., Liu, C.K., Kweon, I.S.: Non-local spatial propagation\nnetwork for depth completion. arXiv preprint arXiv:2007.10042 3(8) (2020)\n19. Dai, J., et al.: Deformable convolutional networks. In: Proceedings of the IEEE\nInternational Conference on Computer Vision, pp. 764–773 (2017)\n20. Zhu, X., Hu, H., Lin, S., Dai, J.: Deformable convnets v2: more deformable, better\nresults. In: Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp. 9308–9316 (2019)\n21. Vaswani, A., et al.: Attention is all you need. Adv. Neural Inf. Proces. Sys. 30,\n5998–6008 (2017)\n22. Dosovitskiy, A., et al.: An image is worth 16x16 words: transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929 (2020)', 'IPE Transformer for Depth Completion 275\n23. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\nto-end object detection with transformers. arXiv preprint arXiv:2005.12872 (2020)\n24. Parmar, N., Ramachandran, P., Vaswani, A., Bello, I., Levskaya, A., Shlens, J.:\nStand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909 (2019)\n25. Zheng, S., et al.: Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv preprint arXiv:2012.15840 (2020)', 'Enhanced Multi-view Matrix\nFactorization with Shared Representation\nSheng Huang, Yunhe Zhang, Lele Fu, and Shiping Wang(B)\nCollege of Mathematics and Computer Science, Fuzhou University,\nFuzhou 350108, China\nAbstract. Multi-view data is widely used in the real world, and tradi-\ntional machine learning methods are not speciﬁcally designed for multi-\nview data. The goal of multi-view learning is to learn practical pat-terns from the divergent data sources. However, most previous researches\nfocused on ﬁtting feature embedding in target tasks, so researchers put\nforward with the algorithm which aims to learn appropriate patternsin data with associative properties. In this paper, a multi-view deep\nmatrix factorization model is proposed for feature representation. First,\nthe model constructs a multiple input neural network with shared hiddenlayers for ﬁnding a low-dimensional representation of all views. Second,\nthe quality of representation matrix is evaluated using discriminators to\nimprove the feature extraction capability of matrix factorization. Finally,the eﬀectiveness of the proposed method is veriﬁed through comparative\nexperiments on six real-world datasets.\nKeywords: Deep learning\n·Matrix factorization ·Shared\nrepresentation ·Multi-view clustering\n1 Introduction\nIn recent decades, matrix factorization has been widely used in image process-\ning [2], clustering analysis [ 12] and recommendation systems [ 10]. Nowadays\ncomputers need to process a large amount of data, and it becomes crucial toﬁnd the hidden low-dimensional representation in the huge amount of data.\nEarlier, many kinds of matrix decomposition methods have been proposed, such\nas singular value decomposition (SVD). Another approach, called non-negativematrix factorization (NMF), is classical and usually used for non-negative inputs.\nAfter that, more extensions were proposed and a typical example is convex and\nsemi-nonnegative matrix factorization [ 3].\nThe ﬁrst author is a student. This work is in part supported by the National Natural\nScience Foundation of China (Grant No. U1705262), the Natural Science Foundation\nof Fujian Province (Grant Nos. 2020J01130193 and 2018J07005).\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 276–287, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_23', 'Enhanced Multi-view Matrix Factorization 277\nIn order to improve the feature extraction capability of matrix factorization,\ndeep matrix factorization is proposed [ 9] to obtain strong feature characteri-\nzation ability by successive decomposition of representation matrix. Previous\ndeep matrix factorization methods are optimized by matrix iteration, and it is\nnecessary to design a new computational method speciﬁcally for diﬀerent restric-tions [ 15]. This situation leads to the dilemma of designing distinct algorithms for\ndiﬀerent constraints, which constitutes a great challenge for the practical use of\nmatrix factorization. Thus, this inspires us to explore a method to make matrixfactorization general. Deep learning is a generalized machine learning method\nthat aims to discover rich hierarchical features, which inspires us to combine\ndeep learning and matrix decomposition to construct a generalized algorithm tosolve the matrix factorization problem.\nGenerative Adversarial Networks (GAN) is a network consisting of a genera-\ntor and a discriminator [ 4], which improves the generative ability of the generator\nand the discriminative ability of the discriminator through an adversarial pro-\ncess [7]. The discriminator has a superior power on the generating ability of the\ngenerator. Inspired by the above, we believe that the discriminator will have a\nlarge improvement on the matrix factorization problem.\nFig. 1. The architecture of the proposed model. Our model uses an identity matrix as\nthe input of the model, and the representation matrix is generated by the encoders,which can be analogized to the objective. Then, the shared representation matrix is\nprojected diﬀerently to each view reconstruction data. θ(·) is the ReLU nonlinear acti-\nvation function.\nIn this paper, we propose a multi-view deep matrix factorization model with\nenhanced discriminators. The framework diagram of this model is shown inFig.1. First, our model uses a shared representation layer. And a multi-layer\nneural network is employed to enhance the feature extraction capability. We\nproject each layer of the network onto a diﬀerent space to achieve a nonlinear', '278 S. Huang et al.\nfeature decomposition and use projected gradient descent to optimize the model.\nThen, in order to map the shared representation layers onto the space of diﬀer-ent perspectives, various multi-layer of nonlinear projection matrices are used in\nthe model, and we optimize the shared representation by reconstruction errors.\nFinally, we use a set of discriminators to optimize the generation of shared repre-sentation. These reconstructed data not only calculate the reconstruction error\nwith the original data, but also are transmitted to the discriminator, thereby\noptimizing the representation matrix and obtaining a better shared representa-tion. In the meantime, the discriminators are also trained. The proposed method\nis compared with several outstanding multi-view clustering algorithms to verify\nits performance, and we perform ablation studies to validate the eﬀect of eachmodule. In summary, our main contributions are as follows:\n– Use a neural network to solve the matrix factorization, which builds a bridge\nbetween deep learning and traditional matrix factorization.\n– Construct a shared representation matrix to extract features from multi-view\ndata matrices.\n– Apply discriminators to matrix decomposition tasks to improve the feature\nextraction capability.\n2 Related Work\nWe ﬁrst introduce some classic matrix factorization methods. An original datamatrix is denoted as X=[x\n1,···,x n]T∈Rn×d, where nis the number of data\npoints and dis the number of features. The objective function of a basic form\nof matrix factorization can be written as\nmin\nH,VLMF=/vextenddouble/vextenddouble/vextenddoubleX−HVT/vextenddouble/vextenddouble/vextenddouble2\nF, (1)\nwhere H∈Rn×kand V∈Rd×k. Here kis the dimension number to be reduced,\nandkis usually much smaller than min {n,d}. This is a simple decomposition,\nand some researchers have extended it to additional forms of constraints. NMF\nis a method that takes a non-negative matrix as input and decomposes it into\ntwo non-negative matrices. Under the non-negative constraints, the optimization\nobjective of NMF can be written as the following minimization problem\nmin\nH,VLNMF=/vextenddouble/vextenddouble/vextenddoubleX−HVT/vextenddouble/vextenddouble/vextenddouble2\nF,\ns.t. H≥0,V≥0. (2)\nTo enhance the feature extraction capability of matrix decomposition and\ndue to the non-linear property caused by the non-negative constraint, matrix\nfactorization is extended to structures with more hierarchies, named deep matrixfactorization. The deep matrix factorization takes the form as', 'Enhanced Multi-view Matrix Factorization 279\nX≈H1VT\n1,\nX≈H2VT\n2VT\n1,\nX≈H3VT\n3VT\n2VT\n1,\n...\nX≈HlVT\nl...VT\n3VT\n2VT\n1, (3)\nwhere Hiis the representation matrix of the output of i-th layer, which is decom-\nposed from Hi−1. To associate with the NMF, here we make Hi≥0. But in\npractice, it is not necessary to force Hlto be a non-negative matrix. Vican be\nconsidered as a projection matrix that maps the representation matrix Hiinto\nthe original space. By continuously decomposing the representation matrix, thematrix factorization can extract more potential features.\n3 Proposed Method\n3.1 Multi-view Matrix Factorization with Shared Representation\nFirst of all, we deﬁne the symbols for ease of explanation. Given multi-view\ndataX={X(1),..., X(p),..., X(P)}andPis the total number of views. Here\nX(p)∈Rn×dpis the input data of the p-th view. For each view of data X(p),a n\nintuitive idea is to decompose them into two matrices groups Hand V.\nBut this is not our ultimate goal, for that is actually not fundamentally dif-\nferent from the single-view methods, and data of each view is still not correlatedwith each other as independent individuals. So we associate that the essence of\nmulti-view data is to collect information from the same object in diﬀerent views.\nEach view should have the same representation in the potential space, therefore,we decompose the data matrices of diﬀerent views into a common representation\nmatrix as the following equations\nX\n(1)≈HlV(1) T\nl...V(1) T\n3V(1) T\n2V(1) T\n1,\n...\nX(P)≈HlV(P)T\nl...V(P)T\n3 V(P)T\n2 V(P)T\n1,\ns.t. H(p)\ni≥0,i∈[1,2,...,l −1], (4)\nwhere Hlis the shared representation of all views. They can be reconstructed to\nthe source data of each view respectively with diﬀerent perspectives under cor-\nresponding mappings of the projection matrices V(p)=V(p)\n1V(p)\n2V(p)\n3...V(p)\nl.\nTo solve this matrix factorization problem, we formulate the objective function\nas the following\nLR=P/summationdisplay\np=1dist/parenleftBig\nX(p),HV(p)T/parenrightBig\n,\ns.t. H(p)\ni≥0,i∈[1,2,...,l −1], (5)', '280 S. Huang et al.\nwhere dist(·,·) means a distance function that can be instantiated to a speciﬁc\nfunction, such as mean square error or binary cross entropy. This is a minimiza-tion problem that aims to make the reconstructed data matrices approach the\nsource matrices as closely as possible. Thus, this allows the proposed model to\nautomatically learn a shared representation and extract the features of all viewsinto a uniﬁed matrix.\nTo solve the above optimization problem, we can use an auto-encoder to\ninstantiate the problem. An auto-encoder is a model that can map features toother spaces. Its mathematical expression is ˆX=g(f(X)) that f(·) is encoder,\ng(·) is decoder, Xis the input data and ˆXis the output. It expressed in terms\nof matrix is ˆX=XED\nTwhere Eis the weighted matrix of encoder and Dis\nthe weighted matrix of decoder. We can use an identity matrix as the input tothe auto-encoder, so that the formula is ˆX=IED\nT=EDT. In this way, we\nconvert the matrix decomposition problem into an auto-encoder model.\n3.2 Enhancement of Feature Extraction Capability\nIn order to enhance the feature extraction ability of matrix factorization, we use a\ndiscriminator module to evaluate the matrices reconstructed by matrix decompo-\nsition. In our model, matrix decomposition not only contains the reconstruction\nerror, but also introduces the discriminators to investigate the quality of recon-struction. The discriminators force our model to extract more representative\nfeature to deceive the discriminators.\nIn our model, the p-th discriminator is D\np(ˆX(p)). Each reconstructed matrix\nhas an independent discriminator to judge separately. The aim of the discrimina-\ntors is to make each reconstructed matrix be judged as false, and each originaldata matrix is judged as true. In this way, the matrix factorization model is\nprompted to ﬁnd better features to reconstruct the data, so as to improve the\naccuracy. Typically, the loss function of the discriminators is\nL\nDis=P/summationdisplay\np=1/parenleftBig\nEx(p)∼preal/bracketleftBig\nln/parenleftBig\nDp/parenleftBig\nx(p)/parenrightBig/parenrightBig/bracketrightBig\n+Eˆx(p)∼pae/bracketleftBig\nln/parenleftBig\n1−Dp/parenleftBig\nˆx(p)/parenrightBig/parenrightBig/bracketrightBig/parenrightBig\n,\n(6)\nwhere xis one of original data and ˆ xis one of reconstructed data. We deﬁne the\ndistribution of original data as prealand reconstructed data as pae. Expectation\nis continuous rather than discrete, but in practice the probability distribution\nof the data can only be obtained by sampling. So that we use the following\nobjective function to optimize the discriminators:\nLD=P/summationdisplay\np=1⎛\n⎝1\nnn/summationdisplay\nj=1ln/parenleftBig\nDp/parenleftBig\nx(p)\nj/parenrightBig/parenrightBig\n+1\nnn/summationdisplay\nj=1ln/parenleftBig\n1−Dp/parenleftBig\nˆx(p)\nj/parenrightBig/parenrightBig⎞\n⎠.(7)\nThis loss function indicates that the discriminators are trained to distinguish\nthe real from the fake. Due to the existence of the discriminators, the goal of', 'Enhanced Multi-view Matrix Factorization 281\nmatrix factorization is not only to reconstruct the matrix, but also to ﬁnd key\nfeatures in the data. This puts stronger pressure on matrix factorization andcan improve its feature extraction capability. When the reconstructed data is\nexpected to cheat the discriminators, its objective function can be represented\nas the following form\nL\nRD=−P/summationdisplay\np=1⎛\n⎝1\nnn/summationdisplay\nj=1ln/parenleftBig\nDp/parenleftBig\nˆx(p)\nj/parenrightBig/parenrightBig⎞\n⎠. (8)\nThe objective function is to make the discriminators tend to determine the recon-\nstructed data as true. In turn, the reconstruction loss term in the total loss func-\ntion ensures that the model does a better job on the premise of completing the\ntarget of matrix factorization, instead of deceiving the discriminators with trickswithout any boundary.\nIn view of the above, the objective function of our proposed model is shown\nas follows\nL\n1=LR+λdLRD, (9)\nL2=LD. (10)\nThese objective functions consist of two components: reconstruction loss L1and\ndiscriminators loss L2.λdis the parameter to adjust the inﬂuence of two loss\nterms. We summarize Algorithm 1to train our model and output the shared\nrepresentation Hl.\n4 Experiments\nIn this section, we conduct comparative experiments with other clustering algo-\nrithms to verify the eﬀectiveness of our model. The datasets and experimental\nsetups are introduced and the experimental results are analyzed.\nAlgorithm 1. Enhanced Multi-view Matrix Factorization with Shared Repre-\nsentation\nInput : Multi-view data X={X(1),..., X(p),..., X(P)}, parameter λdand the number\nof iterations maxiter.\nOutput : The shared representation H l.\n1:fort=1→maxiter do\n2: Forward propagation in the model using the identity matrix as input;\n3: Compute the reconstruction loss L1by Equation 9;\n4: Optimize the auto-encoder by backward propagation the reconstruction loss L1;\n5: Compute the discriminators loss L2by Equation 10;\n6: Optimize the discriminators by backward propagation the loss L2;\n7:end for\n8:return The shared representation H l.', '282 S. Huang et al.\n4.1 Datasets\nTo verify the eﬀectiveness of our proposed method, we perform comparative\nexperiments on six real-world datasets.\nMSRC-v11is a dataset of 210 object images from 7 classes. It contains 24-D\ncolour moments, 576-D HOG features, 512-D GIST features, 256-D local binary\npatterns, and 254-D CENTRIST features.\nWikipedia2is an article dataset that consists of 693 documents with 10\ncategories, and we extract each entry as two feature representations. They are\ntwo-view data sources come with 10-D and 128-D low-dimensional features.\nALOI3is an 1,079 images dataset of object from 10 classes. Each image is\nextracted four types of features, including 64-D RGB color histograms, 64-D HSV\ncolor histograms, 77-D color similarities, and 13-D Haralick texture features.\nNUS-WIDE4is a web images dataset of 1,600 images from 8 classes. This\ndataset is extracted as 6 views of features: 64-D color histograms, 144-D color\ncorrelograms, 73-D edge direction histograms, 128-D wavelet textures, 225-Dblock-wise color moments, and 500-D bag of words.\nMNIST\n5is a digit images dataset that contains 2,000 handwritten numbers\n‘0’-‘9’. There are six views of features in them, including 30 features of IsoPro-jection, 9 features of linear discriminant analysis, and 9 features of neighborhood\npreserving embedding.\nCaltech101\n6is an object image dataset of 9,144 images from 101 classes,\nincluding cellphone, brain, face, etc. There are about 40 to 800 images per cate-\ngory on average and most categories have about 50 images. Each image is roughlyscaled to 300 ×200 pixels.\nWe summarize these datasets in Table 1. The table contains the number of\nsamples, views, features and classes.\nTable 1. A summary of datasets\nDataset ID Datasets #S a m p l e s #V i e w s # Total features # Classes Data types\n1 MSRC-v1 210 5 1,622 7 Object image\n2 Wikipedia 693 2 138 10 Documents\n3 ALOI 1,079 4 218 10 Object image\n4 NUS-WIDE 1,600 6 1,134 8 Web image\n5 MNIST 2,000 3 48 10 Digit image\n6 Caltech101 9,144 6 3,766 101 Object image\n1http://riemenschneider.hayko.at/vision/dataset/task.php?did=35 .\n2http://lig-membres.imag.fr/grimal/data.html .\n3http://aloi.science.uva.nl/ .\n4https://lms.comp.nus.edu.sg/wp-content/uploads/2019/research/nuswide .\n5http://yann.lecun.com/exdb/mnist/ .\n6http://www.vision.caltech.edu/Image Datasets/Caltech101/Caltech101.html .', 'Enhanced Multi-view Matrix Factorization 283\n4.2 Experimental Setup\nWe will show our experimental setup in this section. First we introduce the\nclustering methods used as comparison algorithms, including a single-view clus-tering method k-means and eight multi-view clustering algorithms. They are\nMLAN [ 8] (Multi-view Learning with Adaptive Neighbours), MVKSC [ 5] (Multi-\nview Kernel Spectral Clustering), MLSSC [ 1] (Multi-view Low-rank Sparse\nSubspace Clustering), MSC-IAS [ 13] (Multi-view Subspace Clustering with\nIntactness-Aware Similarity), MCGC [ 14] (Multi-view Consensus Graph Clus-\ntering), BMVC [ 16] (Binary Multi-view Clustering), GMC [ 11] (Graph-based\nMulti-view Clustering) and AMCDMD [ 6] (Auto-weighted Multi-view Cluster-\ning via Deep Matrix Decomposition).\n(a) ALOI (b) NUS-WIDE (c) MNIST\n(d) MSRC-v1 (e) Wikipedia (f) Caltech101\nFig. 2. Convergence curves of the objective function value. The proposed method\nalmost converges within 800 iterations on each dataset.\nWe use the default parameters on the comparison algorithms and run them\non all datasets. For our proposed method, we use a fully connected layer network\nto generate the representation matrix with the neuron numbers of {1000, 500,\n200,k}. And a symmetric fully connected layer network is used as a generator.\nOur discriminators also use fully connected layers with the neuron numbers of\n{200, 100, 50, 1 }.W eu s e k-means on the representation matrix to obtain the\nclustering results.\nFor the parameter λd, we adjust the optimal value by a hyperparameter opti-\nmization method. We use the ReLU activation function to activate each layer.For all algorithms, we perform 10 repetitions of the experiments and calculate\ntheir means and standard deviations. Finally, we report three evaluation criteria,', '284 S. Huang et al.\nincluding accuracy (ACC), normalized mutual information (NMI) and adjusted\nrand index (ARI).\n4.3 Experimental Results\nIn this section, we show the detailed results of the experiments performed by\nour algorithm. First, the convergence curves of our method on the six datasetsare shown in Fig. 2. From ﬁgures we can see that our method converges to a\nrelatively stable loss value by 500 iterations on the majority of datasets.\nIn Tables 2,3and4, we show the multi-view clustering performance of all\nthe algorithms used for comparison. From the table we can see that our method\nachieves the best clustering accuracy performance on four of the six datasets,and the second best performance on two datasets. And on MNIST, ACC and\nARI is only less than one percent with the best, and NMI is the best result. We\ndemonstrate that the clustering performance of our method is stable on diﬀerentdatasets and can be adapted to diﬀerent scales of datasets by above experiments.\nTable 2. ACC (%) of varying multi-view clustering algorithms. Bold and underlined\nresults are the best and the second best.\nMethod \\Dataset ALOI MNIST MSRC-v1 NUS-WIDE Wikipedia Caltech101\nk-means 47.49 (3.31) 73.90 (7.16) 46.33 (1.67) 32.02 (0.70) 55.82 (3.28) 13.37 (0.45)\nMLAN 58.94 (5.18) 77.09 (0.53) 68.10 (0.00) 34.72 (3.17) 18.18 (0.00) 19.47 (0.64)\nMVKSC 60.43 (0.00) 75.20 (0.00) 48.57 (0.00) 31.13 (0.00) 15.01 (0.00) 12.34 (0.00)\nMLSSC 63.02 (9.51) 83.76 (5.21) 65.19 (8.99) 33.28 (2.25) 52.05 (9.00) 20.61 (7.60)\nMSC-IAS 61.39 (3.98) 80.59 (3.04) 67.91 (6.64) 29.70 (0.67) 46.85 (1.19) 20.77 (0.84)\nMCGC 55.51 (0.00) 88.65 (0.00) 72.38 (0.00) 21.56 (0.00) 14.00 (0.00) 23.63 (0.00)\nBMVC 59.59 (0.00) 62.57 (2.15) 63.81 (0.00) 36.64 (1.32) 54.83 (0.00) 28.78 (0.00)\nGMC 64.88 (0.00) 88.90 (0.00) 74.76 (0.00) 20.06 (0.00) 44.88 (0.00) 19.50 (0.00)\nAMCDMD 62.83 (3.24) 76.28 (3.32) 83.19 (6.43) 35.38 (1.20) 54.72 (2.36) 22.43 (2.36)\nOurs 82.14 (0.10) 88.73 (0.04) 86.05 (0.37) 40.11 (1.00) 62.38 (0.16) 25.33 (0.68)\nTable 3. NMI (%) of varying multi-view clustering algorithms. Bold and underlined\nresults are the best and the second best.\nMethod \\Dataset ALOI MNIST MSRC-v1 NUS-WIDE Wikipedia Caltech101\nk-means 47.34 (2.14) 68.05 (2.27) 40.18 (1.48) 17.53 (0.74) 53.97 (1.11) 30.30 (0.17)\nMLAN 59.37 (4.31) 75.51 (0.70) 62.99 (0.00) 22.84 (1.96) 5.89 (0.01) 25.87 (1.56)\nMVKSC 58.40 (0.00) 63.29 (0.00) 36.79 (0.00) 14.43 (0.00) 0.00 (0.00) 9.82 (0.00)\nMLSSC 63.43 (9.89) 74.61 (2.13) 52.99 (15.74) 19.98 (1.59) 47.19 (7.17) 40.76 (3.81)\nMSC-IAS 70.05 (1.79) 74.49 (0.93) 49.55 (1.70) 21.05 (0.62) 43.39 (1.43) 40.61 (0.34)\nMCGC 55.41 (0.00) 77.39 (0.00) 61.52 (0.00) 11.79 (0.00) 1.12 (0.00) 26.27 (0.00)\nBMVC 54.70 (0.00) 56.22 (0.89) 57.37 (0.00) 19.01 (0.37) 43.80 (0.00) 48.58 (0.00)\nGMC 61.81 (0.00) 77.90 (0.00) 74.22 (0.00) 12.23 (0.00) 36.14 (0.00) 23.79 (0.00)\nAMCDMD 62.93 (4.03) 71.34 (2.09) 74.09 (4.46) 20.29 (2.02) 47.87 (2.46) 41.65 (2.58)\nOurs 86.28 (0.12) 78.06 (0.05) 77.49 (0.93) 20.41 (0.63) 55.53 (0.07) 49.18 (0.68)', 'Enhanced Multi-view Matrix Factorization 285\nTable 4. ARI (%) of varying multi-view clustering algorithms. Bold and underlined\nresults are the best and the second best.\nMethod \\Dataset ALOI MNIST MSRC-v1 NUS-WIDE Wikipedia Caltech101\nk-means 32.98 (2.89) 63.87 (4.28) 26.93 (1.70) 9.02 (0.71) 39.82 (2.68) 28.89 (0.35)\nMLAN 34.54 (5.55) 68.85 (0.95) 50.39 (0.00) 13.76 (3.5) 0.48 (0.01) –0.39 (0.12)\nMVKSC 43.77 (0.00) 61.69 (0.00) 23.12 (0.00) 10.42 (0.00) 0.00 (0.00) 3.63 (0.00)\nMLSSC 54.77 (9.50) 73.28 (4.68) 44.92 (9.22) 12.80 (1.75) 37.46 (9.22) 16.01 (9.98)\nMSC-IAS 53.24 (3.49) 67.32 (0.81) 31.00 (1.98) 11.44 (0.65) 30.11 (1.63) 12.68 (0.80)\nMCGC 35.42 (0.00) 78.75 (0.00) 52.23 (0.00) 3.49 (0.00) –0.21 (0.00) 0.38 (0.00)\nBMVC 40.77 (0.00) 47.75 (2.12) 48.75 (0.00) 13.52 (0.42) 32.76 (0.00) 22.38 (0.00)\nGMC 32.90 (0.00) 79.17 (0.00) 64.00 (0.00) 4.24 (0.00) 14.48 (0.00) –0.42 (0.00)\nAMCDMD 40.46 (5.02) 55.80 (2.06) 67.81 (6.89) 13.15 (1.13) 30.62 (4.72) 20.84 (4.23)\nOurs 78.47 (0.14) 78.33 (0.06) 73.16 (0.93) 16.51 (1.01) 44.77 (0.18) 21.88 (0.97)\nBesides, the clustering results of compared algorithms on MNIST dataset\nare exhibited in Fig. 3, where the t-SNE method is adopted to map the original\ndata onto a 2-dimensional subspace. The data points are colored by the output\nlabels from each compared algorithm to compare the eﬀectiveness of each algo-\nrithm. It is apparent that the proposed method acquires encouraging clusteringperformance on the MNIST dataset from the above ﬁgures.\n4.4 Ablation Study\nIn order to verify whether adding discriminators has an improvement on the\nperformance of matrix factorization, we conduct ablation experiments. In this\nsubsection, we show the performance with and without adding discriminators.\nWe complete this experiment on real-world datasets. The experimental resultsare shown in Table 5. We can see that after adding the discriminators, the per-\nformance of our model is improved on all tested datasets, from the large scale\nto the small scale. This proves the validity of the enhancement of our proposedmodel.\nTable 5. Ablation study of the proposed model on six real-world datasets\nMethod \\Dataset ALOI MNIST MSRC-v1 NUS-WIDE Wikipedia Caltech101\nOurs without Discriminators 60.98 81.74 64.29 28.22 51.95 21.43\nOurs with Discriminators 82.14 88.73 86.05 40.11 62.38 25.33', '286 S. Huang et al.\n-60 -50 -40 -30 -20 -10 0 10 20 30 40-50-40-30-20-10010203040\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n(a) MLAN-60 -50 -40 -30 -20 -10 0 10 20 30 40-50-40-30-20-10010203040\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n(b) MVKSC-60 -50 -40 -30 -20 -10 0 10 20 30 40-50-40-30-20-10010203040\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n(c) MLSSC\n-60 -50 -40 -30 -20 -10 0 10 20 30 40-50-40-30-20-10010203040\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n(d) MSC-IAS-60 -50 -40 -30 -20 -10 0 10 20 30 40-50-40-30-20-10010203040\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n(e) MCGC-60 -50 -40 -30 -20 -10 0 10 20 30 40-50-40-30-20-10010203040\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n(f) BMVC\n-60 -50 -40 -30 -20 -10 0 10 20 30 40-50-40-30-20-10010203040\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n(g) GMC-60 -50 -40 -30 -20 -10 0 10 20 30 40-50-40-30-20-10010203040\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n(h) AMCDMD-60 -50 -40 -30 -20 -10 0 10 20 30 40-50-40-30-20-10010203040\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n(i) Ours\nFig. 3. Visualizations of the learned clustering results for eight compared algorithms\non MNIST dataset using t-SNE.\n5 Conclusion\nThis paper proposed a multi-view matrix factorization method that used a\nshared representation matrix to represent multi-view data and extracted featuresfrom all views. We used discriminators to enhance the feature extraction capa-\nbility of matrix factorization and demonstrated the eﬀectiveness of the modality.\nWe conducted experiments on six real-world datasets to show the superiority ofthe performance of our approach relative to other multi-view clustering methods.\nIn the future work, we will further focus on exploring the connection between\nshared representation matrices and feature representations thus providing higherquality features for subsequent tasks.', 'Enhanced Multi-view Matrix Factorization 287\nReferences\n1. Brbic, M., Kopriva, I.: Multi-view low-rank sparse subspace clustering. Patt.\nRecogn. 73, 247–258 (2018)\n2. Chen, W., Lu, X.: Unregistered hyperspectral and multispectral image fusion with\nsynchronous nonnegative matrix factorization. In: Proceedings of the Third Chi-\nnese Conference on Pattern Recognition and Computer Vision, pp. 602–614 (2020)\n3. Ding, C.H.Q., Li, T., Jordan, M.I.: Convex and semi-nonnegative matrix factor-\nizations. IEEE Trans. Patt. Anal. Mach. Intell, 32(1), 45–55 (2010)\n4. Goodfellow, I.J., et al.: Generative adversarial nets. In: Proceedings of the Twenty-\neighth Conference on Neural Information Processing Systems, pp. 2672–2680(2014)\n5. Houthuys, L., Langone, R., Suykens, J.A.K.: Multi-view kernel spectral clustering.\nInf. Fus. 44, 46–56 (2018)\n6. Huang, S., Kang, Z., Xu, Z.: Auto-weighted multi-view clustering via deep matrix\ndecomposition. Pattern Recogn. 97, 107015 (2020)\n7. Li, Z., Wang, Q., Tao, Z., Gao, Q., Yang, Z.: Deep adversarial multi-view clustering\nnetwork. In: Proceedings of the Twenty-Eighth International Joint Conference on\nArtiﬁcial Intelligence, pp. 2952–2958 (2019)\n8. Nie, F., Cai, G., Li, X.: Multi-view clustering and semi-supervised classiﬁcation\nwith adaptive neighbours. In: Proceedings of the Thirty-First AAAI Conference\non Artiﬁcial Intelligence, pp. 2408–2414 (2017)\n9. Sun, G., Cong, Y., Zhang, Y., Zhao, G., Fu, Y.: Continual multiview task learning\nvia deep matrix factorization. IEEE Trans. Neural Netw. Learn. Syst. 32(1), 139–\n150 (2021)\n10. Wang, H., Ding, S., Li, Y., Li, X., Zhang, Y.: Hierarchical physician recommenda-\ntion via diversity-enhanced matrix factorization. ACM Trans. Knowl. Discov. Data\n15(1), 1:1–1:17 (2021)\n11. Wang, H., Yang, Y., Liu, B.: Gmc: graph-based multi-view clustering. IEEE Trans.\nKnowl. Data Eng 6, 1116–1129 (2020)\n12. Wang, S., Chen, Z., Du, S., Lin, Z.: Learning deep sparse regularizers with appli-\ncations to multi-view clustering and semi-supervised classiﬁcation. IEEE Trans.\nPatt. Anal. Mach. Intell. (2021). https://doi.org/10.1109/TPAMI.2021.3082632\n13. Wang, X., Lei, Z., Guo, X., Zhang, C., Shi, H., Li, S.: Multi-view subspace clus-\ntering with intactness-aware similarity. Patt. Recogn. 88, 50–63 (2018)\n14. Zhan, K., Nie, F., Wang, J., Yang, Y.: Multiview consensus graph clustering. IEEE\nTrans. Image Process. 28(3), 1261–1270 (2019)\n15. Zhao, H., Ding, Z., Fu, Y.: Multi-view clustering via deep matrix factorization.\nIn: Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence, pp.\n2921–2927 (2017)\n16. Zheng, Z., Li, L., Shen, F., Shen, H.T., Shao, L.: Binary multi-view clustering.\nIEEE Trans. Patt. Anal. Mach. Intell. 41(7), 1774–1782 (2019)', 'Multi-level Residual Attention Network\nfor Speckle Suppression\nYu L e i1,2, Shuaiqi Liu1,2,3(B), Luyao Zhang1,2, Ling Zhao1,2, and Jie Zhao1,2\n1College of Electronic and Information Engineering, Hebei University, Baoding 071002, China\n2Machine Vision Technology Innovation Center of Hebei Province, Baoding 071000, China\n3National Laboratory of Pattern Recognition (NLPR), Institute of Automation,\nChinese Academy of Sciences, Beijing 100190, China\nAbstract. In order to achieve effective speckle suppression, we propose a multi-\nlevel residual attention network by combining with multi-level block and residual\nchannel attention network, which is suitable for speckle suppression. Firstly, the\nnetwork model performs a simple shallow feature extraction for the input noiseimage through two convolution layers. Then, the residual attention network is used\nto extract the deep features. Finally, a convolution layer and residual learning are\nused to generate the ﬁnal denoised image. Experimental results show that theproposed method can effectively suppress the noise and preserve the edge details\nof the image.\nKeywords: Speckle suppression ·Deep learning ·Residual attention\n1 Introduction\nSynthetic aperture radar (SAR) [ 1] is a kind of imaging radar with high resolution. The\nremoval of speckle noise in SAR image is very important for the subsequent applicationof SAR image. These methods are mainly divided into two categories: spatial ﬁltering\nalgorithm and frequency-domain ﬁltering algorithm. Spatial ﬁltering algorithms mainly\nuse local adjacent pixels to suppress noise, among which typical denoising algorithmsinclude Frost ﬁlter [ 2] and Lee ﬁlter [ 3]. In order to better perform image denoising,\nBuades et al. [ 4] proposed the idea of non-local means (NLM). Later, Dabov et al. [ 5]\nextended this idea to the transform domain, and proposed matching adjacent pixel imagesand integrating similar blocks in a three-dimensional matrix called block-matching and\n3D ﬁltering (BM3D). Obviously, SAR images also have non-local similarity, so the\nidea of non-local denoising can also be applied to the ﬁeld of speckle suppression. Forexample, the SAR image denoising algorithm which combines non-local ideas with the\ntransform domain (SAR-BM3D) [ 6].\nThe ﬁrst author is a student.\nThis research was funded by National Natural Science Foundation of China under grant 62172139,\nthe Post-graduate’s Innovation Fund Project of Heibei University under grant HBU2021ss002,Natural Science Foundation of Hebei Province under grant F2018210148, F2019201151 and\nF2020201025, Science Research Project of Hebei Province under grant BJ2020030.\n© Springer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 288–299, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_24', 'Multi-level Residual Attention Network for Speckle Suppression 289\nBased on speckle suppression algorithm in the frequency domain, the noisy image\nis transformed from the spatial domain to the frequency domain, and the coefﬁcients inthe transform domain are processed, then the processed image is inversely transformed\nback to the spatial domain, ﬁnally achieving the purpose of removing the noise. The\ndenoising algorithm based on transform domain mainly realizes image denoising throughstatistical priori in wavelet domain and multi-scale geometric transform domain, such\nas SAR image denoising in Shearlet domain [ 7]. Although the methods based on the\nspatial domain and the transform domain have certain denoising effects on SAR images,in the process of denoising, many algorithms will produce other image quality problems,\nfor example, the image is too blurred, the edge detail information is lost too much, and\nartifacts are introduced, etc.\nIn recent years, convolutional neural networks (CNN) have been widely used in\nvarious image processing problems, such as image classiﬁcation, image segmentation,\nedge detection and image denoising. Therefore, scholars have also done a lot of research\nand applied the method based on CNN to the ﬁeld of speckle suppression. Compared with\ntraditional non-CNN denoising methods, CNN-based methods are superior to traditionalmethods in denoising and image detail preservation. Inspired by the model framework\nof feature fusion attention network [ 8], we construct a new subnetwork by using residual\nand channel attention ideas, and propose a novel speckle suppression network: multi-level residual attention denoising network (MRANet). Experimental results show that the\nproposed algorithm has a good denoising effect on simulated and real SAR images. The\ncontributions of proposed algorithm are as follows: (1) The multi-layer recursive blocksare introduced to facilitate the extraction of more feature information. (2) The residual\nchannel attention network is proposed, which can effectively improve the denoising\nperformance of the model through local residuals learning and global residuals learning.(3) Embedding a channel attention block at the end of the entire network can effectively\nmaintain the details of the image while denoising, thereby avoiding the denoised image\nfrom being too smooth or over-sharpened.\n2 Related Work\nMany CNN methods have been applied to the denoising problem of SAR images andachieved good results. Chierchia et al. [ 9] proposed a speckle suppression algorithm\n(SAR-CNN) based on residual learning, and the whole network contains 17 convolu-\ntional layers. The algorithm ﬁrst performs a homomorphic transformation on the inputimage, and then uses residual learning to reduce speckle from the extracted noise to\nrestore a clean image. Although the denoising ability of this algorithm is very strong, it\ncannot keep the detailed information of the image very well. In addition, since logarith-\nmic transformation and corresponding exponential transformation are required during\nCNN model training, the network cannot be trained end-to-end.\nWang et al. [ 10] proposed a GAN-based SAR denoising algorithm. This method\ncombines three different loss functions for end-to-end training to achieve a better effect\nof suppressing speckle. Later, Wang et al. [ 11] proposed an image speckle suppression\nconvolutional neural network (ID-CNN). The whole network has 8 layers. This model\ncan be processed end-to-end without the need to transform the input image. Compared\nwith SAR-CNN, ID-CNN combines Euclidean loss and TV loss for training, which', '290 Y . Lei et al.\ncan obtain a better denoising effect, but still lose some details. In order to expand the\nreceptive ﬁeld, Zhang et al. [ 12] proposed a SAR despekling algorithm (SAR-DRN)\nbased on dilated convolution, combined skip connection and residual learning to carry\nout the design of the network framework, so that the denoising network performance has\nbeen improved. The method of SAR images denoising based on deep learning enablesthe network to extract image features from training data, and better learn noise models\nto achieve better speckle suppression performance than traditional methods, and the\nrunning time is less than traditional methods. Therefore, based on the study of SARimage speckle model and CNN theory, we also propose a new type of network model\nsuitable for speckle suppression--MRANet.\n3 Proposed Method\nThe framework of MRANet is mainly divided into three parts, including shallow featureextraction block (SFB), deep feature extraction block (DFB) and reconstruction block. As\nshown in Fig. 1, SFB consists of two 3 ×3 convolutional layers, and DFB is composed of\nrecursive group (RG), skip connection and channel attention (CA) in the form of cascade.\nThe reconstruction block is only implemented by a 3 ×3 convolution operation.\nFig. 1. Overall framework of the proposed MRANet.\nLet xdenotes the input of the network. As shown in Fig. 1, the input noise image x\nis ﬁrst extracted by two convolutional blocks for shallow feature extraction, that is\nfo=HC2(x) (1)\nwhere forepresents the output after shallow extraction, which is used as the input of\nthe second part. HC2represents the function of two convolution block.\nContinuous blocks can reduce the depth and computational complexity of the net-\nwork model, and the introduction of skip connections can also improve the training\nspeed of network. Therefore, the middle deep feature extraction block is designed in a\ncascade way. The DFB block is composed of mRGs and a CA through skip connection,\nand each RG is realized by cascading nresidual attention blocks (RCBs). Let the output\nof the ith RG block be frgi\nfrgi=RG i(RG i−1···(RG 1(f0))) (2)\nwhere f0denotes the input of the second part, and RG irepresents the realization func-\ntion of the RG block. The RG block denotes composed of nRCBs, a 3 ×3 convolution\noperation and local residual learning, which can be expressed as\nRG i=C(Bn(Bn−1···(B1(frg(i−1))))) +frg(i−1) (3)', 'Multi-level Residual Attention Network for Speckle Suppression 291\nwhere Bnrepresents the nth RCB implementation function, Cdenotes the convolution\noperation, and frg(i−1)represents the input of the ith RG.\nThe RCB block is composed of parametric rectiﬁed linear unit (PReLU), convolution,\nrectiﬁed linear unit (ReLU), CA and residual learning. It can allow a small amount of\nunimportant information is passed, so as to focus on effective information and furtherimprove network performance. In the experiment, m=3 in RG and n=10 in RCB were\nset, and 64 ﬁlter coefﬁcients were output in each RG. The output of each RG is fused,\nthat is\nf\nRG=concat (frg1,frg2,frg3... frgm) (4)\nThen fRGis fed back to the reconstruction block through a channel attention block,\nand the reconstruction module is realized by only a convolution, which can be expressedas\nz=C(H\nCA(fRG)) (5)\nwhere zrepresents the obtained residual map (ie, noise), Crepresents the convolution\noperation, and HCAdenotes the realization function of the channel attention module.\nFinally, the ﬁnal clean image y(y=x−z) is obtained through residual learning.\nThe proposed RCB model are shown in Fig. 2. Firstly, the input feature map is put into\ntwo residual attention blocks for feature extraction, and then the global residual attention\nis performed by PReLU, convolution, ReLU activation function and CA. Through the\noverall feature analysis and extraction, the output feature map is obtained.\nFig. 2. Architecture of the proposed residual channel network.\nSince the general CNN processing feature mapping treats each channel equally, it\nwill reduce the learning ability between channels, thereby inhibiting the representation\nability of the deep network. Therefore, we introduce the channel attention model andput it at the end of the residual block to form a residual channel attention network,\nand CA is added in the reconstruction stage, so that the network can ignore irrelevant\ninformation, and pay more attention to noise information to facilitate the ﬁnal generationof high-quality images. The channel attention module is shown in Fig. 3.\nThe loss function is mainly used to guide the training of the model and to estimate\nthe relationship between the predicted value and the true value. If the two deviate faraway, the loss function will get a very large value, and the parameters will be updated\nthrough back propagation. Further reduce te value of the loss function to achieve the\npurpose of continuous learning. Conversely, the smaller the loss function, the closer the', '292 Y . Lei et al.\nFig. 3. Channel attention module.\npredicted value and the true value, and the better the robustness of the model. In this\npaper, L1 loss is selected for model training. Given a training set with Npair of images\n{xi\nnoise,yi\ngt}N\ni=1and the loss function can be expressed as\nL(/Phi1) =1\nNN/summationdisplay\ni=1||yi\ngt−MRANet (xi\nnoise)||1 (6)\nwhere Nrepresents the number of noise-free image pairs, /Phi1denotes the set of\nnetwork parameters, ygtrepresents the noise free image, xnoisedenotes the input noise\nimage of the network, and MRANet represents the output of the entire network model.\n4 Experimental Results\nIn order to verify the effectiveness of the proposed residual attention network, we conduct\nexperimental veriﬁcation on synthetic data and real data sets, as described below.\n4.1 Datasets and Evaluation Index\nTraining datasets. We used the UC Merced Land Use dataset [ 13] for training. This\ndataset is mainly used for remote sensing image classiﬁcation, and the size of eachimage is 256 ×256. We randomly selected 400 images as the training set. The artiﬁcial\nsimulation of multiplicative noise was to add multiplicative noise to the 400 training\nimages as the training data of the network. The color image is ﬁrstly converted to asingle-channel grayscale image, and then the grayscale noise image is processed to get\nthe ﬁnal clean image.\nT est datasets. The test data are divided into simulated and real image. For simulated\nimages, on the one hand, a noise image with four looks (L =2, 4, 8 and 10) is randomly\nselected in the Set12 data set for testing. For the denoising test of real SAR images,\nfour real SAR images are used to evaluate the proposed algorithm visually, as shown in\nFig. 4. All the images in Fig. 4are 256 ×256. These real SAR images are available at\nwww.sandia.gov . For the convenience of description, the four images are named SAR1,\nSAR2, SAR3 and SAR4.\nIn order to verify the effectiveness of the proposed algorithm, in the simulation data,\ndue to the existence of clear original images, we use peak signal-to-noise ratio (PSNR)\nand structural similarity index (SSIM) [ 14] to measure the denoising effect of various\ndenoising algorithms. The larger the SSIM value, the more the details of the denoised\nimage are restored and the closer it is to the original image. The larger the PSNR value,\nthe stronger the denoising ability of the denoising algorithm and the clearer the image.', 'Multi-level Residual Attention Network for Speckle Suppression 293\n(a) SAR1 (b) SAR2 (c) SAR3  (d) SAR4 \nFig. 4. Original images used in the test experiments. (a)–(d) are real SAR images.\nFor real image denoising, since there is no clear image, we choose the following four\nindicators to measure the denoising effect: equivalent numbers of looks (ENL) [ 15], edge\npreservation degree based on the ratio of average ratio (EPD-ROA) (horizontal (HD) and\nvertical direction (VD)) [ 16], unassisted measure of the quality (UM) [ 17] and TIME.\nENL is used to measure the smoothness of the image. The larger the value, the smoother\nthe ﬂat area of the image. It is also used to reﬂect the degree of contamination of the image\nby coherent speckle noise. The higher the ENL, the less noise in the image. EPD-ROAis used to evaluate the ability of image detail edge preservation, which can be divided\ninto horizontal and vertical parts. The closer the value is to 1, the stronger the image\ndetail and edge preservation ability. UM is used to test the overall denoising ability ofthe image. The smaller the UM value, the better the denoising ability.\nThen, in order to verify the effectiveness of the proposed algorithm for speckle\nsuppression, we test it on simulated and real SAR images and compare it with the fol-\nlowing seven methods: Non-local SAR image denoising algorithm based on wavelet\ndomain (SAR-BM3D) [ 6], Frost ﬁlter [ 2], Shearlet domain Bayesian threshold shrink-\nage denoising algorithm based on sparse representation (BSS-SR) [ 7], fast and ﬂexible\nbased on non-subsampled clipping domain Denoising algorithm (FFDNet) [ 18], SAR\nimage denoising algorithm based on FFDNet and continuous cyclic translation (FFDNet-CCS) [ 19], SAR image denoising algorithm based on convolutional neural network and\nguided ﬁltering (CNN-GFF) [ 20] and SAR image denoising algorithm based on CNN\nprior (IRCNN) [ 21]. Finally, the performance of the proposed algorithm is analyzed in\nobjective indicators and subjective vision.\nThe training environment used in the experiment is Windows 10 system. Model\nframework is Pytorch. The GPU of the machine is NVIDIA GeForce RTX 2080 Ti.CUDA10.1 and CUDNN10.1 are used to accelerate GPU computing power and speed\nup training. It takes about 13 h to train the complete model. The entire network is trained\n40,000 times, using the ADAM [ 22] optimization method for training. β1=0.9,β2=\n0.999, batch-size is 2, and the initial learning rate (lr) is set to 0.0001. In the training\nprocess, the training image with a size of 256 ×256 is cropped into image patch with a\nsize of 64 ×64, and random rotation and horizontal ﬂipping are performed at 90, 180,\nand 270° to achieve data enhancement.\n4.2 Experiment on Synthetic Speckled Image\nFor the simulated image, we randomly select an image in Set12, and add coherent noises\nof different look numbers to the clean image for testing. The looks are 2, 4, 8, and 10', '294 Y . Lei et al.\nrespectively. Table 1shows the denoising performance of various denoising algorithms.\nThe objective indicators PSNR and SSIM in Table 1are used to measure the denoising\nperformance. The red font indicates the best and the blue indicates the second best. It\ncan be seen that the objective evaluation indicators of our algorithm are basically the\nbest, and the PSNR value is signiﬁcantly improved by about 1 dB compared with otherCNN methods.\nTa b l e 1 . Quantitative evaluation results for simulated SAR image.\nLooks Index SAR-\nBM3D BSS-\nSR Frost IRCNN FFDNet FFDNet-\nCCS CNN-\nGFF Pro-\nposed \nL=2 PSNR 29.54 27.76 27.62 27. 63 27.67 27.55 27.43 29.67\nSSIM 0.6832  0.6611 0.4785 0.3831 0.3749 0.3934 0.3066 0.6643 \nL=4 PSNR 30.48 29.04 28.68 28.54 28.54 28.50 28.15 31.59 \nSSIM 0.7299 0.7186 0.5911 0.5351 0.5076 0.5301 0.4289 0.7927 \nL=8 PSNR 31.88 30.31 29.91 29.60 29.88 29.84 29.03 32.92 \nSSIM 0.7725 0.7425 0.6912 0.6601 0.6682 0.6819 0.5601 0.7956 \nL=10 PSNR 32.18 30.56 30.33 29.82 30.42 30.36 29.34 33.08 \nSSIM 0.7798 0.7486 0.7257 0.6875 0.7195 0.7336 0.6023 0.7984 \n4.3 Experiment on Real Image\nFour real SAR images were selected to test the corresponding denoising algorithm to\nverify the effectiveness of the proposed algorithm. Figure 5shows the denoising effect\nof each denoising algorithm on the SAR1 image.\nAs can be seen from the magniﬁcation area in Fig. 5, the denoising images of SAR-\nBM3D and BSS-SR are blurred with too much detail loss. The SAR-BM3D has a strong\ndenoising effect on smooth areas, but the edge details are lost seriously. The magniﬁcation\nregion of CNN-GFF is very similar to that of the original SAR1, indicating that thedenoising effect of the algorithm is poor. The denoising effect of IRCNN is good, but\nfalse edges appear in ﬂat areas. Frost and FFDNet-CCS have some denoising effects,\nbut introduce some artiﬁcial textures. The denoised image of the proposed algorithmcan retain more details, and the edge details can be fully preserved while the speckle is\neffectively suppressed.\nTable 2shows the objective evaluation index results obtained by each denoising\nalgorithm when denoising SAR1 image. It can be seen from the Table 2that our ENL\nvalue is second only to SAR-BM3D, but much higher than other denoising methods. Our\nalgorithm also has the lowest UM value, which indicates that the proposed algorithm has\nthe strongest comprehensive denoising ability. The EPD-ROA index of our algorithm is\nhigher than other algorithms, which indicates that our algorithm has better image edgeretention ability. In terms of running time, the proposed algorithm also has the shortest\nrunning time compared with other algorithms.\nFigures 6and 7respectively show the renderings after denoising of SAR2 and SAR3\nimages by each denoising algorithm. As can be seen from Figs. 6and 7, when denoising\nSAR2, the images denoised by SAR-BM3D, BSS-SR and Frost were blurred, and the\ndetails of the original image were completely lost. However, other speckle suppression', 'Multi-level Residual Attention Network for Speckle Suppression 295\n(a) BSS-SR (b) Frost (c) SAR-BM3D  (d) IRCNN \n(e) CNN-GFF (f) FFDNet (g) FFDNet-CCS (h) Proposed  \nFig. 5. Despeckling results on the real SAR1.\nTa b l e 2 . Quantitative evaluation results for real SAR1.\nMethod ENL  UMEPD-ROA TIME(s)HD  VD\nSAR-BM3D 166.5003 51.0335 0.8081 0.7725 68.2135 \nBSS-SR 86.6347 63.3830 0.4350 0.5065 6.3939 \nFrost 40.1545 35.5885 0.7732 0.7819 7.8311 \nIRCNN 25.9335 32.5849 0.8663 0.8502 2.1575\nFFDNet 18.6832 88.7781 0.9063 0.8833 2.8121 \nFFDNet-CCS 28.4617 148.38 35 0.8260 0.8141 2.7841 \nCNN-GFF 12.4384 100.6929 0.9075 0.8923 5.6132 \nProposed 145.6307 28.5329 0.9141 0.9040 0.2754 \nalgorithms based on CNN also have a certain amount of noise points in the ampliﬁcation\nregion. The proposed algorithm can effectively suppress the speckle while preserving\nthe edge and detailed texture information of the image.\nTable 3presents the objective evaluation index results obtained by each denoising\nalgorithm in SAR2 images. As can be seen from Table 3, the ENL value of our algo-\nrithm is poor, but compared with other CNN-based speckle suppression algorithms, theENL value of our algorithm is the highest. In the UM index, the value of the proposed\nalgorithm is the lowest, which also indicates that the proposed algorithm has stronger\ncomprehensive denoising ability. In terms of EPD-ROA index, the proposed algorithm\nis also the highest, which indicates that our algorithm has a better ability of image edge\npreservation. In terms of running time, compared with other algorithms, our algorithmalso has the shortest running time.\nTable 4shows the objective evaluation index results obtained by each denoising\nalgorithm in SAR3 images. As can be seen from Table 4, the ENL value of the proposed\nalgorithm is poor, but compared with other CNN-based speckle suppression algorithms,\nthe ENL value of the proposed algorithm is the highest. The ENL and EPD-ROA values of\nthe proposed algorithm are also the highest, which indicates that the proposed algorithm', '296 Y . Lei et al.\n(a) BSS-SR (b) Frost (c) SAR-BM3D  (d) IRCNN \n(e) CNN-GFF (f) FFDNet (g) FFDNet-CCS (h) Proposed  \nFig. 6. Despeckling results on the real SAR2.\n(a) BSS-SR (b) Frost (c) SAR-BM3D  (d) IRCNN \n(e) CNN-GFF (f) FFDNet (g) FFDNet-CCS (h) Proposed  \nFig. 7. Despeckling results on the real SAR3.\nTa b l e 3 . Quantitative evaluation results for real SAR2.\nMethod ENL  UMEPD-ROA TIME(s)HD  VD\nSAR-BM3D 3.7011 92.3219 0.7431 0.8340 65.4424 \nBSS-SR 4.4846 63.1376 0.5678 0.6083 2.3139 \nFrost 3.2515 28.8393 0.8373 0.8402 7.4867 \nIRCNN 2.3433 12.0083 0.8355 0.8635 2.7126 \nFFDNet 1.9762 37.8296 0.8923 0.9014 2.9386 \nFFDNet-CCS 2.3706 23.6286 0.7968 0.8443 2.2009 \nCNN-GFF 1.8382 56.5609 0.9012 0.9087 2.3969 \nProposed 2.9261 11.2305 0.9106 0.9249 0.2504 ', 'Multi-level Residual Attention Network for Speckle Suppression 297\ncan not only effectively suppress the speckle, but also fully preserve the edge and texture\ninformation of the image. Compared with other denoising algorithms, the running timeof our algorithm is much less than that of other denoising algorithms.\nTa b l e 4 . Quantitative evaluation results for real SAR3.\nMethod ENL  UMEPD-ROA TIME(s)HD  VD\nSAR-BM3D 4.9058 39.0508 0.5707 0.6800 62.6117 \nBSS-SR 5.4611 38.1924 0.3399 0.4146 2.1841 \nFrost 4.1862 49.2760 0.6061 0.6429 8.2414 \nIRCNN 3.3376 39.0926 0.7030 0.7455 1.9533 \nFFDNet 2.7059 380.1645 0.7354 0.7910 2.5625\nFFDNet-CCS 3.2829 305.6754 0.6106 0.6796 2.2588 \nCNN-GFF 2.2515 176.5678 0.7385 0.7821 2.8814 \nProposed 3.5132 35.5219 0.7560 0.8071 0.2370 \nFigure 8shows the effect of denoising the SAR4 image by each denoising algo-\nrithm. As can be seen from the visual diagram in Fig. 8, the proposed algorithm can\neffectively suppress the speckle and has a strong ability to restore the texture details of\nthe image. From the ampliﬁcation area in Fig. 8, it can be seen that Frost, CNN-GFF\nand FFDNet cannot effectively suppress speckle. Over-smooth phenomenon exists in\nthe image denoised by SAR-BM3D and BSS-SR. FFDNet, IRCNN and CNN-GFF have\ninsufﬁcient noise suppression capability, while FFDNet-CCS has artiﬁcial texture. The\nproposed algorithm can not only suppress the speckle effectively, but also preserve the\nedge and texture information of the image well.\n(a) BSS-SR (b) Frost (c) SAR-BM3D  (d) IRCNN \n(e) CNN-GFF (f) FFDNet (g) FFDNet-CCS (h) Proposed  \nFig. 8. Despeckling results on the real SAR4.\nTable 5shows the objective evaluation index results obtained by each denoising\nalgorithm in SAR4 images. As can be seen from Table 5, for SAR4, the indicators of our\nalgorithm are the best, and compared with other denoising algorithms, our algorithm is\nsigniﬁcantly improved. Because SAR4 has less texture information, the EPD-ROA is', '298 Y . Lei et al.\ngenerally lower, but our algorithm is still the best. Therefore, in general, the proposed\nalgorithm has a better comprehensive denoising effect.\nTa b l e 5 . Quantitative evaluation results for real SAR4.\nMethod ENL  UMEPD-ROA TIME(s)HD  VD\nSAR-BM3D 31.3483 35. 5472 0.4717 0.5071 58.4861 \nBSS-SR 33.9264 76.3399 0.1321 0.2290 2.4597 \nFrost 22.3186 34.5700 0.4546 0.5735 7.2027 \nIRCNN 21.5150 51.2553 0.5370 0.6335 1.7841 \nFFDNet 15.7984 41.4648 0.6558 0.6890 2.4043 \nFFDNet-CCS 21.9737 34. 7462 0.5887 0.6145 2.1708 \nCNN-GFF 11.9831 76.5049 0.6503 0.6812 2.3497 \nProposed 34.8494 19.2200 0.6651 0.6970 0.2529 \n5 Conclusion\nIn this paper, we propose an end-to-end SAR denoising network, which achieves denois-\ning by cascading multiple blocks and combining the residual channel attention network.\nOn the one hand, more block recursive cascade can keep the diversity of features. On theother hand, the channel module can focus on the characteristics of need, while ignoring\nunimportant information. Therefore, in the part of depth extraction, we use local and\nglobal residual channel attention to analyze and process the noise information, so as togenerate high quality images in the reconstruction stage. Finally, the proposed algorithm\nis validated on real and simulated data. The results show that the proposed algorithm\nhas strong denoising ability and can fully preserve the image details. In the future work,we will continue to study this problem and improve it, and further apply the denoising\ntask to other high-level tasks.\nReferences\n1. Moreira, A., Prats-Iraola, P ., Y ounis, M., Krieger, G., Hajnsek, I., Papathanassiou, K.P .: A\ntutorial on synthetic aperture radar. IEEE Geosci. Remote Sens. Mag. 1(1), 6–43 (2013)\n2. Frost, V .S., Stiles, J.A., Shanmugan, K.S., Holtzman, J.C.: A model for radar images and\nits application to adaptive digital ﬁltering of multiplicative noise. IEEE Trans. Pattern Anal.Mach. Intell. 4(2), 157–166 (1982)\n3. Lee, J.S., Jurkevich, L., Dewaele, P ., Wambacq, P ., Oosterlinck, A.: Speckle ﬁltering of\nsynthetic aperture radar images : a review. Remote Sens. Rev. 8(4), 313–340 (1994)\n4. Buades, A., Coll, B., Morel, J.-M.: A non-local algorithm for image denoising. In: 2005 IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005),\nvol. 2, pp. 60–65 (2005)\n5. Dabov, K., Foi, A., Katkovnik, V ., Egiazarian, K.: Image denoising with block-matching and\n3D ﬁltering. In: Image Processing: Algorithms and Systems, Neural Networks, and Machine\nLearning, vol. 6064, p. 606414 (2006)', 'Multi-level Residual Attention Network for Speckle Suppression 299\n6. Parrilli, S., Poderico, M., Angelino, C.V ., V erdoliva, L.: A nonlocal SAR image denoising\nalgorithm based on LLMMSE wavelet shrinkage. IEEE Trans. Geosci. Remote Sens. 50(2),\n606–616 (2012)\n7. Liu, S.Q., Hu, S.H., Xiao, Y ., An, Y .L.: Bayesian Shearlet shrinkage for SAR image de-\nnoising via sparse representation. Multidimension. Syst. Signal Process. 25(4), 683–701\n(2013). https://doi.org/10.1007/s11045-013-0225-8\n8. Qin, X., Wang, Z., Bai, Y ., Xie, X., Jia, H.: FFA-Net: feature fusion attention network for\nsingle image dehazing. Proceedings of the AAAI Conference on Artiﬁcial Intelligence 34,\n11908–11915 (2020)\n9. Chierchia, G., Cozzolino, D., Poggi, G., V erdoliva, L.: SAR image despeckling through\nconvolutional neural networks. In: 2017 IEEE International Geoscience and Remote SensingSymposium (IGARSS), pp. 5438–5441 (2017)\n10. Wang, P ., Zhang, H., Patel, V .M.: Generative adversarial network-based restoration of speck-\nled SAR images. In: 2017 IEEE 7th International Workshop on Computational Advances inMulti-Sensor Adaptive Processing (CAMSAP), pp. 1–5 (2017)\n11. Wang, P ., Zhang, H., Patel, V .M.: SAR image despeckling using a convolutional neural\nnetwork. IEEE Signal Process. Lett. 24(12), 1763–1767 (2017)\n12. Zhang, Q., Y uan, Q., Li, J., Yang, Z., Ma, X.: Learning a dilated residual network for SAR\nimage despeckling. Remote Sens. 10(2), 196 (2018)\n13. Yang, Y ., Newsam, S.: Bag-of-visual-words and spatial extensions for land-use classiﬁca-\ntion. In: Proceedings of the 18th SIGSPA TIAL International Conference on Advances in\nGeographic Information Systems, pp. 270–279 (2010)\n14. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P .: Image quality assessment: from error\nvisibility to structural similarity. IEEE Trans. Image Process. 13(4), 600–612 (2004)\n15. Lee, J.-S.: Speckle analysis and smoothing of synthetic aperture radar images. Comput. Graph.\nImage Process. 17(1), 24–32 (1981)\n16. Ma, X., Shen, H., Zhao, X., Zhang, L.: SAR image despeckling by the use of variational\nmethods with adaptive nonlocal functionals. IEEE Trans. Geosci. Remote Sens. 54(6), 3421–\n3435 (2016)\n17. Gomez, L., Ospina, R., Frery, A.C.: Unassisted quantitative evaluation of despeckling ﬁlters.\nRemote Sens. 9(4), 389 (2017)\n18. Zhang, K., Zuo, W., Zhang, L.: FFDNet: toward a fast and ﬂexible solution for CNN-based\nimage denoising. IEEE Trans. Image Process. 27(9), 4608–4622 (2018)\n19. Liu, S., et al.: SAR speckle removal using hybrid frequency modulations. IEEE Trans. Geosci.\nRemote Sens. 59, 3956–3966 (2021)\n20. Liu, S., et al.: Convolutional neural network and guided ﬁltering for SAR image denoising.\nRemote Sens. 11(6), 702 (2019)\n21. Zhang, K., Zuo, W., Gu, S., Zhang, L.: Learning deep CNN denoiser prior for image restora-\ntion. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 2808–2817 (2017)\n22. Kingma, D.P ., Ba, J.L.: Adam: a method for stochastic optimization. In: ICLR 2015 :\nInternational Conference on Learning Representations 2015 (2015)', 'Suppressing Style-Sensitive Features via\nRandomly Erasing for Domain\nGeneralizable Semantic Segmentation\nSiwei Su1, Haijian Wang1, and Meng Yang1,2(B)\n1School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou,\nChina\n2Key Laboratory of Machine Intelligence and Advanced Computing (SYSU),\nMinistry of Education, Guangzhou, China\nyangm6@mail.sysu.edu.cn\nAbstract. Domain generalization aims to enhance robustness of mod-\nels to diﬀerent domains, which is crucial for safety-critical systems in\npractice. In this paper, we propose a simple plug-in module to promote\nthe ability of generalization for semantic segmentation networks withoutextra loss function. Firstly, we rethink the relationship between seman-\ntics and style in the sight of feature maps, and divide the channels of\nthem into two kinds (i.e. style-sensitive channels and semantic-sensitivechannels) via the variance of Gram matrix. Secondly, with the assump-\ntion that the domain shift mainly lies in style, a random erasure method\nis proposed to style-sensitive-channel features with the hope of learning\ndomain invariant features and preventing model from over-ﬁtting to spe-\nciﬁc domain. Extensive experiments demonstrate that the generalizationof our proposed method outperforms existing approaches.\nKeywords: Domain generalization\n·Semantic segmentation\n1 Introduction\nDeep convolution neural networks (DCNN) have seen its soaring success in\nmany visual understanding tasks concerning their performance on the bench-\nmark datasets. However, when applied on the real-world unseen data, DCNN\nmodels often fail to perform at their best due to the discrepancy between the\ntraining and test samples. Thus it is of great signiﬁcance to reduce the perfor-\nmance gap between the target and source domains, especially for those safety-critical systems, i.e. autonomous driving. In response to such issues, an intuitive\nway is to collect and label data as many samples as possible. However, it is\nprohibitively expensive and impractical to manually label more unseen samples,especially for densely classiﬁcation tasks, e,g. semantic segmentation. Recently,\nresearchers instead tend to focus on developing more robust models to mitigate\nthe domain gap issue without plenty of manually labeling.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 300–311, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_25', 'Suppressing Style-Sensitive Features via Randomly Erasing 301\nFig. 1. Illustrating diﬀerences between ours and others: (a) An image always\nincludes semantic information and style information. (b) Other works apply soft con-\nstrain. (c) Our work applies random erasure.\nDomain adaptation (DA) is expected to align distribution on the setting of\nsource (synthetic) domain with labels and target (real-world) domain without\nlabels, which greatly relieves the tediousness of labeling real-world data. How-ever, despite best eﬀorts, we are unable to exhaustively collect all real-world data\ndue to its massive complexity and unpredictability. Therefore, once DA network\nis applied in agnostic environment, their accuracy will drop signiﬁcantly.\nRecently, Domain generalization (DG) have drawn increasing attention due\nto its ability to overcome the above inadequacy. In DG, no target samples and\nlabels are available, and its goal is to learn a robust model that can learn domaininvariant feature, generalizing well to all unseen new data. A study [ 3]h a ss h o w n\nthat it is a preferable choice to normalize features for generalization networks.\nHowever, the normalization standardizes the whole features in an image, ignoringadverse impact of style information in them during training. An existing state-\nof-the-art method [ 7] applies selected whitening loss (ISW loss) to suppress style\nelement in images. However, ISW loss is just soft constrain during training, whichcan’t prevent style information from feeding into network explicitly and still\npotentially leads to over-ﬁtting in speciﬁc domain. As shown in Fig. 1, in order\nto reduce style information as possible and strengthen the semantic informationin model learning, we present a strongly restrictive plug-in module in semantic\nsegmentation network for domain generalization. To the best of our knowledge,\nour method is the ﬁrst attempt to decouple the channels of features into two\ncategories via Gram matrix [ 28]: semantic-sensitive channels and style-sensitive\nchannels. As mentioned above, we are motivated to train a robust network byremoving the features of style-sensitive channels and preserving semantic infor-\nmation in images. Thus how to distinguish semantics and style information is', '302 S. Su et al.\nCross-\nentropy \nLossLabel\nSCM\nRE\nPredictionISW Loss\nFig. 2. Illustrating our framework: We train DeepLabv3+ [ 19] with labeled source\ndomain at the beginning of Kepochs. After K-th and before K+1 - t he p o c h ,w e\nfeed image and image-transformation to get style-sensitive channels with the help of\nGram matrix and randomly erase style-sensitive-channel features in upcoming epochs.\nFollowing [ 7], we apply ISW Loss and Cross-entropy loss to supervise our training net-\nwork. SCRE, SCM and RE in ﬁgure represent randomly erasing style-sensitive-channel\nfeatures module, getting style-sensitive channels and randomly erasing respectively.\nthe key issue. We feed the model with image and image-relevant style trans-\nformation to calculate the diﬀerences between channel-dimensions. With these\ndiﬀerences, we ﬁlter style-sensitive channels of feature maps and randomly erasepartial feature values on these channels. Overall, our main contribution can be\nconcluded as follows:\n– We are the ﬁrst to explicitly decouple the style-sensitive-channel and\nsemantic-sensitive-channel features. Therefore, the latter, which are widely\nconsidered to be domain invariant in domain generalization, can be better\npreserved.\n– We relief the model learning from over-ﬁtting to domain-speciﬁc style fea-\ntures and enhance the domain invariant features by a plug-in module, which\nrandomly erases style-sensitive-channel features.\n– Extensive experiments and ablation studies are conducted to illustrate the\neﬀectiveness and superiority of our proposed method.\n2 Related Work\nIn this section, we give a brief review to the related work in the paper, i.e.\nsemantic segmentation and domain generalization.\n2.1 Semantic Segmentation\nSemantic Segmentation is a basic branch in computer vision, receiving wide\nattention in recent year, since Fully Convolutional Network (FCN) [ 12]i sﬁ r s t', 'Suppressing Style-Sensitive Features via Randomly Erasing 303\ntransform convolutional classiﬁcation network to fully convolutional network for\nsemantic segmentation. Based on FCN, current models employing either encoder-decoder [ 13,14] or spatial pyramid pooling [ 15–17] greatly boost the accuracy\nof the task. For instance, PSPNet [ 17] applies pooling kernels to capture repre-\nsentations at various resolutions. DeepLab Family [ 15,18,19] employs an Atrous\nSpatial Pyramid Pooling (ASPP) module rather than reducing resolutions, with\ndilated convolutions [ 20] to expand convolution perspective. Up to now, although\nother semantic segmentation networks emerge in an endless stream, DeepLabv3+[19] is still a classical semantic segmentation architecture, which is widely in\ndownstream work. As our domain generalization method is proposed for seman-\ntic segmentation, we adopt DeepLabv3+ as well as other related works.\n2.2 Domain Generalization\nBlanchard et al. [ 1] is the pioneer of domain generalization, which essen-\ntially solves out-of-distribution problem. Thereafter, researchers have developednumerous outstanding algorithms to promote the development of domain gener-\nalization. For instance, Instance Normalization [ 8] and batch normalization [ 9]\nare utilized in parallel by Pan et al. [ 3] to modify the marginal architecture and\nprevent features from falling into speciﬁc styles. Li et al. [ 6] achieves distribu-\ntion alignment in the hidden representation of an auto-encoder using maximum\nmean discrepancy. Li and Gong et al. [ 5] resorted to adversarial learning with\nauxiliary domain classiﬁers to learn domain-agnostic features. And Ghifary et\nal. [10] introduces multi-task auto-encoders to learn feature robust to invariant\ndomain. Furthermore, Muandet et al. [ 11] presented a kernel-based optimization\nalgorithm, called Domain-Invariant Component Analysis, to learn an invariant\ntransformation by minimizing the dissimilarity across domains. Inconsistently,our method does not require extra networks, layers and complex adversarial\ntraining strategy (e.g. domain classiﬁers) for learning robustness models. Our\nmethod is just a plug-in module with negligible calculation and we wish ourproposed can be a supplement to any domain generalization task, despite just\napplied for segmentation in this paper.\n3M e t h o d\nIn this section, we ﬁrst introduce the overview in Subsect. 3.1and framework in\nSubsect. 3.2of our method in outline. We then detail our plug-in module in Sub-\nsect.3.3involving how to select the style-sensitive channels and how to randomly\nerase the features in them. At last, we show our training loss in Subsect. 3.4.\n3.1 Overview\nWe argue that the channels of learning features should be divided into style-\nsensitive channels and semantic-sensitive channels due to the indisputable fact\nthat the networks always fall into over-ﬁtting when trained with single style', '304 S. Su et al.\ndata. In respond to such issue, our method are concluded as followed: ﬁrstly,\nwe detect style diﬀerences between image and it transformation and then ﬁndout the style-sensitive channels. After that, we randomly erase the features of\nthe style-sensitive channels to prevent the model from over-ﬁtting in speciﬁc\ndomains, thereby enhancing the domain generalization ability of segmentationmodels. Furthermore, by erasing style-sensitive-channel features, the model will\nbe forced to pay more attention to other semantic-sensitive-channel channels.\n3.2 Framework\nGiven source-domain images X\nswith pixel-wise labels Ysand a set of unseen\ntarget domain Dt={D1,...,D K}. Our goal is to generalize segmentation mod-\nelsMtraining with source domain images Xsto unseen target domains. To\nachieve this, we extract feature map f=M(x)∈RC×H×Wof an image x;a n d\nbased on the assumption that the style shifts in domain includes color and blur-\nriness, we employ augmentation function τ(x), e.g. color jittering and Gaussian\nblurring, widely used in fully-supervised tasks, to change the style of images\nwithout changing its semantics; then we feed τ(x)t oMto get feature map,\ni.e.f/prime=M(τ(x))∈RC×H×W. After that we combine fandf/primeto explore the\nstyle-sensitive channels. And then, we can randomly erase the feature of the cor-\nresponding style-sensitive channels in the upcoming learning. Additionally, our\nnetwork is trained under supervision of ISW loss and Cross-entropy loss. Theoverall framework is illustrated in Fig. 2.\n3.3 Randomly Erasing Style-sensitive-Channel Features\nIn this section, we will introduce our proposed plug-in module of randomly eras-\ning style-sensitive-channel features comprehensively, as shown in Fig. 3. As men-\ntioned above, we feed an original image and its transformation to model respec-\ntively to obtain the features f∈R\nC×H×Wandf/prime∈RC×H×Wrespectively. Next,\nwe ﬂatten them to ¯f=flatten (f)∈RC×HWand¯f/prime=flatten (f/prime)∈RC×HW\nrespectively. In the above notation, Cdenotes the number of channels, Hand\nWare the spatial dimensions of the feature maps. For convenience, we will use\n¯Fas the uniﬁed notation of ¯fand¯f/prime. In the style transfer, Gram matrix is often\nused to represent the style of images [ 28]. Just like them, we measure the style\nof our images by Gram matrix:\nGram =¯F◦¯FT∈RC×C(1)\nWe standardize the learning features ¯Fto¯Fsthrough an instance normalization\nshown in [ 8]. It is very worthy of our reminding that normalization does not\nchange the distribution of values, that is, larger values are still larger and thesmaller values are still smaller after regularization. Therefore, the standardiza-\ntion does not aﬀect our purpose of obtaining style-sensitive channels:\n¯F\ns=(diag(Gram µ))−1\n2⊙(¯F−μ⊙1T) (2)', 'Suppressing Style-Sensitive Features via Randomly Erasing 305\nFig. 3. Illustrating of our plug-in module . The orange solid box indicates the\nstream of getting style-sensitive channels by feeding image and its transformation. The\nred solid box shows the stream of erasing style-sensitive-channel features and the red\ndashed box in it represents erasing features on one channel. SCRE, SCM and REin ﬁgure represent randomly erasing style-sensitive-channel features module, getting\nstyle-sensitive channels and randomly erasing respectively. (Color ﬁgure online)\nwhere ⊙is element-wise multiplication, and diag(Gµ)∈RC×1denotes the col-\numn vector consisting of diagonal elements in Gram . Note that each diagonal\nelement is copied along with the spatial dimension HWfor element-wise multi-\nplication. Therefore, we can get the normalized Gram matrix:\nGrams =¯Fs◦¯FsT∈RC×C(3)\nSpeciﬁcally, we denote the normalized Gram matrix from ¯FsasGrams and from\n¯F/primesasGrams /prime. And then we calculate the variance matrix between Grams and\nGrams /primeto catch diﬀerences between them, as shown the orange solid box in\nFig.3. Formally, the variance matrix V∈RC×Cis deﬁned as:\nV=1/NN/summationdisplay\ni=1Δ2\ni (4)\nfrom mean M GiandΔ2\nifori-th image from two normalized Gram matrices, i.e.\nM Gi=1\n2(Grams i+Grams /primei) (5)\nΔ2\ni=1\n2(Grams i−M Gi)2+(Grams /primei−M Gi)2(6)\nwhere Nis the number of samples. As a result, Vcan include all style shifts in our\nargumentation technology. Following [ 7],Vis split into kclusters with K−means', '306 S. Su et al.\nclustering, which are separated to two groups according to the variance values.\nSpeciﬁcally, the mclusters with greater values compose a group, denoted as H\nand the remaining clusters compose the other group. The kandmare set to 3\nand 1 in our network. Thus we can get a style-sensitive mask matrix as followed:\nMask i,j=/braceleftbigg1if V i,j∈H\n0otherwise(7)\nThen we sum up per row in Mask :\nKi=C/summationdisplay\nj=0Mask i,j (8)\nwhere K∈RC×1is a column vector. The value of Kimeans how sensitive the i-\nth channel is to style. We set a parameter ρ∈(0,1) to determine the proportion\nof style-sensitive channels:\nSSC i=/braceleftbigg1ifK i∈top(ρ|K|)\n0otherwise(9)\nwhere topk(·),|K|represent the top ·largest values in Kand the length of\nK. Specially, SSC i= 1 means that the i-th channel is a style-sensitive chan-\nnel. Finally, we erase the style-sensitive-channel features with the motivation of\nremoving style information:\nEi,j∈[0,1]∼Bernouli (p) (10)\nFe=F⊙SSC⊙E+F⊙(1−SSC) (11)\nwhere Bernouli (p) denotes taking 1 with probability punder Bernouli Distri-\nbution, otherwise 0. And Fe,F⊙SSC⊙EandF⊙(1−SSC) denote fea-\nture maps, style-sensitive-channel features with random erasure and semantic-sensitive-channel features respectively. ⊙is an element-wise multiplication and\nSSC is coped to the dimension as F∈R\nC×H×W. After that, we feed Feto\nmodel and get its prediction ˆY.\n3.4 Training Loss\nTraining our model requires two losses, one is Cross-entropy loss commonly used\nin full-supervised segmentation, and the other is ISW loss from [ 7]. Overall, our\nloss function can be recorded as:\nL=1\nHW/summationdisplay\nh,w/summationdisplay\nc−y(h,w,c )log ˆy(h,w,c )+E[||Grams ⊙M||] (12)\nwhere HandWis the height and weight of the image, and ˆ y(·)is the prediction\nof a pixel. And Edenotes the arithmetic mean.', 'Suppressing Style-Sensitive Features via Randomly Erasing 307\n4 Experiment\nIn this section, we conduct extensive experiments on simulation-to-others and\nreal-to-others domain generalization for semantic segmentation task. We com-pare our method with state-of-the-art methods on several datasets: Cityscapes\n[22], BDD-100K [ 23], Mapillary [ 24], SYNTHIA [ 25] and GTAV [ 26]. Specially,\nCityscapes [ 22], BDD-100K [ 23] and Mapillary [ 24] are real-world datasets, while\nGTAV [ 26] and SYNTHIA [ 25] are synthetic datasets. And we discuss the advan-\ntage of our method through ablation studies with the setting of randomly eras-\ning all style-sensitive-channel features and randomly erasing features. Followingcommon practice in semantic segmentation, the mean intersection over union\n(mIoU) will be reported in the all experiments.\n4.1 Datasets\nCityscapes [22] consists of 5000 ﬁne-annotated and 20000 coarse-annotated\nreal-world urban traﬃc scene images, which is widely used in semantic segmen-\ntation. We only use the ﬁne-annotated images for our experiments, 2975, 500,and 1525 for training, validation, and test respectively.\nBDD-100K [23] is a real-world dataset that contains diverse urban driving\nscene videos. The videos are split into training (70K), validation (10K) andtesting (20K) sets. The frame at the 10-th second in each video is annotated for\nimage tasks.\nMapillary [24] contains 25000 high resolution images annotated into 66 object\ncategories all around the world. Specially, it is captured at various conditions\nregarding weather, season, and daytime.SYNTHIA [25] is the subset of SYNTHIA-RAND-CITYSCAPES, which con-\ntain 9400 pixel-level annotated images synthesized from a virtual.\nGTA V [26] is another synthetic dataset with 24966 driving-scene images gener-\nated from Grand Theft Auto V game engine. Specially, It contains 12403, 6382,\nand 6181 images for training, validation, and test.\n4.2 Implementation Details\nWe implement our framework in PyTorch [ 21], and our experiments are con-\nducted on NVIDIA GetForce 1080Ti GPU, the memory of which is 11 GB.\nIn all experiments, we adopt DeepLabv3+ as our segmentation architectureand Resnet-50 as backbone, following [ 7]. The optimization of our network is\nStochastic Gradient Descent with a initial learning late of 1e–2, momentum of\n0.9 and the decay of polynomial learning scheduling [ 27] is set to 0.9. Besides, The\nmaximum training iterations is 40K in all experiments. We conducted common\naugmentations such as color-jitering, Gaussian blur, random cropping, random\nhorizontal ﬂipping, and random scaling with the range of [0 .5,2.0] to prevent the\nover-ﬁtting. For the argumentation function in inferring style-sensitive channels,\nwe apply color jittering and Gaussian blur, due to the style shifts in diﬀer-\nent domain rose by color and blurriness. It is worth noted that we empirically', '308 S. Su et al.\nget style-sensitive channels between 5-th epoch and 6-th epoch in the train-\ning stage. The reason for choosing this timing is that the network has learnedmost of the semantic in images, and it has not been over-ﬁtted. Moreover, we\ntrain DeepLabV3+ with instance normalization layers in the ﬁrst ﬁve epochs\nand after that, we add Selected Whitening Loss [ 7] and our random erasure of\nstyle-sensitive-channel features to train model.\nAs for the selection of the sensitive channel parameter ρand probability pin\nBernouli , we randomly set it to 0 .3 and 0 .8. Therefore, The random erasure is\nconducted to 0 .3×Cchannels and approximately 20% of style-sensitive features\nare erased.\n4.3 Performance Comparison\nWe compare our approach to the current state-of-the-art methods on two domain\ngeneration scenarios: real (Cityscapes) to others (BDD-100K, Mapillary, GTAVand SYNTHIA) in Table 1, simulation (GTAV) to others (Cityscapes, BDD-\n100K, Mapillary and SYNTHIA) in Table 2.\nReal to others in Table 1. Here, training dataset is Cityscapes, validation\ndatasets are BDD-100K, Mapillary, GTAV and SYNTHIA. Our method achieves\na improvement over the best published results of the same backbone (ResNet-50),\ndemonstrating the generalization of our method. In detail, we achieve state-of-the-art accuracy in Mapillary, GTAV and SYNTHIA. However, the accuracy in\nBDD-100K in our method has fallen slightly, with only −0.20.\nTable 1. The comparision of Semantic segmentation performance mIoU on real-world\ndataset Cityscapes without accessing to others. Resnet-50 and DeepLabV3+ are basic\narchitectures in all models for fair comparision.\nModel (Cityscapes) BDD-100K Mapillary GTAV SYNTHIA Cityscapes\nBaseline 44.96 51.68 42.55 23.29 77.51\nIBN-Net [ 3] 48.56 57.04 45.06 26.14 76.55\nRobustNet [ 7] 50.73 58.64 45.00 26.20 76.41\nOurs 50.53 58.85 45.95 26.75 76.20\nSimulation to others in Table 2. As mentioned above, GTAV is a synthetic\ndataset, which can demonstrate the generalization when our network trains withsynthetic images. As shown in Table 2, our method achieves the best results\ncompared to other domain generalization methods in all datasets. Speciﬁcally,\nour method get improvement of +1.02, +0.44, +1.54 and +0.91 respectively inCityscapes, BDD-100K, Mapillary and SYNTHIA. This proves the superioty of\nour method in synthetic datasets. It is noted that the performance on GTAV\nitself has a slight drop, about 1.37 to Baseline, 0.82 to IBN-Net [ 3] and 0.02 to\nRoubustNet [ 7].', 'Suppressing Style-Sensitive Features via Randomly Erasing 309\nTable 2. The comparision of Semantic segmentation performance mIoU on synthetic\ndataset GTAV without accessing to others. Resnet-50 and DeepLabV3+ are basic\narchitectures in all models for fair comparision.\nModel (GTAV) Cityscapes BDD-100K Mapillary SYNTHIA GTAV\nBaseline 28.95 25.14 28.18 26.23 73.45\nIBN-Net [ 3]33.85 32.30 37.75 27.90 72.90\nRobustNet [ 7]36.58 35.20 40.33 28.30 72.10\nOurs 37.60 35.64 41.54 29.21 72.08\n4.4 Ablation Study\nTo analyze the eﬀectiveness of our proposed model, we conduct ablation studies\non the following setting: erasing all style-sensitive-channel features to prove the\nrationality of our random erasure and randomly erasing all channels features toshow the necessity of selecting style-sensitive channels. As Table 3shown, erasing\nall style-sensitive-channel features drop the accuracy sharply, the reason of which\nis that style-sensitive channels contain more style information, and does not meanthat they don’t have semantic information. This is why we call style-sensitive\nchannels not style channels. About randomly erasing the all channels, although\nits performance does not drop as much as the above setting, the decline of itsaccuracy shows that it doesn’t make sense due to its erasing semantic features.\nThis is because randomly erasing all features is easy to remove semantic features,\nwhich has a large proportion in channels.\nTable 3. Ablation study of our method. SCRE represents Randomly Erasing Style-\nsensitive- Channel features, SCAE means Erasing AllStyle-sensitive- Channel features\nand ACRE is noted for Randomly Erasing the features of AllChannels.\nModel (GTA V) Cityscapes BDD-100K Mapillary SYNTHIA GTA V\nRobustNet [ 7] 36.58 35.20 40.33 28.30 72.10\nOurs (RobustNet[ 7]+SCRE) 37.60 35.64 41.54 29.21 72.08\nRobustNet [ 7]+SCAE 32.99 30.64 35.48 25.64 71.49\nRobustNet [ 7]+ACRE 35.59 33.65 39.84 27.48 70.7\n5 Conclusion\nIn this work, we rethink the composition of learning features: style-sensitive-\nchannel features and semantic-sensitive-channel features. In order to enhance\nthe generalization ability of network, we propose a plug-in module to erase the\nfeatures of style-sensitive channels randomly without additional loss function.And a lot experiments on two setting (i.e., simulation to real and real to sim-\nulation) demonstrate the advantage of our method when comparing with other\nstate-of-the-art methods.', '310 S. Su et al.\nAcknowledgement. This work is partially supported by National Natural Science\nFoundation of China (Grants no. 61772568), Guangdong Basic and Applied Basic\nResearch Foundation (Grant no. 2019A1515012029), and Youth science and technologyinnovation talent of Guangdong Special Support Program.\nReferences\n1. Blanchard, G., Lee, G., Scott, C.: Generalizing from several related classiﬁcation\ntasks to a new unlabeled sample. Adv. Neural Inf. Process. Syst. 24, 2178–2186\n(2011)\n2. Yue, X., Zhang. Y., Zhao, S., et al.: Domain randomization and pyramid consis-\ntency: simula-tion-to-real generalization without accessing target domain data. In:\nProceedings of the IEEE/CVF International Conference on Computer Vision, pp.2100–2110 (2019)\n3. Pan, X., Luo, P., Shi, J., et al.: Two at once: enhancing learning and generalization\ncapacities via ibn-net. In: Proceedings of the European Conference on ComputerVision (ECCV), pp. 464–479 (2018)\n4. Volpi, R., Murino, V.: Addressing model vulnerability to distributional shifts over\nimage transformation sets. In: Proceedings of the IEEE/CVF International Con-ference on Computer Vision, pp. 7980–7989 (2019)\n5. Li, Y., Tian, X., Gong, M., et al.: Deep domain generalization via conditional\ninvariant adver-sarial networks. In: Proceedings of the European Conference onComputer Vision (ECCV), pp. 624–639 (2018)\n6. Li, H., Pan, S.J., Wang, S., et al.: Domain generalization with adversarial feature\nlearning. In: Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp. 5400–5409 (2018)\n7. Choi, S., Jung, S., Yun, H., et al.: RobustNet: improving domain generalization in\nurban-scene segmentation via instance selective whitening. In: Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11580–\n11590 (2021)\n8. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Instance normalization: the missing ingre-\ndient for fast stylization. arXiv preprint arXiv:1607.08022 (2016)\n9. Ioﬀe, S, Szegedy, C.: Batch normalization: accelerating deep network training by\nreducing internal covariate shift. In: International Conference on Machine Learning,pp. 448–456. PMLR (2015)\n10. Ghifary, M., Kleijn, W.B., Zhang, M., et al.:Domain generalization for object recog-\nnition with multi-task autoencoders. In: Proceedings of the IEEE InternationalConference on Computer Vision, pp. 2551–2559 (2015)\n11. Muande, K., Balduzzi, D., Sch¨ olkopf, B.: Domain generalization via invariant fea-\nture representation. In: International Conference on Machine Learning, pp. 10–18.\nPMLR (2013)\n12. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmenta-tion. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 3431–3440 (2015)\n13. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomed-\nical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.\n(eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015).\nhttps://doi.org/10.1007/978-3-319-24574-4\n28', 'Suppressing Style-Sensitive Features via Randomly Erasing 311\n14. Badrinarayanan, V., Kendall, A., Cipolla, R.: Segnet: a deep convolutional encoder-\ndecoder architecture for image segmentation. IEEE Trans. Patt. Anal. Mach. Intell.\n39(12), 2481–2495 (2017)\n15. Chen, L.C., Papandreou, G., Kokkinos, I., et al.: Deeplab: semantic image seg-\nmentation with deep convolutional nets, atrous convolution, and fully connected\nCRFS. IEEE Trans. Patt. Aanal. Mach. Intell. 40(4), 834–848 (2017)\n16. He, K., Zhang, X., Ren, S., et al.: Spatial pyramid pooling in deep convolutional\nnetworks for visual recognition. IEEE Trans. Pattern Anal. Mach. Intell. 37(9),\n1904–1916 (2015)\n17. Zhao, H., Shi, J., Qi, X., et al.: Pyramid scene parsing network. In: Proceedings of\nthe IEEE Conference on Computer Vsion and Pattern Recognition, pp. 2881–2890(2017)\n18. Chen, L.C., Papandreou, G., Schroﬀ, F., et al.: Rethinking atrous convolution for\nsemantic image segmentation. arXiv preprint arXiv:1706.05587 (2017)\n19. Chen, L.C., Zhu, Y., Papandreou, G., et al.: Encoder-decoder with atrous separable\nconvolution for semantic image segmentation. In: Proceedings of the European\nConference on Computer Vision (ECCV), pp. 801–818 (2018)\n20. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. arXiv\npre-print arXiv:1511.07122 (2015)\n21. Paszke, A., Gross, S., Massa, F., et al.: Pytorch: an imperative style, high-\nperformance deep learning library. arXiv preprint arXiv:1912.01703 (2019)\n22. Cordts, M., Omran, M., Ramos, S., et al.: The cityscapes dataset for semantic\nurban scene understanding. In: Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pp. 3213–3223 (2016)\n23. Yu, F., Chen, H., Wang, X., et al.: Bdd100k: a diverse driving dataset for het-\nerogeneous multitask learning. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 2636–2645 (2020)\n24. Neuhold, G., Ollmann, T., Rota Bulo, S., et al.: The mapillary vistas dataset for\nsemantic understanding of street scenes. In: Proceedings of the IEEE InternationalConference on Computer Vision, pp. 4990–4999 (2017)\n25. Ros, G., Sellart, L., Materzynska, J., et al.: The synthia dataset: a large collection\nof synthetic images for semantic segmentation of urban scenes. In: Proceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 3234–3243\n(2016)\n26. Richter, S.R., Vineet, V., Roth, S., Koltun, V.: Playing for data: ground truth\nfrom computer games. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV\n2016. LNCS, vol. 9906, pp. 102–118. Springer, Cham (2016). https://doi.org/10.\n1007/978-3-319-46475-6\n7\n27. Liu, W., Rabinovich, A., Berg, A.C.: Parsenet: looking wider to see better. arXiv\npreprint arXiv:1506.04579 (2015)\n28. Li, Y., Fang, C., Yang, J., et al.: Universal style transfer via feature transforms.\narXiv preprint arXiv:1705.08086 (2017)', 'MAGAN: Multi-attention Generative\nAdversarial Networks for T ext-to-Image\nGeneration\nXibin Jia, Qing Mi, and Qi Dai(B)\nBeijing University of Technology, Beijing 100124, China\ndaiqi@emails.bjut.edu.cn\nAbstract. Although generative adversarial networks are commonly used in text-\nto-image generation tasks and have made great progress, there are still some prob-lems. The convolution operation used in these GANs-based methods works on\nlocal regions, but not disjoint regions of the image, leading to structural anomalies\nin the generated image. Moreover, the semantic consistency of generated imagesand corresponding text descriptions still needs to be improved. In this paper, we\npropose a multi-attention generative adversarial networks (MAGAN) for text-to-\nimage generation. We use self-attention mechanism to improve the overall qualityof images, so that the target image with a certain structure can also be generated\nwell. We use multi-head attention mechanism to improve the semantic consistency\nof generated images and text descriptions. We conducted extensive experimentson three datasets: Oxford-102 Flowers dataset, Caltech-UCSD Birds dataset and\nCOCO dataset. Our MAGAN has better results than representative methods such\nas AttnGAN, MirrorGAN and ControlGAN.\nKeywords: Text-to-image generation ·Generative adversarial networks ·\nAttention mechanism\n1 Introduction\nThe task of Text-to-image (T2I) generation is to input a text description and output\nan image. This task has great potential in many ﬁelds. As GANs [ 1] have made great\nprogress in generating real images, T2I generation has also made great progress.\nIn 2016, Reed et al. [ 2] used GANs for the T2I generation task. They used recurrent\nneural networks (RNNs) [ 3] to encode natural language descriptions into text vectors,\nand controlled the generated content through text vectors. They also added CNN to thenetwork, which improved the overall authenticity of the generated images and the speed\nof network convergence. Most existing methods [ 4–12] are based on this idea. However,\nthey have the following problems.\nOne problem is that GANs are not effective in generating objects with a certain struc-\nture, and the objects in the generated images have incomplete structures. This problem is\ncaused by the dependence of GANs on CNN, CNN mainly acts on the local area of eachposition of the image, and the long-distance dependence between disjoint regions in an\n© Springer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 312–322, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_26', 'MAGAN: Multi-attention Generative Adversarial Networks 313\nimage needs to be achieved by a multi-layer CNN. The multi-layer CNN will cause the\nproblem of excessive calculation, so GANs may have a very bad effect on the generationof objects with a certain structure. For this reason, we add the self-attention module to\nachieve long-distance dependency modeling and make generated images more realistic\nand believable.\nAnother problem is that these methods do not take into account related words well\nwhen generating image subregions. Although AttnGAN [ 7] used the attention mech-\nanism for the ﬁrst time in T2I generation, so that when generating a subregion of theimage, it will consider the inﬂuence of different words on the subregion, but each word\nhas a different meaning from a different perspective. AttnGAN may only consider the\nimpact of some words on a certain subregion when generating images, and does notconsider the impact of other words on the subregion from different perspectives. For\nthis reason, we propose a multi-head attention module that enables model to compre-\nhensive consider all words, and enhances the consistency of generated images and text\ndescriptions.\nThe main contributions are summarized as follows:\n(1) We propose a self-attention mechanism in the network. The self-attention mecha-\nnism helps images establish long-distance dependence, which can further improvethe overall authenticity of the image.\n(2) We propose a multi-head mechanism in the network. The multi-head mechanism\nhelps model to focus on different aspects of text information and improves theconsistency of generated images and text descriptions.\n2 Related Work\n2.1 T ext-to-Image Generation\nIn 2016, Reed et al. [ 2] proposed GAN-INT-CLS based on CGAN [ 14] and DCGAN\n[15], which inputs a text as a condition vector into the network to control the generation\nof image. Reed et al. [ 4] also proposed GAWWN. Because of GAWWN can only gen-\nerate a resolution of 64 ×64 images, Zhang et al. [ 5] proposed StackGAN. StackGAN\nincludes two stages. A multi-stage network can be used to obtain images with a reso-\nlution of 256 ×256. Since the StackGAN could not train completely, Zhang et al. [ 6]\nproposed the StackGAN++. StackGAN++ includes three stages and could train com-\npletely. Xu et al. [ 7] proposed the AttnGAN. AttnGAN used the attention mechanism\nthat will pay attention to related words when generating different areas of the image,thereby synthesizing ﬁne-grained details. Qiao et al. [ 8] proposed MirrorGAN, which\nconverts the generated image into a new text, and compares it with the original text as a\nreconstruction loss to improve the consistency of generated images and text descriptions.\n2.2 Attention Mechanism\nThe attention mechanism has been used in target machine translation [ 16] and image\ncaptioning [ 17]. Transformer [ 13] used the self-attention mechanism to calculate the', '314 X. Jia et al.\ninﬂuence of other words in the same sequence on the word when encoding a word.\nTranformer achieved the most advanced results in the machine translation model atthe time. SAGAN [ 18] ﬁrst used the self-attention mechanism for GANs. GANs are\ngood at generating images with less structural constraints. The self-attention mechanism\ncan build long-distance dependencies between different regions of the image so thatGANs can generate images with higher structural constraints. SAGAN has also achieved\nthe most advanced results on ImageNet [ 19]. In response to the problem of excessive\ncalculation of the self-attention module in SAGAN, Cao et al. [ 20] simpliﬁed the self-\nattention mechanism and reduced the amount of calculation when the improvement\neffect was the same.\n2.3 Multi-head Mechanism\nAttnGAN [ 7] is one of the classic networks of multi-stage GANs. Many T2I works are\nmodiﬁed on AttnGAN. The three stages of the network generate images with differentresolutions. A word-level attention mechanism is used between each two stages, so that\nthe generator will pay attention to different words. However, a subregion can be very\nrelated to multiple texts from different perspectives. In order to make the model pay\nattention to different aspects of text information, we combine the multi-head mechanism\nof the encoder in Transformer [ 13] with the attention mechanism in AttnGAN. Different\nheads consider the words related to the subregion from different perspectives, and ﬁnally\nsummarize the information under the multi-head, so that the model can comprehensively\nconsider the relevant text information when generating a subregion.\n3 Multi-Attention Gan (MAGAN)\n3.1 Model Overview\nAs shown in Fig. 1. Given a text description, we ﬁrst use a pre-trained RNN [ 7]t o\nencode this description into sentence-level features s∈RDand word-level features\nw=/braceleftbig\nwl|l=0,..., L−1}∈ RD∗L,Dis the dimension of the feature, Lis the number\nof words, then we apply conditioning augmentation (CA) [ 5] to sentence-level features,\nthen the conditionally augmented sentence-level features and random noise sampled in\nthe Gaussian distribution are input into the ﬁrst stage of the network. The overall processcan be expressed as:\nV\n0=F0(z,sca), (1)\nHi−1=FSA i−1(Vi−1),i∈{1,2,..., m−1}, (2)\nXi=Gi(Hi−1),i∈{0,1,2,..., m−1}, (3)\nVi=Fi/parenleftbig\nHi−1,FMH i−1(Hi−1,w)/parenrightbig\n,i∈{1,2,..., m−1}, (4)\nWhere zis a random noise, Viis the i-stage visual feature, FSA iis the self-attention\nmodule, FMH iis the multi-head attention module, Xiis the i-stage generated image, Fi\nis the visual feature transformers.', 'MAGAN: Multi-attention Generative Adversarial Networks 315\nFig. 1. The architecture of MAGAN.\nThe network has three stages. The visual features Viobtained at each stage will\nﬁrst pass through a self-attention module FSA i, and then the visual features Hiafter the\nself-attention module will be input to different generators Gito generate images with\ncorresponding resolutions. Hiwill obtain the visual features weighted by the attention\nmechanism through the multi-head attention module FMH i. Then Hiis combined with\nthe visual features weighted by the multi-head attention mechanism. The visual feature\nViafter fusion will be used as the input of the next stage of the network.\n3.2 Self-attention Module\nThe visual features obtained after upsampling at each stage will ﬁrst pass through a self-\nattention module. NLNet [ 21] obtains the attention map by calculating the relationship\nbetween each location, and calculates the importance of other location features to the\ncurrent location feature through the softmax function. Finally the attention map assignedwith the weight is aggregated with the features of all positions. Here we use the simpliﬁed\nself-attention mechanism in GCNET [ 20].\nAs shown in Fig. 2. We calculate the visual features V\nidirectly through a\nconvolutional layer to obtain a global attention map fi:\nfi=Wk(Vi), (5)\nfiuse softmax function to assign a weight to the feature of each position to get pi:\npi=softmax (fi), (6)\nThen we multiply piand the visual feature Vito obtain the visual feature, and then use a\nconvolutional layer to convert this visual feature to obtain the feature ti, so that the ﬁnal\nfeature tiis consistent with the input Vidimension:\nti=Wv(Vipi), (7)', '316 X. Jia et al.\nFig. 2. Self-attention module.\nFinally, the element-wise add operation is used to integrate the global attention map into\nthe features of each location to complete the modeling of long-distance dependencies:\nFSA i(Vi)=Vi+ti, (8)\n3.3 Multi-head Attention Module\nAs shown in Fig. 3. We combine the multi-head mechanism [ 13] in the transformer\nwith AttnGAN’s attention mechanism [ 7] as a multi-head attention module. The module\ncontains two inputs: word-level features wand visual features Vi. The word-level feature\nwis ﬁrst changed in dimension, wis changed from D×LtoC×L. We obtain the attention\nmap by calculating the word-level feature wafter the dimension transformation and the\nvisual feature Vi, then we normalize the attention map through the softmax function and\nmatrix transpose. Finally, we obtain the attentive word-context feature by calculating the\nword-level feature wafter the dimension transformation and the normalized attention\nmap:\nAttj\ni−1=/summationdisplay L−1\nl=0(Ujwl)(softmax (VT\ni−1(Ujwl)))T,j∈{1,2,..., n}, (9)\nThe multi-head mechanism calculates the attentive word-context feature of multiple\nattention mechanisms in multiple spaces, and uses the element-wise add operation for\nthe attentive word-context features in these multi-head spaces:\nFMH i(Vi−1,w)=/summationdisplay n−1\nj=0Attj\ni−1,,j∈{1,2,..., n−1}, (10)\nFinally, concatenation with Vi−1to obtain the visual features Vi, and Viwill be used as\nthe input of the next stage of the network.\nThrough the attention mechanism, the generator can take into account the correlation\nof different words when generating the subregion of the image. However, a word has\ndifferent meanings from different angles. When generating the subregion of the image,we comprehensively consider more word information through the multi-head mecha-\nnism, and ﬁnally summarize the word information in different spaces through the add\noperation, so that the generated images are more in line with the text descriptions.', 'MAGAN: Multi-attention Generative Adversarial Networks 317\nFig. 3. Multi-head attention module.\n3.4 Objective Functions\nThe overall loss function of generator G is deﬁned as:\nLG=/summationdisplay\niLGi+λ1LDAMSM , (11)\nThe generator loss function includes adversarial loss and the text-image matching loss\nDAMSM [ 7].λ1is the hyperparameter used to balance these two losses. In the i-th stage,\nthe adversarial loss of the corresponding generator is deﬁned as:\nLGi=−1\n2[Ex∼PGilogD i(x)+Ex∼PGilogD i(x,s)], (12)\nWhere xis the image generated by the generator, sis the given text, the ﬁrst part is the\nunconditional adversarial loss which makes the image more realistic, and the latter part\nis the conditional adversarial loss which makes the image match the text description.\nThe overall loss function of the discriminator D is deﬁned as:\nLD=/summationdisplay\niLDi, (13)\nIn the i-th stage, the adversarial loss of the corresponding discriminator is deﬁned as:\nLDi=−1\n2[Ex∼PdatalogD i(x)+Ex∼PGilog(1−Di(x)),\n+Ex∼PdatalogD i(x,s)+Ex∼PGilog(1−Di(x,s))], (14)\nSame as the generator, the ﬁrst part is the unconditional loss, and the latter part is the\nconditional loss.\n4 Experiments\n4.1 Datasets\nWe use Caltech-UCSD Birds dataset [ 22], Oxford-102 Flowers dataset [ 23] and COCO\ndataset [ 24]. Each image has many descriptions.', '318 X. Jia et al.\n4.2 Evaluation Metric\nWe choose Inception Score [ 25] (IS) and Fréchet Inception Distance [ 26] (FID) as\nevaluation indicators.\nIS is deﬁned as:\nI=exp(ExDKL(p(y|x)||p(y))), (15)\nWhere xis the generated image, yis the image label predicted by the pre-trained Inception\nV3 network [ 27], and IS calculates the Kullback-Leibler (KL) divergence between the\nconditional distribution and the marginal distribution. High IS means that images are\nwell generated.\nFID is deﬁned as:\nF(r,g)=/bardblμr−μg/bardbl2+trace/parenleftBig\nΣr+Σg−2(Σ rΣg)1/2/parenrightBig\n, (16)\nWhere ris the real data, gis the generated data, μr,μg,/Sigma1r,/Sigma1gare the means and\ncovariance matrixs of real data distribution and generated data distribution, respectively.Low FID means that images are well generated.\n4.3 Implementation Details\nThe optimizers all use Adam [ 28] optimizer, which β\n1is set to 0, β2is set to 0.999.\nTraining 600 epochs.\n4.4 Quantitative Results\nAs shown in Table 1. Compared with AttnGAN [ 7], the IS of MAGAN on Oxford-102\ndataset has increased from 3.75 to 4.11, the IS of MAGAN on CUB dataset has increased\nfrom 4.36 to 4.63, and the IS of MAGAN on COCO dataset increased from 25.89 to26.27. As shown in Table 2. Compared with AttnGAN, the FID of MAGAN on CUB\ndataset has dropped from 23.98 to 19.25, and the FID of MAGAN on COCO dataset\ndropped from 35.49 to 27.53. Compared with other methods, we can see that our methodhas obtained higher IS and lower FID on three datasets.\n4.5 Qualitative Results\nWe ﬁrst compare the images generated by representative methods and our proposed\nMAGAN on CUB dataset and Oxford-102 dataset. The results in Fig. 4and Fig. 5have\na resolution of 256 ×256.\nIn general, these mothods can generate images with higher resolution, which can\nbasically match the natural language description. Y ou can see that the birds generated\nby our proposed MAGAN have more real and clear details, such as the third column,for the text description, in the image generated by our method, the bird’s neck has black\nstripes, and the color of the neck is clearly generated, while other methods such as\nAttnGAN [ 7] do not clearly generate black stripes.', 'MAGAN: Multi-attention Generative Adversarial Networks 319\nTa b l e 1 . IS of representative methods and MAGAN on Oxford-102, CUB and COCO datasets.\nMethods Oxford-102 CUB COCO\nGAN_CLS_INT [ 2]2.66±0.03 2.88±0.04 7.88±0.07\nGAWWN [ 4] – 3.62±0.07 –\nStackGAN [ 5] 3.20±0.01 3.70±0.04 8.45±0.03\nStackGAN++ [ 6] 3.26±0.01 4.04±0.05 –\nHDGAN [ 11] 3.45±0.07 4.15±0.05 11.86 ±0.18\nAttnGAN [ 7] 3.75±0.02 4.36±0.03 25.89 ±0.47\nMirrorGAN [ 8] – 4.56±0.05 26.47 ±0.41\nControlGAN [ 9] – 4.58±0.09 24.06 ±0.60\nLeicaGAN [ 10] 3.92±0.02 4.62±0.06 –\nOurs 4.11 ±0.06 4.63 ±0.06 26.27 ±0.58\nTa b l e 2 . FID of representative methods and MAGAN on CUB and COCO datasets.\nMethods CUB COCO\nStackGAN++ [ 6]35.11 33.88\nAttnGAN [ 7] 23.98 35.49\nDMGAN [ 12] 16.09 32.64\nOurs 19.25 27.53\nIntuitively, there are three bird’s feet in the image generated by MirrorGAN [ 8]i n\nthe sixth column, and our method does not have this problem. Overall, it looks better\nthan other methods. The background of the image is also more natural, such as the ﬁrst\ncolumn and fourth column. The image background we generate is more delicate thanother image background.\nThen we compared the images generated by several methods on the Oxford-102\ndataset. It can be seen from the Fig. 5that the colors of the ﬂowers generated by methods\nsuch as AttnGAN are not bright enough, the petals have no sense of hierarchy, and the\nstamens are not clear. Our method can capture details, and the generated image is more\nlayered and background is more realistic.\nThe above proves the generation effect of our method, the generated image is more\nrealistic and reasonable, the semantic consistency corresponding to the text description\nis higher.', '320 X. Jia et al.\n The bird has a\nyellow breast\nand belly and a\nsmall bill.A bird with a red\nbreast, red eyebrow\nand a red crown.A yellow bird with\na black stripe on\nits neck,black bill\nand black wings\nwith a stripe ofyellow and white.As m a l l  g r a y\nand brown bird\nwith white tipped\nfeathers along the\nwings and a whitechest.A small bird that is\nalmost completely\nblue but has black\nfeathers on the tip\nof its wings, andshort, fat beak.This small gray\nbird has white\neyebrows and\nwhite and dark\ngray stripes on itsbreast and belly.This bird has soli\nd\nblack wings and a\nsolid black head.This bird is white\nwith blue and has\na very short beak.\nStackGAN+ +\nAttnGAN\nMirrorGAN\nOurs\nFig. 4. Examples results by StackGAN++, AttnGAN, MirrorGAN and MAGAN on CUB dataset.\nThis flower has\npetals that are\npink with yellow\nstemanThis flower haspetals that are\nyellow with\nyellow stamen.This odd looking\n flower has pink\npetals with darkred veins and lightyellow towards thecenter of the flower.The petals of the\nflower are long\nare pointy, and has\na vibrant orangecolor.The middle of the\nflower is brown in\ncolor and the petals\nare light pink in\ncolor.This flower has\npetals that are\npurple with yellow\nstamen.This flower has\nmulticolored petals\nthat are varied\nshades of peach\nand yellow.This flower features\nsmall delicate purple\npetals with a yellow\ntinge in the center.\nStackGAN++\nAttnGAN\nMirrorGAN\nOurs\nFig. 5. Examples results by StackGAN++, AttnGAN, MirrorGAN and MAGAN on Oxford-102\ndataset.\n4.6 Ablation Studies\nWe conduct ablation studies to verify the effectiveness of the two modules in MAGAN.\nAs shown in Table 3. Baseline stands for AttnGAN, SA stands for self-attention module,\nMH stands for multi-head attention module. Compared with the basic network, the IS of\nthe CUB dataset has increased to 4.48 and 4.50 respectively, and the IS of the Oxford-102\ndataset has increased to 3.93 and 3.90 respectively. This shows that the two modules bothplay a positive role in image generation. Combining the two modules can increase the', 'MAGAN: Multi-attention Generative Adversarial Networks 321\nIS of the CUB dataset to 4.63 and the IS of the Oxford-102 dataset to 4.11. Combining\nthe two modules can make the generators generate higher quality images.\nTa b l e 3 . The performance of different components of our module on CUB and Oxford-102\ndatasets.\nArchitecture CUB Oxford-102\nBaseline 4.36±0.03 3.75±0.02\nSA 4.48±0.05 3.93±0.04\nMH 4.50±0.04 3.90±0.03\n(SA)+(MH) 4.63 ±0.06 4.11 ±0.06\n5 Conclusion\nWe propose a new architecture called MAGAN for T2I generation tasks. Our proposed\nthe self-attention module helps non-adjacent regions to build long-distance correlation,\nwhich helps generators to generate higher quality image. Our proposed multi-head atten-tion module can consider all text information, which improves the semantic consistency.\nExperiment results demonstrate our MAGAN has excellent performance in text-to-image\ngeneration.\nAcknowledgement. This work is supported by Beijing Natural Science Foundation under No.\n4202004.\nReferences\n1. Goodfellow, I., Xu, B., et al.: Generative adversarial nets. In: NIPS, pp. 2672–2680 (2014)\n2. Reed, S., Akata, Z., et al.: Generative adversarial text to image synthesis. In: ICML, pp. 1060–\n1069 (2016)\n3. Cho, K., Gulcehre, C., Schwenk, H., et al.: Learning phrase representations using RNN\nencoder-decoder for statistical ma-chine translation. In: EMNLP (2014)\n4. Reed, S., Akata, Z., et al.: Learning what and where to draw. In: NIPS, pp. 217–225 (2016)\n5. Zhang, H., Xu, T., Li, H., et al.: StackGAN: text to photo-realistic image synthesis with\nstacked generative adversarial networks. In: ICCV , pp. 5907–5915 (2017)\n6. Zhang, H., Xu, T., Li, H., et al.: StackGAN++: realistic image synthesis with stacked\ngenerative adversarial networks. In: TPAMI, pp. 1947–1962 (2018)\n7. Xu, T., Zhang, P ., Huang, Q., et al.: AttnGAN: ﬁne-grained text to image generation with\nattentional generative adversarial networks. In: CVPR, pp. 1316–1324 (2018)\n8. Qiao, T., Zhang, J., Xu, D., et al.: MirrorGAN: learning text-to-image generation by\nredescription. In: CVPR, pp. 1505–1514 (2019)\n9. Li, B., Qi, X., et al.: Controllable text-to-image generation. In: NIPS, pp. 2065–2075 (2019)\n10. Qiao, T., Zhang, J., et al.: Learn, imagine and create: text-to-image generation from prior\nknowledge. In: NIPS, pp. 885–895 (2019)', '322 X. Jia et al.\n11. Zhang, Z., Xie, Y ., Yang, L.: Photographic text-to-image synthesis with a hierarchical-ly-\nnested adversarial network. In: CVPR, pp. 6199–6208 (2018)\n12. Zhu, M., Pan, P ., Chen, W., et al.: DM-GAN: dynamic memory generative adversarial networks\nfor text-to-image synthesis. In: CVPR, pp. 5802–5810 (2019)\n13. Shazeer, N., Jones, L., et al.: Attention is all you need. In: NIPS, pp. 5998–6008 (2017)\n14. Mirza, M., Osindero, S.: Conditional generative adversarial nets. arXiv preprint arXiv:1411.\n1784 (2014)\n15. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with deep\nconvolutional generative adversarial networks. In: ICLR (2016)\n16. Bahdanau, D., Cho, K., Bengio, Y .: Neural machine translation by jointly learning to align\nand translate. In: ICLR (2015)\n17. Xu, K., Ba, J., Kiros, R., et al.: Show, attend and tell: neural image caption generation with\nvisual attention. In: ICML (2015)\n18. Zhang, H., Goodfellow, I., Metaxas, D., et al.: Self-attention generative adversarial networks.\nIn: ICML, pp. 7354–7363 (2019)\n19. Russakovsky, O., Deng, J., Su, H., et al.: ImageNet large scale visual recognition challenge.\nIn: IJCV , pp. 211–252 (2015)\n20. Cao, Y ., Xu, J., Lin, S., et al.: GCNet: non-local networks meet squeeze-excitation networks\nand beyond. In: ICCV (2019)\n21. Wang, X., Girshick, R., Gupta, A., et al.: Non-local neural networks. In: CVPR (2018)22. Wah, C., Branson, P ., Welinder, P ., et al.: The Caltech-UCSD Birds-200-2011 Da-taset.\nCalifornia Institute of Technology, Technical Report CNS-TR-2011-001 (2011)\n23. Nilsback, M., Zisserman, A.: Automated ﬂower classiﬁﬁcation over a large number of classes.\nIn: ICVGIP , pp. 722–729 (2008)\n24. Lin, T.-Y ., et al.: Microsoft COCO: common objects in context. In: Fleet, D., Pajdla, T., Schiele,\nB., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp. 740–755. Springer, Cham (2014).https://doi.org/10.1007/978-3-319-10602-1_48\n25. Salimanx, T., Goodfellow, I., Zaremba, W., et al.: Improved techniques for training GANs.\nIn: NIPS, pp. 2226–2234 (2016)\n26. Heusel, M., Ramsauer, H., et al.: GANs trained by a two time-scale update rule con-verge to\na local nash equilibrium. In: NIPS, pp. 6626–6637 (2017)\n27. Szegedy, C., Ioffe, S., Shlens, J., et al.: Rethinking the inception architecture for com-puter\nvision. In: CVPR, pp. 2818–2826 (2016)\n28. Kingma. D., Ba, J.: Adam: a method for stochastic optimization. In: ICLR (2015)', 'Dual Attention Based Network with\nHierarchical ConvLSTM for Video Object\nSegmentation\nZongji Zhao and Sanyuan Zhao(B)\nSchool of Computer Science and Technology, Beijing Institute of Technology,\nBeijing, China\nzhaosanyuan@bit.edu.cn\nAbstract. Semi-supervised Video object segmentation is one of the\nmost basic tasks in the ﬁeld of computer vision, especially in the multi-\nobject case. It aims to segment masks of multiple foreground objects in\ngiven video sequence with annotation mask of the ﬁrst frame as prior\nknowledge. In this paper, we propose a novel multi-object video segmen-\ntation model. We use the U-Net architecture to obtain multi-scale spatialfeatures. In the encoder part, the spatial attention mechanism and chan-\nnel attention is used to enhance the spatial features simultaneously. We\nuse the recurrent ConvLSTM module in the decoder to segment diﬀerentobject instances in one stage and keep the segmentation object consis-\ntent over time. In addition, we use three loss functions for joint training\nto improve the model training eﬀect. We test our network on the popularvideo object segmentation dataset DAVIS2017. The experiment results\ndemonstrate that our model achieves state-of-art performance. Moreover,\nour model achieves faster inference runtimes than other methods.\nKeywords: Video object segmentation\n·ConvLSTM ·Attention\n1 Introduction\nVideo object segmentation is one of the most important tasks in the ﬁeld of com-\nputer vision. With the wide application and rapid development of deep learningin the ﬁeld of computer vision, video object segmentation methods based on\nconvolutional neural networks have achieved better results on video object seg-\nmentation benchmark. Traditional video object segmentation methods usually\nrequire manual design and capture appearance features such as color, bright-\nness, texture, and temporal features such as optical ﬂow and pixels matching,while the method based on CNN uses the feature representation learned from\nthe large-scale video object segmentation dataset to build the appearance model\nand the motion model in the video sequence.\nThis work was supported by the National Natural Science Foundation of China\n(61902027).\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 323–335, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_27', '324 Z. Zhao and S. Zhao\nWe tackle the video object segmentation problem in the semi-supervised\nsetting, where the ground truth mask of the ﬁrst frame is given and the aimis to estimate multiple objects masks in each subsequent frames. The recent\nsemi-supervised video object segmentation methods can be mainly categorized\ninto three classes: 1) online learning based; 2) feature matching based and 3)mask propagation based approaches. For the ﬁrst class, the annotation mask is\nused to online ﬁne-tune the pretrianed object segmentation network in the test\nphase. Although online learning methods can achieve impressive performance,the process of ﬁne-tuning takes up a lot of computing resources. The second class\nuses pixels similarities to match the object in the video frame with the object\nin the ﬁrst frame but often neglects both the object structure and appearanceinformation due to unordered pixels.\nIn order to lighten the model and increase the runtime speed while main-\ntaining performance, we choose a method based on mask propagation, and use\nthe prediction result of the previous frame to enhance the features of the cur-\nrent frame. In response to the problems in the mask propagation methods, wepropose that spatial attention and channel attention act on diﬀerent features to\nobtain valuable information in the feature maps, and use the recurrent ConvL-\nSTM decoder to gradually restore the scale of the feature maps. At the sametime, we use temporal information such as the prediction of the previous frame\nand the mask of the other objects to predict the mask of the current object in\nthe current frame.\nThe main contributions of our work are listed below:\n– We use the criss-cross attention module to obtain the spatial context infor-\nmation of the input frame at low computational cost, and use the channel\nattention module to obtain the channel semantic dependencies in the high-level feature, to obtain an enhanced spatial representation of the feature.\n– We propose a intra-frame self IOU loss function, which is used to evaluate\nthe diﬀerence of the mask of diﬀerent objects in the same frame, and jointlytrain the model with two other common loss functions.\n– We test our model on the DAVIS2017 [ 10], demonstrating that our model\nachieves state-of-art performance. Our model can achieve real-time speed of45.5 fps on GTX 1080Ti GPU.\n2 Related Work\n2.1 Semi-supervised Video Object Segmentation\nOnline Learning. Online Learning based methods ﬁne-tune the pre-trained\nnetwork by using the ﬁrst annotated frame to ﬁne-tune the pre-training network\nduring the test phase. For example, Caelles et al. proposed OSVOS [ 1]b a s e d\non FPN, which is still a static segmentation method in essence that learns theappearance feature of the object in the video sequence through an online learn-\ning process. In addition, OSVOS use object-speciﬁc appearance information to\nexpand the object’s mask, which greatly improves the result. Since the temporal', 'Dual Attention Based Network with Hierarchical ConvLSTM 325\ninformation contained in the video sequence is not considered, rapid changes\nin the object scale and shape will cause large errors. In order to overcome thelimitations of OSVOS, OnAVOS [ 13] updates the network online using training\nexamples selected based on the conﬁdence of the network and the spatial conﬁg-\nuration. Maninis et al. [ 6] combined the instance-level semantic information by\nusing instance proposals to improve segmentation performance.\nFeature Match Based. Feature matching based method calculates the simi-\nlarity matrix by using the features of the reference pixel and the target pixel, and\nfurther predicts these target pixels according to the learned similarity relation-\nship. Such as Yoon et al. [ 16] use the siamese network to extract features from the\nreference frame and prediction frame, obtain the pixel-level similarity through\nthe fully connected layer. VideoMatch [ 3] simultaneously match the foreground\nfeatures and background features in the reference frame and the target frame to\nmake the prediction result better. Seoung et al. [ 8] proposed a memory networks,\nthe past frames with object masks form an external memory, and the currentframe as the query is segmented using the mask information in the memory.\nMask Propagation Based. The method based on mask propagation uses the\ninitial reference frame and previous frame information to maintain the temporal\nand spatial consistency of the segmentation results. MaskTrack [ 9] combines the\nsegmentation result of the previous frame with the current frame to form a four-channel input feature to guide the segmentation network to predict. The deep\nsiamese encoder-decoder network [ 7] makes full use of the advantages of mask\npropagation and object detection, and can predict stably without any online\nlearning and post-processing procedures.\n3M e t h o d\nFig. 1. The Framework. For each frame of a video sequence, it is processed by the\nfollowing modules: the feature extraction network, the spatial context Branch (consists\nof three convolution layer and spatial attention module), the skip-connection (includes\na channel attention module), and the recurrent ConvLSTM decoder.', '326 Z. Zhao and S. Zhao\n3.1 Framework\nThe basic encoder-decoder architecture of video object segmentation model our\nproposed is based on RVOS [ 12]. The encoder uses ResNet50 [ 2] as the backbone,\nwhich is responsible for extracting high-level image semantic features. Let {It∈\nRW×H×3}T\nt=1denotes the input video with Tframes. ResNet will obtain multi-\nscale features for each frame ft={ft,2,ft,3,ft,4,ft,5}, at the same time, the spatial\ncontext branch extract the shallow spatial features HtofTinput video frames,\nand use the spatial attention mechanism to focus on the contextual information\nSt. The feature fusion module merges the middle level features ft,3obtained\nby the ResNet with the spatial context feature Stto obtain the fusion feature\nffu s e\nt. The fusion feature and other feature maps of the ResNet are reduced the\nnumber of channels by the skip-connection convolution and sent into decoder.Notice that the channel attention module is applied to the ﬁfth layer feature f\n/prime\nt,5\nof the feature extraction network to capture semantic dependencies of feature\nchannels and obtain the channel attention feature Ct. The decoder is based on\nthe hierarchical recurrent ConvLSTM module, using features obtained by the\nencoder f/prime\nt={f/prime\nt,2,f/prime\nt,3,f/prime\nt,4,Ct}, predict a set of object segmentation masks for\neach frame {M t,1,..., M t,N}. The overall framework is shown in Fig. 1.\n3.2 Encoder\nSpatial Context Branch. In order to obtain spatial context information while\nreducing the computational overhead,we use a simple spatial context branch,\nwhich contains three convolutional layers with the stride 2. In this way, the\nfeature map {Ht}T\nt=1with a size of 1 /8 of the input frame will be obtained,\nwhich has a larger spatial size than the deep feature map of the feature extraction\nmodule, so it will contain richer spatial details information, while having fewer\nfeature channels.\nFig. 2. Spatial Attention Module.', 'Dual Attention Based Network with Hierarchical ConvLSTM 327\nSpatial Attention Module. The spatial feature will focus on the eﬀective\npart of the feature through the spatial attention module to obtain global contextinformation. In order to further reduce the amount of parameters and calcula-\ntions of the spatial attention module, we use the criss-cross attention module [ 4]\nto collect contextual information between pixels in both horizontal and verticaldirections, and iteratively diﬀuse the horizontal and vertical context to the global\ncontext feature. As shown in Fig. 2, for the spatial feature H\nt∈Rc×w×hint-th\nframe, ﬁrstly, two feature maps Q∈Rc/prime×w×hand K∈Rc/prime×w×hare generated\nthrough two 1 ×1 convolutions respectively to reduce the dimension of the input\nfeature map. Next, the correlation matrix A∈R(h+w−1)×(w×h)is obtained from\nQand K. Speciﬁcally, the vector Qu∈Rc/primeof each position uin the feature map\nQis combined with the feature vector Ωu∈R(h+w−1)×c/primeof the points which\nare in the same row or the same column of position uon the feature map Kby\nmatrix multiplication to get the degree of correlation di,u∈D:\ndi,u=QuΩ/intercal\ni,u (1)\nwhere i=[ 1,2,...,h +w−1], and D∈R(h+w−1)×(w×h). Then, after Dpasses a\nsoftmax layer, the correlation matrix Ais obtained. Next, we use the correlation\nmatrix to update the input feature Ht.\nH/prime\nu=h+w−1/summationdisplay\ni=0Ai,uΦi,u+Hu (2)\nwhere Φu∈R(h+w−1)×cis the set of points which are in the same row or the\nsame column with the position uin feature Ht,H/prime\nuis the vector of the position\nuin the updated feature map H/prime.\nEach position in the feature H/primeobtained by the above steps gathers the\ncontext information of the points in the same row and the same column. We\niterate the criss-cross attention module twice to obtain richer spatial featuresincluding global context information. Speciﬁcally, we input the output feature\nH\n/primeof the ﬁrst criss-cross attention module into the second one, and after the\nsame steps, obtain the spatial context feature S, so that, Each position u/primeinH/prime\ncan aggregate the feature information of other position Ω/prime\nuin the same row and\nthe same column. In the previous criss-cross attention module, these points have\nbeen aggregated to obtain the context information of position in the same rowand the same column. Compared with the non-local module that is also used\nto extract global context dependencies, iterating twice the criss-cross attention\nmodule can achieve similar functions, and can greatly reduce the amount ofcalculation required by the spatial attention module.\nFinally, for the input video frame of the t-th frame, the output feature S\nt\nof the spatial context branch will be fused with the features obtained by the\nfeature extraction network through a feature fusion module as shown in Fig. 3.\nChannel Attention Module. We use a channel attention module on high-\nest level feature ft,5of the ResNet to mine the dependencies between channels,', '328 Z. Zhao and S. Zhao\nFig. 3. Feature fusion module.\nFig. 4. Channel attention module.\nbecause each channel of high-level features contains rich semantic information,\nand the dependencies between channels can enhance the semantic feature rep-\nresentation of high-level features. First, the input feature f/prime\nt,5∈RC5×H5×W5of\nthe channel attention module is reshaped to B∈RC5×N, where N=H5×W5,\nC5,H5,W 5respectively indicate the number of channels, height and width of\nthe input feature f/prime\nt,5. Then obtain the channel correlation matrix by performing\na matrix multiplication between Band the transpose of B. Finally, we apply a\nsoftmax layer to obtain the channel attention map X∈RC5×C5:\nxji=exp(Bi·Bj)\nΣC5\ni=1exp(Bi·Bj)(3)\nwhere xjidenotes i-th channel’s impact on the j-th channel. In addition, we use\nmatrix multiplication between the transpose of the attention map Xand Band\nshape result to RC5×H5×W5. Finally, sum with the original feature map after\nweighting to get the ﬁnal output result C∈RC5×H5×W5:\nCj=βC5/summationdisplay\ni=1(xjiBi)+Bj (4)\nwhere βis a learnable weight. As shown in Eq. 4, the ﬁnal feature of each channel\nis obtained by the weighted sum of the feature of each channel and the originalfeature, so the long-range semantic dependencies of the channel dimension is\nobtained.', 'Dual Attention Based Network with Hierarchical ConvLSTM 329\n3.3 Recurrent ConvLSTM Decoder\nOur decoder is a hierarchical recurrent ConvLSTM network. The ConvL-\nSTM [ 11] module recursively in spatial and temporal dimensions. The recurrence\nin the spatial dimension aims to predict multiple objects, and the recurrence in\ntemporal dimensions can make the prediction of the same object on diﬀerent\nframes have the same index. The structure is shown in Fig. 1, which consists of\nconcatenate and upsampling operation of the feature maps, alternately placed\nwith the ConvLSTM module.\nFor the features of the encoder f/prime\nt={f/prime\nt,2,ffu s e\nt,f/prime\nt,4,Ct}, the decoder out-\nputs a set of objects segmentation masks {M t,1,..., M t,i,..., M t,N}, where M t,i\ndenotes the segmentation mask of the i-th object in the t-th frame. For each video\nsequence, the object number of segmentation results obtained by the decoder is\nﬁxed. If the object disappears in the video sequence, the value of the prediction\nwill be 0 in the subsequent frame. In the prediction process, the index of diﬀer-ent object in the spatial recurrence is not speciﬁed in the ﬁrst frame, but the\noptimal match between the predicted mask and annoted mask is obtained by\ncalculating the diﬀerence between those.\nCompared with the traditional ConvLSTM network in the temporal domain,\nthe recurrent ConvLSTM module acting in the temporal domain and the spatial\ndomain can predict multi-objects based on more input information. The inputof the k-th layer ( k∈{5,4,3,2}) ConvLSTM module for the i-th object in\nthet-th frame h\nt,i,kdepends on the following input feature: (1) The feature\nf/prime\nt,i,kextracted by the encoder part from the t-th frame, (2) The output state of\nConvLSTM of the ( k+ 1)-th layer, (3) The hidden state ht,i−1,kof the previous\nobject in the same frame of the current ConvLSTM layer, (4) The hidden state of\nthe same object in the previous frame ht−1,i,k, (5) The prediction mask M t−1,i\nof the same object in the previous frame:\nhinput=[UP 2(ht,i,k +1),f/prime\nt,k,M t−1,i]\nhstate=[ht,i−1,k,ht−1,i,k] (5)\nNotice that the input of the ConvLSTM corresponding to the ﬁfth level feature\nhinputis deﬁned as:\nhinput=[Ct,M t−1,i] (6)\nWe use the annoted mask of the ﬁrst frame as the prior knowledge, so that the\nnetwork can track and segment multiple objects corresponding to the annotation\nlabel in subsequent frames. The input of the ConvLSTM module of the k-th\nlayer is the feature obtained by concatenate the ×2 upsampled hidden state of\nthe upper ConvLSTM layer, the spatial feature map of the current layer, and\nthe label mask yi,kwith the corresponding scale:\nhinput=[UP 2(ht,i,k +1),f/prime\nt,k,yi,k] (7)\nAnd the input state hstateof the ﬁrst object in each frame is:\nhstate=[Z,ht−1,i,k] (8)', '330 Z. Zhao and S. Zhao\nThrough Nrecurrences, the decoder generates Nobject segmentation results\nfor each frame, and each result corresponds to a reference object in the ﬁrst videoframe, where the value of the object that does not appear in the video is 0 in\nthe prediction result.\n3.4 Loss Function\nIn this paper, we use the IOU loss, the Balanced Binary Cross-Entropy loss, and\nthe intra-frame self IOU loss to joint supervise the training process. We takes the\nIOU loss as one of the loss functions, aims to punish the inconsistency between\nthe predicted object and the ground truth mask. We also use the Balanced BCEloss to pay attention to the sparse foreground pixels by calculating the ratio of\nthe number of foreground pixels to the number of pixels in the ground truth as\nthe weight, and weighting the obtained Cross-Entropy loss. In order to avoid too\nmuch overlap between the predicted masks, we propose the intra-frame self IOU\nloss function LOSS\nself, the loss uses the IOU between the prediction results of\nobjects in the same frame as a penalty, and implements self-supervised on the\ndiﬀerence between objects in a frame.\nLOSS self=n/summationdisplay\ni,j=0; i/negationslash=jM t,i∩M t,j\nM t,i∪M t,j(9)\nFinally, our loss function is the weighted sum of the IOU loss, Balanced\nBinary Cross-Entropy loss and the intra-frame self IOU loss:\nLOSS =αLOSS IOU+βLOSS BalancedBCE +γLOSS self (10)\n4 Experiments\n4.1 Implementation Details\nWe choose the ResNet50 [ 2] with a pre-trained weight on ImageNet as the feature\nextraction network. In order to reduce the amount of network parameters and\ncalculations, we apply the 3 ×3 depth-wise separable convolutions for other\nmodules, except for the ResNet network.\nWe resize the input video frame to 240 ×427, The batchsize of the training\nprocess is set to 2, that is, two video sequences are input for each batch, and\neach sequence has 8 frames. We choose the Adman optimizer, the learning rateof the ResNet is set to 1 e−6, and the learning rate of other parts is 1 e−3. We\napply rotation, translation, shearing and scaling on the original image for data\naugmentation. According to the experiment, the weight of each loss function is\nset as α=1,β=2,γ=0.2.\n4.2 Dataset\nWe measured the eﬀect of our model on the recent video object segmentation\ndataset DAVIS 2017 [ 10]. DAVIS 2017 consists of 60 videos in the training set,\n30 videos in the validation set and 30 videos in the test-dev set. Each video\nsequence contains multiple objects, and the video duration is 3–6 s.', 'Dual Attention Based Network with Hierarchical ConvLSTM 331\n4.3 Ablation Study\nEncoder. To test the eﬀectiveness of the modules used in the encoder, this\nsection conducts ablation study on the spatial context branch and the channelattention module. Table 1illustrates the impact of spatial context branch and\nchannel attention mechanism on model performance. We use ResNet50 with a\ndecoder based on one layer recurrent ConvLSTM as the baseline. The train-ing process uses weighted sum of three loss functions. From the results, it can\nbe found that adding the spatial attention module and the channel attention\nmodule respectively, the average value of JandFhave improved compared\nwith the baseline. Speciﬁcally, the second row of Table 1shows that applying\nspatial context branch, J&Fmean has a 1.2 point improvement compared to\nthe baseline. Adding feature fusion module and sptial attention module on thisbasis can further improve network performance. What’s more, the ﬁfth row of\nTable 1shows that after the channel attention module is added separately, J&F\nmean is improved by 1.4 compared with the baseline. Finally, the joint use of thespatial context branch with spatial attention module and the channel attention\nmodule achieves the highest performance. The J&Fmean is 2.0 higher than\nthe baseline network.\nTable 1. Ablation study on DAVIS2017 validation set with diﬀerent spatial mod-\nules. SCB †:spatial context branch with only conv layers, FFM :feature fusion module,\nSCB:spatial context branch with sptial attention module, CAM :channel attention\nmodule\nMethod J&F J F\nMean↑Mean↑Recall ↑Decay ↓Mean↑Recall ↑Decay ↓\nBaseline 80.1 75.8 84.5 –3.9 84.3 89.4 –4.4\n+SCB † 81.3 77.0 85.4 –4.3 85.6 90.4 –4.3\n+SCB †+FFM 81.9 77.6 85.6 –4.3 86.2 90.6 –4.1\n+SCB+FFM 82.0 77.6 85.7 –4.3 86.3 90.6 –4.4\n+CAM 81.5 77.2 85.5 –4.2 85.9 90.8 –4.7\n+SCB+FFM+CAM 82.1 77.8 85.8 –4.3 86.5 90.8 –4.1\nLoss Function. As shown in the Table 2, the ﬁrst row and the second row\nrepresent the case of using the IOU loss and the Balanced BCE loss respectively,and IOU loss is signiﬁcantly better than Balanced BCE loss. The third and fourth\nrows in the Table 2are added to the intra-frame self IOU loss we proposed as the\nauxiliary loss on the basis of the use of IOU loss and Balanced BCE loss. Afterthe auxiliary loss is added, the performance is improved. Compared with only\nusing the IOU loss, Jmean and Fmean of LOSS\nIOU+LOSS selfimproves 0.9\nand 1.3 respectively. The two results of LOSS BalanceBCE +LOSS selfimproved\nby 2.5 and 1.5 compared with only using the Balanced BCE loss. The ﬁfth row of\nthe Table 2shows the results of using three loss functions at the same time, and', '332 Z. Zhao and S. Zhao\nTable 2. Ablation study of the three loss functions on the DAVIS2017 validation set\nLOSS IOU LOSS BalanceBCE LOSS selfJMean FMean\n√76.6 85.2\n√64.3 73.9\n√ √77.5 86.5\n√ √66.8 75.4\n√ √ √77.8 86.5\ntheJmean and Fmean reach 81.8 and 90.5 respectively. This shows that the\njoint supervision of three loss function during the training process can improvethe performance of the model.\nA ss h o w ni nT a b l e 3, that the parameters and the number of ﬂoating-\npoint operation of the criss-cross attention module we used are 0.005M and0.067GFLOPS, respectively, which are both 1/3 of the non-local module under\nthe same setting, meanwhile, the Jmean and Fmean are similar to the results\nobtained by the non-local module. By executing the criss-cross attention moduletwice in succession, the global context dependence of each pixel position can be\nobtained without increasing the parameter amount.\nTable 3. Comparative experiment results of the inﬂuence of criss-cross attention mod-\nule and non-local module on model parameters\nModule Parameters FLOPs JMean FMean\nCriss-Cross attention module 0.005M 0.067G 77.6 86.3\nNon-local module 0.017M 0.215G 77.5 85.5\n4.4 Semi-supervised Video Object Segmentation\nTable 4gives the overall results of our method on the DAVIS 2017 validation\nset. We set the same structure as the model of the last row of Table 1.T h e\nother methods in the Table 4are all obtained from the DAVIS2017 benchmark.\nOurJ&Fmean are 82.2, which surpasses other methods in the benchmark.\nAnd our model inference speed is also faster than the SSM-VOS. The inference\nspeed of SSM-VOS reach 22.3 fps, while our method with ResNet50 can reach\n45.5 fps on the NVIDIA GTX1080Ti. This is due to the fact that our model\nadopts strategies such as criss-cross attention module and depth-wise separableconvolution to control the amount of model parameters and calculations.\n4.5 Visualization\nFigure 5shows the visual eﬀect of the qualitative evaluation of the DAVIS2017\ndataset. The segmentation results of each video sequence in the ﬁgure show that', 'Dual Attention Based Network with Hierarchical ConvLSTM 333\nTable 4. Quantitative comparison of other methods on the DAVIS2017 val sets.\nMethod J&F J F Runtime (fps)\nMean ↑Mean ↑Recall ↑Decay ↓Mean ↑Recall ↑Decay ↓\nCFBI [ 15] 81.9 79.1 - - 84.6 - - -\nSTM [ 8] 81.75 79.2 88.7 8.0 84.3 91.8 10.5 -\nPReMVOS [ 5] 77.85 73.9 83.1 16.2 81.8 88.9 19.5 -\nMHP-VOS [ 14]76.15 73.4 83.5 17.8 78.9 87.2 19.1 -\nSSM-VOS [ 17] 77.6 75.3 - 11.7 79.9 - 15.3 22.3\nOurs 82.1 77.8 85.8 –4.3 86.5 90.8 –4.1 45.4\nour model can deal with various challenging scenarios of semi-supervised video\nobject segmentation tasks, such as the serious occlusion between the objects inthe second and ﬁfth rows, the rapid changes in the shape and scale of the object\nin the ﬁrst and fourth row, the blurred background of the ﬁsh tank in the third\nrow.\nFig. 5. The visualization results of the semi-supervised video object segmentation task\non the DAVIS2017 dataset.\n5 Conclusion\nWe propose an end-to-end framework for real-time video multi-object segmen-\ntation. We set up a spatial context branch with spatial attention module to\nobtains the long range context dependencies in the features and to avoid theglobal spatial attention module from occupying too much computing resources.\nIn addition, we also use the channel attention mechanism for the highest level\nfeatures of the feature extraction network to obtain the semantic dependenciesbetween channels. Finally, the feature with diﬀerent scale are sent to the recur-\nrent ConvLSTM decoder through the skip-connection conv, and the multi-object\nvideo segmentation mask is obtained through step-wise upsample of the featuremap. In the DAVIS2017 benchmark, our model has achieved the best results\ncompared with the latest methods in semi-supervised tasks. At the same time,\nthe model inference speed is 45.5 fps.', '334 Z. Zhao and S. Zhao\nReferences\n1. Caelles, S., Maninis, K., Pont-Tuset, J., Leal-Taix´ e, L., Cremers, D., Gool, L.V.:\nOne-shot video object segmentation. In: 2017 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR 2017), pp. 5320–5329. IEEE Computer\nSociety (2017)\n2. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR\n2016), pp. 770–778. IEEE Computer Society (2016)\n3. Hu, Y.-T., Huang, J.-B., Schwing, A.G.: VideoMatch: matching based video object\nsegmentation. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV\n2018. LNCS, vol. 11212, pp. 56–73. Springer, Cham (2018). https://doi.org/10.\n1007/978-3-030-01237-3 4\n4. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., Liu, W.: Ccnet: criss-cross\nattention for semantic segmentation. In: 2019 IEEE/CVF International Conference\non Computer Vision (ICCV 2019), pp. 603–612. IEEE (2019)\n5. Luiten, J., Voigtlaender, P., Leibe, B.: Premvos: proposal-generation, reﬁnement\nand merging for the Davis challenge on video object segmentation 2018. In: The\n2018 DAVIS Challenge on Video Object Segmentation-CVPR Workshops, vol. 1,\np. 6 (2018)\n6. Maninis, K., et al.: Video object segmentation without temporal information. IEEE\nTrans. Pattern Anal. Mach. Intell. 41(6), 1515–1530 (2019)\n7. Oh, S.W., Lee, J., Sunkavalli, K., Kim, S.J.: Fast video object segmentation by\nreference-guided mask propagation. In: 2018 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR 2018), pp. 7376–7385. IEEE Computer Society(2018)\n8. Oh, S.W., Lee, J., Xu, N., Kim, S.J.: Video object segmentation using space-time\nmemory networks. In: 2019 IEEE/CVF International Conference on Computer\nVision, ICCV 2019. pp. 9225–9234. IEEE (2019)\n9. Perazzi, F., Khoreva, A., Benenson, R., Schiele, B., Sorkine-Hornung, A.: Learn-\ning video object segmentation from static images. In: 2017 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR 2017), pp. 3491–3500. IEEE\nComputer Society (2017)\n10. Pont-Tuset, J., Perazzi, F., Caelles, S., Arbelaez, P., Sorkine-Hornung, A.,\nGool, L.V.: The 2017 DAVIS challenge on video object segmentation. CoRR\nabs/1704.00675 (2017)\n11. Shi, X., Chen, Z., Wang, H., Yeung, D., Wong, W., Woo, W.: Convolutional LSTM\nnetwork: a machine learning approach for precipitation nowcasting, pp. 802–810(2015)\n12. Ventura, C., Bellver, M., Girbau, A., Salvador, A., Marqu´ es, F., Gir´ o-i-Nieto, X.:\nRVOS: end-to-end recurrent network for video object segmentation. In: IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR 2019), pp. 5277–5286.Computer Vision Foundation/IEEE (2019)\n13. Voigtlaender, P., Leibe, B.: Online adaptation of convolutional neural networks for\nvideo object segmentation. In: British Machine Vision Conference 2017 (BMVC\n2017). BMVA Press, London (2017)\n14. Xu, S., Liu, D., Bao, L., Liu, W., Zhou, P.: MHP-VOS: multiple hypotheses prop-\nagation for video object segmentation. In: IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR 2019), pp. 314–323. Computer Vision Founda-\ntion/IEEE (2019)', 'Dual Attention Based Network with Hierarchical ConvLSTM 335\n15. Yang, Z., Wei, Y., Yang, Y.: Collaborative video object segmentation by\nforeground-background integration. In: Vedaldi, A., Bischof, H., Brox, T., Frahm,\nJ.-M. (eds.) ECCV 2020. LNCS, vol. 12350, pp. 332–348. Springer, Cham (2020).https://doi.org/10.1007/978-3-030-58558-7\n20\n16. Yoon, J.S., Rameau, F., Kim, J., Lee, S., Shin, S., Kweon, I.S.: Pixel-level match-\ning for video object segmentation using convolutional neural networks. In: IEEEInternational Conference on Computer Vision (ICCV 2017), pp. 2186–2195. IEEE\nComputer Society (2017)\n17. Zhu, W., Li, J., Lu, J., Zhou, J.: Separable structure modeling for semi-supervised\nvideo object segmentation. IEEE Trans. Circuits Syst. Video Technol. 99, 1–1\n(2021)', 'Distance-Based Class Activation Map for\nMetric Learning\nYeqing Shen1, Huimin Ma2(B), Xiaowen Zhang1, Tianyu Hu2,\na n dY u h a nD o n g1,3\n1Tsinghua University, Beijing, China\n{shenyq18,zhangxw18 }@mails.tsinghua.edu.cn\n2University of Science and Technology Beijing, Beijing, China\n{mhmpub,tianyu }@ustb.edu.cn\n3Tsinghua Shenzhen International Graduate School, Shenzhen, China\ndongyuhan@sz.tsinghua.edu.cn\nAbstract. The interpretability of deep neural networks can serve as reli-\nable guidance for algorithm improvement. By visualizing class-relevantfeatures in the form of heatmap, the Class Activation Map (CAM) and\nderivative versions have been widely exploited to study the interpretabil-\nity of softmax-based neural networks. However, CAM cannot be adopteddirectly for metric learning, because there is no fully-connected layer\nin metric-learning-based methods. To solve this problem, we propose a\nDistance-based Class Activation Map (Dist-CAM) in this paper,\nwhich can be applied to metric learning directly. Comprehensive experi-\nments are conducted with several convolutional neural networks trained\non the ILSVRC 2012 and the result shows that Dist-CAM can achievebetter performance than the original CAM in weakly-supervised local-\nization tasks, which means the heatmap generated by Dist-CAM can\neﬀectively visualize class-relevant features. Finally, the applications ofDist-CAM on speciﬁc tasks, i.e., few-shot learning, image retrieval and\nre-identiﬁcation, based on metric learning are presented.\nKeywords: Neural network interpretability\n·Class activation map ·\nMetric learning\n1 Introduction\nThe past few years have witnessed a major development in deep learning, and\nimpressive achievements have been made in several tasks, e.g., recognition, detec-\ntion and reinforcement learning. However, current deep neural networks are fac-ing the problem of interpretability. Speciﬁcally, deep neural networks contain a\nlarge number of learnable parameters, which must be trained on a grand scale of\ndata using gradient descent strategy, and thus the prediction results of the neuralnetwork are less possible to be appropriately interpreted based on the parameters\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 336–347, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_28', 'Distance-Based Class Activation Map for Metric Learning 337\nof the network. Many studies [ 2,12,21] have been trying to solve this problem\nof network interpretability, and one representative approach is to visualize theoutput of the neural network, of which Class Activation Map (CAM) [ 20]h a s\nmade signiﬁcant progress. To be more speciﬁc, CAM is applied to the classiﬁca-\ntion network based on Convolutional Neural Network (CNN). This classiﬁcationnetwork extracts the features of images through CNN, and the features will then\nbe combined through fully connected layer to obtain the prediction results. To\nvisualize the classiﬁcation network, CAM combines the feature map of CNN withthe weights of fully connected layers, based on which the heatmap of the image\nto be recognized can be generated. However, this approach relies heavily on the\nfully connected layers of the classiﬁcation network, thereby making it diﬃcultto be applied to metric learning which lacks fully connected layer.\nMetric learning aims at learning a representation function which maps objects\ninto a CNN network. The object’s similarity should be reﬂected in the distance\nof the CNN network, i.e., the distance between similar objects is as reduced\nas possible while dissimilar objects are far from each other. This approach hasbeen extensively applied in image retrieval [ 1,4,15,18], re-ID [ 5,9,10,17]a n d\nfew-shot learning [ 3,6–8,14], etc. Therefore, it is of great value to interpret the\nneural networks in metric learning. Facing this problem of interpretability, wepropose a Dist-CAM to achieve better interpretability of neural networks in\nmetric learning. In particular, the current study focuses on solving the problem\nof CAM in its limited applicability to broader network architectures, and triesto extend CAM to metric learning based on the idea of class activation.\nA ss h o w ni nF i g . 1, in a recognition network with fully connected layer, the\nfeature maps of diﬀerent channels combine with the weights of the fully con-nected layer. The class with the highest probability in the output is considered\nas the ﬁnal prediction result. On the other hand, diﬀerent regions of the heatmap\ngenerated by CAM have distinct responses. Particularly, a region with a higherresponse indicates that the features of this area have more contributions to the\noutput. Therefore, CAM actually establishes a relationship between the output\nof the classiﬁcation network and diﬀerent regions of the original image in which\na stronger response implies a greater relevance.\nBy contrast, in a metric learning network without fully connected layer in\nFig.2, the features of the sample are ﬁrstly extracted by CNN, and then the\ndistance between the features of the test sample and those of the training sample\nis calculated in the metric module. The class of the training sample which hasthe smallest distance from the test sample is considered as the prediction result.\nFollowing this idea of class prediction in metric learning, the Dist-CAM proposed\nby the current study establishes a relationship between the prediction resultsof metric learning and diﬀerent locations of the original image. To be more\nspeciﬁc, the main idea is to calculate the distance between the feature maps\nof the training sample and those of the test sample in diﬀerent channels, inorder to evaluate their relevance. The obtained relevance score is used as the\nweights corresponding to the diﬀerent channels of the feature maps of the test\nsample, in which a channel with a higher relevance score has a greater weight.', '338 Y. Shen et al.\nThe relevance between features of the test sample in a particular location and\nthe class of the training sample is reﬂected in the responses of diﬀerent locationsin the generated heatmap, and a higher response suggests a greater relevance\nbetween the feature and the training set.\nThe main contribution of the current paper is that we propose the Dist-\nCAM which can be employed in the interpretation of the neural networks in\nmetric learning. To evaluate the quality of heatmaps generated by Dist-CAM,\ncomprehensive experiments have been conducted on weakly-supervised local-ization tasks in ImageNet [ 13]. The results show that the regions with higher\nresponses in the heatmaps generated by Dist-CAM are indeed the class-relevant\nfeatures, which clearly indicates that Dist-CAM can be used in metric learningto eﬀectively interpret neural networks. Additionally, this paper also compares\nthe performance of CAM and Dist-CAM in weakly-supervised localization tasks,\nand the results show that Dist-CAM with the same backbone network as CAM\ncan achieve more accurate localization. To the best of our knowledge, this is\nthe ﬁrst method that can be used to generate class activation map for metriclearning. In order to evaluate the applicability of Dist-CAM, the visualization\nresults in several metric learning tasks are also presented in Sect. 5.\n2 Distance-Based Class Activation Map\nDetails on CAM in softmax-based methods and Dist-CAM in metric learning as\nwell as their comparison will be introduced in this section.\nClass Activation Map\nSoftmax-based learning refers to neural networks with fully-connected(FC) lay-\ners, which calculates probabilities of diﬀerent classes, and predicts the ﬁnal result\nwith the softmax layer. The operation of softmax layer is shown in Eq. 1,i nw h i c h\nˆyis the prediction of softmax and viis the value of vector generated by FC layers.\nˆy=evi\n/summationtext\njevi(1)\nThe architecture of CAM [ 20] for the softmax-based classiﬁcation networks is\nshown in Fig. 1(a), in which FC means FC layer. Test feature map Q∈RW×H×K\nis encoded by a convolutional neural network (CNN) backbone with test sample\nas input. The width and height of feature map are denoted as Wand H,a n d\nthe number of its channels is denoted as K. Furthermore, the feature map of\nthe k-th channel are denoted as Qk∈RW×H. Then Global Average Pooling\n(GAP) calculates the average of feature map in each channel, which is denotedasGAP(·). Finally, as shown in Eq. 2, the output of the GAP is combined with\nweight of FC layers to obtain the conﬁdence of each class. For a speciﬁc class c,\nw\nc\nkis the FC layer weight of the k-th channel and ycis the class conﬁdence.\nyc=/summationdisplay\nkwc\nkGAP(Qk) (2)', 'Distance-Based Class Activation Map for Metric Learning 339\nFig. 1. Class activation map. In softmax-based learning, CAM combines the feature\nmaps with FC weight to generate heatmaps. (a) The framework of softmax-base learn-\ning with CAM module. (b) A simpliﬁed illustration of CAM generation process.\nAs Eq. 3shows, heatmap of class cis obtained by combining test feature\nmaps with FC layers weight of class cin the CAM module. Diﬀerent regions\nof the heatmap have distinct response. A larger response indicates a greater\ncorrelation between the region and the class c. High response region can be\nregarded as activation, so the heatmap of class cis the class cactivation map.\nMc\ncam=/summationdisplay\nkwc\nkQk (3)\nAs Fig. 1(b) shows, the ﬁnal prediction is class 1, because the score of class 1\nis 0.55 and the score of class 2 is 0.45. Heatmap 1 and heatmap 2 are generatedby combining test feature maps with FC weight 1 and FC weight 2. A higher\nresponse in heatmap cindicates a greater correlation between the region and the\nclass c.\nHowever, due to the dependence on the weight of the FC layer, CAM can only\nbe applied in the softmax-based methods, and cannot be used in metric learning.\nTo solve this problem, the distance-based Class Activation Map is designed toanalyze the models in metric learning.', '340 Y. Shen et al.\nDistance-Based Class Activation Map\nA ss h o w ni nF i g . 2, the feature of unlabeled test sample Qis predicted in\nmetric module according to the distances to other training sample features S=\n{S0,S1,··· ,S N}. In addition, for class c, we denote the i-th feature of training\nsample of the k-th channel as sc\ni,k.\nFig. 2. Distance-based class activation map. In metric learning, Dist-CAM combines\nthe feature maps with weight to generate heatmaps. (a) The framework of metric\nlearning with Dist-CAM module. (b)A simpliﬁed illustration of Dist-CAM generation\nprocess.\nAs Eq. 4shows, the distance Dcis calculated between the feature of training\nsample Scand that of test sample Qvia the GAP layer, where the operation L1(·)\nrefers to element-wise 1-norm. The k-th dimension in the vector Dcrepresents\nthe diﬀerence between Scand Qin the k-th channel. It is clear that the diﬀerence', 'Distance-Based Class Activation Map for Metric Learning 341\nis positively correlated with the distance value. Therefore, the smaller value of\nDc\nkis, the closer Qkis to Sc\nk. If the feature is closer to the training sample cin\nthe channel k, it is supposed that the k-th channel plays a more important role\nin the prediction.\nDc=L1(GAP(Sc)−GA P (Q)),Dc∈RK(4)\nSimilar to the combination of the GAP outputs in Eq. 2, the output is\nobtained by the summation of Dcin metric learning. However, the largest\nresponse of class conﬁdence ycby the combination in Eq. 2determines the out-\nput class c, while the smallest summation of Dcdetermines the output class\nc. Therefore, we transform DctoWca ss h o w ni nE q . 5, where norm(·)i st h e\noperation of element-wise normalization. Thus the coeﬃcient Wreﬂects the dis-\ntances from the diﬀerent channels of Qto training sample Sc. The bigger the\nweight Wc\nkis, the more relevant the k-th channel is with the class c.\nWc=norm(1\nDc),Wc∈RK(5)\nMc\nDistCAM =/summationdisplay\nkWc\nkQk(6)\nMc\nDistCAM is calculated by combining Qkwith weight vector Wc\nkas shown in\nEq.6. The diﬀerent responses in Dist-CAM heatmap reﬂect distinct level of class\nactivation. High response region can be regarded as activation, so the heatmap of\nclass cis the class cactivation map. As shown in Fig. 2(b), the prediction is class\n1, because the distance in feature space between test sample and training sample\n1 is 6, and the distance in feature space between test sample and training sample2 is 8. In the Dist-CAM module, weight of class W\ncis reciprocal of distance\nvector Dc. Heatmap 1 and heatmap 2 are generated by combining test feature\nmaps with weight 1 and weight 2. A higher response in Dist-CAM heatmap c\nindicates a greater correlation between the region and the class c.\nIn Dist-CAM heatmap, a red pixel represents a large activation value, and\na blue pixel represents a small activation value. The region with a large acti-vation value suggests that feature of this region has a large contribution to the\nﬁnal prediction. As shown in Fig. 3, the response regions of Dist-CAM heatmaps\ngenerated with diﬀerent categories of prototypes are also diﬀerent. The inputimage contains a cat and a dog. The Dist-CAM modules measure the feature\nwith the prototypes of cat, dog, bird, and table respectively. Among visualiza-\ntion results, the Dist-CAM heatmaps of cat class and dog class respond signiﬁ-cantly at the locations of the two objects, while the other two activation regions\nare chaotic and scattered, which means Dist-cam eﬀectively activates the class-\nrelevant regions.\nP\nc\nk=1\nN/summationdisplay\niSc\n(i,k),Pc\nk∈RW×H(7)\nIn some metric learning applications, the prediction of test sample is deter-\nmined by the nearest class prototype, instead of the nearest training sample.', '342 Y. Shen et al.\nFig. 3. Framework of Distance-based class activation Map. Diﬀerent colors represent\nthe corresponding images and prototypes of diﬀerent categories (i)–(iv). The inputimage is encoded by a feature encoder, and the features are fed to Dist-CAM modules to\nclass-wise calculate the distance-based Class Activation Maps with diﬀerent prototypes.\nThe Dist-CAM heatmaps of cat class and dog class respond signiﬁcantly at the locationsof the two objects, while the other two activation regions are chaotic and scattered.\nThe prototype of class cis denoted as Pc, which is obtained by calculating the\nmean value of features of all training samples in class c. The way to calculate\nprototype Pcis shown in Eq. 7,i nw h i c h Nis the number of training samples. In\nthese applications, the prototype Pcreplaces the training sample Scto calculate\nthe distance Dcand generate the Dist-CAM heatmap Mc\nDistCAM .\n3 Experiments\nWeakly-supervised localization task is chosen for the evaluating the performance\nof Dist-CAM for the following reasons. On the one hand, neural networks in clas-\nsiﬁcation task and those in weakly-supervised localization task share the samekind of label, i.e., both are trained with class label. On the other, both weakly-\nsupervised localization task and class activation task focus on localizing class-\nrelevant regions. Therefore, weakly-supervised localization task is appropriate tobe used in the experiments.\nExperiments are conducted to quantitatively evaluate the weakly-supervised\nlocalization ability of Dist-CAM on the ILSVRC 2012 benchmark [ 13]. We adopt\ntwo main CNN backbones: ResNet and DenseNet. For each backbone, we have\ntrained the model on ILSVRC 2012 training set to obtain softmax-based modelswith FC layers and distance-based metric learning models, respectively.\nIn order to ensure the fairness of the experiment, N in Eq. 7is set to the\nnumber of training samples of each class when calculating the prototype in met-ric learning. Besides, we drop the last GAP layer and the FC layer of models to\nobtain a multi-channel feature map with spatial position information for metric\nlearning experiments. For example, the input image with the size of 224 ×224×3\nis encoded by a modiﬁed ResNet backbone into a feature map with the size of', 'Distance-Based Class Activation Map for Metric Learning 343\nTable 1. Localization error (top-1) of the Dist-CAM on ILSVRC2012 valsets.\nBackbone Method Top-1 error\nResNet-18 CAM 59.5\nResNet-18 Dist-CAM 56.7\nResNet-50 CAM 56.2\nResNet-50 Dist-CAM 54.7\nResNet-101 CAM 56.6\nResNet-101 Dist-CAM 54.9\nDenseNet-161 CAM 62.4\nDenseNet-161 Dist-CAM 57.6\nDenseNet-169 CAM 62.3\nDenseNet-169 Dist-CAM 56.6\nDenseNet-201 CAM 62.5\nDenseNet-201 Dist-CAM 56.4\n2048×7×7. When predicting the class, the feature map is transformed into a\n2048-dimensional feature of test sample by the GAP layer. Then the network cal-\nculates the distances between the 2048-dimensional feature and each prototype\nand selects the nearest one as the prediction. When calculating the Dist-CAM,the network measures the channel-wise distances of the 2048-dimensional fea-\nture and the nearest prototype and obtains the channel weighting coeﬃcients\nby reciprocal and normalization. Finally, a weighted summation of the origi-nal 2048 ×7×7 feature map with the coeﬃcients is performed to obtain the\nDist-CAM with the size of 7 ×7.\nTo compare with the original CAM, we evaluate the localization ability of\nDist-CAM with the same error metric (top-1) on the ILSVRC 2012 [ 13]val\nset. The intersection of union (IoU) threshold is set to generate bounding boxes\non the positions with strong responses. Compared with groundtruth, when the\nIoU between them is lower than 0.5, the prediction is considered as a wrong\nlocalization.\nAs shown in Table 1, Dist-CAM has achieved a lower top-1 error than the\noriginal CAM, which indicates the eﬀectiveness of our method. Without the FC\nlayer, Dist-CAM has achieved accurate localization of discriminative features,which is eﬀective to interpret the model in metric learning. Meanwhile, the Dist-\nCAM can also be regarded as a promotion of CAM, which can be used in the\nsoftmax-based methods by dropping the last FC layer.\n4 Metric-Based Applications\nDist-CAM is applied to three main tasks of metric learning, including few-shot\nlearning, image retrieval and re-identiﬁcation. Experiments are conducted to\nshow the visualization results to explore network interpretability. Note that in', '344 Y. Shen et al.\nfew-shot learning, support sample refers to training sample mentioned in Sect. 2\nand query sample refers to test sample. In image retrieval and re-identiﬁcation,training sample mentioned in Sect. 2is deﬁned as gallery sample and test sample\nis deﬁned as probe sample.\nFig. 4. Dist-CAM visualization for metric-based applications. (a) Dist-CAM heatmaps\nfor few-shot learning on miniImageNet; (b) Dist-CAM heatmaps for image retrieval on\nCUB2011; Dist-CAM heatmaps for pedestrian re-identiﬁcation on Market-1501.\n4.1 Few-Shot Learning\nFew-shot learning is one of the most signiﬁcant applications for metric learn-\ning [3,6–8,14]. Studies on few-shot learning aim to overcome the lack of support\nsamples to obtain a useful recognition model. Inspired by the idea of meta-\nlearning, most of the current approaches adopt a number of tasks similar to thetarget one to train a meta-learner. Speciﬁcally, the majority of current main-\nstream methods are based on metric learning frameworks. To predict the class\nof unlabeled query sample, the metric learning frameworks measure the dis-tances among the unlabeled query sample and the class prototypes, which are\ncalculated from the labeled support samples.\nAs mentioned in Sect. 1, the lack of FC layers limits the original CAM meth-\nods to analyze the model performance. To explore the network interpretability\nof meta-learning, Dist-CAM is used to localize the discriminative features and\nanalyze the advantages of metric-based few-shot learning.\nWe adopt ResNet-12 as a backbone and train the model [ 7] by meta-learning\nstrategy. The performance has achieved a state-of-the-art performance with77.14±0.42 in the-5-way-1-shot task. Figure 4(a) shows Dist-CAM for few-shot\nlearning on mini-Imagenet [ 11] dataset. According to the setting of the one-shot\ntask, only one support sample is used to calculate the prototype. It can be seenthat the activation location of the fox in the above image is concentrated on the\nears and tail, and the activation location of the sailboat in the image below is\nconcentrated on the sail and hull. These are indeed the class-deﬁning featuresrelated to these classes. Results show that Dist-CAM is a useful tool to ana-\nlyze the metric-based few-shot learning, which can also be used to improve the\ntraining strategies by analyzing failure cases.', 'Distance-Based Class Activation Map for Metric Learning 345\n4.2 Image Retrieval\nMetric learning is widely used in image retrieval to search images by images. For\nsoftmax-based methods, it is exhausting to retrain the model when a novel classsample appears. By contrast, there is no need to retrain the model in metric\nlearning when adding a novel class. Therefore, image retrieval is an important\napplication of metric learning, trying to retrieve the same object or class asthe probe sample from the existing gallery samples [ 1,4,15,18]. Diﬀerent from\nthe classiﬁcation algorithm with ﬁxed output classes, the metric learning algo-\nrithm increases the inter-class interval and reduces the intra-class variance bydesigning the loss function during the training process. Images are encoded by\nbackbone and retrieved according to distance calculated by metric module. In\nthe application stage of image retrieval, even if new categories are added, thetrained backbone can be used for prediction in metric learning, which is diﬃcult\nfor classiﬁcation algorithms with a ﬁxed number of classes.\nDist-CAM is used to analyze an image retrieval model trained on the\nCUB2011 [ 16], a dataset containing 200 categories of birds, of which ResNet-\n50 is used as the backbone. According to the settings of the image retrieval task,\nonly one gallery sample that is the closest to the probe sample is used to cal-\nculate the Dist-CAM heatmaps. It can be seen from Fig. 4(b) that Dist-CAM\nheatmaps have a good weakly supervised localization capability. The activationlocation is concentrated on the region of the birds. Therefore, it clearly shows\nthat Dist-CAM can be used to analyze the results and failure cases of the image\nretrieval model.\n4.3 Re-identiﬁcation\nAs a sub-problem of image retrieval, re-identiﬁcation aims to ﬁnd the expected\ntarget in the image library [ 5,9,10,17]. During the test phase, the probe images to\nbe retrieved are used to calculate feature distances with gallery images. Then top-\nk images are retrieved according to feature distances. Plenty of re-identiﬁcation\nalgorithms are based on metric learning to avoid retraining the model when\nadding a novel class as mentioned in previous section.\nWe adopt the ResNet-50 as a backbone and train the model with center loss\nand reranking strategy [ 10]. The rank-1 accuracy achieves 94.5 and the average\nprecision achieves 85.9 on the Market-1501 [ 19]. According to the settings of\nthe re-identiﬁcation, only one gallery sample that is the closest to the probesample is used to calculate the prototype. Figure 4(c) shows Dist-CAM of the\nprobe samples in the Market-1501 dataset for pedestrian re-identiﬁcation. It can\nbe seen that the discriminative features are concentrated on the human clothes.The Dist-CAM heatmaps in Fig. 4(c) reﬂect that clothes are an important feature\nfor re-identiﬁcation in these probe samples.\n5 Conclusion\nIn this paper, we propose Distance-based Class Activation Map for deep\nmetric learning to explore the interpretability of metric-based networks. On', '346 Y. Shen et al.\nweakly supervised object localization tasks on ILSVRC 2012 [ 13], comprehensive\nexperiments are conducted and the result shows that Dist-CAM is better thanthe original CAM with ResNet or DenseNet as backbones. Besides, we demon-\nstrate the visualization of Dist-CAM in speciﬁc applications of metric learning,\nincluding few-shot learning, image retrieval and re-identiﬁcation. In the future, itis worthwhile to adopt the Dist-CAM to guide the innovation and improvement\nof metric learning methods.\nAcknowledgments. This work was supported by the National Natural Science\nFoundation of China (No.U20B2062), the fellowship of China Postdoctoral Science\nFoundation (No.2021M690354), the Beijing Municipal Science & Technology Project(No.Z191100007419001).\nReferences\n1. Cakir, F., He, K., Xia, X., Kulis, B., Sclaroﬀ, S.: Deep metric learning to rank.\nIn: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 1861–1870 (2019)\n2. Chattopadhay, A., Sarkar, A., Howlader, P., Balasubramanian, V.N.: Grad-\nCAM++: generalized gradient-based visual explanations for deep convolutional\nnetworks. In: 2018 IEEE Winter Conference on Applications of Computer Vision\n(WACV), pp. 839–847 (2018)\n3. Chu, W., Wang, Y.F.: Learning semantics-guided visual attention for few-shot\nimage classiﬁcation. In: 2018 25th IEEE International Conference on Image Pro-cessing (ICIP), pp. 2979–2983 (2018). https://doi.org/10.1109/ICIP.2018.8451350\n4. Ge, W., Huang, W., Dong, D., Scott, M.R.: Deep metric learning with hierarchi-\ncal triplet loss. In: Proceedings of the European Conference on Computer Vision\n(ECCV), pp. 272–288 (2018)\n5. Hao, Y., Wang, N., Li, J., Gao, X.: Hsme: Hypersphere manifold embedding for\nvisible thermal person re-identiﬁcation. In: Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, vol. 33, pp. 8385–8392 (2019)\n6. Li, X., et al.: Learning to self-train for semi-supervised few-shot classiﬁcation. In:\n33rd Conference on Neural Information Processing Systems. vol. 32, pp. 10276–\n10286 (2019)\n7. Liu, J., Song, L., Qin, Y.: Prototype rectiﬁcation for few-shot learning. In: ECCV,\nvol. 1. pp. 741–756 (2019)\n8. Liu, L., Zhou, T., Long, G., Jiang, J., Yao, L., Zhang, C.: Prototype propaga-\ntion networks (PPN) for weakly-supervised few-shot learning on category graph.\nIn: Proceedings of the Twenty-Eighth International Joint Conference on ArtiﬁcialIntelligence, pp. 3015–3022 (2019)\n9. Liu, W., Wen, Y., Yu, Z., Li, M., Raj, B., Song, L.: Sphereface: deep hypersphere\nembedding for face recognition. In: 2017 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 6738–6746 (2017)\n10. Luo, H., Gu, Y., Liao, X., Lai, S., Jiang, W.: Bag of tricks and a strong baseline for\ndeep person re-identiﬁcation. In: 2019 IEEE/CVF Conference on Computer Visionand Pattern Recognition Workshops (CVPRW), pp. 0–0 (2019)\n11. Rusu, A.A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S., Hadsell,\nR.: Meta-learning with latent embedding optimization. In: International Confer-\nence on Learning Representations (2018)', 'Distance-Based Class Activation Map for Metric Learning 347\n12. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-\nCAM: visual explanations from deep networks via gradient-based localization. Int.\nJ. Comput. Vis. 128(2), 336–359 (2020)\n13. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition. In: International Conference on Learning Representations 2015\n(ICLR 2015) (2015)\n14. Snell, J., Swersky, K., Zemel, R.S.: Prototypical networks for few-shot learning.\nAdv. Neural Inf. Process. Syst. 30, 4077–4087 (2017)\n15. Song, H.O., Jegelka, S., Rathod, V., Murphy, K.: Deep metric learning via facility\nlocation. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 2206–2214 (2017)\n16. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The Caltech-UCSD\nBirds-200-2011 Dataset. Tech. Rep. CNS-TR-2011-001, California Institute of\nTechnology (2011)\n17. Wang, H., Zhu, X., Xiang, T., Gong, S.: Towards unsupervised open-set person re-\nidentiﬁcation. In: 2016 IEEE International Conference on Image Processing (ICIP),\npp. 769–773 (2016). https://doi.org/10.1109/ICIP.2016.7532461\n18. Wang, J., Zhou, F., Wen, S., Liu, X., Lin, Y.: Deep metric learning with angular\nloss. In: 2017 IEEE International Conference on Computer Vision (ICCV), pp.\n2612–2620 (2017)\n19. Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., Tian, Q.: Scalable person\nre-identiﬁcation: a benchmark. In: IEEE International Conference on Computer\nVision (2015)\n20. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features\nfor discriminative localization. In: 2016 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR). IEEE Computer Society (2016)\n21. Zhou, Y., Zhu, Y., Ye, Q., Qiu, Q., Jiao, J.: Weakly supervised instance segmen-\ntation using class peak response. In: 2018 IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 3791–3800 (2018)', 'Reading Pointer Meter Through One Stage\nEnd-to-End Deep Regression\nZhenzhen Chao1, Yaobin Mao1(B), and Yi Han2\n1Nanjing University of Science and Technology, Nanjing 210094, China\nmaoyaobin@njust.edu.cn\n2Zhejiang Huayun Information Technology Co., LTD., Hangzhou 310008, China\nAbstract. The recognition of analog pointer meters under nature environment is\ncommonly a challenge task due to many inﬂuences like types of meters, shooting\nangle, lighting condition, etc. Most existing recognition algorithms consist ofmultiple steps including the detection and extraction of dial, scale marks and\npointers followed by reading calculation, which is complex and sensitive to image\nquality. To address this issue, a one-stage, end-to-end recognition method forpointer meter based on deep regression is proposed in this paper. The proposed\nmethod simultaneously locates the position of the end point of a pointer, obtains\na meter reading and determines whether the pointer exceeds the normal rangethrough a fully convolutional neural network. Without complicated image pre-\nprocessing and post-processing, the algorithm can read multiple meters in one\nimage just through a simple one-round forward inference. Experimental resultsshow that the recognition accuracy achieves 92.59% under ±5% reading error,\nand the processing speed reaches approximately 25 FPS on a NVIDIA GTX 1080\nGPU.\nKeywords: Pointer meter ·Automatic meter reading ·Deep regression\n1 Introduction\nIn power stations there exist numerous analogy pointer meters that need manually read\nand monitor periodically which is a cumbersome work since this manual data collectionis of low efﬁciency, high labor intensity, and being easily impacted by external factors.\nTherefore, it is of crucial importance to develop a reliable method to automatically read\nin those pointer meters’ indications.\nMultiple methods have been proposed and applied in automatic meter reading [ 1,2],\nhowever, most of them need two stage operations, namely recognition after detection.\nThose methods require the extraction of dials, scale marks and pointers which is noteasily obtained and severely affected by image quality, thus could not suit for practical\napplications. The method proposed in this paper integrates detection and recognition\ninto one neural network model and accomplishes the tasks via just one round inference.\nSince the endpoint of a gauge’s pointer represents the value of the meter which\ncontains most important information, in our method, we just locate it and needn’t detect\n© Springer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 348–360, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_29', 'Reading Pointer Meter Through One Stage End-to-End Deep Regression 349\nmeter dials and scales. To reliably extract the endpoint on a meter, the CenterNet [ 27]i s\nemployed, as we know object can be detected through keypoint estimation by regression.As mentioned in Mask R-CNN [ 32], multi-task design leads to the uniformity and\nthe consistency of the entire network model which outperforms single task design in\nperformance.\nAll above considerations lead to the outcome of a one-stage, end-to-end pointer meter\nrecognition method that is based on a deep neural network. By utilizing the multi-task\nlearning capability of the deep network, three different output modules are conductedafter a backbone network. Those three modules act simultaneously and respectively\nperform pointer location, meter reading regression, and measuring range checking. As\nan end-to-end model, the proposed method needn’t image pre-processing usually usedin most others. The main contributions of this paper are summarized as follows:\n(1) A simultaneous meter locating and reading recognition model is designed through\ndeep regression which is robust against environment noise.\n(2) The proposed model regresses a normalized value in [0, 1] instead of the absolute\nmeter value by compressing measure range, which makes it easier to converge.\n(3) A classiﬁcation module is set up for outrange checking. In this way, an extra\nconstrain is imposed that signiﬁcantly improves the accuracy of the readingregression.\nThe remaining of the paper is organized as follows: Sect. 2reviews related work on\npointer meter recognition. Section 3details the method proposed in the paper. Section 4\nreports the experimental results. At last, the paper is concluded in Sect. 5.\n2 Related Work\nGenerally, existing meter reading algorithms can be catalogued into three classes, namely\nrecognition by dial detection [ 3–18], recognition by keypoint detection [ 19–22] and\ndirect reading through regression [ 23–26].\nThe ﬁrst two classes of methods need the extraction of objects like dial, pointer and\nscale marks as human doing, while the third one regresses the indications of the metersby deep neural networks. No matter which kind of the method needs explicit dial region\ndetection before reading the indication. Therefore, if DNN is used, at least two networks\nare required respectively for detection and recognition.\n2.1 Recognition by Dial Detection\nThe majority of current researches belongs to this paradigm, which can be sub-divided\ninto two categories: algorithms based on traditional image processing technology and\nmethods based on deep learning.\nTraditional Image Processing Based Algorithms. The traditional image processing\nbased algorithms usually ﬁrst perform pre-process followed by template matching [ 5,7,\n9] or Hough transform [ 3,4,6,8,10] to extract the dial regions. Then the Hough line', '350 Z. Chao et al.\ndetection or line ﬁtting methods are employed to ﬁnd pointers and scale marks on the\nbinarized images. Even those algorithms operate fast, they suffer from noise and imagedegradation, therefore are less robust and unusable in practical applications.\nDeep Learning Based Algorithms. To make the algorithms more robust, in recent\nyears, deep learning is widely applied to pointer meter recognition. Liu [ 11], Zhang\n[12] and Wang [ 13] respectively use Faster R-CNN to detect meter regions then utilize\nimage processing algorithms to extract pointer and scales to get meter readings. Alter-\nnatively, Jiaqi He [ 14] and Zengguang Zhang [ 15] use MASK R-CNN to identify meter\nregion followed by a Hough transformation to detect the positions of pointers. Evendeep learning is used in above methods, they still rely on traditional image-processing\nalgorithms in value reading.\nRecently, some researches begin to apply deep learning on both dial detection and\npointer detection. Jilin Wan et al. [ 16] used Faster R-CNN to detect dial region and pointer\nregion, followed by an improved U-Net model to segment the pointer and scale marks.\nJiale Liu, etc. [ 17] used MASK R-CNN to detect meter region and pointer position,\nhowever, the raw image needs to be carefully pre-processed. Peilin He [ 18] has improved\nthe MASK R-CNN model by PrRoIPooling and used it to detect the dial and the pointer.\nDue to the complexity of the DNN, those algorithms generally are time and memory\nconsuming and only can be run on speciﬁc environments with powerful computational\ncapability.\n2.2 Recognition by Keypoint Detection\nSometimes meter as a whole is difﬁcult to detect especially in clutter environment, how-\never keypoints contain useful information and may be easier to detect which are drawn\nmuch attention to some researchers. Junkai Wang [ 19] uses a multi-task convolutional\nnetwork to detect meter and four kinds of keypoints, which is followed by pointer detec-tion and reading calculation. Yixiao Fang [ 20] uses a Mask R-CNN to simultaneously\ndetect meter and keypoints, then works out meter reading through the calculation of the\npointer angle. Ni Tao et al. [ 21] ﬁrst use SSD to detect an instrument front, then extract\nkeypoints to calculate the reading. Xiaoliang Meng et al. [ 22] use an optimized two-stage\nconvolutional network to detect the keypoints like starting points, ending points of indi-\ncators and the rotation center of the meter pointer. A rotating virtual pointer algorithmis utilized to obtain the ﬁnal reading through pointer’s angle.\nAbove mentioned methods can process a variety of meters without complex image\npre-processing, however they are heavily relied on exact keypoint detection that could\nnot be applied on blur images.\n2.3 Direct Recognition Through Deep Regression\nSome researchers start to utilize deep neural networks to directly perform reading regres-\nsion on detected and calibrated meter images. Cheng Dai, etc. [ 23] use a four-layer CNN\nto regress meter reading, however their method needs complex image pre-processing.\nWeidong Cai, etc. [ 24] expand the dataset by rotating the pointers in meter images which', 'Reading Pointer Meter Through One Stage End-to-End Deep Regression 351\nis subject to regression by use of a CNN. The proposed method has a complex training\nprocess since it requires pre-training a classiﬁcation model and ﬁne-tuning a regressionmodel.\nKunfun Peng [ 25] ﬁrst utilizes an improved Faster R-CNN to detect the dial region\nand then calibrates the meter image by G-RMI algorithm. A CNN is used for pointerdirection regression as well as reading calculation. Hao Zhang, etc. [ 26] utilize YOLOv4\nto detect the dial region and then regress the meter reading based on a deep neural\nnetwork. All those methods will train multiple deep neural networks that increases thecomplexity of the algorithm and leads to large computational consumption.\n3 Method Proposed in the Paper\nAs the position of the pointer uniquely corresponds to a meter reading, we can ﬁnd that\nthe most crucial information for meter recognition is the location of the endpoint of a\npointer. Given that the measure ranges of variant pointer meters vary greatly, it is difﬁcultto directly regress an absolute reading via a DNN. In this paper, the meter range is ﬁrst\nscaled into [0, 1] then subject to regression. Suppose the absolute indication of a meter\nisV, the relationship of Vand the normalized reading value is described in formula ( 1),\nwhere, S\n1is the starting point and S2is the end point of the scale, and prepresents the\nnormalized reading.\nV=p(S2−S1)+S1 (1)\nSometimes, the pointer may fall out of the measurement range or dial region due to error.\nTherefore, we design a network branch to check the outrange of the pointer.\nThe overall architecture of the network is illustrated in Fig. 1, where a resized image is\nfed into a CNN backbone for feature extraction which followed by three output branches\nthat are established respectively to locate the end point of the pointer, directly regressthe normalized meter reading and perform outrange checking.\nreading\n(relative to 0~1)\n0 or 1:\nOut of normal range or notInput: Raw imageKeypoint Detection Module\nbackbone\nCNN\nResNet18-DCNHeatmap: to Detect\nLocation of pointer pointProjected to Initial \nimage\nAffineTransform\nReading Regress Module\nClassification Module0.46reading is in normal range0.88\nreading is in normal range\nFig. 1. The overall structure of our framework', '352 Z. Chao et al.\n3.1 Backbone Network\nThe network backbone used here is ResNet18-DCN [ 27], which consists of ResNet [ 28]\nand a deformable convolution network [ 33].\nResNet is widely used in feature extraction which consists of a series of residual\nblock. The backbone network of our model consists of ResNet18 layers followed by three\nlayers of deformable convolution and transposed convolution to improve the accuracyof the feature extraction as shown in Fig. 2.\nRM 4 \n512\nchannelsDCN+TCN\n256\nchannels\nDCN+TCN\n128\nchannelsDCN+TCN\n64\nchannels\nConv 1\n64_7×7Pool\n3×3\nRM l \n64\nchannelsRM 2 \n128\nchannels\nRM 3 \n256\nchannels\nConvolution operationDeformable convolution\nTransposed convolution\nRM：ResNet Module ；  DCN：Deformable convolution ；  TCN： Transposed convolution1/4 1/8\n1/16 1/321/16\n1/81/4\nFig. 2. Backbone framework. The numbers above or under the boxes are the resize factors relative\nto the input image.\n3.2 Loss Function\nFocal loss [ 29] is used for keypoint detection as shown in Eq. ( 2), where α=2,β=4\n[27].Nis the number of pointers, Y/logicalandtext\nxycis the predicted value and Yxycis the ground truth\nvalue.\nLk=−1\nN/summationdisplay\nxyc⎧\n⎨\n⎩/parenleftBig\n1−Y/logicalandtext\nxyc/parenrightBigα\nlog/parenleftBig\nY/logicalandtext\nxyc/parenrightBig\nifYxyc=1\n/parenleftbig\n1−Yxyc/parenrightbigβ/parenleftBig\nY/logicalandtext\nxyc/parenrightBigα\nlog/parenleftBig\n1−Y/logicalandtext\nxyc/parenrightBig\notherwise(2)\nL1 loss is used for meter reading regression. As shown in Eq. ( 3),pkrepresents the\npointer point k,R/logicalandtext\npkis the predicted relative reading of pk,Rkis actual reading and Nis\nthe number of pointers.\nLreading =1\nNN/summationdisplay\nk=1/vextendsingle/vextendsingle/vextendsingleR/logicalandtext\npk−Rk/vextendsingle/vextendsingle/vextendsingle (3)', 'Reading Pointer Meter Through One Stage End-to-End Deep Regression 353\nThe outrange checking is a binary classiﬁcation process, therefore a binary cross-entropy\nloss function is used as shown in Eq. ( 4), where Ckrepresents the state of the k-th pointer\npoint, whether it exceeds the normal range or not. C/logicalandtext\nkis the prediction output of the\nclassiﬁcation module and Nis the number of pointers.\nLclassify =−1\nNN/summationdisplay\nk=1/bracketleftBig\nCk∗log/parenleftBig\nC/logicalandtext\nk/parenrightBig\n+(1−Ck)∗log/parenleftBig\n1−C/logicalandtext\nk/parenrightBig/bracketrightBig\n(4)\nThe total loss is the weighted summation of above three losses as shown in Eq. ( 5),\nwhere, λreading represents the weight of the regression loss and λclassify represents the\nweight of the classiﬁcation loss. We set λreading =2 andλclassify =1 in all the following\nexperiments.\nLtotal=Lk+λreading Lreading +λclassify Lclassify (5)\n3.3 Model Training\nFor keypoint detection, a two-dimensional Gaussian function is used to make the ground\ntruth labels of a heatmap, as shown in Eq. ( 6), where ( x, y) is the location of each pixel,\nand(xt,yt)is the position of the pointer’s endpoint, σPis a standard deviation which is\nset to 1 in our experiments.\nYxyc=exp/parenleftBigg\n−(x−xt)2+(y−yt)2\n2σ2p/parenrightBigg\n(6)\nSuppose there are Nmeters in an image, therefore N-dimensional mask labels respec-\ntively for regressing module and classiﬁcation module are set. They are denoted as\nMregand Mclas, in which element Mreg[k] and Mclas[k] are set to 1 and the remain\nelements are set to 0. With the help of mask labels, the model updates only when thereexist pointers during the calculation of the regressing loss and the classiﬁcation loss. The\ndown-sampling layers of the Resnet-18 are initialized with ImageNet pretrained weights\nwhilst the up-sampling layers are initialized randomly.\nThe input image resolution is set to 544 ×960 with the original aspect ratio\nunchanged for both training and testing. The data are augmented with random scaling(scale factor in between [0.6, 1.4]), random shifting (shift distance in between [ −0.2,\n0.2]), random rotating (rotation angle in between [ −45°, 46°]), and color jittering. To\nkeep the image contents not changed signiﬁcantly, the operation probability is set to 0.4.Other training parameters are itemized as follows: batch-size is set to 20, learning rate\nis 1.25e −4 for 200 epochs with 10 times drops at 160 and 180 epochs respectively.\nTable 1gives hardware and software conﬁguration for both training and testing.', '354 Z. Chao et al.\nTa b l e 1 . Hardware and software environment for training and testing\nItem Speciﬁcation\nCPU Intel(R) Core(TM) i7-7700\nGPU GTX 1080\nRAM 32G\nGPU memory 8G\nOS Ubuntu 18.04.5 LTS\nDeep learning framework Pytorch\nCUDA 10.2.89\ncudnn 7.6.5\n4 Experimental Results\n4.1 Dataset\nMost images of the dataset used in this paper are from the “2018 Global Artiﬁcial\nIntelligence Application Competition (NanJing)” [ 31], which contains more than 20\ntypes of meters captured in different view angles and light conditions. The other images\nare captured from some substations. Our dataset contains total 813 training images and114 test images. Several typical examples are shown in Fig. 3.\nFig. 3. Typical samples from our dataset\nAll endpoints of the pointers in dataset are tagged by software Labelme, and manually\ncorrespond to normalized reading values that ﬁnally are saved in json format.\n4.2 Experiments\nModel T esting. 114 different kinds of meter images with variants of shooting angels\nand lighting conditions are subject to testing. To quantitatively evaluate the performance,', 'Reading Pointer Meter Through One Stage End-to-End Deep Regression 355\nthree metrics including precision ( P), recall ( R), and F-value are used which are deﬁned\nin formula ( 7)t o( 9).\nP=TP\n(TP+FP)(7)\nR=TP\n(TP+FN)(8)\nF=2×P×R\n(P+R)(9)\nWhere TPdenotes true positive, FPdenotes false positive and FNdenotes false negative.\nTo determine correct detection, a distance measure is deﬁned in formula ( 10), in which\nTis a threshold and Lis the length of the pointer.\nT=0.14×L (10)\nThe detection results for endpoints of pointers are shown in Table 2.\nTa b l e 2 . Detection results for endpoints of pointers\nMetrics TP FP FN R/% P/% F/%\nEndpoints 134 1 1 99.26 99.26 99.26\nTo evaluate the accuracy of the meter recognition, allowance error is respectively set\nto±5% and ±10% of the full range of the meter. Results of the reading accuracy are\nshown in Table 3.\nTa b l e 3 . Meter reading accuracy\n#Meters #Correct reading\n(±5%)/(±10%)Accuracy ( ±5%)/% Accuracy ( ±10%)/%\nReading 135 125/132 92.59 97.78\nFor outrange checking, if the output of the binary classiﬁcation is higher than 0.8\nwhile the ground-truth is 1, or the output is lower than 0.5 while the ground-truth is 0,\nthe classiﬁcation is judged as correct. Final results of the classiﬁcation test are shown in\nTable 4.\nTa b l e 4 . Accuracy of classiﬁcation\n#Meters #Correct classiﬁcation Accuracy/%\nClassiﬁcation 135 134 99.26', '356 Z. Chao et al.\nTa b l e 5 . Model size and operation speed\nModel Speed Model size\nResnet18-DCN 25 FPS 166M\nWe also tested the inference speed and recorded the model size, which are shown in\nTable 5. Notice that the inference was performed on an NVDIA GTX 1080 GPU with\n8G memory.\nComparative Experiment. We have compared our method with a template matching\nmethod [ 30] and a keypoint detection based method [ 22] on the same testing dataset.\nThe testing results are shown in Table 6.\nTa b l e 6 . Comparison among different methods\nMethod End-to-end Accuracy\n(±5%)/%Accuracy\n(±10%)/%\nTemplate matching [ 30]No 68.42 72.28\nKeypoint detection [ 22]No 82.98 86.10\nOur method Yes 92.59 97.78\nAs reported in Table 6, compared with other methods, our method achieves the\nhighest recognition accuracy at 92.59% under ±5% reading error. Some typical results\nare shown in Fig. 4, from which one can ﬁnd that even under clutter scenes and uneven\nillumination our method still can get stable results.\n4.3 Additional Experiments\nDifferent Output Modules. To demonstrate the necessity of the outrange checking\nmodule, we trained two kinds of frameworks, one with the classiﬁer while the otherwithout. The experimental result is shown in Table 7. As one can see, the conﬁguration\nof the outrange checking module improves the performance signiﬁcantly.\nWith or Without DCN Module. To demonstrate the necessity of DCN module, two\nnetwork architectures respectively with and without DCN are subject to reading recog-nition. The experiment results recorded in Table 8show that ResNet-DCN performs\nbetter than ResNet, from which we can infer that the irregular feature extraction yields\nbetter accuracy on reading regression.\nDifferent Keypoint Detection. We chose different kinds of key points as feature to\nregress meter reading, one is the endpoint of the pointer, another is the center point of\nthe meter. Models using different keypoints are trained and subject to test, the ﬁnal result', 'Reading Pointer Meter Through One Stage End-to-End Deep Regression 357\n(a)Meter1  Predicted:0.56; Truth :0.56\nMeter2  Predicted:0.92; Truth :0.93(b)Meter 1  Predicted:0.58; Truth:0.59\nMeter 2  Predicted:0.03; Truth:0.03\n(c)Predicted:0.01; Truth:0.00 (d)Predicted:0.32; Truth:0.31\n(e)Predicted:0.13; Truth:0.15 (f)Predicted:0.18; Truth:0.21\nFig. 4. Some typical recognition results\nTa b l e 7 . The necessity of the outrange checking module\nMethod Accuracy ( ±5%)/%\nKeypoint +reading regress 82.63\nKeypoint +reading regress +classiﬁcation 92.59\nis shown in Table 9. We can perceive that there is an approximately 7% improvement\nwith the use of the endpoint, which suggests that the endpoint provides more crucial\ninformation.\nDifferent Function Transformation. The meter readings are normalized to [0, 1], thus\nthe regressing loss is relatively lower than other parts. We use variants of functions to\nincrease the regressing loss. The testing results are shown in Table 10, where we can', '358 Z. Chao et al.\nTa b l e 8 . The improvement of DCN module\nModel Accuracy ( ±5%)/%\nResNet18 88.15\nResNet18-DCN 92.59\nTa b l e 9 . Regression with different kinds of keypoints\nMethod Accuracy ( ±5%)/%\nCenter point of meter 85.27\nEndpoint of pointer 92.59\nT able 10. Accuracy under different function transformation\nFunction transformation Accuracy ( ±5%)/%\n1/(R/logicalandtext\npk,Rk) 80.85\n−log(R/logicalandtext\npk,Rk) 89.88\n4∗(R/logicalandtext\npk,Rk) 92.59\nsee the function transformation of 4 ∗(R/logicalandtext\npk,Rk)has the highest accuracy of reading\nrecognition.\n5 Conclusion\nIn this paper, a one-stage, end-to-end recognition method for multiple types of pointermeters is proposed. The method can perform the multi-task inference through a single\ndeep neural network. Fully utilizing the ﬁtting and learning capabilities of the deep\nnetwork, our model is capable of locating the endpoint of the pointer, and obtainingthe meter reading as well as making a outrange checking simultaneously just with one\nround inference. The experimental results show that the proposed method can achieve a\nhigh recognition accuracy at a running speed of 25 FPS on an NVDIA GTX 1080 GPU.Furthermore, the proposed method is of good versatility and adapted to different image\nquality, thus has potential application in complex real-world industrial scenarios.\nReferences\n1. Xiong, G.L., Xiao, W.M., Wang, X.M.: Review of pointer meter detection and recognition\nmethod based on vision. Transducer Microsyst. Technol. 39(346(12)), 6–8+14 (2020)', 'Reading Pointer Meter Through One Stage End-to-End Deep Regression 359\n2. Han, S.C., Xu, Z.Y ., Yin, Z.C.: Research review and development for automatic reading\nrecognition technology of pointer instruments. Comput. Sci. 45(6), 54–57 (2018)\n3. Liu, Y ., Shi, K., Zhang, Z., Hu, Z., Liu, A.: A stable and reliable self-tuning pointer type meter\nreading recognition based on gamma correction. In: Liang, Q., Wang, W., Liu, X., Na, Z.,\nJia, M., Zhang, B. (eds.) CSPS 2019. LNEE, vol. 571, pp. 2448–2458. Springer, Singapore\n(2020). https://doi.org/10.1007/978-981-13-9409-6_297\n4. Li, X., Yin, P ., Duan, C.: Analog gauge reader based on image recognition. J. Phys. Conf.\nSer. 1650 (3), 032061 (2020). 8p.\n5. Zhang, M.L.: pointer instrument recognition based on Halcon template matching optimiza-\ntion. Artif. Intell. Robot. Res. 09(2), 123–130 (2020)\n6. Lai, H.W., Kang, Q., Pan, L.: A novel scale recognition method for pointer meters adapted\nto different types and shapes. In: 15th International Conference on Automation Science and\nEngineering (CASE). IEEE (2019)\n7. Zhang, X.F., Huang, S.: Research on pointer identifying and number reading algorithm of\nmulti-class pointer instruments. Electr. Meas. Instrum. 57(16), 147–152 (2020)\n8. Zhang, Z.F., Wang, F.Q., Tian, E.L.: Reading recognition method of pointer meter on the\nbasis of machine vision. Control Eng. China 3, 581–586 (2020)\n9. Shen, Y .Q., Xiong, W.H., Huang, W.M., Xu, W.: Instrument recognition based on template\nmatching and hough circle detection, vol. 31, no. 4, pp. 69–73 (2021)\n10. Gao, L.: Discussion on improving the accuracy of automatic recognition algorithm for circular\npointer instrument. Instrumentation 28(04), 5–8 (2021)\n11. Liu, K.: Recognition of the Analog Display Instrument Based on Deep Learning. Huazhong\nUniversity of Science and Technology, China (2017)\n12. Zhang, X., Dang, X., Lv, Q.: A pointer meter recognition algorithm based on deep learning.\nIn: 3rd International Conference on Advanced Electronic Materials, Computers and Software\nEngineering (AEMCSE) (2020)\n13. Wang, L., Wang, P ., Wu, L.: Computer vision based automatic recognition of pointer\ninstruments: data set optimization and reading. Entropy 23(3), 272 (2021)\n14. He, J.Q.: Research and Application of Automatic Recognition of Dial Instrument Based on\nDeep Learning. Beijing University of Posts and Telecommunications, China (2020)\n15. Zhang, Z.G.: Research on the Method of Pointer Instrument Reading Recognition Based on\nDeep Learning. Harbin Engineering University, China (2020)\n16. Wan, J.L., Wang, H.F., Guan, M.Y .: an automatic identiﬁcation for reading of substation\npointer-type meters using faster R-CNN and U-Net. Power Syst. Technol. 44(08), 3097–3105\n(2020)\n17. Liu, J., Wu, H.Y ., Chen, Z.H.: Automatic identiﬁcation method of pointer meter under complex\nenvironment. In: 12th International Conference on Machine Learning and Computing, ICMLC\n(2020)\n18. He, P .L.: Deep Learning-Based Recognition Algorithm for Industry Meters and Its Applica-\ntions. University of Electronic Science and Technology of China (2020)\n19. Wang, J.K.: A Thesis Submitted in Partial Fulﬁllment of the Requirements for the Degree for\nthe Master of Engineering. Huazhong University of Science & Technology, China (2019)\n20. Yang, Y .X.: Research and application of pointer Meter Reading Recognition Algorithm Based\non Key point detection. Zhejiang University, China (2020)\n21. Ni, T., Miao, H., Wang, L.: Multi-meter intelligent detection and recognition method under\ncomplex background. In: 39th Chinese Control Conference (CCC) (2020)\n22. Meng, X., Cai, F., Wang, J.: Research on reading recognition method of pointer meters based\non deep learning combined with rotating virtual pointer. In: 5th International Conference on\nInformation Science, Computer Technology and Transportation (ISCTT) (2020)', '360 Z. Chao et al.\n23. Dai, C., Gan, Y ., Zhuo, L.: Intelligent ammeter reading recognition method based on deep\nlearning. In: IEEE 8th Joint International Information Technology and Artiﬁcial Intelligence\nConference (ITAIC) (2019)\n24. Cai W., Ma B., Zhang L.: A pointer meter recognition method based on virtual sample\ngeneration technology. Measurement 163, 107962 (2020).\n25. Pen, H.F.: Research on Pointer Meter Reading Recognition Method based on Deep Learning.\nUniversity of Science and Technology of China (2020)\n26. Zhang, H., Zhou, G.J., Wang, F.L.: Deep learning-based instrument recognition technology\nfor sea ascending pressure station. Ind. Control Comput. 34(03), 56–57+60 (2021)\n27. Zhou, X., Wang, D., Krähenbühl, P .: Objects as points. arXiv preprint arXiv:1904.07850\n(2019)\n28. He K., Zhang X., Ren S.: Deep residual learning for image recognition. IEEE (2016)\n29. Lin T.Y ., Goyal P ., Girshick R.: Focal loss for dense object detection. In: Proceedings of the\nIEEE International Conference on Computer Vision, pp. 2980–2988 (2017)\n30. Weng, Z.H.: Research on Remote Reading Recognition and Location of Single Pointer and\nDigital Instrument. University of Electronic Science and Technology of China (2019)\n31. https://www.kesci.com/home/competition/5b387f92257415006c47183a\n32. He, K., Gkioxari, G., Dollár, P .: Mask R-CNN. IEEE Trans. Pattern Anal. Mach. Intell. 39,\n128–140 (2017)\n33. Zhu, X., Hu, H., Lin, S., Dai, J.: Deformable ConvNets v2: more deformable, better results.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 9308–9316 (2019)', 'Deep Architecture Compression with\nAutomatic Clustering of Similar Neurons\nXiang Liu1,2, Wenxue Liu1,L i - N aW a n g1, and Guoqiang Zhong1(B)\n1College of Computer Science and Technology, Ocean University of China,\nQingdao, China\ngqzong@ouc.edu.cn\n2Innovation Center, Ocean University of China, Qingdao, China\nAbstract. The more complex the deep neural networks (DNNs) are,\nthe more diverse the learning tasks they can be applied to. However,\nfor complex DNNs, it is diﬃcult to deploy them on to the edge devices,\nwhich have limited computation and storage resources. In this paper,we propose an automatic neurons clustering (ANC) approach for deep\narchitecture compression, it can reduce the computation and storage\nconsumption without degrading the model performance. Speciﬁcally, anautomatic clustering algorithm is used to discover similar neurons in each\nlayer of the deep architecture, then the similar neurons and the corre-\nsponding connections are merged based on the results of automatic clus-\ntering. After ﬁne-tuning, a more compact and less storage space occupied\nneural network is obtained, with no performance degradation comparedto the original deep architecture. This compression method is fully appli-\ncable to fully connected layer and convolutional layer, both of which are\ncommon modules of popular DNNs. The analysis of neuron redundancyin DNNs is performed on a deep belief network (DBN), and it is veriﬁed\nthat there is great redundancy among neurons in DNNs. To verify the\neﬀectiveness of the proposed ANC, we conducted experiments on DBNand VGGNet-16 using MNIST, CIFAR-10 and CIFAR-100 datasets. The\nexperimental results demonstrate that our method can eﬀectively per-\nform deep architecture compression without losing network performance.After ﬁne-tuning, it can even obtain higher accuracy than the original\nnetwork. In addition, the superiority of ANC is further demonstrated by\ncomparing it with related network compression methods.\nKeywords: Deep architecture compression\n·Compact neural\nnetwork ·Automatic neurons clustering.\n1 Introduction\nDeep learning have made breakthroughs in many ﬁelds, such as speech recogni-\ntion, computer vision, and natural language processing. Recently, many excellent\ndeep learning models have been proposed, such as Transformer [ 26,27] and multi-\nlayer perceptron (MLP) [ 3,25], however, the architecture that dominates learning\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 361–373, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_30', '362 X. Liu et al.\ntasks is still convolutional neural network (CNN). The residual networks [ 10] suc-\ncessfully overcome the problem of gradient collapse and disappearance in ultra-deep networks, enabling CNNs to became deeper to cope with increasingly com-\nplex and diﬃcult tasks. The consequence is that increasingly large computation\nand storage resources are required. For example, AlexNet [ 16] has 60M parame-\nters, VGGNet-16 [ 24] has 138M parameters, and Gpipe [ 14] even reaches 556M.\nAlthough the high accuracy is often delivered by large DNNs, the requirements\nfor computation resource and storage space make them diﬃcult to be directlydeployed on resource-constrained devices. At the same time, in the era of 5G and\nIoT (Internet of Things), edge AI becomes more and more signiﬁcant. Hence how\nto deploy high-performance DNNs on these edge devices becomes a critical prob-lem. In this paper, we propose an automatic neurons clustering method for deep\narchitecture compression, which aims to compress the scale of DNNs’ parame-\nters without degrading model performance, thus reducing the computational con-\nsumption and ultimately facilitating the development of edge AI.\nFig. 1. The left ﬂow chart shows the overall process of our work. Note that the “Original\nNetwork” is trained suﬃciently, and the step of “Automatic Neurons Clustering” is\nillustrated as the right diagram.\nIn this work, a DNN is ﬁrst trained suﬃciently. We then use an automatic\nclustering algorithm to adaptively determine the number of clusters of neu-\nrons in each layer, and those neurons with the same clustering center are con-sidered to be similar neurons. These similar neurons and their connections are\nthen merged to reduce the redundancy degree in the network. Finally, after ﬁne-\ntuning, a compact neural network can be obtained without any loss of modelperformance. The overall ﬂow chart and the neuron clustering diagram are illus-\ntrated as Fig. 1. Comparing to previous similar network compression methods,\nthe main innovation of ANC is that similar neurons can be merged automaticand adaptively, without any manual operations.', 'Deep Architecture Compression with Automatic Clustering 363\nThe following sections are organized as below. In Sect. 2, some of the main-\nstream network compression methods are introduced. In Sect. 3, we analyze the\nredundancy of neurons in DNNs and explain how to perform automatic neuron\nclustering. In Sect. 4, a large number of experiments with comparison to related\napproaches are reported to show the eﬀectiveness and superiority of ANC app-roach. Finally in Sect. 5, this paper is concluded, and the possible directions for\nfurther research are discussed.\n2 Related Work\n[8] systematically summarizes the eﬀective methods for deep neural network com-\npression, such as low-rank decomposition, pruning, Huﬀman coding and weight\nquantization. Subsequently, more novel network compression approaches such as\nknowledge distillation [ 11,22] and structural re-parameterization [ 3,4] have been\nproposed one after another. A series of network compression methods that are\nof high relevance to this paper are introduced in this section.\n2.1 Pruning\nPruning approach achieves network compression by reducing the number of\nparameters, and it can be divided into two types: unstructured pruning [ 9]a n d\nstructured pruning. Unstructured pruning sets the unimportant parameters to\nzero without removing them, while structured pruning removes the information\nof the whole ﬁlter directly. Since the unstructured pruning approach does not\nremove those zeroed weights and corresponding connections, it cannot signif-icantly improve the computational eﬃciency of the network, Therefore, struc-\ntured pruning approach is considered as the more eﬀective implementation.\nIn 2016, Lebedev et al. proposed the Improved Brain Damage method [ 17].\nThe group sparse regularization is added to the standard training process in this\nmethod, therefore ﬁlters smaller than a given threshold can be pruned in the\nform of neuron groups. For CNNs, judging the importance of the convolutionkernels by the van value of the parameters is a well-tested method. Li et al.\nmeasured the importance of convolutional kernels by the weight parameter after\nL1 regularization and these kernels were removed in their entirety [ 18].\n2.2 Knowledge Distillation\nHinton et al. argued that the output of a network with superior performance\nmight contain some hidden but valid information, and proposed the conceptof Knowledge Distillation (KD) in 2015 [ 11]. KD approach using both the real\nlabels and the softmax output from a complex network (Teacher) as learning\ntargets for a simple one (Student) to induce Students to learn something thatthe real labels cannot provide.\nRemero et al. proposed FitNet based on KD [ 22], an approach that introduces\nnot only the output of ﬁnal layer, but also the feature representation of middle', '364 X. Liu et al.\nlayers of the teacher network as knowledge to train the student network. Liu et al.\n[19] further implemented layers resection for DNNs using middle layer knowledge\ndistillation. Chen et al. [ 1] combined Generative Adversarial Networks (GAN) [ 7]\nand teacher-student networks to propose a data-free KD method that does not\nrequire the involvement of raw training data. Zagoruyko et al. [ 28]p r o p o s e da\nmechanism for transferring attention as knowledge, allowing the student network\nto learn a spatial attention map similar to that of the teacher network, thereby\nimproving the performance of the student network.\n2.3 Lightweight Network Architecture Design\nThe main idea of lightweight network architecture design is to design compact\ncomputational module to replace the modules with large number of param-\neters. For example, SqueezeNet extensively uses 1 ×1 convolution instead of\n3×3 to achieve the performance of AlexNet while reducing the computational\ncomplexity and memory consumption of the networks [ 15]. The MobileNet fam-\nily [12,13,23] and ShuﬄeNet family [ 21,29] use deep separable convolution and\npointwise group convolution instead of standard convolution respectively, reduc-ing the computation and storage consumption of the neural networks while keep-\ning the performance unchanged.\n3 Automatic Neurons Clustering\nIn this section, we ﬁrst analyze the redundancy of neurons based on DBN to\nestablish a practical basis for neurons clustering. Then the automated neuronclustering algorithm is introduced in detail.\n3.1 Neurons Redundancy Analysis\nBased on visualization method, we analyze the redundancy of neurons in deep\nbelief networks (DBN). The architecture of DBN is shown in Table 1.\nTable 1. Network architecture of DBN.\nLayers Number of input nodes Number of output nodes\nLayer 1 784 500\nLayer 2 500 500\nLayer 3 500 2000\nLayer 4 2000 10\nTraining this DBN on MNIST dataset, and after trained suﬃciently, we ana-\nlyze the distribution of its neurons using the t-SNE method. The result is plotted\nin Fig. 2. As it is illustrated in Fig. 2, the distribution of neurons in DBN is very', 'Deep Architecture Compression with Automatic Clustering 365\ndense. For example, in Fig. 2(c), many scattered points are clustered together\nand there are many very dense regions. This proves that after training, manyneurons in the network learn similar features, i.e., there is redundancy among\nthe neurons.\nFig. 2. The neurons distribution of original DBN trained suﬃciently on MNIST\ndataset. (a), (b) and (c) separately show the ﬁrst, second and third hidden layer.\n3.2 Automatic Neurons Clustering Algorithm\nA consensus should be established that those neurons learned similar features\ncan be merged as one to reduce the network’s consumption of computation andstorage with no damage of performance. Determining and merging those similar\nneurons eﬃciently, is exactly the problem that this subsection is trying to solve.\nDeﬁne the ith neuron in the llayer as the set of weights wand bias b:\nneu\nl\ni=<wl\ni,1,wl\ni,2,··· ,wl\ni,n l−1,bl\ni>. (1)\nIn Eq. ( 1),wl\ni,jis the weight connecting the ith neuron in lth layer and the j\nth neuron in l-1th layer, bl\niis the bias of the ith neuron in lth layer, and nl−1\nstands for the amount of neurons in the l-1th layer. To facilitate operations on\nneurons in the lth layer, these neurons are merged into a set:\nSl=< neul\n1, neul2,··· , neul\nnl>. (2)\nBefore being merged, these neurons in the set Slare ﬁrst clustered via the\nautomatic clustering algorithm: mean shift , which is proposed by Fukunaga\n[6].\nAs illustrated in Fig. 3[5], the main concepts involved in the mean shift\nclustering algorithm are region of interest, center of mass, and mean shift vector.\nthe key of the algorithm is to calculate the mean shift vector of the centroidsbased on the density of data points in the region of interest, and then shift\nthe centroids based on the mean shift vector the key is to calculate the mean\ndrift vector of the centroids based on the density of data points in the regionof interest, and then shift the centroids based on the mean drift vector and\niterate until the centroids no longer change. During this process, the number of', '366 X. Liu et al.\nFig. 3. Schematic of mean shift clustering algorithm [ 5].\noccurrences of data points in each region of interest is counted, and this number\nis used as the basis for the ﬁnal clustering.\nThe main process of mean shift algorithm is: for a data point xin the dataset,\nthere are many points xiin its region of interest, and the sum of the oﬀsets\nrequired to move the point xto each point xiis calculated and averaged, so that\nthe mean shift vector is obtained (the direction of the vector is the directionof the dense distribution of surrounding points). Then the point xis moved\naccording to the mean shift vector. The above process is iterated until certain\nconditions are met.\nThe number of clustering centers in each layer, that is, the number of groups\ninto which the neurons can be divided, is obtained adaptively by means shift\nalgorithm. The k\nlis used to denote the number of clustering centers in the lth\nlayer that are automatically obtained, then the set of all clustering centers in\nthe lth layer is:\nPl=<pl\n1,pl2,··· ,pl\nkl>, (3)\nin which pl\nidenotes the ith cluster center in the lth layer. Also, the cluster\npartitioning information of neurons in the lth layer is expressed as:\nRl=<rl\n1,rl\n2,··· ,rl\nnl>, (4)\nin which rl\nidenotes the cluster to which the ith neuron in the lth layer is\nautomatically clustered. In the lth layer, neurons assigned to the same cluster\nare merged into the clustering center of such cluster.\nAssuming that the neurons divided into the same cluster by the mean\nshift method can be considered as identical within the acceptable error (the\n“acceptable error” is decided by the number of clustering centroids), i.e. thereisneu\nl\ni=neul\njifrl\niis equal to rl\nj, the activation value of the ith neuron in the\nl+1layer can be recalculated as:', 'Deep Architecture Compression with Automatic Clustering 367\nal+1\ni=f⎛\n⎝nl/summationdisplay\nj=1wl+1\ni,jal\nj+bl+1\ni⎞\n⎠, (5)\n=f⎛\n⎝kl/summationdisplay\np=1(/summationdisplay\nrj=pwl+1\ni,jal\nj)+ bl+1\ni⎞\n⎠, (6)\n≈f⎛\n⎝kl/summationdisplay\np=1(/summationdisplay\nrj=pwl+1\ni,jˆal\np)+ bl+1\ni⎞\n⎠, (7)\n=f/parenleftBiggkl/summationdisplay\np=1ˆwl+1\ni,pˆal\np+bl+1\ni/parenrightBigg\n. (8)\nIn the derivation of Eq. ( 7)t oE q .( 8),ˆwl+1\ni,p=/summationtext\nrj=pwl+1\ni,jthen denotes the\nmerging process of the relevant connections in the network. ˆ al\npin Eq. ( 7)a n d\n(8) denotes the activation value of the pth clustering center in the llayer. Note\nthat since the above derivation is based on the assumption that the neurons\nassigned to the same cluster are considered as identical within the acceptableerror, the obtained result in Eq. ( 8) is an approximation. However, also due to\nthe errors leading to this approximation are within acceptable limits, the network\nafter neurons clustering can be quickly restored to the initial level with simpleﬁne-tuning.\n4 Experiment and Analysis\nIn this section, the feasibility and eﬀectiveness of ANC method for deep architec-\nture compression proposed in this paper will be experimentally veriﬁed. Based\non three dataset, i.e. MNIST, CIFAR-10, and CIFAR-100, the proposed app-roach was deployed on three networks including the fully connection network\n(DBN), and the VGGNet-16.\n4.1 Experiment on the Fully Connected Network\nThis experiment was performed on the MNIST dataset using a DBN, where the\nbenchmark network’s architecture is shown in Table 1, and its error percentage\nin validation set was 1 .03%. First, the bandwidth parameter of the mean shift\nalgorithm was adjusted to 0.004 (the closer the parameter is to 0, the more the\nclusters centroids are obtained). With this parameter, a compressed network A\nwith compression ratio of 75 .45% was obtained. This compressed network’s error\npercentage in validation set was 0 .99% after 40 epochs of ﬁne-tuning. The scatter\nplot of the neuron distribution of this compressed network is illustrated in Fig. 4\nwith the same method as Fig. 2. It can be seen that the distribution of neurons\nin the compressed DBN is more dispersed than in Fig. 2, and many dense areas\nhave become sparse.', '368 X. Liu et al.\nFig. 4. The neurons distribution of compressed DBN with 75.45% parameter compres-\nsion ratio comparing to the original. (a), (b) and (c) separately show the ﬁrst, secondand third hidden layer.\nBy setting the bandwidth parameter to 0.1, a compression network Bwith\na parameter compression ratio of 92 .81% was obtained. After 60 epochs of ﬁne-\ntuning, the compressed network Bwas restored to the same accuracy as the\noriginal network. To investigate whether the good performance of the above-\nFig. 5. The epoch-accuracy curve in validation set of reconstructed tiny DBN. The\nhorizontal axis is epoch and the vertical axis is the accuracy in validation set.\nobtained compressed network was caused by the tiny structure, a tiny DBN with\nthe same architecture as the compressed network Bwas reconstructed. It was\ntrained after being initialized, and the curve of its validation set accuracy during\ntraining is plotted as Fig. 5. The reconstructed net’s validation set error rate was\n3.21%, which was higher than all the above results. This proved that the good\nperformance of the compressed network originates from our ANC method.\nTo further demonstrate the advantage of ANC method, we replace the clus-\ntering algorithm with the k-means, which is used by MSN [ 30], and specify the\nsame number of cluster centroids for each layer of the network as in the mean shift\nmethod. For a fair comparison, the compressed network was ﬁne-tuned with thesame epoch as that obtained by the mean shift method after being compressed.\nﬁnally, the results of this comparison are summarized with the results of the other\nexperiments in this subsection into Table 2. Table 2proved that with the same\ncompression ratio, compressing network by using ANC method outperforms the', 'Deep Architecture Compression with Automatic Clustering 369\nTable 2. The Comparison Experimental results obtained by ANC, MSN, and recon-\nstructed net on the MNIST dataset. The “# of A” means the “compressed network A”\nwhich is obtained from original DBN by using our method, and it is same for the “#\nofB”. The “# of K-means” is the compressed network obtained from MSN method,\nwhich has the same architecture with the “compressed network B”. Finally, the “# ofRe-cons” is the reconstructed tiny DBN.\nComparative idicators Benchmark #o f A#o f B#o fK - m e a n s[ 30]# of Re-cons\nAmount of params (M) 1.67 0.41 0.12 0.12 0.12\nCompression ratio (%) - 75.45 92.81 92.81 92.81\nError rate in val (%) 1.03 0.99 1.03 1.56 3.21\nreconstructed tiny network and manually conﬁrming the number of clustering\ncentroids via k-means. This is a preliminary demonstration of the eﬀectiveness\nand superiority of the compression method proposed in this paper.\n4.2 Experiment on Convolutional Networks\nTable 3. The comparison experimental results obtained by ANC, and k-means on\nthe CIFAR-10 dataset. The “# of A” means the “compressed VGGNet-16 A” which is\nobtained from original VGGNet-16 by using our method, and it is same for the “# of\nB”. The “ KofA” is the compressed VGGNet-16 A obtained by using k-means method,\nand it is same for the “ KofB”.\nComparative idicators Benchmark #o f A#o f B KofA KofB\nAmount of params (M) 33.65 3.15 2.43 3.15 2.43\nCompression ratio of params (%) - 90.64 92.78 90.64 92.78\nFLOPS ( ×108) 6.65 4.19 4.00 4.19 4.00\nCompression ratio of FLOPS (%) - 36.99 39.85 36.99 39.85\nError rate in val (%) 6.38 6.35 6.27 6.41 6.97\nThe experiments in the previous subsection demonstrate the eﬀectiveness of\nthe ANC method when compressing fully connected networks. To further ver-\nify the eﬀectiveness on more complex datasets and CNNs, we conducted vali-\ndation experiments and comparison experiments on the CIFAR-10/CIFAR100\ndataset using the VGGNet-16. The error rate in validation set of the fullytrained VGGNet-16 on the CIFAR-10 dataset was 6 .38%, which was used as the\nbenchmark.\nBy setting the bandwidth of mean shift to 0.05, a compressed VGGNet-\n16 A with 90.64% parameters compression ratio and 36.99% FLOPs compression\nratio was obtained. The validation set error rate of Compressed VGGNet-16 A\nwithout ﬁne-tuning was 7.93%, with almost no performance loss comparing to', '370 X. Liu et al.\nthe benchmark. After only 20 epochs of ﬁne-tuning, the validation set error rate\nof the compressed network A dropped to 6.35%, which was lower than that ofthe benchmark.\nThe compressed VGGNet-16 B with the parameters compression ratio of\n92.78% was obtained by setting the bandwidth parameter to 0.2. After 60 epochsof ﬁne-tuning, its validation set error rate decreased 0.11% compared to the\nbenchmark.\nAlso, for VGGNet-16, we conducted comparison experiments with the k-\nmeans method (MSN) [ 30]. The comparison experiments were performed on the\nCIFAR-10 dataset and the experimental results are summarized in Table 3.\nThe experimental results show that the compressed network obtained by\nusing k-means clustering loses some accuracy relative to the benchmark at both\ncompression rates, while the ANC method even improves the accuracy. This\nexperiment proved that the ANC method still maintains eﬀectiveness and sta-\nbility when deploy on large CNNs.\nFurther, experiments on the CIFAR-100 dataset were conducted, and the\nresults are shown in Table 4.\nFinally, Table 5shows the results of the comparison experiments between\nANC and the three other network compression methods on the CIFAR-10 andCIFAR-100. To be fair, the exact same network architecture as in the paper [ 18]\nwas used in the experiments. It was easy to see from Table 5that the compressed\nmodels obtained by our method deliver higher accuracy relative to the thatobtained by the other methods in the case of very similar compression rates. It\nwas worth mentioning that during the experiments, we found that ANC has less\ndamage to the original network than the other three methods. Speciﬁcally, afterusing ANC to compress the networks without ﬁne-tuning and testing it directly\non the CIFAR-10, the accuracy of validation set was 81.77%. The same test using\nt h em e t h o dp r o p o s e di np a p e r[ 18] yielded an accuracy of 32.54% . This further\ndemonstrated that ANC can compress the networks more safely and eﬃciently\nthan other methods [ 2,18,20], and with little damage to the original network at\nlow compression rates.\nTable 4. Results of VGGNet-16 with diﬀerent compression ratios obtained on CIFAR-\n100.\nModels Error Params P-Compression FLOPS F-Compression\nVGGNet16 26.38% 34.02 M - 6.65×108-\nCompressed Net A 26.20% 8.95 M 73.69% 5.58×10816.09%\nCompressed Net B 26.33% 5.68 M 83.30% 4.56×10831.43%', 'Deep Architecture Compression with Automatic Clustering 371\nTable 5. Comparison of ANC and two correlation network compression methods on\nthe CIFAR-10/100 dataset. # in table stands for “Compression ratio”, while Δ standsfor “Change of accuracy”\nMethods #o nC I F A R - 1 0 Δo nC I F A R - 1 0 # on CIFAR-100 Δ on CIFAR-100\nNet slimming [ 20]88.5% +0 .14% 76.0% +0 .22%\nPruning ﬁlters [ 18]88.5% −0.54% 75.1% −1.62%\nGSM [ 2] 88.5% +0 .19% 76.5% +0 .08%\nOurs 88.5% +0.19% 75.9% +0.25%\n5 Conclusion and Discussion\nIn this paper, a novel deep architecture compression method is proposed. The\nassumption that there is redundancy in neurons is obtained by visualizing and\nanalyzing the distribution of neurons in DNNs. Based on this assumption, wedesign an automatic neurons clustering algorithm to obtain and merge simi-\nlar neurons in networks. Experimental results demonstrate that the compressed\nmodels obtained by our method achieve better performance. Moreover, since theproblem of optimal initialized cluster centroids selection is avoided, the ANC\nmethod always achieves better performance at the same compression ratio com-\nparing to those neural networks compression methods that manually specify the\ncluster centroids (e.g., k-means based methods [ 30]). And the more complex the\nnetworks are, the better results can be achieved by ANC.\nThe compressed method in this paper focuses on the convolutional neural\nnetworks, but there are still many other networks with diﬀerent architectures,\nsuch as RNN, LSTM, and Transformer, which are also widely used in the ﬁeld ofdeep learning. How to implement the ANC method speciﬁcally in these networks\nand whether it has good results still need to be veriﬁed. Therefore, future work\ncan explore how to perform deep architecture compression by the ANC methodin those non-convolutional neural networks.\nAcknowledgement. This work was supported by the National Key Research and\nDevelopment Program of China under Grant No. 2018AAA0100400, the Joint Fundof the Equipments Pre-Research and Ministry of Education of China under Grant No.\n6141A0 20337, the Science and Technology Program of Qingdao under Grant No. 21-\n1-4-ny-19-nsh, the Natural Science Foundation of Shandong Province under Grant No.ZR2020MF131, and the Open Fund of Engineering Research Center for Medical Data\nMining and Application of Fujian Province under Grant No. MDM2018007. Thanks to\nZhaoxu Ding for his assistance in writing this paper.\nReferences\n1. Chen, H., et al.: Data-free learning of student networks. In: 17 International Con-\nference on Computer Vision, Seoul, pp. 3513–3521. IEEE Computer Society (2019)', '372 X. Liu et al.\n2. Ding, X., Ding, G., Zhou, X., Guo, Y., Han, J., Liu, J.: Global sparse momentum\nSGD for pruning very deep neural networks. In: 32nd Annual Conference on Neural\nInformation Processing Systems, Vancouver, pp. 6379–6391 (2019)\n3. Ding, X., Zhang, X., Han, J., Ding, G.: Repmlp: re-parameterizing convolutions\ninto fully-connected layers for image recognition. CoRR abs/2105.01883 (2021)\n4. Ding, X., Zhang, X., Ma, N., Han, J., Ding, G., Sun, J.: Repvgg: making vgg-style\nconvnets great again. CoRR abs/2101.03697 (2021)\n5. of Electronic Engineering, D., Computer Science, U.o.M.: senmentation and clus-\ntering. [EB/OL] (2012). https://www.eecs.umich.edu/vision/teaching/EECS442\n2012/lectures/seg cluster.pdf\n6. Fukunaga, K., Hostetler, L.D.: The estimation of the gradient of a density function,\nwith applications in pattern recognition. IEEE Trans. Inf. Theory 21(1), 32–40\n(1975)\n7. Goodfellow, I.J., et al.: Generative adversarial networks. CoRR abs/1406.2661\n(2014)\n8. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-\nwork with pruning, trained quantization and huﬀman coding. In: 4th InternationalConference on Learning Representations (ICLR), San Juan (2016)\n9. Hassibi, B., Stork, D.G.: Second order derivatives for network pruning: Optimal\nbrain surgeon. In: 5th Advances in Neural Information Processing Systems, Denver,Colorado, pp. 164–171. Morgan Kaufmann, Denver (1992)\n10. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: 4th IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas,pp. 770–778. IEEE Computer Society (2016)\n11. Hinton, G.E., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.\nCoRR abs/1503.02531 (2015)\n12. Howard, A., et al.: Searching for mobilenetv3. In: 17th International Conference\non Computer Vision, Seoul, pp. 1314–1324. IEEE Computer Society (2019)\n13. Howard, A.G., et al.: Mobilenets: eﬃcient convolutional neural networks for mobile\nvision applications. CoRR abs/1704.04861 (2017)\n14. Huang, Y., et al.: Eﬃcient training of giant neural networks using pipeline par-\nallelism. In: 33rd Annual Conference on Neural Information Processing Systems(NIPS), Vancouver, pp. 103–112 (2019)\n15. Iandola, F.N., Moskewicz, M.W., Ashraf, K., Han, S., Dally, W.J., Keutzer, K.:\nSqueezenet: Alexnet-level accuracy with 50x fewer parameters and ¡1mb modelsize. CoRR abs/1602.07360 (2016)\n16. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-\nvolutional neural networks. In: 26th Advances in Neural Information ProcessingSystems, Lake Tahoe. pp. 1106–1114 (2012)\n17. Lebedev, V., Lempitsky, V.S.: Fast convnets using group-wise brain damage. In:\n4th IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas,\npp. 2554–2564. IEEE Computer Society (2016)\n18. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning ﬁlters for eﬃcient\nconvnets. In: 5th International Conference on Learning Representations (ICLR),\nToulon (2017)\n19. Liu, X., Wang, L., Liu, W., Zhong, G.: Incremental layers resection: a novel method\nto compress neural networks. IEEE Access 7, 172167–172177 (2019)\n20. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning eﬃcient convo-\nlutional networks through network slimming. In: 16th International Conference onComputer Vision, Venice, pp. 2755–2763. IEEE Computer Society (2017)', 'Deep Architecture Compression with Automatic Clustering 373\n21. Ma, N., Zhang, X., Zheng, H.-T., Sun, J.: ShuﬄeNet V2: practical guidelines for\neﬃcient CNN architecture design. In: Ferrari, V., Hebert, M., Sminchisescu, C.,\nWeiss, Y. (eds.) Computer Vision – ECCV 2018. LNCS, vol. 11218, pp. 122–138.Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01264-9\n8\n22. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets:\nhints for thin deep nets. In: 3rd International Conference on Learning Representa-tions (ICLR), San Diego (2015)\n23. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Mobilenetv 2:\nInverted residuals and linear bottlenecks. In: 6th IEEE Conference on Computer\nVision and Pattern Recognition, Salt Lake City, pp. 4510–4520. IEEE Computer\nSociety (2018)\n24. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition. In: 3rd International Conference on Learning Representations\n(ICLR), San Diego (2015)\n25. Tolstikhin, I.O., et al.: Mlp-mixer: An all-mlp architecture for vision. CoRR\nabs/2105.01601 (2021)\n26. Vaswani, A., et al.: Attention is all you need. In: 31st Annual Conference on Neural\nInformation Processing Systems, Long Beach, pp. 5998–6008 (2017)\n27. Wu, B., et al.: Visual transformers: token-based image representation and process-\ning for computer vision. CoRR abs/2006.03677 (2020)\n28. Zagoruyko, S., Komodakis, N.: Paying more attention to attention: improving the\nperformance of convolutional neural networks via attention transfer. In: 5th Inter-\nnational Conference on Learning Representations, Toulon. OpenReview.net (2017)\n29. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shuﬄenet: an extremely eﬃcient convolu-\ntional neural network for mobile devices. In: 6th IEEE Conference on Computer\nVision and Pattern Recognition, Salt Lake City, pp. 6848–6856. IEEE ComputerSociety (2018)\n30. Zhong, G., Liu, W., Yao, H., Li, T., Sun, J., Liu, X.: Merging similar neurons for\ndeep networks compression. Cogn. Comput. 12(3), 577–588 (2020)', 'Attention Guided Spatio-Temporal\nArtifacts Extraction for Deepfake\nDetection\nZhibing Wang1,2,X i nL i1,2, Rongrong Ni1,2(B), and Yao Zhao1,2\n1Institute of Information Science, Beijing Jiaotong University, Beijing 100044, China\n{19120311,rrni }@bjtu.edu.cn\n2Beijing Key Laboratory of Advanced Information Science and Network Technology,\nBeijing 100044, China\nAbstract. Recently, deep-learning based model has been widely used\nfor deepfake video detection due to its eﬀectiveness in artifacts extrac-\ntion. Most of the existing deep-learning detection methods with the\nattention mechanism attach more importance to the information inthe spatial domain. However, the discrepancy of diﬀerent frames is\nalso important and should pay diﬀerent levels of attention to tempo-\nral regions. To address this problem, this paper proposes an Atten-tion Guided LSTM Network (AGLNet), which takes into consideration\nthe mutual correlations in both temporal and spatial domains to eﬀec-\ntively capture the artifacts in deepfake videos. In particular, sequential\nfeature maps extracted from convolution and fully-connected layers of\nthe convolutional neural network are receptively fed into the attentionguided LSTM module to learn soft spatio-temporal assignment weights,\nwhich help aggregate not only detailed spatial information but also\ntemporal information from consecutive video frames. Experiments onFaceForensics++ and Celeb-DF datasets demonstrate the superiority of\nthe proposed AGLNet model in exploring the spatio-temporal artifacts\nextraction.\nKeywords: Spatio-temporal artifacts\n·Attention ·Deepfake detection\n1 Introduction\nAdvances in deep learning technology have made it increasingly easier for gener-\native models to synthesize compelling face forgery videos which are even indis-tinguishable for human eyes. These forgery videos are likely to be abused by\nmalicious users to cause severe societal problems or political threats [ 10]. The\npotential threats may include pornographic videos of a victim whose face is syn-thesized, a chief executive oﬃcer who comments on his company’s performance\nThis work was supported in part by the National Key Research and Development of\nChina (2018YFC0807306), National NSF of China (U1936212), Beijing Fund-Municipal\nEducation Commission Joint Project (KZ202010015023).\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 374–386, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_31', 'Attention Guided Spatio-Temporal Artifacts Extraction 375\nthat may exert certain inﬂuence on the stock market, or even realistically look-\ning videos of state leaders who seem to make inﬂammatory comments they havenever actually made. Therefore, the ability of detecting whether a face has been\nmanipulated in a video sequence is crucial to social stability.\nWith the increasing concerns over face forgery videos, there has been a\nsurge of interest in developing deepfake detection methods with signiﬁcant\nprogress in the past two years. Generally speaking, the existing work can be\ndivided into hand-crafted feature based method [ 7,12,22] and deep learning\nbased method [ 1,13–15,18]. The hand-crafted feature based method mainly\nfocuses on the visual artifacts in forgery videos. Most of these methods detect\nface forgery videos depending on prior knowledge of face manipulation methods.Although they have achieved great results in speciﬁc dataset [ 14], they are not\noptimal due to the visual artifacts which are easily removed with the develop-\nment of generative technology. The deep learning methods which directly treat\ndeepfake detection tasks as a binary classiﬁcation can automatically learn fea-\ntures from a huge number of labeled data, and perform better than traditionalapproaches relying on manually-designed features. Early deep learning based\nmethods mainly focused on spatial information. Some methods [ 4,24,25]i n t r o -\nduce attention mechanism, but they mainly focus on the more important regionin the spatial domain, lacking the attention in the temporal domain.\nIn recent years, recurrent neural network (RNN) based approaches have\nattracted tremendous attention. Many variants of RNN are widely employed indeepfake detection, such as Long Short-Term Memory (LSTM) [ 5], and convo-\nlutional LSTM (Conv-LSTM) [ 21]. The RNN like LSTM which exploits possible\ninter-frame dissimilarities is eﬀective for processing face manipulation becausethe forgery videos are generated frame by frame. Despite the promising progress,\nthe existing methods still have a lot of room for improvement. In previous\nresearch, the knowledge embedded in diﬀerent-level layers of CNN was not fullyexploited in face manipulation detection. What’s more, the existing LSTM based\nsolutions lack learning capacity of discriminative spatio-temporal feature repre-\nsentation without explicitly extracting the most informative information in spa-\ntial and temporal dimensions of the videos, for they usually learn the temporal\nfeatures by equally treating the consecutive frames. However, in practice, dif-ferent frames might convey quite diﬀerent information for deepfake detection.\nSimilarly, along with the spatial dimension, the diﬀerences between the visual\ninformation from diﬀerent position are usually undistinguished in the existingLSTM solutions.\nIn this paper, we propose an Attention Guided LSTM Network (AGLNet)\nfor deepfake video detection. Our AGLNet model is constructed by face featureextraction module and the attention guided LSTM module. The face feature\nextraction module is based on pretrained EﬃcienNet-B4 network, which extracts\nhigh-level features from the fully-connected layer and spatial features from themiddle-level convolution layer. The extractd feature maps are respectively fed\ninto the fully-connected LSTM (FC-LSTM) and the convolutional LSTM (Conv-\nLSTM) learn the spatio-temporal information. Afterwards, the temporal atten-', '376 Z. Wang et al.\ntion module and spatio-temporal attention module are designed to focus on the\ncritical artifacts information in videos. For the high-level fully connected fea-ture with semantic information, our temporal attention module can adaptively\nlearn frame-level attention features at each step of FC-LSTM. For the middle-\nlevel convolution features with detailed artifacts, our spatio-temporal attentionmodule can explicitly allocate content and temporal dependent attention to the\noutput of each deep feature in the video sequence. Finally, we concatenate the\noutputs from two attention module and feed them to the fully connected layerfor binary classiﬁcation. A large-scale evaluation is conducted across two large\ndatasets which consist of thousands of real and deepfake videos. Its results show\nthat our approach is highly eﬀective in detecting deepfake videos.\n2 Related Work\nHand-Crafted Feature-Based Approach. In traditional approaches of deep-\nfake detection, suitable hand-crafted features play a critical role in training a\nhigh-performance classiﬁer. Li et al. [ 7] proposed a method based on abnormal\neye-blinking to identify deepfake videos which usually do not have closed eyes.\nThe work of [ 22] found that the creation of deepfake videos leads to inconsisten-\ncies between head poses estimated with all facial landmarks and those estimatedwith the central region. In [ 12], some visual artifacts such as the defects of reﬂec-\ntion details near eyes, and the imprecise geometry of both nose and teeth were\nexploited to determine whether a video was a deepfake video. [ 6] presented the\nFace X-ray to detect the trace of manipulation around the boundary regions of\nfake faces. Li et al. [ 8] proposed a detection method based on post processing\nartifacts from the generation process. The Face Warping Artifacts (FWA) wasemployed to capture the clues in generated videos. However, when there exist no\nobvious speciﬁc artifacts in deepfake videos with the development of generative\ntechnologies, these methods might be invalidated.\nDeep Learning-Based Approach. In recent years, with the development of\ndeep learning, many deep learning networks have been proposed and applied fordeepfake detection. These deep learning based methods usually be categorized\ninto two groups: methods that employ inter-frame features and those that explore\nintra-frame features. The methods based on inter-frame features mainly explorevisual artifacts within single frames to obtain discriminant features. Afchar\net al. [ 1] proposed two detection methods which focus on the mesoscopic fea-\ntures, including Meso-4 and MesoInception-4. In work of [ 14], XceptionNet was\nintroduced for deepfake detection, which could extract eﬀective features for this\ntask. Nguyen et al. [ 13] proposed a model which leverages capsule network to\ndetect fabricated images and videos. Unlike the approaches based on inter-frame\nfeatures, the methods based on intra-frame features capture high-level features\nfrom the temporal domain by using deep neural network. Sabir et al. [ 15]p r e -\nsented a detection approach which extracts features from CNN and passes the\nfeatures to Recurrent Neural Network to capture the temporal information. Tariq', 'Attention Guided Spatio-Temporal Artifacts Extraction 377\net al. [ 18] proposed a Convolutional LSTM based Residual Network (CLRNet)\nmodel using Convolutional LSTM cells, which can capture the spatio-temporalinformation directly from an input image sequence. Masi et al. [ 11]p r e s e n t e d\na method for deepfake detection based on a two-branch network structure that\nisolates digitally manipulated faces by learning to amplify artifacts while sup-pressing the high-level face content.\nAttention Mechanism-Based Approach. To explore manipulated regions of\ndeepfake detection in frames, Dang et al. [ 4] introduced an attention mech-\nanism to process and improve the feature maps for the classiﬁcation task.\nZi et al. [ 25] proposed to exploit a CNN structure together with an attention\nmask to better diﬀerentiate between the realand the fake faces. In work of [ 24],\na multi-attentional network for deepfake detection was introduced to pay atten-\ntion to diﬀerent local regions in images. Chen et al. [ 3] proposed a method based\non spatial and frequency domain combination and attention mechanism. How-\never, all of these attention models above are mainly based on spatial domain,\nwhich didn’t utilize the temporal information. According to this, we proposean attention guided LSTM network for deepfake video detection. It is beneﬁcial\nfor artifacts extraction to put more eﬀorts to develop spatio-temporal atten-\ntion model which can learn diﬀerent focusing weights for diﬀerent frames in thetemporal dimension and diﬀerent focusing weights for diﬀerent locations in the\nspatial dimension.\n3 The Proposed Method\nTo detect deepfake videos, we present an Attention Guided LSTM Network. Our\nproposed model as shown in Fig. 1consists of the face feature extraction module\nand the attention guided LSTM module and classiﬁcation.\n3.1 Face Feature Extraction Module\nIn our proposed method, we utilize EﬃcienNet-B4 network [ 17] pretrained on\nImageNet as a feature extraction network. The EﬃcienNet-B4 network is a con-\nventional convolutional neural network using 7 stages of the convolutional andidentity blocks to classify the image into 1000 classes for image classiﬁcation.\nWe modify the pretrained Eﬃcient-B4 network by eliminating the last layer and\ninserting the new fully-connected layer with 1024 dimensions for face featureextraction. Inspired by the complementarity of feature representation in diﬀer-\nent CNN layers, each frame is fed into EﬃcienNet-B4 to extract the high-level\nfully-connected features and middle-level convolutional features from the lastfully-connected layer and convolutional layer in the ﬁfth stage, respectively. The\nextracted features will be used as the input of the subsequent Attention Guided\nLSTM Module.', '378 Z. Wang et al.\nFig. 1. An overall architecture of the Attention Guided LSTM Network. The eﬀective\nCNN is employed to extract features in videos, and followed by the attention guided\nLSTM module with temporal attention module (TAM) and spatio-temporal attention\nmodule (STAM), where/circlemultiplytextdenotes matrix multiplication, and/circleplustextdenotes the matrix\nsum. GAP and AVE respectively denote global average pooling and average operation.\n3.2 Attention Guided LSTM Module\nThanks to the memory mechanism and forgetting mechanism, LSTM can weigh\nthe contribution of current and previous observations, and automatically update\nitself to determine the amount of information needed to be forgotten and remem-bered at each time step. Therefore, LSTM is capable of acquiring the discrimi-\nnative information and learning the dependency in sequential features. Because\nthe features from diﬀerent layers have diﬀerent dimensions, we introduce convo-lutional LSTM and fully-connected LSTM to model spatio-temporal information\nfor the extracted features. Due to the small convolution ﬁlter and the weak atten-\ntion ability of LSTM, the convolutions can only rely on the local information', 'Attention Guided Spatio-Temporal Artifacts Extraction 379\ncontained in the small receptive ﬁeld in the spatial domain and the short con-\nsecutive frame sequence in the temporal domain. This leads to the severe loss ofthe spatio-temporal correlations without considering the global information in\nthe whole feature maps and the whole frame sequence. To address this problem,\nwe introduce an attention mechanism for our spatio-temporal model, which cangenerate more discriminative feature maps by transforming the original feature\nmaps along the temporal and spatial dimension.\nTemporal Attention Module. We ﬁrst elaborate on the attention mecha-\nnism along the temporal dimension and show an eﬃcient capture of the long-\nrange temporal dependencies across multiple frames. Speciﬁcally, inspired byself-attention mechanism [ 19], the relationship between any two points in the\ninput feature maps proves to be helpful to convey the long-range information.\nTherefore, we design a temporal attention module (TAM), which can computethe attention response at each position in the input feature maps by taking\nall other positions and their weighted average into consideration. Formally, we\nre-denote the feature maps x\ntasxt=[x1,x2,···,xT]∈RT×K, which are con-\nvolved with kernel WθandWφto generate attention features,\nθt(xt)=Wθ∗xt,φt(xt)=Wφ∗xt, (1)\nwhere ∗,WθandWφrespectively denote convolution operation and the weight\nmatrices with K/2 channels. Then the attention map can be calculated as:\nσt(s)=exp(s)/summationtextC\niexp(s),s=θt(xt)Tφt(xt) (2)\nwhere σt(s)∈RK/2×K/2is the attention map, which indicates the weights of all\npositions in the attention features. We deﬁne the temporal attention module as:\nFt=ht(σt(xt)gt(xt)) + xt, (3)\ngt(xt)=Wg∗xt,ht(xt)=Wh∗xt, (4)\nwhere shape of FtisT×K,Wgis a 1D trainable kernel with the same size as\nWφ.Whare the weight matrices with K channels. σ(s) has the form of softmax\nfunction. Thus the formulation of temporal attention module can be rewrittenas:\nF\nt=ht(softmax (θt(xt)Tφt(xt))gt(xt)+xt. (5)\nThen we use average operation for Ftto get the ﬁnal temporal feature Gt,\nGt∈R1×K.\nSpatio-Temporal Attention Module. We design a spatio-temporal attention\nmodule (STAM) according to the temporal and spatial characteristics of the\nfeatures from Conv-LSTM. Instead of directly employing two similar attention\nmechanisms which capture temporal and spatial attention separately, we focusour attention simultaneously on a single module, which learns the attention', '380 Z. Wang et al.\nweights adaptively in spatial and temporal domain. Similarly, we can also give\nthe deﬁnition of the spatio-temporal attention module:\nFs=hs(softmax (θs(xs)Tφs(xs))gs(xs)+xs,Fs∈RT×K×H×W,(6)\nwhere the input feature maps are denoted as: xs=[x1,x2,···,xT]∈\nRT×K×H×W,θs,gsandφsare 1×1×1 3D convolution operations to reduce the\nnumber of input channel to K/2 for eﬃcient computation, hsis 3D convolution\noperation with K channels. To better aggregate the key informative locations of\nfeature map, a new feature vector Hs∈RT×Kis simply produced by generated\nby dealing with the feature maps of each frame through global average poolingoperation. This feature vector will be feed into the temporal attention module\nto obtain the spatio-temporal feature G\ns,Gs∈R1×K.\n3.3 Classﬁcation\nTo integrate the temporal, spatial and semantic information of videos, we con-\ncatenate the outputs from TAM and STAM and feed them to the fully connected\nlayer for binary classiﬁcation. In the test stage, the ﬁnal prediction of the networkwill merge the outputs of the two branches as follows:\nG=concat {G\ns,Gt}, (7)\nFor the deepfake detection classiﬁcation, we use the cross-entropy loss as follows:\nL=−1\nN/summationdisplay\ni=1[yi·log(pi)+( 1 −yi)·log(1−pi)], (8)\nwhere yiis the one-hot vector of the ground-truth of video, piis the predicted\nprobability vector.\n4 Experiments\nIn this section, we ﬁrst introduce the datasets used in the experiments. Then\nwe describe our experimental setup and some implementation details. Next, We\npresent the experimental results and compare them with those of other state-of-the-art methods.\n4.1 Datasets\nWe conduct experiments on two datasets: FaceForensics++ [ 14], Celeb-DF [ 9].\nFaceForensics++ dataset consists of 1000 real videos and four sub-databases that\nproduce 1000 face forgery videos via diﬀerent methods respectively, i.e. DeepFake\n(DF), FaceSwap (FS), Face2Face (F2F) and NeuralTexture (NT). Since eachsub-database contains 1,000 videos, we split it into a training and a test set,\nwhich consist of 950 and 50 videos respectively. Celeb-DF [ 9] dataset is one\nof the largest deepfake video datasets, which is composed of more than 5,000manipulated videos taken from celebrities. In this experiment, we use the train\na dataset which consists of 1000 fake videos and 500 real videos. For test dataset,\nwe use 50 fake videos and 50 real videos.', 'Attention Guided Spatio-Temporal Artifacts Extraction 381\n4.2 Setup and Implementation Details\nIn our experiment, ﬁrst of all, we extracted 9 samples which contains ten con-\nsecutive frames for each video. Then, we used multi-task CNN (MTCNN) [ 23]\nto crop the face from the frame. All the frames are resized to a 224 ×224 res-\nolution. The convolutional features from the ﬁfth stage of the EﬃcienNet-B4network are extracted as the middle level representations. The fully connected\nfeatures are extracted from the last fully connected layer as high level represen-\ntations. For each frame, the output size of the feature maps is 160 ×14×14 and\n1×1024 respectively. In Conv LSTM, the convolutional kernels for input-to-state\nand state-to-state transitions are of size 3 ×3 with 512 channels. In addition,\nthe two-layer FC-LSTM structure is employed and has the hidden size of 1024.The network is trained using Stochastic Gradient Descent (SGD) algorithm. The\ntraining epochs are set to 20. The batch size is set to 8. The weight decay is set to\n0.0001. The initial learning rate is set to 0.01. After every 3 epochs on the train-ing set, the learning rate decays to a tenth of original. We choose the one which\nhas best performance on the test set. The model is implemented with PyTorch\nframework using two GeForce RTX 2080Ti GPUs. We evaluate the performanceof the methods in terms of their accuracy: the higher the accuracy is, the better\nthe performance is.\nTable 1. Quantitative results (Acc (%)) on Celeb-DF dataset and FaceForensics++\ndataset with four diﬀerent manipulation methods, i.e. DeepFake (DF), Face2Face(F2F), FaceSwap (FS), NeuralTextures (NT).\nMethods DF F2F FS NT Celeb-DF\nAfchar et al. [ 1]82.67 80.22 78.78 66.00 53.78\nRossler et al. [ 14]95.56 93.89 94.22 84.89 86.56\nNguyen et al. [ 13]94.11 96.44 97.67 90.22 83.33\nLi and Lyu [ 8] 96.78 97.22 96.33 91.67 69.89\nWang et al. [ 20]95.89 93.56 94.78 85.11 95.44\nBonettini et al. [ 2]98.33 97.44 98.67 93.56 96.89\nOurs 99.44 99.33 99.78 94.44 98.78\n4.3 Comparison with State-of-the-Art Methods\nWe compare our AGLNet with recent deep learning based detection methods,\nincluding XceptionNet [ 14], FWA [ 8], I3D [ 20] and the other state-of-the-art\nmethods [ 1,2,13] in Table 1. Firstly, to evaluate the proposed AGLNet’s ability\nto capture defects introduced by diﬀerent manipulation methods, the model is\ntrained and tested on diﬀerent manipulation methods in FaceForensics++ [ 14].\nThe experimental results indicate that the proposed method achieves state-of-\nthe-art performance, which demonstrates that the AGLNet is capable of captur-\ning various kinds of defects introduced by diﬀerent manipulation methods. Then', '382 Z. Wang et al.\nthe Celeb-DF [ 9] dataset is also tested. As the results shown in Table 1, although\nthe Celeb-DF dataset is very realistic, the proposed model can eﬀectively capturethe defects. The results show that our best result outperforms many methods,\nwhich indicates the importance of attention mechanism and demonstrates the\neﬀectiveness of AGLNet. Meanwhile, we can ﬁnd that there is a performance gapbetween our method and the state-of-the-arts. We conjecture that there are two\nreasons for this phenomenon. First, the combination of feature maps in diﬀerent\nCNN layers is useful, which can enrich the video representation to help artifactextraction. Second, temporal attention module and spatio-temporal attention\nmodule play an important role in our model, which can guide LSTM to learn\nmore important area. Diﬀerent from the methods [ 2], our attention mechanism\ncan exploit both long-range temporal dependencies across multiple frames and\nlong-distance spatial dependencies within each frame, thus enabling the extract-\ning of the discriminative global information at both inter-frame and intra-frame\nlevels.\nTable 2. Componet study of Attention Guided LSTM Network (AGLNet) on Celeb-\nDF dataset. TAM and STAM denote temporal attention module and spatio-temporal\nattention module, respectively.\nXceptionNet EﬃcienNet Conv-LSTM FC-LSTM TAM STAM Acc (%)\n√86.56\n√89.67\n√ √96.44\n√ √96.78\n√ √ √97.56\n√ √ √97.44\n√ √ √ √ √98.78\n4.4 Ablation Study\nComponent Analysis. As shown in Table 2, the eﬀectiveness of each part of\nour proposed framework is evaluated. To give a more comprehensive compari-\nson, a variety of models are implemented. First, we evaluate the performance ofthe XceptionNet and EﬃcienNet-B4 to choose better backbone. Besides, to ver-\nify whether diﬀerent level features and cooperation of spatio-temporal domains\ncan improve the performance, we separately train FC-LSTM and Conv-LSTM,which respectively handle the fully-connected features and middle-level convo-\nlution features. The fully-connected features contain global information and the\nmiddle-level features contain more detail information. From the experimentalresults listed in Table 2, we can see that our model achieves better performance\nthrough combining diﬀerent level information. Moreover, the attention mod-\nules play an important role in our approach, which are designed to integrate', 'Attention Guided Spatio-Temporal Artifacts Extraction 383\nspatio-temporal information for diﬀerent-level features. The ablation studies ver-\nify the performance of separate attention modules in AGLNet. We can concludethat the AGLNet model exhibits higher accuracy than those without any atten-\ntion modules, which can guide the model to ﬁnd key information for better\nclassiﬁcation.\nFig. 2. Grad-CAM results: Ten frames are overlayed with the important attention\nregions highlighted using Grad-CAM. [ 16].\nTable 3. Results on using diﬀerent video clip length.\n5 10 15\nAcc (%) 97.44 98.78 97.78\nInput Video Clip Length. The AGLNet model works over an input video clip\nto capture the spatio-temporal attentions. Therefore, it is important to see the\nconnections between the input video clip length and the performance of AGLNet\nmodel. In Table 3we compare the performance using diﬀerent lengths including\n5, 10 and 15 respectively on the Celeb-DF dataset. It is obvious that with a\nsmall input length, our AGLNet model can provide much worse performance on\nthe Celeb-DF dataset. This is because the short input frame sequence conveysless spatio-temporal information. Besides, the experiment result shows the long\ninput length also reduces the performance of model, which is mainly because\nLSTM models are less eﬀective in learning high-level video representations or\ncapturing long-term relations.', '384 Z. Wang et al.\n4.5 Visualization\nBesides the above mentioned quantitative and qualitative evaluations, the gener-\nated attention maps at each prediction step are also visualized in Fig. 2. Because\nfully-connected feature is not convenient to mapped to the corresponding frames\nfor temporal attention visualization, we only illustrate the attention maps com-\nputed by spatio-temporal attention module. These illustrated examples revealtremendous eﬀect of our spatio-temporal attention. As shown in these exam-\nples, Fig. 2visualizes features characterized by spatial attention module, spatio-\ntemporal attention module on the Celeb-DF dataset. It can be observed thatEﬃcienNet with spatial attention module [ 2] focuses more on boundary of face\nregion. Compared spatial attention module, our proposed AGLNet with spatio-\ntemporal attention module better characterizes face artifacts regions by repre-senting feature maps that cover the multi frame.\n5 Conclusion\nIn this paper, we propose a novel Attention Guided LSTM Network for deep-\nfake detection, which attempts to exploit the discriminative information at both\nspatial domain and temporal domain. The key advantage of our architecture isthe attention mechanism. We present an attention guided LSTM module with\nthe temporal attention module and the spatio-temporal attention module, which\nimproves the artifacts extraction based on diﬀerent layers of the feature extrac-tion networks. The former aims to pay more attention to the high-level fea-\ntures with semantic information, while the latter can adaptively distinguish key\ninformation for middle-level convolution features with detailed artifacts. Our\nAGLNet simultaneously distinguishes the characteristics in temporal and spa-\ntial dimensions, and further improves the capability of the LSTM with morepowerful artifacts extraction. The experiments on several state-of-the-art meth-\nods and two diﬀerent datasets have demonstrated that our AGLNet can obtain\nthe state-of-the-art performance for deepfake detection.\nReferences\n1. Afchar, D., Nozick, V., Yamagishi, J., Echizen, I.: MesoNet: a compact facial video\nforgery detection network. In: 2018 IEEE International Workshop on InformationForensics and Security (WIFS), pp. 1–7. IEEE (2018)\n2. Bonettini, N., Cannas, E.D., Mandelli, S., Bondi, L., Bestagini, P., Tubaro, S.:\nVideo face manipulation detection through ensemble of CNNs. In: 2020 25th Inter-\nnational Conference on Pattern Recognition (ICPR), pp. 5012–5019. IEEE (2021)\n3. Chen, Z., Yang, H.: Manipulated face detector: joint spatial and frequency domain\nattention network. arXiv e-prints, arXiv-2005 (2020)\n4. Dang, H., Liu, F., Stehouwer, J., Liu, X., Jain, A.K.: On the detection of digital\nface manipulation. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 5781–5790 (2020)', 'Attention Guided Spatio-Temporal Artifacts Extraction 385\n5. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),\n1735–1780 (1997)\n6. Li, L., et al.: Face X-ray for more general face forgery detection. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n5001–5010 (2020)\n7. Li, Y., Chang, M.C., Lyu, S.: In ictu oculi: exposing AI created fake videos by\ndetecting eye blinking. In: 2018 IEEE International Workshop on Information\nForensics and Security (WIFS), pp. 1–7. IEEE (2018)\n8. Li, Y., Lyu, S.: Exposing deepfake videos by detecting face warping artifacts. arXiv\npreprint arXiv:1811.00656 (2018)\n9. Li, Y., Yang, X., Sun, P., Qi, H., Lyu, S.: Celeb-DF: a large-scale challenging\ndataset for deepfake forensics. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 3207–3216 (2020)\n10. Lyu, S.: DeepFake detection: current challenges and next steps. In: 2020 IEEE\nInternational Conference on Multimedia & Expo Workshops (ICMEW), pp. 1–6.IEEE (2020)\n11. Masi, I., Killekar, A., Mascarenhas, R.M., Gurudatt, S.P., AbdAlmageed, W.: Two-\nbranch recurrent network for isolating deepfakes in videos. In: Vedaldi, A., Bischof,H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12352, pp. 667–684.\nSpringer, Cham (2020). https://doi.org/10.1007/978-3-030-58571-6\n39\n12. Matern, F., Riess, C., Stamminger, M.: Exploiting visual artifacts to expose deep-\nfakes and face manipulations. In: 2019 IEEE Winter Applications of Computer\nVision Workshops (WACVW), pp. 83–92. IEEE (2019)\n13. Nguyen, H.H., Yamagishi, J., Echizen, I.: Capsule-forensics: using capsule networks\nto detect forged images and videos. In: ICASSP 2019–2019 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2307–2311.\nIEEE (2019)\n14. Rossler, A., Cozzolino, D., Verdoliva, L., Riess, C., Thies, J., Nießner, M.: Face-\nForensics++: learning to detect manipulated facial images. In: Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 1–11 (2019)\n15. Sabir, E., Cheng, J., Jaiswal, A., AbdAlmageed, W., Masi, I., Natarajan, P.: Recur-\nrent convolutional strategies for face manipulation detection in videos. Interfaces\n(GUI) 3(1), 80–87 (2019)\n16. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-\nCAM: visual explanations from deep networks via gradient-based localization. In:\nProceedings of the IEEE International Conference on Computer Vision, pp. 618–626 (2017)\n17. Tan, M., Le, Q.: EﬃcientNet: rethinking model scaling for convolutional neural net-\nworks. In: International Conference on Machine Learning, pp. 6105–6114. PMLR\n(2019)\n18. Tariq, S., Lee, S., Woo, S.S.: A convolutional LSTM based residual network for\ndeepfake video detection. arXiv preprint arXiv:2009.07480 (2020)\n19. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npp. 7794–7803 (2018)\n20. Wang, Y., Dantcheva, A.: A video is worth more than 1000 lies. Comparing 3DCNN\napproaches for detecting deepfakes. In: 2020 15th IEEE International Conference\non Automatic Face and Gesture Recognition (FG 2020), pp. 515–519. IEEE (2020)\n21. Xingjian, S., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.C.: Convolu-\ntional LSTM network: a machine learning approach for precipitation nowcasting.\nIn: Advances in Neural Information Processing Systems, pp. 802–810 (2015)', '386 Z. Wang et al.\n22. Yang, X., Li, Y., Lyu, S.: Exposing deep fakes using inconsistent head poses. In:\nICASSP 2019–2019 IEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), pp. 8261–8265. IEEE (2019)\n23. Zhang, K., Zhang, Z., Li, Z., Qiao, Y.: Joint face detection and alignment using\nmultitask cascaded convolutional networks. IEEE Sig. Process. Lett. 23(10), 1499–\n1503 (2016)\n24. Zhao, H., Zhou, W., Chen, D., Wei, T., Zhang, W., Yu, N.: Multi-attentional\ndeepfake detection. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 2185–2194 (2021)\n25. Zi, B., Chang, M., Chen, J., Ma, X., Jiang, Y.G.: WildDeepfake: a challenging\nreal-world dataset for deepfake detection. In: Proceedings of the 28th ACM Inter-national Conference on Multimedia, pp. 2382–2390 (2020)', 'Learn the Approximation Distribution of\nSparse Coding with Mixture Sparsity\nNetwork\nLi Li1, Xiao Long2,3, Liansheng Zhuang1,2(B), and Shafei Wang4\n1School of Data Science, USTC, Hefei, China\nlszhuang@ustc.edu.cn\n2School of Information Science and Technology, USTC, Hefei, China\n3Peng Cheng Laboratory, Shenzhen, China\n4Northern Institute of Electronic Equipment, Beijing, China\nAbstract. Sparse coding is typically solved by iterative optimization\ntechniques, such as the ISTA algorithm. To accelerate the estimation,neural networks are proposed to produce the best possible approxima-\ntion of the sparse codes by unfolding and learning weights of ISTA. How-\never, due to the uncertainty in the neural network, one can only obtain apossible approximation with ﬁxed computation cost and tolerable error.\nMoreover, since the problem of sparse coding is an inverse problem, the\noptimal possible approximation is often not unique. Inspired by theseinsights, we propose a novel framework called Learned ISTA with Mix-\nture Sparsity Network (LISTA-MSN) for sparse coding, which learns to\npredict the best possible approximation distribution conditioned on theinput data. By sampling from the predicted distribution, LISTA-MSN\ncan obtain a more precise approximation of sparse codes. Experiments\non synthetic data and real image data demonstrate the eﬀectiveness ofthe proposed method.\nKeywords: Sparse coding\n·Learned ISTA ·Mixture Sparsity Network\n1 Introduction\nSparse coding (SC) has shown great success in uncovering global information from\nnoisy and high dimensional data such as image super-resolution [ 17], image classi-\nﬁcation [ 15], and object recognition [ 13]. The main goal of SC is to ﬁnd the sparse\nrepresentation from over-complete dictionary. To solve this problem, classic meth-\nods are based on high dimensional optimization theory, such as proximal coordi-\nnate descent [ 9], Least Angle Regression [ 8] and proximal splitting methods [ 2].\nAmong these methods, Iterative Shrinkage-Thresholding Algorithm (ISTA) [ 5]i s\nthe most popular one, which belongs to proximal-gradient method. It has been\nproven that ISTA converges with rate 1 /t, where tis the number of the itera-\ntions/layers, and its computational complexity is too high. To address this issue,\nL. Li and X. Long—Contributed equally.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 387–398, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_32', '388 L. Li et al.\nBeck and Teboulle proposed the FISTA [ 3] algorithm converges more rapidly in\n2009. The major diﬀerence with ISTA is the introduction of a “momentum” term.The algorithms mentioned above belong to traditional iterative approaches.\nHowever, even though the FISTA accelerates the speed of convergence a lot,\nthese algorithms are still too slow for practical applications such as real-timeobject recognition.\nTo further increase the speed of ISTA, Gregor and LeCun proposed a train-\nable version of ISTA called Learned ISTA (LISTA) [ 11] by unfolding ISTA\nstructure into a recurrent neural network (RNN). Unlike the traditional iter-\native approaches, LISTA is highly computationally eﬃcient during the inference\nperiod. Once the learnable parameters of the neural network are trained, it canquickly estimate a solution of sparse codes by passing the input through a ﬁxed\nrecurrent neural network rather than solving a series of convex optimization\nproblems. Moreover, LISTA yields better estimation result than ISTA on new\nsamples for the same number of iterations/layers. This idea has led to a profu-\nsion of literature [ 6,18]. For one thing, they follow the idea of LISTA and modify\nthe structure to use more historical information. For another, some works change\nshared weights to layer-wise weights and get better performance. All these meth-\nods have achieved impressive performance in solving sparse coding.\nBut the existing neural network-based methods have suﬀered from the follow-\ning drawbacks. First, they ignore the fact that the LASSO problem is an inverse\nproblem and the optimal solution may be not unique. For example, in a superresolution task [ 12] a low-resolution image could be explained by many diﬀerent\nhigh-resolution images. In this case, the neural network designed to predict a\nspeciﬁc value is not eﬀective. Second, due to the uncertainty of deep learning, itis hard to get a unique accurate solution of sparse codes with ﬁxed iterations.\nIn fact, neural network-based methods (e.g., LISTA) can only obtain a possi-\nble approximation of sparse codes with ﬁxed computational cost and tolerableerror [ 11]. Therefore, the best possible approximation of sparse codes should sat-\nisfy some kind of distribution. We argue that, it is more reasonable to predict\nthe distributions of possible approximation of sparse codes instead of the speciﬁc\nvalue of best possible approximation.\nInspired by the above insights, this paper proposes a novel framework called\nLearned ISTA with Mixture Sparsity Network (LISTA-MSN) for sparse coding.\nThe key idea is to introduce a novel module called Mixture Sparsity Network\n(MSN) to predict the distribution of best possible approximation of sparse codes.Speciﬁcally, the proposed framework ﬁrst uses the popular LISTA network to\nobtain an initial estimate solution of sparse coding. The LISTA framework can\nbe replaced by any other neural networks such as ALISTA [ 14], Coupled-LISTA\n(LISTA-CP) [ 6], etc. Then, our framework introduces the Mixture Sparsity Net-\nwork to predict the distribution of the best possible approximations according\nto the initial estimate solution. At last, the proposed framework samples theﬁnal optimal results of sparse coding from the distribution. Note that, it is a\nchallenging problem to model the conditional probability density function of the\ntarget sparse vectors conditioned on the input vector. To address this problem,', 'Learn the Approximation Distribution of Sparse Coding 389\nMixture Sparsity Network adds an additional penalty to the popular framework\nof Mixture Density Network [ 4] so that it can ensure the sparsity of network’s\noutputs as we hope. Experiments on synthetic data and real image data have\nshown that the proposed LISTA-MSN framework can signiﬁcantly improve the\naccuracy and convergence speed of sparse coding.\nIn summary, our main contributions are as follows:\n– A novel framework called LISTA-MSN is proposed to learn fast approxima-\ntions of sparse codes. Diﬀerent from previous works (such as LISTA [ 11],\nSC2Net [ 18] and ALISTA [ 14]), the proposed framework learns to predict the\ndistribution of the best possible approximation of sparse codes instead of the\nbest possible approximation. To the best of our knowledge, this is the ﬁrstwork to learn a trainable neural network to predict the distribution of the\noptimal solution of sparse coding. Moreover, the proposed framework is very\nﬂexible, where the LISTA can be replaced with any other neural network for\nsparse coding.\n– Mixture Sparsity Network is proposed to model the conditional probability\ndensity function of the target sparse vectors conditioned on the input vector.\nIt ensures that the data sampling from the predicted distribution is sparse\nenough, which makes the ﬁnal results meaningful.\n2 The Proposed Method\nIn this section, we will introduce the LISTA-MSN framework. The architectureof LISTA-MSN is shown in Fig. 1. Speciﬁcally, it ﬁrst uses the LISTA network to\nobtain a coarse estimation of sparse codes. Then, the framework introduces the\nmixture sparsity network to predict the distribution of the best possible approx-\nimations according to the initial estimation. Finally, the framework samples theﬁnal optimal sparse codes from the distribution. Note that, the proposed method\nadds penalty to the mixture density network [ 4] to ensure the sparsity of results.\nFig. 1. The architecture of LISTA-MSN.\n2.1 LISTA Network\nGiven the input data X=[x1,x2,...,xN]∈Rn×N, sparse coding aims to learn\nthe over-complete dictionary D=[d1,d2,...,dm]∈Rn×mand the sparse repre-\nsentation Z=[z1,z2,...,zN]∈ Rm×N. And the goal of LISTA Network is to', '390 L. Li et al.\ncalculate the coarse estimation ˆZ=[ ˆz1,ˆz2,...,ˆzN]∈ Rm×N, where ˆzqis\nthe coarse estimation of sparse codes of xqwith limited iterations. Speciﬁcally,\nLISTA is:\nz(t+1)=hθ(Wex+Sz(t))t=0,...,K −1 (1)\nˆz=z(K),a n d Kis ﬁxed number of steps. θ=[θ1,...,θ m] is a trainable vector.\nThe variables We,Sandθare learned by the given training data. So, the LISTA\ncan be replaced by any other neural networks such as ALISTA [ 14], Coupled-\nLISTA (LISTA-CP) [ 6], etc.\n2.2 Mixture Sparsity Network\nHaving the coarse estimation ˆzgenerated from LISTA Network, in this stage,\nthe mixture sparsity network is proposed to model the conditional distribution\nof sparse codes zon the coarse estimation ˆzas the linear combination of kernel\nfunction:\nP(z|ˆz)=M/summationdisplay\ni=1αi(ˆz)φi(z|ˆz) (2)\nwhere Mis the number of kernel function, φiis the kernel function, αi(ˆz)i s\nthe mixture coeﬃcients, which can be regarded as prior probability of the sparsecodezbeing generated from i\nthkernel given the coarse estimation ˆ z. In practice,\nthe Gauss density function is often chosen as the kernel function:\nφi(z|ˆz)=exp\n(2π)m\n2m/producttext\nj=1σij(ˆz)⎧\n⎨\n⎩−m/summationdisplay\nj=1(zj−μij(ˆz))2\n2σij(ˆz)2⎫\n⎬\n⎭(3)\nwhere zjis the jthelement of target data z,μijandσijdenote the jthelement\nof mean and standard deviation of the ithkernel. A diagonal matrix is used\ninstead of the covariance matrix of Gauss density function from the concern of\ncomputational cost. The parameters Θ={α,μ,σ }are outputs of network which\ndepend on the coarse estimation ˆz. Since the ˆzis calculated by LISTA Network,\nthe distribution of the sparse code is conditioned on input data. A simple full-\nconnected network is constructed to generate the parameters Θ={α,μ,σ}like\nFig.1. Diﬀerent activation functions are applied to these parameters in order to\nsatisfy the restrictions. For the coeﬃcient α(ˆz),αi>0a n d/summationtextM\ni=1αi=1 ,w e\nuse the SoftMax function [ 10]. For the standard deviation of kernel σ(ˆz)w h i c h\nshould be positive, we choose a modiﬁed Elu function [ 7]:\nf(t)=/braceleftBigg\nt+1, ift≥0;\nγ[exp(t)−1] + 1 ,otherwise(4)\nwhere γis a scale parameter.\nNext, to ensure the sparsity of the ﬁnal result, it is noteworthy that the\nelement of μwhich corresponds to zero elements in sparse codes should be very', 'Learn the Approximation Distribution of Sparse Coding 391\nclose to zero when the network was trained. So, we use the coordinate-wise\nshrinking function h/epsilon1as sparsity restriction of μto keep it sparse:\nh/epsilon1(t)=sign(t)(|t|−/epsilon1)+ (5)\nHere, /epsilon1is a hyperparameter with the positive value which could be set to a\nsmall number e.g., 0.01. The shrinking function h/epsilon1could change the small\nnumber to zero. If the output of linear layer id denoted as o∈R(2m+1)M=\n{oα\ni,oμ\ni1,...,oμ\nim,oσ\ni1,...,oσ\nim}M\ni=1, the parameters Θare:\nαi=exp(oα\ni)/summationtextM\nj=1exp(oα\nj)μij=h/epsilon1(oμ\nij)σij=Elu(oσ\nij) (6)\nThe whole network is optimized by using the negative log-likelihood loss (NLL)\nof (4) and ( 5), the loss function is deﬁned as:\nE=−N/summationdisplay\nq=1Eq\nEq=−ln(M/summationdisplay\ni=1αi(ˆzq)φi(zq|ˆzq))(7)\nSo, the derivatives of Eqwith respect to output of linear layer oare calculated\nas follows:\nπi=αiφi/summationtextM\nj=1αjφj\n∂Eq\n∂oα\nj=αi−πi(8)\n∂Eq\n∂oσ\nij=πi/braceleftBigg\n1\nσij−(zj−μij)2\nσ3\nij/bracerightBigg\nf/prime(oσ\nij) (9)\n∂Eq\n∂oμ\nij=πi/braceleftBigg\n(μij−zj)\nσ2\nij/bracerightBigg\nδ(|oμ\nij|>λ) (10)\nwhere f/primedenotes the derivative of function f,δ(t) is deﬁned as follow:\nδ(t)=/braceleftBigg\n1,iftis true;\n0,otherwise(11)\nSo, the standard back-propagation is guaranteed, and the algorithm is shown in\nAlgorithm 1 and Algorithm 2.\nFinally, after getting the parameters Θ, we need to sample the ﬁnal results\nfrom the learned distribution. Luckily, we get the mixture of Guass density func-\ntion by which the sampling approach can be easily implemented. The location is\nobtained by randomly sampling according to αand the corresponding center is', '392 L. Li et al.\nthe result of sampling. Actually, what interested us more is the most likely value\nof the output. The most likely value for output vector is calculated by maximumthe conditional distribution, but this procedure might be computationally costly.\nA faster approximation is to use the center of kernel function which has highest\nprobability of being sampled, from ( 4) and ( 5):\nind= arg max\ni{αi} (12)\nThe corresponding center μindis the most likely output. And the result of sample\nwill be sparse as long as the h/epsilon1activation for μis used.\nAlgorithm 1. LISTA-MSN: fprop and bprop\nfprop (x,z,W e,S ,θ)\nVariable Z(t),C(t),B,Θare stored for bprop\nB=Wex;Z(0) = hθ(B)\nfort=1 to Kdo\nC(t)=B+SZ(t−1)\nZ(t)=hθ(C(t))\nend for\nˆz=Z(K)\no=Wmˆz\nαi=exp (ˆoα\ni)/summationtextM\nj=1exp (ˆoα\nj)μij=h/epsilon1(ˆoμ\nij)σij=f(ˆoσ\nij)\nΘ={α,μ,σ }\nbprop (z∗,x,W e,S ,W m,θ,δWe,δS,δWm,δθ)\nZ(t),C(t),BandΘwere stored in fprop\nInitialize: δB=0 ; δS=0 ; δθ=0\nδois calculated by (10) −(12)\nδWm(t)=δoˆzT;δˆz=WT\nmδo;δZ(K)=δˆz\nfort=Kdown to 1 do\nδC(t)=hθ/prime(C(t))⊙δZ(t))\nδθ=δθ−sign(C(t))⊙δC(t)\nδB=δB+δC(t)\nδS=δS+δC(t)Z(t−1)T\nδZ(t−1) =STδC(t)\nend for\nδB=δB+hθ/prime(B)⊙δZ(0)\nδθ=δθ−sign(B)⊙hθ(B)δZ(0)\nδWe=δBXT\n3 Experiment\nIn this section, we evaluate the performance of the proposed framework on syn-\nthetic data and real image data, and compare it with other state-of-the-art algo-rithms, including ISTA, FISTA, LISTA, ALISTA, and Coupled-LISTA. Similar\nto previous works [ 1,3,11], we adopt Prediction error and Function value error as', 'Learn the Approximation Distribution of Sparse Coding 393\nthe criteria. Prediction error is the squared error between the input data and pre-\ndicted codes in a certain dictionary. And cost function error is the cost betweencurrent prediction and best prediction. These criteria can evaluate the accuracy\nand convergence of the algorithm. During the experiments, we ﬁnd that all the\nevaluated methods perform stable when λranges between 0.1 and 0.9. So, the spar-\nsity parameter λ=0.5 is ﬁxed in all the experiments, which means the diﬀerence\nduring experiments is just the algorithms themselves. All the algorithms are imple-\nmented by python and pytorch, and run the experiments on a DGX-1v server.\n3.1 Synthetic Experiments\nFor synthetic case, the dictionary D∈R\nn×mof standard normal distribution\nis generated. Once Dis ﬁxed, a set of Gaussian i.i.d. samples {xi}N\ni=1∈Rnare\ndraw. In this case, we set m= 256, n= 64, Ntrain= 40000 and Ntest= 10000.\nIn this subsection, two experiments are carried out to verify the performance\nof the proposed method. First, we compare the LISTA-MSN with several tradi-\ntional algorithms by measuring their prediction error to verify whether LISTA-\nMSN outperforms the traditional iterative approaches. Figure 2shows the pre-\ndiction error of several algorithms for diﬀerent iterations or layers (For ISTA\nand FISTA, the number of iterations changes. For LISTA and LISTA-MSN, thenumber of layers changes.) From Fig. 2, we can observe that LISTA-MSN can\nalways achieve the lowest prediction error in diﬀerent depth compared with other\nalgorithms. Even a 10 layers LISTA-MSN can achieve the same error as the ﬁvetimes depth of LISTA or FISTA. Furthermore, in order to verify whether the\nproposed framework is eﬀective under diﬀerent estimation network structures.\nSo, we use the proposed framework for veriﬁcation under the two network struc-tures (ALISTA and Coupled-LISTA) respectively. Figure 3and Fig. 4show that\nfor diﬀerent estimation network structures, the proposed framework also outper-\nforms the original network structure. And it can decrease the prediction errornearly by 10% at the same network depth and structure.\nFig. 2. Evolution of prediction error in ISTA, FISTA, LISTA, LISTA-MSN on simu-\nlated data', '394 L. Li et al.\nFig. 3. Evolution of prediction error in Coupled-LISTA, Coupled-LISTA-MSN on sim-\nulated data\nFig. 4. Evolution of prediction error in ALISTA, ALISTA-MSN on simulated data\nSecond, we compare the convergence performance of the proposed method\nwith other algorithms by calculating the function value error. Figure 5and\nFig.6show that the proposed method converges faster than traditional iterative\napproaches (ISTA and FISTA) and reaches a lower overall cost than other meth-\nods. Besides, we can also ﬁnd that under the same network structure, using the\nproposed framework can improve the convergence speed of the original algorithm.', 'Learn the Approximation Distribution of Sparse Coding 395\nFig. 5. Evolution of the function value error in ISTA, FISTA, LISTA, LISTA-MSN on\nsimulated data\nFig. 6. Evolution of the function value error in four algorithms on simulated data\n3.2 Digit Data Experiments\nAbout real image data, we use the handwritten digits dataset from scikit-learn\n[Pedregosa et al., 2011]. The digits dataset contains 60,000 training images and10,000 test images, where each image size is 8 ×8 and sampled from digits (0\nto 9). We randomly sample m= 256 samples from dataset and normalize it to\ngenerate the dictionary D. Besides the above, all the image data is processed to\nremove its mean and normalize its variance.\nIn the digit dataset, we also compare several algorithms with the proposed\nmethod by measuring their prediction error. From Fig. 7,8and Fig. 9,w ec a n\nobserve that the results are very similar to the synthetic data. The proposed\nmethod outperforms the traditional iterative approaches. And under diﬀerent\nestimation network structures (LISTA, ALISTA and Coupled-LISTA) with theuse of our framework, the prediction error can also decrease. The interesting', '396 L. Li et al.\nFig. 7. Evolution of prediction error in ISTA, FISTA, LISTA, LISTA-MSN on digit\ndata\nFig. 8. Evolution of prediction error in Coupled-LISTA, Coupled-LISTA-MSN on digit\ndata\nthing observed is that all the algorithms perform better on synthetic dataset\nthan digit dataset. For this result, the dictionary D of the digit data has a muchricher correlation structure than the simulated Gaussian dictionary, which is\nknown to impair the performance of learned algorithms [ 16].\n3.3 Sparsity Analysis\nAs mentioned earlier, how to guarantee the sparsity of results from the learned\ndistribution is a very important thing. And the sparse regulation is put forwardto solve this problem. Therefore, we experimented to prove the importance of\nsparse regulation to MSN by comparing the sampling results’ sparsity of the', 'Learn the Approximation Distribution of Sparse Coding 397\nFig. 9. Evolution of prediction error in ALISTA, ALISTA-MSN on digit data\nproposed method with or without sparse regulation under the same estimation\nnetwork structure. We calculate the proportion of zero elements in the ﬁnal\nresults. Table 1shows that no matter how the dataset changes, under a certain\nestimation network structure with sparse regulation can always keep the sparsity\nof the results. This also guarantees that the ﬁnal results are meaningful.\nTable 1. The sparsity of the results whether use sparse regulation under a certain\nnetwork structure\nDatasets Whether usesparse regulation LISTA-MSN ALISTA-MSN Coupled-\nLISTA-MSN\nSynthetic data Without 35.67% 38.52% 40.28%\nWith 97.57% 94.31% 95.46%\nDigit data Without 34.30% 33.60% 37.59%\nWith 94.67% 93.69% 93.55%\n4 Conclusion\nThis paper proposes a novel framework called LISTA-MSN for sparse coding,\nwhich signiﬁcantly improves the accuracy of sparse coding. Diﬀerent from exist-\ning neural network-based methods, the proposed framework learns the approx-\nimation distribution of sparse codes, then obtains the ﬁnal optimal solution bysampling from the learned distribution. Furthermore, the proposed framework\nis very ﬂexible. Mixture sparsity network can be combined with various neural\nnetworks as estimation networks for sparse coding, including LISTA, ALISTA,and Coupled-LISTA. Experimental results show that the proposed framework\ncan signiﬁcantly improve the accuracy of sparse coding.', '398 L. Li et al.\nAcknowledgment. This work was supported in part to Dr. Liansheng Zhuang by\nNSFC under contract (U20B2070, No. 61976199), and in part to Dr. Houqiang Li by\nNSFC under contract No. 61836011.\nReferences\n1. Ablin, P., Moreau, T., Massias, M., Gramfort, A.: Learning step sizes for unfolded\nsparse coding. In: Advances in Neural Information Processing Systems, pp. 13100–\n13110 (2019)\n2. Bauschke, H.H., Combettes, P.L., et al.: Convex Analysis and Monotone Operator\nTheory in Hilbert Spaces, vol. 408. Springer, New York (2011). https://doi.org/\n10.1007/978-1-4419-9467-7\n3. Beck, A., Teboulle, M.: A fast iterative shrinkage-thresholding algorithm for linear\ninverse problems. SIAM J. Imaging Sci. 2(1), 183–202 (2009)\n4. Bishop, C.M.: Mixture density networks (1994)\n5. Blumensath, T., Davies, M.E.: Iterative thresholding for sparse approximations.\nJ. Fourier Anal. Appl. 14(5–6), 629–654 (2008). https://doi.org/10.1007/s00041-\n008-9035-z\n6. Chen, X., Liu, J., Wang, Z., Yin, W.: Theoretical linear convergence of unfolded\nISTA and its practical weights and thresholds. In: Advances in Neural InformationProcessing Systems, pp. 9061–9071 (2018)\n7. Clevert, D.A., Unterthiner, T., Hochreiter, S.: Fast and accurate deep network\nlearning by exponential linear units (ELUs) (2015)\n8. Efron, B., Hastie, T., Johnstone, I., Tibshirani, R., et al.: Least angle regression.\nAnn. Stat. 32(2), 407–499 (2004)\n9. Friedman, J., Hastie, T., H¨ oﬂing, H., Tibshirani, R., et al.: Pathwise coordinate\noptimization. Ann. Appl. Stat. 1(2), 302–332 (2007)\n10. Gold, S., Rangarajan, A., et al.: Softmax to softassign: neural network algorithms\nfor combinatorial optimization. J. Artif. Neural Netw. 2(4), 381–399 (1996)\n11. Gregor, K., LeCun, Y.: Learning fast approximations of sparse coding. In: Proceed-\nings of the 27th International Conference on International Conference on MachineLearning, pp. 399–406 (2010)\n12. Ledig, C., et al.: Photo-realistic single image super-resolution using a generative\nadversarial network. In: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), July 2017\n13. Lee, H., Ekanadham, C., Ng, A.Y.: Sparse deep belief net model for visual area\nV2. In: NIPS (2007)\n14. Liu, J., Chen, X., Wang, Z., Yin, W.: ALISTA: analytic weights are as good as\nlearned weights in LISTA. In: International Conference on Learning Representa-tions (2019)\n15. Mairal, J., Bach, F.R., Ponce, J., Sapiro, G., Zisserman, A.: Discriminative learned\ndictionaries for local image analysis. In: 2008 IEEE Conference on Computer Visionand Pattern Recognition, pp. 1–8 (2008)\n16. Moreau, T., Bruna, J.: Understanding trainable sparse coding via matrix factor-\nization. Stat 1050, 29 (2017)\n17. Yang, J., Wright, J., Huang, T.S., Ma, Y.: Image super-resolution via sparse rep-\nresentation. IEEE Trans. Image Process. 19(11), 2861–2873 (2010). https://doi.\norg/10.1109/TIP.2010.2050625\n18. Zhou, J.T., et al.: SC2Net: sparse LSTMs for sparse coding. In: Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence (2018)', 'Anti-occluded Person Re-identiﬁcation\nvia Pose Restoration and Dual Channel\nFeature Distance Measurement\nBin Wu , Keyang Cheng(B), Chunyun Meng , and Sai Liang\nSchool of Computer Science and Communication Engineering, Jiangsu University,\nZhenjiang 212013, China\nkycheng@ujs.edu.cn\nAbstract. In real scenes, persons are often blocked by obstacles. The\npurpose of occluded person re-identiﬁcation is to identify the occluded\npersons in the non-shared view camera. In this paper, we propose a newframework, anti-occluded person re-identiﬁcation model via pose restora-\ntion and dual channel feature extraction (PRAO). The network is divided\ninto two modules: person pose repair module (PPR) and dual channelfeature extraction module (DCFE). (1) In the person pose repair mod-\nule, the instance segmentation network is used to detect the occlusion\nin the person image, and then the pre-trained edge smoothing GAN (e-GAN) is used to repair the person image. (2) In the dual channel feature\nextraction module, we change the original single channel person predic-\ntion structure into a dual channel person matching structure, which canaccurately align the person features of the two channels, reduce the noise\ngenerated by image generation, and improve the identiﬁcation accuracy\nof persons in the case of occlusion. Finally, a large number of experi-ments on occluded and non-occluded datasets show the performance of\nthe method.\nKeywords: Person re-identiﬁcation\n·Feature expression ·Similarity\nmeasure\n1 Introduction\nWith the development of science and technology, person re-identiﬁcation is more\nand more widely used, person re-identiﬁcation is considered to be a sub-questionof image retrieval. The purpose of person re-identiﬁcation is to ﬁnd persons with\nthe same ID in camera without sharing vision. Recently, great progress has been\nmade in person re-identiﬁcation [ 1–4]. However, due to the challenges such as\nocclusion, person re-identiﬁcation has remained in the academic ﬁeld, and there\nis still a big gap from the practical application. As shown in Fig. 1, persons are\neasily blocked by obstacles, if the occluded features are also learned as person\nfeatures in the model, it may lead to wrong retrieval results.\nThe ﬁrst author is student.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 399–410, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_33', '400 B. Wu et al.\nFig. 1. Persons were occluded by diﬀerent things.\nRecently, some works [ 5,7] have tried to solve the problem of person occlusion,\nbut there are still some shortcomings in these works: (1) To ensure that theperson image in the gallery is always the holistic person, but it does not conform\nto the real scene; (2) In the case of inconsistent occlusion areas, it is hard to align\nthe person features that are not occluded; (3) It is diﬃcult to detect the human\nsemantic key-points, when person are occluded, the detection eﬀect of human\nsemantic key-points is still aﬀected, for example, the prediction of conﬁdenceand position coordinates is not accurate enough, as shown in Fig. 2.\nFig. 2. Existing problem of person re-identiﬁcation with occlusion.\nIn order to solve the existing problems. The proposed model is diﬀerent from\nthe traditional occlusion person re-identiﬁcation model. In this model, the pose\nrepair module (PPR) is used to reconstruct the occluded area in the occludedperson image, and the holistic person pose is generated as far as possible to\nimprove the performance of pose estimation. Then the dual channel feature\nextraction module (DCFE) is used to align the features. The contributions ofour model are as follows:\n(1) The person pose repair module is proposed to erase the occlusion in the per-\nson image and repair the overall person pose, so as to improve the perfor-\nmance of pose estimation and realize the overall person to overall matching.\n(2) A dual channel feature extraction module is proposed, which uses feature\ndistance measurement for feature matching to focus the network attentionon the whole person area and reduce the noise outside the person area due\nto image generation.\n(3) We have done a lot of experiments on occluded and whole Reid datasets,\nand the experimental results show that the modiﬁed model is better than\nthe existing methods.', 'Anti-occluded Person Re-identiﬁcation via Pose Restoration 401\n2 Related Works\n2.1 Traditional ReID\nAs a speciﬁc person retrieval problem, person re-identiﬁcation is widely studied\non non-overlapping cameras. The early work of person re-identiﬁcation is mainly\nto construct person features by hand [ 8] or metric learning methods [ 9,26]. With\nthe rise of deep learning, person re-identiﬁcation technology based on deep learn-\ning has achieved high accuracy [ 10–13]. However, there is still a big gap between\nthe research-oriented scene and the actual scene in people’s daily life. The keychallenge is that a large amount of noise caused by diﬀerent perspectives, pose,\nimage texture and occlusion hinders the network learning. Traditional person re-\nidentiﬁcation methods mainly focus on the matching of the overall person image,but can not deal with the occluded image well, so it limits the application in the\nactual monitoring scene.\n2.2 Occluded ReID\nOccluded person re-identiﬁcation methods aim to ﬁnd a given occluded per-\nson image from the camera video surveillance which does not share the ﬁeld ofview [ 14]. This task is more challenging because the person image contains the\nnoise information of occlusion or the occlusion area is inconsistent, which leads\nto the spatial alignment dislocation. Zhuo et al. [ 15] used occluded/non occluded\nbinary classiﬁcation loss to distinguish occluded images from the overall person\nimage, there is no fundamental solution to the occlusion problem. Miao et al. [ 5]\nproposed a pose guided feature alignment method to match the occluded imagesin query and gallery, However, this does not take into account the negative eﬀect\nof occlusion on pose estimation. On this basis, Wang et al. [ 7] proposed a method\nto distinguish features and robust alignment by learning high-order relations andtopological information. Zhang et al. [ 6] proposed a mask guided de-occlusion\nmethod to deal with the problem of person occlusion. This solves the problem\nof person being blocked to some extent, but brings in new noise.\n3 Proposed Method\nThis part mainly introduces the framework we proposed, including the personpose repair module (PPR) to remove the occlusion and repair the complete\nperson pose, and the dual channel feature extraction module (DCFE) to extract\nthe global and local features of person image. These two modules are trained inan end-to-end manner, and Fig. 3shows our overall model.\n3.1 Person Pose Repair\nIn this part, we propose a new person pose restoration method, which is used to\nremove the occlusion in the person image, repair the complete person pose, and', '402 B. Wu et al.\nQuery\n image\nGallery\n image\nFull-body \nimageFast-rcnn GANResnet \nlayer 1Resnet \nlayer 2Resnet \nlayer 3Resnet \nlayer 4Pose \nesƟmator\nPose \nesƟmatorLocal features\nLocal featuresGlobal features\nDistance \nmeasure\nFig. 3. An overview of the PRAO model.\nrealize the holistic image to the holistic image matching of the person image as\nfar as possible. This part can be divided into two steps: the ﬁrst step is to detect\nthe occlusion and add mask to it; In the second step, the proposed e-GAN is\nused to generate the complete person pose. Through the experiment, it is foundthat the traditional GAN network does not have good smoothness for the mask\nedge when generating complete person pose. In order to solve this problem, we\npropose an edge smooth GAN (e-GAN). We hope that the e-GAN network willpay more attention to the features of the edge when it is generated.\nThe ﬁrst step is to detect the occlusion. We assume that the target person\nis in the center of the image, while the main body including other persons isthe occlusion, which is eﬀective in most of the Reid public datasets. Firstly, the\noccluded person image is input into the instance segmentation model. Here, we\nretrain the instance segmentation model Fast-RCNN to detect occlusion. If theinstance segmentation network detects other subjects except the central target\nperson, we add mask to it.\nGeneratorDiscriminatorTrue/False\nRandom erase\nWeight matrix\nFig. 4. The training process of e-GAN.', 'Anti-occluded Person Re-identiﬁcation via Pose Restoration 403\nIn the second step of training e-GAN network, we randomly add mask to\nthe holistic person Reid dataset. Speciﬁcally, for a number of person images I\nrandomly select a rectangular area Imof size ( wm,h m), and use blank pixel value\nto cover. Suppose that the area of the image Iand the area ImareS=w×h\nandSm=wm×hm, respectively. To determine a unique rectangular region, we\ninitialize a point P=(xm,ym) at random, if xm+wm≤w,a n d ym+hm≤h,\nWe will select this rectangular area, otherwise we will repeat the above process\nuntil we ﬁnd a suitable rectangular area. After the image is randomly added tothe mask, the e-GAN network is pre-trained with the original image as the label.\nThe speciﬁc training process is shown in Fig. 4above.\nIn order to make the edge of person pose image smoother, we add random\nmask to person image and generate edge weight matrix W. The weight matrix\nat location ( i,j) is deﬁned as follows,\nW\n(i,j)=/braceleftbigg\n0.9µ;μ= min |i−wm|+|j−hm|\n0;∀(i,j)∈Sm(1)\nThe loss function of e-GAN network training is as follows:\nmin\nGmax\nDV(D,G)=Ex∼Pdata (x)[log(D(x))]+Em∼Pm(m)[log(1 −D(G(m)))] (2)\nAmong them, xis the real person image of the input network, mis the person\nmask image after random erasure, Pdata(x) is the real person data distribution,\nandPm(m) is the data distribution of the person mask image.\nIn the stage of repairing person pose image, we only keep the branches used to\ngenerate person image. Then the image with mask in the previous step is input\ninto the pre-trained e-GAN network, which outputs an image with completeperson pose image X. As shown in Fig. 5.\nFast-rcnn\nAdd maskPre-trained \ne-GAN\nFig. 5. Process of repairing complete person pose image.\n3.2 Dual Channel Feature Extraction\nThe purpose of this module is to extract the features of human semantic key-\npoints. Because the person pose repair module can repair the complete person\npose as much as possible, it is improved on the basis of the traditional occlusionperson re-identiﬁcation network. After a lot of experiments, we found that the\nperformance of the dual channel matching structure is better than that of the\nsingle channel prediction structure.', '404 B. Wu et al.\nResnet Pose \nes/g415mator\nPose \nes/g415matorDistance \nmeasureClassiﬁer Loss\nClassiﬁer LossDistance LossTriplet Loss\nTriplet Loss17 18\nʴ\nʴConcat\nConcat\nFig. 6. An overview of dual channel feature extraction module.\nThe structure diagram of the module is shown in Fig. 6. First, the prepro-\ncessed person image Xis given, we can get their feature Fg, and key-points\nheatmaps H, through ResNet-50 model and pose estimation model. The dimen-\nsion of key-points heatmaps is 24 ×8×17. Using matrix broadcast B(·)a n d\nglobal average pooling g(·), we can get human semantic local feature Flguided\nby key points, in which the dimension of human semantic local feature is\n24×8×17×2048.\nFl=g(B(G⊗H)) (3)\nAfter extracting the local features of human pose Fl, in order to enhance\nthe performance of the dual channel feature extraction module, we use feature\ndistance measurement method dto determine whether the two images in the\nmodule are the same kind of person. Among them, fis the fully connected\nlayer, Fl1andFl2are the features extracted after convolution of two channels.\nd=exp(f/bardblFl1−Fl2/bardbl1)\n1 + exp( f/bardblFl1−Fl2/bardbl1)(4)\nDistance Loss. The loss is used to optimize the feature distance measurement\ndThe loss is deﬁned as follows.\nLd=⎧\n⎨\n⎩ylogd+( 1−y)log(1 −d)\ny=1 ;ifF l1=Fl2\ny=0 ;otherwise(5)\nLoss Function. We use classiﬁcation loss Lcls, triples loss Ltriand distance\nlossLdas our optimization objectives. Among them, kis the number of human\nsemantic key-points, Ciis the conﬁdence level at the ith key-points, Fgis the\nglobal feature extracted by ResNet-50, Fla,F lp,F lnare the anchor feature, neg-\native sample feature and positive sample feature of person image respectively,\nandαis the margin. yis used to judge whether the persons in two channels are', 'Anti-occluded Person Re-identiﬁcation via Pose Restoration 405\nthe same person. If they are the same person, then y= 1, otherwise y=0 ,λ\nis constants, which are used to balance the relationship between diﬀerent lossfunctions.\nL=1\nkk/summationdisplay\ni=1Ci(Lcls+Ltri)+λL d (6)\nLcls=−log(Fg) (7)\nLtri=/bracketleftBig\nα+/bardblFla−Flp/bardbl2\n2−/bardblFla−Fln/bardbl2\n2/bracketrightBig\n+(8)\n4 Experiment\n4.1 Datasets and Evaluation Metrics\nIn order to prove the eﬀectiveness of our method in person re-identiﬁcation\nwith occlusion and without occlusion, we evaluate it on occluded-DukeMTMC\nand two non- occluded person re-identiﬁcation datasets Market1501 and\nDukeMTMC-ReID, respectively.\nOccluded-DukeMTMC [5]:It is the largest occluded person re-identiﬁcation\ndataset so far, and its training set contains 15618 images, covering a total of 702identities. The test set contains 1110 identities, including 17661 gallery images\nand 2210 query images. The experimental results on the occluded Duke-MTMC\ndata set show the superiority of our model in the case of person occlusion, andit does not need any manual clipping process as preprocessing.\nMarket1501 [16]:The dataset is collected in Tsinghua University campus, and\nthe images come from 6 diﬀerent cameras, one of which is low pixel. The training\nset contains 12936 images, and the test set contains 19732 images. There are 751\npeople in the training data and 750 people in the test set.\nDukeMTMC-ReID [17,18]:The dataset was collected at Duke University with\nimages from 8 diﬀerent cameras. The training set contains 16522 images, the testset contains 17661 images and 2228 query images. There are 702 people in the\ntraining data, with an average of 23.5 training data for each class. It is the largest\nperson re-identiﬁcation dataset at present.\nEvaluation Metrics: We use most of the standard indicators in the literature\nof person re-identiﬁcation, namely cumulative matching characteristic (CMC)curves and mean average precision (mAP), to evaluate the quality of diﬀerent\nperson re-identiﬁcation models.\n4.2 Implementation Details\nWe use ResNet-50 [ 19] as our backbone network, and make a small modiﬁcation\nto it: delete the average pooling layer and fully connected layer, the size of the', '406 B. Wu et al.\ninput image is adjusted to 256 ×128, the batch size of training is set to 64, and\nthe training epoch number is set to 60. Using Adam optimizer, the exponentialdecay rate of ﬁrst-order moment estimation is 0.5, the exponential decay rate\nof second-order moment estimation is 0.999, and the initial learning rate is 0.1.\nIn order to ensure the stability of training, a batch normalization layer is addedafter extracting the local features of person pose each time. The initial learning\nrate is 3.5e −4, and it decays to 0.1 when the epoch training reaches 30 to 60.\n4.3 Comparative Experiments\nIn this subsection, we compare four methods, which are holistic person ReID\nmethods, holistic ReID methods with key-points information, partial ReID meth-ods and occluded ReID methods, in order to explore the better performance of\nour model in person occlusion, as show in Table 1. We have also conducted exper-\niments on two common holistic ReID datasets Market-1501 and DukeMTMC-ReID, to explore our model has good generality, as show in Table 2.\nTable 1. Performance comparison on Occluded-DukeMTMC.\nMethod Rank-1 (%) Rank-5 (%) Rank-10 (%) mAP (%)\nHACNN [ 20]34.4 51.9 59.4 26.0\nPCB [ 21] 42.6 57.1 62.9 33.7\nFD-GAN [ 22]40.8 – – –\nDSR [ 23] 40.8 58.2 65.2 30.4\nSFR [ 24] 42.3 60.3 67.3 32.0\nPGFA [ 5] 51.4 68.6 74.9 37.3\nPARO (ours) 53.9 65.4 75.6 40.9\nTable 2. Performance comparison on the holistic re-id datasets Market-1501 and\nDukeMTMC-ReID\nMethod Market-1501 DukeMTMC-ReID\nRank-1 (%) mAP (%) Rank-1 (%) mAP (%)\nPAN [ 25] 82.8 63.4 71.7 51.5\nDSR [ 23] 83.5 64.2 – –\nTripletLoss [ 26]84.9 69.1 – –\nAPR [ 27] 87.0 66.9 73.9 55.6\nMutiScale [ 28]88.9 73.1 79.2 60.6\nPCB [ 21] 92.4 77.3 81.9 65.3\nPGFA [ 5] 91.2 76.8 82.6 65.5\nPRAO (ours) 93.8 77.0 82.1 67.8', 'Anti-occluded Person Re-identiﬁcation via Pose Restoration 407\nResults on Occluded-DukeMTMC. As we can see, in the case of occlusion,\nthere is no signiﬁcant diﬀerence between the general overall Reid method andthe partial Reid method, which indicates that it is not very good to deal with the\nocclusion problem by only focusing on the local features of the person visible\narea. However, it is not very obvious for the general overall Reid method toimprove the performance of alignment of person features through pose guidance,\nbecause the occlusion areas are inconsistent and cannot be aligned. Our PRAO\nachieves 53.9% Rank-1 accuracy and 40.9% mAP accuracy. This is because wetry our best to repair the person pose features through the pre-trained e-GAN\nnetwork, which makes it easier for the person features to align, and adds the pose\nattention mechanism to suppress the noise information propagation generatedby e-GAN.\nResults on Market-1501 and DukeMTMC-ReID. A ss h o w ni nT a b l e 2,\nour method has achieved the same performance as the most advanced methods\non both datasets, indicating that our method has good versatility.\n4.4 Visualization\nOccluded \nimageGAN e-GAN\nFig. 7. Pose repair results.\nRetrieval Results TargetPGFA\nOurs\nFig. 8. Visual results of person retrieval.\nVisualization of Pose Repair Module Results. The Fig. 7shows the per-\nformance of the pose repair module in our method. It can be seen from the ﬁgure\nthat when the person is occluded by other objects, the person pose repair mod-\nule can remove the occlusion and repair the person pose as much as possible.\nVisualization of Target Person Retrieval Results. Figure 8shows the\nvisual comparison between the new PRAO model and the PGFA model in person\nretrieval results. It can be seen from the ﬁgure that our new model has betterperformance in the case of inconsistent occlusion areas.', '408 B. Wu et al.\nFig. 9. Performance comparison of each module in diﬀerent epochs.\n4.5 Ablation Study\nIn this subsection, we use pose repair module (PPR) and dual channel feature\nextraction module (DCFE) for ablation experiments to explore the results ofperson re-identiﬁcation under occlusion. Firstly, the PPR and DCFE are further\nanalyzed under diﬀerent training epoch, and each module is compared with\nthe traditional occluded person re-identiﬁcation to explore the eﬀectiveness of\nthe model, as shown in Fig. 9. Secondly, we visualize the pose estimation and\nattention heatmap in the network, as shown in Fig. 10. It can be seen that the\nocclusion has a negative impact on the pose estimation to a certain extent. The\neﬀect of pose estimation can be improved through the pose repair module, so as\nto explore the performance of pose repair.\nBefore Pose Repair A/g332er Pose Repair\nFig. 10. After the person pose repair module, we can see that the performance of pose\nestimation has been improved.', 'Anti-occluded Person Re-identiﬁcation via Pose Restoration 409\n5 Conclusion\nIn this paper, we propose a new framework to solve the problem of person\nocclusion. In order to realize the whole to whole matching of person image in the\ncase of occlusion, a person pose repair module (PPR) is proposed to remove theocclusion. Finally, a dual channel feature extraction module (DCFE) is proposed\nto extract person features to further improve the performance of person re-\nidentiﬁcation. We have done a lot of experiments on occluded dataset and globaldataset to prove the performance of the framework.\nAcknowledgments. This research is supported by National Natural Science Foun-\ndation of China (61972183, 61672268) and National Engineering Laboratory DirectorFoundation of Big Data Application for Social Security Risk Perception and Prevention.\nReferences\n1. He, M.-X., Gao, J.-F., Li, G., Xin, Y.-Z.: Person re-identiﬁcation by eﬀective\nfeatures and self-optimized pseudo-label. IEEE Access 9, 42907–42918 (2021).\nhttps://doi.org/10.1109/ACCESS.2021.3062281\n2. Li, Y., Chen, S., Qi, G., Zhu, Z., Haner, M., Cai, R.: A GAN-based self-training\nframework for unsupervised domain adaptive person re-identiﬁcation. J. Imaging\n7, 62 (2021). https://doi.org/10.3390/jimaging7040062\n3. Fu, Y., et al.: Horizontal pyramid matching for person re-identiﬁcation. In: AAAI\n(2019)\n4. Hu, M., Zeng, K., Wang, Y., Guo, Y.: Threshold-based hierarchical clustering\nfor person re-identiﬁcation. Entropy 23, 522 (2021). https://doi.org/10.3390/\ne23050522\n5. Miao, J., Wu, Y., Liu, P., Ding, Y., Yang, Y.: Pose-guided feature alignment for\noccluded person re-identiﬁcation. In: 2019 IEEE/CVF International Conference\non Computer Vision (ICCV), pp. 542–551 (2019). https://doi.org/10.1109/ICCV.\n2019.00063\n6. Zhang, P., Lai, J., Zhang, Q., Xie, X.: MGD: mask guided de-occlusion framework\nfor occluded person re-identiﬁcation. In: Cui, Z., Pan, J., Zhang, S., Xiao, L., Yang,\nJ. (eds.) IScIDE 2019. LNCS, vol. 11935, pp. 411–423. Springer, Cham (2019).https://doi.org/10.1007/978-3-030-36189-1\n34\n7. Wang, G., et al.: High-order information matters: learning relation and topology\nfor occluded person re-identiﬁcation. In: 2020 IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pp. 6448–6457 (2020). https://doi.org/\n10.1109/CVPR42600.2020.00648\n8. Ma, B., Su, Y., Jurie, F.: Covariance descriptor based on bio-inspired features for\nperson re-identiﬁcation and face veriﬁcation. Image Vis. Comput. 32(6–7), 379–390\n(2014)\n9. Chang, Y., et al.: Joint deep semantic embedding and metric learning for person\nre-identiﬁcation. Pattern Recogn. Lett. 130, 306–311 (2020)\n10. Zhu, X., et al.: Heterogeneous distance learning based on kernel analysis-synthesis\ndictionary for semi-supervised image to video person re-identiﬁcation. IEEE Access\n8, 169663–169675 (2020). https://doi.org/10.1109/ACCESS.2020.3024289\n11. Wu, W., Tao, D., Li, H., Yang, Z., Cheng, J.: Deep features for person re-\nidentiﬁcation on metric learning. Pattern Recogn. 110, 107424 (2021)', '410 B. Wu et al.\n12. Vidhyalakshmi, M.K., Poovammal, E., Bhaskar, V., Sathyanarayanan, J.: Novel\nsimilarity metric learning using deep learning and root SIFT for person re-\nidentiﬁcation. Wirel. Pers. Commun. 117(3), 1835–1851 (2020). https://doi.org/\n10.1007/s11277-020-07948-1\n13. Zhang, S., Chen, C., Song, W., Gan, Z.: Deep feature learning with attributes for\ncross-modality person re-identiﬁcation. J. Electron. Imaging 29(03), 033017 (2020)\n14. Wang, H., Haomin, D., Zhao, Y., Yan, J.: A comprehensive overview of person\nre-identiﬁcation approaches. IEEE Access 8, 45556–45583 (2020)\n15. Zhuo, J., Chen, Z., Lai, J., Wang, G.: Occluded person re-identiﬁcation. In: 2018\nIEEE International Conference on Multimedia and Expo (ICME), pp. 1–6. IEEE\n(2018)\n16. Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., Tian, Q.: Scalable person re-\nidentiﬁcation: a benchmark. In: ICCV (2015)\n17. Ristani, E., Solera, F., Zou, R., Cucchiara, R., Tomasi, C.: Performance measures\nand a data set for multi-target, multi-camera tracking. In: Hua, G., J´ e g o u ,H .( e d s . )\nECCV 2016. LNCS, vol. 9914, pp. 17–35. Springer, Cham (2016). https://doi.org/\n10.1007/978-3-319-48881-3 2\n1 8 .Z h e n g ,W . S . ,L i ,X . ,X i a n g ,T . ,L i a o ,S . ,L a i ,J . ,G o n g ,S . :P a r t i a lp e r s o nr e -\nidentiﬁcation. In: ICCV (2015)\n19. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 770–778 (2016)\n20. Li, W., Zhu, X., Gong, S.: Harmonious attention network for person re-\nidentiﬁcation. In: CVPR (2018)\n21. Sun, Y., Zheng, L., Yang, Y., Tian, Q., Wang, S.: Beyond part models: person\nretrieval with reﬁned part pooling (and a strong convolutional baseline). In: Fer-rari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol.\n11208, pp. 501–518. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-\n01225-0\n30\n22. Ge, Y., Li, Z., Zhao, H., Yin, G., Yi, S., Wang, X., et al.: FD-GAN: pose-guided\nfeature distilling GAN for robust person re-identiﬁcation. In: NIPS (2018)\n23. He, L., Liang, J., Li, H., Sun, Z.: Deep spatial feature reconstruction for partial\nperson re-identiﬁcation: alignment-free approach. In: CVPR (2018)\n24. He, L., Sun, Z., Zhu, Y., Wang, Y.: Recognizing partial biometric patterns. arXiv\npreprint arXiv:1810.07399 (2018)\n25. Zheng, Z., Zheng, L., Yang, Y.: Unlabeled samples generated by GAN improve the\nperson re-identiﬁcation baseline in vitro. In: ICCV (2017)\n26. Hermans, A., Beyer, L., Leibe, B.: In defense of the triplet loss for person re-\nidentiﬁcation. arXiv preprint arXiv:1703.07737 (2017)\n27. Lin, Y., et al.: Improving person re-identiﬁcation by attribute and identity learning.\nPattern Recogn. 95, 151–161 (2019)\n28. Chen, Y., Zhu, X., Gong, S.: Person re-identiﬁcation by deep learning multi-scale\nrepresentations. In: ICCVW (2017)', 'Dynamic Runtime Feature Map Pruning\nPei Zhang1,2, Tailin Liang1,2, John Glossner1,2,L e iW a n g1, Shaobo Shi1,2,\nand Xiaotong Zhang1(B)\n1University of Science and Technology, Beijing 100083, China\n{pei.zhang,tailin.liang}@xs.ustb.edu.cn,\n{jglossner,wanglei,zxt}@ustb.edu.cn\n2Hua Xia General Processor Technologies, Beijing 100080, China\nsbshi@hxgpt.com\nAbstract. High bandwidth requirements in edge devices can be a bot-\ntleneck for many systems - especially for accelerating both training and\ninference of deep neural networks. In this paper, we analyze feature mapsparsity for several popular convolutional neural networks. When con-\nsidering run-time behavior, we ﬁnd a good probability of dynamically\ndisabling many feature maps. By evaluating the number of 0-valuedactivations within feature maps, we ﬁnd many feature maps that can\nbe dynamically pruned. This is particularly eﬀective when a ReLU acti-\nvation function is used. To take advantage of inactive feature maps, wepresent a novel method to dynamically prune feature maps at runtime\nreducing bandwidth by up to 11.5% without loss of accuracy for image\nclassiﬁcation. We further apply this method on Non-ReLU activation\nfunctions by allowing the output of the activation function to be within\nan epsilon of 0. Additionally, we also studied how video streaming appli-cations could beneﬁt from bandwidth reduction.\nKeywords: Dynamic pruning\n·Deep learning ·Accelerating neural\nnetworks\n1 Introduction\nDeep Neural Networks (DNN) have been developed to identify relationships in\nhigh-dimensional data [ 1]. Recent neural network designs have shown superior\nperformance over traditional methods in many domains including voice synthesis,\nobject classiﬁcation, and object detection. DNNs have many convolutional layersto extract features from the input data and map them to a latent space. These\nfeatures maps then become the input to subsequent layers.\nDNNs are required to be trained before inference can be performed. Training\ntypically uses backward propagation and gradient computations to learn parame-\nter values. The inference stage does not use backward propagation and therefore\nthe computing requirements are signiﬁcantly reduced. However, modern deepneural networks have become quite large with hundreds of hidden layers and\nupwards of a billion parameters [ 2]. With increasing size, it is no longer possi-\nble to maintain data and parameters in the processor’s cache. Therefore data\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 411–422, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_34', '412 P. Zhang et al.\nmust be stored in external memory, resulting in signiﬁcantly increased band-\nwidth requirements. Many researchers have studied reducing DNN bandwidthusage and proposed methods of compressing networks. Results have shown the\nnumber of parameters can be signiﬁcantly reduced with minimal or no loss of\naccuracy. Previous work includes parameter quantization [ 3], low-rank decom-\nposition [ 4], and network pruning which we fully describe as follows.\nNetwork pruning involves taking a designed neural network and removing\nneurons with the beneﬁt of reducing computational complexity, power dissipa-tion, and memory bandwidth. Neurons can often be removed without signiﬁcant\nloss of accuracy. Network pruning generally falls into the categories of static\npruning and dynamic pruning [ 5].\nStatic pruning chooses which neurons to remove before the network is\ndeployed. Neurons with 0-values or that are not contributing to classiﬁcation\nresults are removed. Statically pruned networks may optionally be retrained [ 6].\nRetraining may produce higher accuracy classiﬁcations [ 7], but requires signiﬁ-\ncant computation time. A problem with static pruning is the resulting networkstructure is often irregular. Additionally, a ﬁxed network is unable to take advan-\ntage of 0-valued input (image) data.\nDynamic pruning determines at runtime which parameters will not partici-\npate in the inference stage. It can overcome limitations of static pruning. When\ninput data is changing, it can adjust the number of pruned parameters to reduce\nbandwidth usage and power dissipation. One implementation of dynamic run-time pruning considers any parameters that are trained as 0-values within a\nprocessing element (PE) in such a way that the PE is inhibited from participat-\ning in the computation [ 8].\nA kernel is comprised of pre-trained coeﬃcient matrices stored in external\nmemory. The kernel (or a collection of kernels) is a ﬁlter that has the ability to\nidentify features in the input data. Most dynamic runtime pruning approachesremove kernels of computation [ 6,9]. In this approach, bandwidth is reduced by\nsuppressing the loading of coeﬃcients often called weights.\nAnother approach to pruning CNNs is to dynamically remove feature maps.\nIn this approach, inactive feature maps are removed at runtime. This type of\npruning is the focus of this paper. Our approach proposes not to remove ker-nels but speciﬁc feature maps that do not contribute to the eﬀectiveness of the\nnetwork. This is done dynamically at runtime and may be combined with other\nstatic pruning techniques.\nThis paper is organized as follows. Section 2discusses sparsity in several\nstandard CNNs and presents our approach to dynamically pruning feature maps.\nSection 3analyzes experimental results. In Sect. 4, we compare our method with\nrelated techniques. Finally, in Sect. 5, we describe our future research.\n2 Runtime Feature Map Pruning\nIn this section we look at sparsity of feature maps and its aﬀect on classiﬁcation\naccuracy. We characterize bandwidth requirements for several CNNs. Then we', 'Dynamic Runtime Feature Map Pruning 413\npresent a method for dynamically marking unused feature maps and discuss the\napplication of bandwidth reduction between image and video applications.\n2.1 Dynamic Pruning\nEven with memory performance improvements, bandwidth is still a limiting fac-\ntor in many neural network designs [ 10]. Standard bus protocols such as PCIe\nlimit the peak available bandwidth within a system. Static pruning can be applied\nwhen designing the network structure. However, for ﬁxed network models that are\nalready in deployment, dynamic runtime pruning can be applied to further reduce\nbandwidth. Our proposed approach reduces the number of feature maps being pro-\ncessed during inference. Unused feature maps do not need to be loaded into theprocessor thereby reducing bandwidth and decreasing inference latency.\nConsider 5 kernels that produce 5 feature maps. Note that in this exam-\nple we consider 1-dimensional (1D) feature maps without loss of generaliza-tion to higher dimensional feature maps. Consider the case where the kernels\nare applied to input data. The activation function (typically ReLU) is then\napplied to the output of each kernel producing the following 5 feature maps:[1,0,0,4],[0,0,0,0],[0,7,2,8],[0,0,0,0],[9,5,3,7]. We deﬁne the feature map ele-\nment sparsity as the number of 0-element values divided by the total number\nof elements. In this case there are 11 0-valued elements and 20 total elements.Therefore the feature map element sparsity is 11/20 = 0 .55. We deﬁne feature\nmap sparsity as the number of feature maps with all 0-valued outputs. In this\ncase 2/5=0.4.\nBecause it is rare that every element in a feature map produces a 0-valued\noutput, we introduce a heuristic criterion called feature map element density .\nFeature map element density is a metric used within a single feature map thatmeasures the percent of 0-valued outputs divided by the total number of outputs.\nThis is distinct from feature map element sparsity that considers all feature maps\nin the layer. We use a feature map element density of 99% meaning that 99% of\noutputs within a single feature map must have 0-valued outputs to be marked\nfor removal.\nIn addition, not all feature maps output are exactly zero but many are close\nto zero. We deﬁne a threshold epsilon ( /epsilon1) to be the distance from zero of the\nfeature map element. It is applied after the activation function and is deﬁned byEq.1. For any absolute value of xgreater than /epsilon1, the function returns x.I ft h e\nabsolute value of xless than /epsilon1, it returns 0.\nactivation(\nx)=/braceleftBigg\nx|x|>/epsilon1\n0|x|≤/epsilon1(1)\nWe proﬁled both feature map element sparsity and feature map sparsity on\nconvolutional layers for some common neural networks. We used the average of\n1000 class images from the ImageNet2012 dataset [ 11]. Figure 1shows feature\nmap element sparsity. The leftmost bar of each network shows feature map', '414 P. Zhang et al.\nFig. 1. Feature map element sparsity ratio of convolutional layers: the x-axis identiﬁes\nsome popular CNNs for a range of pruning thresholds (epsilon). The y-axis shows the\nelement sparsity of feature maps for the network.\nelement sparsity for the case when each element is equal to 0 (epsilon = 0). For\nexample, AlexNet has 27% 0-valued elements while ResNet50 don’t have any.\nFigure 2shows feature map sparsity for a feature map element density of\n99%. Although feature map sparsity is lower than feature map element spar-\nsity, selecting which feature maps to mark for pruning is not resource intensive.\nAllowing values of epsilon to be 0.1 (meaning 99% of all feature map outputsare within 0.1 of zero) shows some networks can prune many more feature maps\n(e.g. ResNet50).\nReLU activation functions typically produce many 0-valued outputs. Leaky\nReLU and some other activation functions which allow small negative values for\ngradient smoothing do not produce many 0-valued outputs [ 12]. ResNet50 and\nDenseNet201 use leaky ReLU and as shown in Fig. 1have fewer 0-sparsity.\nFig. 2. Feature map sparsity on convolutional layers.', 'Dynamic Runtime Feature Map Pruning 415\n2.2 Dynamic Pruning Algorithm\nAlgorithm 1describes a brute-force naive technique for dynamic feature map\npruning. We proﬁle a frame of video and apply the activation function of Eq. 1.\nWe look at the element values in each feature map. If we determine that a\nfeature map has 99% of elements less than a value of /epsilon1, we mark the feature\nmap as inactive and subsequently do not process it. This feature map remains\ninactive until the next time a frame of video is proﬁled (called the recalibration\ntime). Based on Darknet [ 13], an open-source neural network framework, we use\na bitmap array to skip the marked feature maps using the GEMM algorithm.\nOur source code is available at GitHub1.\nAlgorithm 1: Dynamic Feature Map Pruning\ninput : C - the number of feature maps, H - a feature map height, W - a\nfeature map width\noutput: bitmap - the bit map marking whether a feature map can be pruned\n1feature_map_size = H ×W;\n2fori=0 to Cdo\n3 /* cnt - counting 0-value */\n4 cnt = 0;\n5 forj=0 to feature_map_size do\n6 cnt += !! abs(value[i ×feature_map_size + j]) ;\n7 end\n8 /* set a 1% error rate */\n9 bitmap[i] = (0.99 ×feature_map_size) ≤cnt;\n10end\n2.3 Bandwidth Reduction in Video Streams\nVideo processing is particularly appropriate for our dynamic pruning approach\nsince a series of video frames may often reuse feature maps to determine detec-\ntion. In this section, we derive bandwidth saving based on criterion such as theimage size, the number of feature maps, and the frame rate of the video stream.\nFigure 3shows the cosine similarity of diﬀerent frame intervals in three segments\n(beginning, middle, and end) of video from the YouTube-8M data-set [ 14].\nObjects within 15 frames (which is typically half of a standard refresh rate)\ntend to be similar. Meanwhile, the objects’ positions are relatively unchanged.\nIn this case, the bounding box of the objects only moves slightly. Active feature\nmaps tend to be reused and the feature maps marked for skipping remain marked\nand are not loaded. Depending upon the amount of motion in an arbitraryvideo, the length of time feature maps are reused may vary. Hence, we introduce\na heuristic parameter called recalibration time that is the number of frames\nbetween reproﬁling of unused feature maps.\n1https://github.com/bit2erswe2t/darknet-pruning .', '416 P. Zhang et al.\n(a) beginning (b) middle (c) end\nFig. 3. The cosine similarity of diﬀerent frame intervals in three segments of video.\nTo determine bandwidth reduction in the span of one second, consider an N\nlayer network. If the input to the convolutional layer is ci×ni×nisize at a rate of\nfframe per second, where ciis the number of feature maps, and niis the height\nand width of a feature map, we can use the same pruning mark at tframes in\na recalibration stage and according to the pruning mark can skip feature maps\nint−1frames. Using Mas recalibration stages, where kijfeature maps can be\npruned for the ilayer in the jrecalibration stage, the total bandwidth reduction\nRis determined by Eq. 2.\nM =f\nt,R =/summationdisplay N\ni=1/summationdisplay M\nj=1((t−1)×kij×n2\ni)\nsubject to k,t,f,n,f,M,N ∈N+andt≤f(2)\nEquation 2shows greater bandwidth reduction for video streams that have\na high frame rate and large numbers of features maps. However, if the object is\nfast-moving and the recalibration time is short, the bandwidth savings convergesto the single-image classiﬁcation rate.\n3 Experiments\nIn this section, we characterize Top-1 and Top-5 classiﬁcation accuracy usingdynamic feature map pruning for the output of Eq. 1within /epsilon1after the original\nactivation function using a feature map element density of 99%. We further\ncharacterize the bandwidth savings for image and video workloads. We compute\nstatistics by counting the number of feature maps loaded. We do not take into\naccount in these results the eﬀect of small processor caches that are unable tostore an entire feature map.\nTo validate our technique we used the ILSVRC2012-1K image data-set con-\ntaining 1000 classes [ 11]. Table 1shows dynamic feature map pruning with a\nfeature map element density of 99%. The results are characterized on all 1000\nclasses. /epsilon1, from Eq. 1, measures how far away from 0 the output is allowed to\nvary and still be marked for pruning. “Pruned” describes the percent of dynami-cally pruned feature maps. Top-1 and Top-5 show the base reference in the ﬁrst\ncolumns and then the change in accuracy for subsequent columns for increasing\nvalues of /epsilon1.', 'Dynamic Runtime Feature Map Pruning 417\nTable 1. Feature map pruning accuracy vs. /epsilon1with element density = 99%\n/epsilon1 / 0.0 0.01 0.1 1\nNetwork Top-1 Top-5 Top-1 Top-5 Pruned Top-1 Top-5 Pruned Top-1 Top-5 Pruned Top-1 Top-5 Pruned\nAlexNet 56% 79% 56% 79% 2.1% 56% 79% 2.1% 56% 79% 2.5% 0% 1% 79.5%\nMobileNetv2 71% 91% 71% 91% 8.6% 71% 91% 9.7% 71% 91% 11.5% 0% 1% 59.2%\nDenseNet201 77% 93% 77% 93% 0% 77% 93% 0% 77% 93% 0.2% 0% 1% 77.3%\nResNet50 76% 93% 76% 93% 0% 76% 93% 0.1% 76% 93% 1.4% 0% 1% 61.6%\nOur results show that with an /epsilon1=0only ReLu activations in AlexNet and\nMobileNetv2 are pruned. Leaky ReLu used in DenseNet201 and ResNet50 do\nnot contain many 0-valued activations in their feature maps and therefore are\nnot pruned. Using an /epsilon1=0.1for pruning has no signiﬁcant loss in accuracy for\nthe Top-1 and Top-5 while reducing feature map loading up to 11.5%. At /epsilon1=1,\nTop-1 and Top-5 accuracy drops to 0%.\nTable 2. Top-1 classiﬁcation accuracy of images w/o & w/0.1 pruning\nNetwork w/o pruning /epsilon1=0.1pruning Reduced fmap\nloading\nCat Dog Eagle Giraﬀe Horse Cat Dog Eagle Giraﬀe Horse Pruned/all\nAlexNet 40.29% 19.03% 79.03% 36.44% 53.02% 40.24% 18.98% 78.91% 36.43% 52.97% 2.48%\n(10k/398k)\nMobileNetv2 19.25% 39.04% 91.62% 29.62% 42.71% 23.95% 39.67% 91.31% 33.23% 42.32% 11.49%\n(1036k/9013k)\nDenseNet201 43.47% 85.49% 71.41% 32.28% 22.86% 42.52% 85.33% 68.65% 32.46% 22.55% 0.15%\n(47k/30605k)\nResNet50 23.94% 95.11% 67.95% 76.72% 21.12% 23.91% 95.16% 67.99% 77.12% 21.00% 1.37%\n(162k/11831k)\nTable 2shows Top-1 image classiﬁcation accuracy with and without dynamic\nfeature map pruning for /epsilon1=0.1for ﬁve classiﬁcation categories. In some cases,\nthe pruned network outperformed the unpruned (e.g., MobileNetv2 cat image).This is consistent with other researcher’s ﬁndings [ 7,15]. The last column shows\nthe number of feature maps pruned at /epsilon1=0.1. For example, MobileNetv2\nreduced the number of feature maps loaded by 1036k, from 9013k to 7977k.\nThis is approximately an 11.5% savings in the number of feature maps loaded.\nAdditional results for MobileNetv2 not shown in Table 2reveals that\nMobileNetv2 particularly beneﬁted from feature map pruning. With /epsilon1=0prun-\ning, MobileNetv2 reduced 36 out of 54 ReLU activated convolutional layers\nresulting in a feature map loading reduction of 8.6%. AlexNet reduced 3 outof 5 ReLU activated convolutional layers reducing feature map loading by 2.1%.\nOther networks using leaky ReLU, as anticipated, do not have reduced feature\nmap loading with /epsilon1=0.\nFigure 4shows the feature map loading requirements for MobileNetv2 by\nconvolutional layer. The y-axis displays the Mega-bytes of data required to be\nread. The x-axis displays the network layers. The stacked bars show the data', '418 P. Zhang et al.\nFig. 4. Dynamic pruning performance on MobileNetv2 by convolutional layer.\nrequirements with and without dynamic pruning. /epsilon1=0is used and shows that\ndynamic pruning can reduce the image data loading requirements by about 6.2%as averaged across all the convolutional layers. A few layers of MobileNetv2 use\nlinear activation functions and therefore don’t beneﬁt from /epsilon1=0pruning.\nWe also ran AlexNet experiments on Caﬀe\n2with3and without4static prun-\ning. The results show a similar feature map sparsity on convolutional layers\nabout 27%. The feature map pruning without static pruning is 0.79%. Afterstatic pruning is applied, the network still has 0.45% feature map loading could\nbe pruned with dynamic runtime feature map pruning.\nUsing our simulator, we conﬁgured a processor capable of executing a single\nof convolution operations at one time. We ran experiments using MobileNetV2\nat/epsilon1=0.1, element density = 99%, 224×224resolution video stream, and a\nrecalibration interval of 30 frames. The original network bandwidth usage is34.35MB per frame. The 29 frames between recalibration reduces 3.95MB per\nframe. Our method reduces bandwidth requirements by 11.12%. As derived in\nEq.2, depending upon the number of layers and the input image size, we examine\nand mark which feature maps should be pruned once during the recalibration\ninterval.\n4 Related Work\nBandwidth reduction is accomplished by reducing the loading of parameters or\nthe number of bits used by the parameters. This leads to further research onquantization, weight sharing and sparsity (pruning).\nBandwidth reduction using reduced precision values has been previously pro-\nposed. Historically most networks are trained using 32-bit single precision ﬂoat-\ning point numbers [ 16]. It has been shown that 32-bit single precision can be\nreduced to 8-bit integers for inference without signiﬁcant loss of accuracy [ 17–\n19]. Even ternary and binary weights can be applied without signiﬁcant loss of\n2https://github.com/BVLC/caﬀe .\n3https://github.com/songhan/Deep-Compression-AlexNet .\n4http://dl.caﬀe.berkeleyvision.org/bvlc_alexnet.caﬀemodel .', 'Dynamic Runtime Feature Map Pruning 419\naccuracy on trivial datasets [ 20,21]. Our present work does not consider reduced\nprecision parameters but may be incorporated into future research since it iscomplementary to our approach.\nCompressing sparse parameter networks can save both computation and\nbandwidth. Chen [ 22] describes using a hash algorithm to decide which weights\ncan be shared. Their work focused only on fully connected layers and used\npre-trained weight binning rather than dynamically determining the bins dur-\ning training. Han [ 6] describes weight sharing combined with Huﬀman coding.\nWeight sharing is accomplished by using a k-means clustering algorithm instead\nof a hash algorithm to identify neurons that may share weights. We don’t cur-\nrently share weights, but it is possible to combine our technique with weightsharing.\nNetwork pruning is an important component for both memory size and band-\nwidth usage. It also reduces the number of computations. Early research used\nlarge-scale networks with static pruning to generate smaller networks to ﬁt end-\nto-end applications without signiﬁcant accuracy drop [ 23].\nLeCun, as far back as 1990, proposed to prune non-essential weights using\nthe second derivative of the loss function [ 24]. This static pruning technique\nreduced network parameters by a quarter. He also showed that the sparsity ofDNNs could provide opportunities to accelerate network performance.\nGuo [ 25] describes a method using pruning and splicing that compressed\nAlexNet by a factor of 7.7×. This signiﬁcantly reduced the training iterations\nfrom 4800K to 700K. However, this type of pruning results in an asymmetric\nnetwork complicating hardware implementation.\nMost network pruning methods typically prune kernels rather than feature\nmaps [ 6,25]. In addition to signiﬁcant retraining times, it requires signiﬁcant\nhyperparameter tuning. Parameters in fully connected layers often represent the\nmajority of 0-values. AlexNet and VGG particularly have many parameters infully connected layers. Our technique uses dynamic pruning of feature maps\nrather than kernels and requires no retraining. We reduce feature map loading\nto save up to 11.5% bandwidth without loss of accuracy.\nBolukbasi [ 26] has reported a system that can adaptively choose which layers\nto exit early. They format the inputs as a directed acyclic graph with variouspre-trained network components. They evaluate this graph to determine leaf\nnodes where the layer can be exited early. Their work can be considered a type\nof dynamic layer pruning.\nFor instruction set processors, feature maps or the number of ﬁlters used to\nidentify objects is a large portion of bandwidth usage [ 16] - especially for depth-\nwise or point-wise convolutions where feature map computations are a largerportion of the bandwidth [ 27]. Lin [ 8] used Runtime Neural Pruning (RNP) to\ntrain a network to predict which feature maps wouldn’t be needed. This is a type\nof dynamic runtime pruning. They found 2.3×to5.9×acceleration with top-5\naccuracy loss from 2.32% to 4.89%. RNP, as a predictor, may need to be retrained\nfor diﬀerent classiﬁcation tasks and may also increase the original network size.', '420 P. Zhang et al.\nOur technique only requires a single bit per feature map to determine if a feature\nmap will be loaded.\nOur technique prunes by feature map rather than elements. This beneﬁts\ninstruction set processors, particularly signal processors, because data can be\neasily loaded into the processor using sliding windows. Additionally, some prun-ing approaches only work with small datasets [ 28], such as MNIST and CIFAR-\n10. Our pruning technique works for both image and video data-sets.\n5 Future Work\nAt present, our pruning algorithm uses comparison, counting and truncation\nto ﬁnd feature maps that can be pruned. Our future research is focused ontechniques to reduce the cost of marking feature maps for pruning. We also plan\nto investigate the eﬀect of small processor caches that can only store part of\na feature map. Finally, our experiments showed ﬂuctuation in cosine similaritycaused by the change of the frame interval of the video stream. We will study\nhow to select the most suitable frame interval and how to dynamically adjust\nthe recalibration time.\nAcknowledgement. This work was supported by the National Natural Science Foun-\ndation of China, No. 61971031.\nReferences\n1. Lecun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436–444\n(2015). https://doi.org/10.1038/nature14539\n2. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer,\nK.: SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB\nmodel size, February 2016. https://doi.org/10.1007/978-3-319-24553-9 .https://\narxiv.org/abs/1602.07360\n3. Micikevicius, P., et al.: Mixed precision training. In: 6th International Conference\non Learning Representations (ICLR 2018), October 2017. http://arxiv.org/abs/\n1710.03740\n4. Denil, M., Shakibi, B., Dinh, L., Ranzato, M., de Freitas, N.: Predicting parameters\nin deep learning. In: Advances in Neural Information Processing Systems, pp. 2148–2156, June 2013. https://doi.org/10.1016/S0033-3506(78)80031-6 .http://papers.\nnips.cc/paper/5025-predicting-parameters-in-deep-learning\n5. Liang, T., Glossner, J., Wang, L., Shi, S., Zhang, X.: Pruning and quantization for\ndeep neural network acceleration: a survey. Neurocomputing 461, 370–403 (2021).\nhttps://doi.org/10.1016/j.neucom.2021.07.045 ,http://arxiv.org/abs/2101.09671\n6. Han, S., Mao, H., Dally, W.J.: Deep compression: compressing deep neural net-\nworks with pruning, trained quantization and huﬀman coding. 45(4), 199–203\n(2015). arXiv preprint arXiv:1510.00149\n7. Yu, R., et al.: NISP: pruning networks using neuron importance score propagation.\nIn: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n9194–9203. IEEE, June 2018. https://doi.org/10.1109/CVPR.2018.00958 .https://\nieeexplore.ieee.org/document/8579056/', 'Dynamic Runtime Feature Map Pruning 421\n8. Lin, J., Rao, Y., Lu, J., Zhou, J.: Runtime neural pruning. In: 31st Conference\non Neural Information Processing Systems (NIPS 2017), pp. 2178–2188 (2017).\nhttps://papers.nips.cc/paper/6813-runtime-neural-pruning.pdf\n9. LeCun, Y., et al.: Backpropagation applied to handwritten zip code recognition.\nNeural Comput. 1(4), 541–551 (1989). https://doi.org/10.1162/neco.1989.1.4.541 .\nhttp://www.mitpressjournals.org/doi/10.1162/neco.1989.1.4.541\n10. Rhu, M., O’Connor, M., Chatterjee, N., Pool, J., Kwon, Y., Keckler, S.W.: Com-\npressing DMA engine: leveraging activation sparsity for training deep neural net-\nworks. In: 2018 IEEE International Symposium on High Performance Computer\nArchitecture (HPCA), pp. 78–91. IEEE, February 2018. https://doi.org/10.1109/\nHPCA.2018.00017 .http://ieeexplore.ieee.org/document/8327000/\n11. Russakovsky, O., et al.: ImageNet large scale visual recognition challenge. Int. J.\nComput. Vis. 115(3), 211–252 (2015). https://doi.org/10.1007/s11263-015-0816-\ny.http://link.springer.com/10.1007/s11263-015-0816-y\n12. Maas, A.L., Hannun, A.Y., Ng, A.Y.: Rectiﬁer nonlinearities improve neural net-\nwork acoustic models. In: ICML 2013, p. 3 (2013). https://doi.org/10.1016/0010-\n0277(84)90022-2\n13. Redmon, J.: Darknet: Open Source Neural Networks in C (2016). http://pjreddie.\ncom/darknet/\n14. Abu-El-Haija, S., et al.: YouTube-8M: a large-scale video classiﬁcation benchmark.\narXiv preprint https://arxiv.org/abs/1609.08675 , September 2016\n15. Huang, Q., Zhou, K., You, S., Neumann, U.: Learning to prune ﬁlters in convo-\nlutional neural networks. In: Proceedings of the 2018 IEEE Winter Conferenceon Applications of Computer Vision, WACV 2018, pp. 709–718, January 2018.\nhttps://doi.org/10.1109/WACV.2018.00083 .http://arxiv.org/abs/1801.07365\n16. Sze, V., Chen, Y.H., Yang, T.J., Emer, J.S.: Eﬃcient processing of deep\nneural networks: a tutorial and survey. Proc. IEEE 105(12), 2295–2329\n(2017). https://doi.org/10.1109/JPROC.2017.2761740 .http://ieeexplore.ieee.org/\ndocument/8114708/\n17. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\nvol. 7, no. 3, pp. 171–180, December 2015. https://doi.org/10.3389/fpsyg.2013.\n00124.http://ieeexplore.ieee.org/document/7780459/\n18. Ma, Y., Suda, N., Cao, Y., Seo, J.S., Vrudhula, S.: Scalable and modularized\nRTL compilation of convolutional neural networks onto FPGA. In: FPL 2016–26thInternational Conference on Field-Programmable Logic and Applications (2016).\nhttps://doi.org/10.1109/FPL.2016.7577356\n19. Jacob, B., et al.: Quantization and training of neural networks for eﬃcient integer-\narithmetic-only inference. In: 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 2704–2713. IEEE, June 2018. abs/1712.0. https://doi.\norg/10.1109/CVPR.2018.00286 .https://ieeexplore.ieee.org/document/8578384/\n20. Zhou, H., Alvarez, J.M., Porikli, F.: Less is more: towards compact CNNs. In:\nLeibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9908, pp.662–677. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46493-0_40\n21. Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., Bengio, Y.: Binarized neural\nnetworks. In: Advances in Neural Information Processing Systems, pp. 4114–4122(2016). http://papers.nips.cc/paper/6573-binarized-neural-networks\n22. Chen, W., Wilson, J., Tyree, S., Weinberger, K., Chen, Y.: Compressing neural\nnetworks with the hashing trick. In: International Conference on Machine Learning,pp. 2285–2294 (2015). http://arxiv.org/abs/1504.04788', '422 P. Zhang et al.\n23. Buciluă, C., Caruana, R., Niculescu-Mizil, A.: Model compression. In: Proceedings\nof the 12th ACM SIGKDD International Conference on Knowledge Discovery and\nData Mining - KDD 2006, p. 535. ACM Press, New York (2006). https://doi.\norg/10.1145/1150402.1150464 .http://portal.acm.org/citation.cfm?doid=1150402.\n1150464\n24. Cun, Y.L., Denker, J.S., Solla, S.A.: Optimal brain damage. In: Advances in Neural\nInformation Processing Systems, vol. 2, pp. 598–605 (1990). https://dl.acm.org/\ndoi/10.5555/109230.109298\n25. Guo, Y., Yao, A., Chen, Y.: Dynamic network surgery for eﬃcient DNNs. In:\nAdvances in Neural Information Processing Systems, vol. 29, pp. 1379–1387 (2016).\nhttp://papers.nips.cc/paper/6165-dynamic-network-surgery-for-eﬃcient-dnns\n26. Bolukbasi, T., Wang, J., Dekel, O., Saligrama, V.: Adaptive neural networks for\neﬃcient inference. In: Thirty-Fourth International Conference on Machine Learn-\ning, February 2017. https://arxiv.org/abs/1702.07811\n27. Chollet, F., Google, C.: Xception: deep learning with depthwise separable convo-\nlutions. In: The IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), vol. 7, pp. 1251–1258. IEEE, July 2017. https://doi.org/10.1109/CVPR.\n2017.195 .http://ieeexplore.ieee.org/document/8099678/\n28. Anwar, S., Hwang, K., Sung, W.: Structured pruning of deep convolutional neu-\nral networks. ACM J. Emerg. Technol. Comput. Syst. (JETC) 13(3), 32 (2015).\nhttps://doi.org/10.1145/3005348 .https://dl.acm.org/citation.cfm?id=3005348', 'Special Session: New Advances in Visual\nPerception and Understanding', 'Multi-branch Graph Network for\nLearning Human-Object Interaction\nTongtong Wu, Xu Zhang, Fuqing Duan(B), and Liang Chang\nCollege of Artiﬁcal Intelligence, Beijing Normal University, 19 Xinjiekouwai Street,\nHaidian, Beijing 100875, People’s Republic of China\nfqduan@bnu.edu.cn\nAbstract. In this work, we study the task of detecting human-object\ninteractions (HOI) from images, which is deﬁned as detecting triplets of\n(human, predicate, object ). A common practice in the literatures is ﬁrstly\nlocalizing human and object instances and then inferring the triplets orpredicates only as a classiﬁcation task from the detected human-object\npairs. A data sparsity issue arises when inferring the triplets because of\nthe serious data imbalance among HOI classes, while a data varianceissue arises when inferring predicates only since a predicate can carry\ndiﬀerent semantic meanings when being applied to diﬀerent objects. To\nresolve the problem, we propose to decompose HOI classes with a samepredicate into several semantic groups based on the appearance, semantic\ninformation and function of the objects. By doing this, semantic-related\nHOI classes are grouped together to compensate the data sparsity issue,while visually and functionally less related HOI classes are separated to\nrelieve the data variance issue. We reveal multiple levels of decompo-\nsition in diﬀerent granularities can provide richer auxiliary informationto boost the performance. We implement this idea with a multi-branch\ngraph network, while the multiple branches make classiﬁcations based on\ndiﬀerent levels of decompositions. We evaluate our method on popularHICO-Det dataset. Experimental results show that our method achieves\nstate-of-the art performance.\nKeywords: Human-object interaction\n·Graph·Multi-branch ·\nNeural network ·Deep learning\n1 Introduction\nHuman-Object Interaction (HOI) Detection aims at localizing and inferring rela-\ntionships between human and objects in an image, e.g., “ride bicycle”, or “hold\ncup”. A HOI Detection problem is in general deﬁned as a triplet ( human, pred-\nicate, object ), where human andobject are represented by bounding boxes, and\npredicate is the interaction between this ( human ,object ) pair. Thanks to the\nsuccess in object detection [ 18], many powerful oﬀ-the-shelf human and object\ndetectors can be used to localize the human and object instances. Therefore, it\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 425–436, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_35', '426 T. Wu et al.\nis a common practice to use these detectors to address human and object detec-\ntion ﬁrstly and then infer the triplets or infer predicates only as a classiﬁcationtask from the detected human-object pairs. Because of the serious data imbal-\nance among HOI classes, e.g., “kiss horse” is rare in available dataset but “ride\nhorse” is common, a data sparsity issue arises when inferring the HOI triplets.On the other hand, since a predicate can carry diﬀerent semantic meanings when\nbeing applied to diﬀerent objects, e.g., “ride” applied to “bicycle” and “bus”,\na data variance issue arises when inferring predicates only. These issues lead tonegative eﬀects for feature and classiﬁer learning, which greatly increases the\ndiﬃculty of the task.\nTo resolve these issues, we propose decomposing the HOI classes with a same\npredicate into several groups, where each group represents a diﬀerent semantics.\nBy doing this, as Fig. 1shows, semantic-related HOI classes (e.g., “ride bicycle”\nand “ride motorbike”) are grouped together to compensate the data sparsity\nissue, while visually and functionally less related HOI classes (e.g., “ride bus”\nand “ride bicycle”) are separated to relieve the data variance issue. For decom-position of HOI classes with a same predicate, we adopt external knowledge\n(appearance, semantic information and function of objects) to measure the sim-\nilarity of objects, and HOI classes with same predicate and similar objects areconsidered semantic-related. We merge semantic-related HOI classes and treat\nthem as one super class during training. Finally, we design a multi-branch net-\nwork that for each input image we can get several super HOI classiﬁcation resultsbased on multi-level decompositions, and get the ﬁnal HOI classiﬁcation result\nby fusing the super HOI classiﬁcation results. We build our proposed multi-\nbranch network upon graph convolutional network [ 9]. Experiments on the main\nbenchmark dataset demonstrate the eﬀectiveness of our proposed method.\nOur contributions are as follows: (1) To deal with the data sparsity issue and\ndata variance issue in HOI learning, we propose to decompose HOI classes witha same predicate into several semantic groups. For the decomposition, we pro-\npose to learn feature embedding of object classes using multi-modal knowledge\n(appearance, semantic information and function of objects), adopt an unsuper-\nvised method to merge semantic-related HOI classes. (2) We propose a novel\nmulti-branch graph network with each branch making HOI classiﬁcation in a dif-ferent level of decomposition and infer HOIs by fusing the multiple classication\nresults. (3) Our method achieve state-of-the-art performance in HOI detection\non the main HOI benchmark dataset.\n2 Related Work\nIn recent years, beneﬁting from the huge success of deep learning [ 21] and the\navailability of large-scale datasets [ 2,3,10,26], many deep learning based HOI\nmodels are proposed. In early work [ 2], a large benchmark HOI dataset and a\nbasic parallel network structure were proposed. Gkioxari et al. [6] adopted a\nBayesian method that utilizes features of human to predict the distribution of\nobject locations to enhance HOI learning. Attention mechanisms was applied for', 'Multi-branch Graph Network for Learning Human-Object Interaction 427\nFig. 1. HOI classes with the same predicates and similar objects are considered\nsemantic-related and be merged as one super class during training.\ncatching useful clues from contextual information [ 5] and the synergy of human\njoints [ 4]. In [11], Liet al.used external datasets to transfer interactive knowledge\nto reduce negative interaction samples. In [ 1,15,19], several methods was pro-\nposed to implement zero-shot learning to detect unseen HOI relationships. These\nworks are mainly based on convolutional neural network. In recent years, withthe development of graph neural network [ 9] in visual relationships [ 24,25], some\nresearchers have explored its application in HOI problem. In [ 23], a knowledge\ngraph of human-object relationships was constructed and learned to enhanceHOI learning. Qi et al. [16] incorporated structural knowledge and proposed a\ngraph parsing neural network for HOI inference in images or videos.\nAlthough considerable improvement were achieve by above approaches, there\nstill exists a issue. All of them ignore the correlation of HOI classes. They simply\ninfer the HOI triplets or infer predicates only as a classiﬁcation task, which leads\nto a data sparsity issue or a data variance issue, and leads to negative eﬀects forfeature and classiﬁer learning. Compared to these methods, our method focuses\non improving the model by decomposing HOI classes with same predicate into\nseveral semantic groups to address the data sparsity issue and the data varianceissue. HOI classes in same semantic group are merged and considered as one\nsuper class during training. This technique reduces the learning burden of the\nmodel and improves the classiﬁcation performance. We design our network byusing graph neural network to classify redeﬁned super HOI classes. Also, we\ndevelop the graph network into multiple branches to adopt multiple levels of\ndecomposition with diﬀerent granularities. Experimental results show that our\nmulti-branch network establishes new state-of-the-art performance.\n3 Approach\n3.1 Overview\nFigure 2shows the overview of our proposed approach. Our approach consists\nof two main stages, human/object detection and HOI classiﬁcation. In thehuman/object detection stage, we adopt Faster R-CNN [ 18] as other HOI detec-\ntion methods [ 5,11,19]. In the HOI classiﬁcation stage, we conduct the decom-\nposition via object embedding, which is realized by using external knowledge', '428 T. Wu et al.\nFig. 2. Overview of our proposed approach. We ﬁrst detect human/object\ninstances in the input image using Faster R-CNN, then construct a graph with the\ndetected instances and their feature maps. The graph is updated in HGAT (HOI graph\nattention) module, and fed to multi-branch network. In each branch of the network,the graph is updated again in HGAT module, and then fed to fully connected layers to\npredict scores for all human-object pairs. Diﬀerent branches of the network are respon-\nsible to HOI classiﬁcation tasks for diﬀerent super HOI classes. During the inferencestage, the scores of super HOI class are ﬁrst parsed into HOI scores, and then we get\nthe ﬁnal HOI scores by averaging the HOI scores.\n(appearance, semantic and function of objects). The HOI classes with same\npredicate and dissimilar objects is decomposed into diﬀerent semantic groups,\nand the HOI classes with same predicate and similar objects are merged as one\nsuper HOI class (named “sHOI”). Finally, we design a multi-branch graph net-work for HOI classiﬁcation, in which each branch network deals with diﬀerent\nlevels of decomposition.\n3.2 Object Embedding\nThe object embedding aims to measure similarity of features of object classes.\nSince similar objects often have similar appearances, semantics and functions,\nwe conduct the object embedding by utilizing visual appearance embedding,\nsemantic word embedding and function-driven embedding of object classes.Denote P={p\n1,p2,...,p m}as the set of predicates and O={o1,o2,...,o n}\nas the set of object classes. Each HOI class can be represented by a triplet\n(human,p i,oj),i∈{1,...,m },j∈{1,...,n }.\nVisual Appearance Embedding. The inspiration of visual appearance embedding\ncomes from object classiﬁcation task. Object classiﬁcation task is actually a', 'Multi-branch Graph Network for Learning Human-Object Interaction 429\ncorrelation problem between visual features and general representation of object\nclasses. An instance is considered as an object class if the feature vector ofthe instance is highly related to the object classiﬁer parameters. Therefore, we\ncan use parameters of the classiﬁers from external model as visual appearance\nembedding for object classes. Speciﬁcally, we choose a Faster R-CNN model,which is pre-trained on MS-COCO [ 13] dataset and contains classiﬁers for all\nobject classes in O. We extract parameters from the output layer of the model.\nFor each object class o\ni∈O, the parameters from the corresponding classiﬁer\nare used as the visual appearance embedding for the object class. We denote fv\ni\nas the visual appearance embedding of oi.\nSemantic Word Embedding. Word2vec [ 14] is a model for mapping words to vec-\ntors, which is widely used in natural language processing. Word vectors generated\nby the model have good semantics. In our work, we use the open source word2vecmodel [ 17] to generate embeddings for all object classes in O. We denote f\nw\nias\nthe semantic word embedding of oi.\nFunction-Driven Embedding. Function-driven embedding aims to leverage the\nobject similarity encoded in word vectors, and it uses predeﬁned HOI classesin the public dataset. In our work, we choose the predeﬁned HOI classes in the\npublic HICO-Det dataset. For each p\ni∈Pandoj∈O,i f(human,p i,oj)i s\npredeﬁned, then we consider pias a function of oj. The function-driven embed-\nding for each object is a mdimensional multi-hot vector, where 1 represents\nthe object has the function and 0 represents the opposite. We denote ff\nias the\nfunction-driven embedding of oi.\nThe object embedding is the combination of the three embeddings as follows:\nfi=(norm(fv\ni),norm (fw\ni),norm (ff\ni)) (1)\nwhere norm indicates /lscript2-normalization.\n3.3 Multi-branch Network\nAfter conducting object embedding, we use the object embeddings to measure\nthe similarity of object classes to decompose HOI classes with a same predicate\ninto diﬀerent semantic groups and merge semantic-related HOI classes. We con-\nstruct a multi-branch network, and apply multi-level sHOI classiﬁcation tasks\nwith multiple levels of decomposition with diﬀerent granularities. In the follow-\ning sections, we ﬁrst introduce the multiple levels of sHOI, then introduce thebranches of the network, and ﬁnally introduce the training and inference process.\nMultiple Levels of sHOI. After obtaining the object embedding set F=\n{f\n1,f2,...,f n}, we cluster it into kclusters using Euclidean distance. Objects in\nthe same cluster are considered highly similar, the same predicate applied to dis-\nsimilar objects is considered to have diﬀerent semantics, and it is decomposedinto diﬀerent groups. The clustering has to be done only once before train-\ning. Considering that one decomposition strategy can lead to over-decomposing\nor under-decomposing for some predicates, we select multiple kvalues, and', '430 T. Wu et al.\nadopt multiply levels of decomposition with diﬀerent granularities. We denote\nK={k1,k2,...,k b}as the set of cluster numbers, where bis the number\nof branches. For each k∈K, we construct a label transformation matrix\nTk∈ Rck∗c0where c0is the number of HOI classes and ckis the number\nof sHOI classiﬁcation of kthbranch. The tij∈Tkdenotes the ( i,j)-th entry of\nthe matrix, and tij= 1 means that the ithsHOI class contains the jthHOI class\nwhile tij= 0 means opposite.\nBranches of Network. Each branch network consists of a HGAT (HOI graph\nattention) module and two fully connected layers. The dimension of the ﬁrst\nfully connected layer is 1024, while the dimension of the second fully connected\nlayer is related to the number of sHOI classiﬁcation in the branch. The branchesare independent of each other, and the parameters of diﬀerent branches are\nnot shared. All branches share the same input. For each input image, we ﬁrst\nconstruct a graph. The graph is updated once in the backbone, and then sent tobranches and updated once again in the HGAT module of branches. The purpose\nis to generate multiple diﬀerent graphs to ﬁt diﬀerent levels of sHOI classiﬁcation\ntasks. The output of the HGAT module in each branch is a graph, which is parsedinto human-object pairs for following inferring. For each human-object pair, we\nuse a concatenation of the human node, the object node and the edge between\nthem and feed it into the two fully connected layers to predict sHOI scores. Thediﬀerences between diﬀerent branches are reﬂected in the output of the branch.\nFor each k\ni∈K,t h eithbranch outputs scores for cisHOI classes.\nTraining. During training, we use an average of sHOI losses for all branches and\nhuman-object pairs. We denote yijas the original HOI label for ithhuman node\nandjthobject node, and /hatwideyijkas the output sHOI score of kthbranch. The total\nloss function is:\nLoss=1\nnh∗no∗nknh/summationdisplay\ni=1no/summationdisplay\nj=1nk/summationdisplay\nk=1FL(Tk∗yij,/hatwideyijk) (2)\nwhere nh,noandnkis the number of human nodes, object nodes and branches,\nandFLis a sigmoid cross entropy function with focal loss [ 12].\nInference. During the inference stage, we ﬁrst parse sHOI scores into HOI scores,\nand then average them as the ﬁnal outputs for each human-object pair. Formally,the ﬁnal HOI score is represented as:\nS\nij=sh\ni∗so\nj∗1\nnknk/summationdisplay\nk=1(TT\nk∗/hatwideyijk) (3)\nwhere sh\niandso\njare the detection conﬁdence of the human and the object.\n3.4 Graph Network\nIn the graph, we use nodes to represent instances and edges to represents spatial\nlayouts. The information of instances comes from oﬀ-the-shelf detection model,', 'Multi-branch Graph Network for Learning Human-Object Interaction 431\nand has been processed to ﬁt the graph structure. In the graph update process,\nwe refer to the graph attention mechanism [ 20] and make improvements. We\ncall the updating method HGAT. We will ﬁrst introduce the process of graph\nconstruction, then introduce the HGAT module.\nGraph Construction. Given an image, we ﬁrst detect all instances in it. Each\ninstance is represented by a bounding box and a class annotation, and used to\nbuild a graph node. We use a Faster R-CNN with ResNet-50 [ 8] backbone, and\nuse the output convolutional feature map of res4 block to build nodes. For each\nbounding box, we crop the region, pass it through a ROI-Pooling layer, a resid-\nual block, and an Average-Pooling layer. The residual block is the res5 blockof ResNet-50. After these, each instance is embedded as a vector. Considering\nincorrect or incomplete detection results, we add an extra node which takes the\nentire feature map as content to ensure the integrity of the contextual informa-tion. For convenience, the extra node is considered the same as the other instance\nnodes. We add directed edges to any two nodes to form a fully connected graph.\nEach edge is a spatial embedding representing the spatial layout between theconnected node pairs. We denote B\ni=(xi\n1,yi\n1,xi2,yi\n2)a n d Bj=(xj\n1,yj\n1,xj2,yj\n2)\nas two bounding boxes of nodes, and Bu=(xu\n1,yu\n1,xu2,yu\n2) as the smallest union\nbounding box containing the two bounding boxes. Here ( x1,y1) is the top-left\ncorner coordinate and ( x2,y2) is the bottom-right corner coordinate. The spatial\nembedding of the node pair is:\nfsp=FC/parenleftBigg\nxi\n1\nWu,yi\n1\nHu,xi\n2\nWu,yi\n2\nHu,xj\n1\nWu,yj\n1\nHu,xj\n2\nWu,yj\n2\nHu,Ai\nAu,Aj\nAu,\nAi\nAI,Aj\nAI,xi\n1−xj\n1\nxj\n2−xj\n1,yi\n1−yj\n1\nyj\n2−yj\n1,logxi\n2−xi\n1\nxj\n2−xj\n1,logyi\n2−yi\n1\nyj\n2−yj\n1/parenrightBigg\n(4)\nwhere Wu,Huare the width and height of union bounding box, AiandAj\nare the area of the two bounding boxes, AuandAIare the area of the union\nbounding box and the input image. The FCrepresents two fully connected layers\nwith 256 and 512 dimensions.\nHGAT Module. Denote G=(V,E) as the graph. Vis the set containing N\nnodes, and vi∈V,i∈{1,2,...,N }is the ithnode vector. Eis the set of edges,\nandeij∈E,i∈{1,2,...,N },j∈{1,2,...,N },i/negationslash=jis the directed edge from vi\ntovjrepresented by the spatial embedding of the node pair. At each iteration k,\nwe ﬁrst compute the hidden state of nodes, then calculate an adjacency matrix,and ﬁnally use the adjacency matrix to update nodes. We denote d\nk\nvanddk\neas\nthe dimensions of nodes and edges in kthiteration. The hidden state of node is\nas:\nhk\ni=Wk\n1∗vk\ni (5)\nwhere W1∈Rdk+1\nv∗dk\nvis learnable parameters achieved through a fully connected\nlayer without bias. The calculation of the adjacency matrix Atakes node pair\nfeatures and edge features as input:\naij=leaky relu(Wk\n2∗(hk\ni,hkj,ekij)+bk\n2) (6)', '432 T. Wu et al.\nAij=exp(aij)/summationtextN\nd=1 ,d/negationslash=iexp(aid)(7)\nwhere W2∈R2dk+1\nv+dk\neandbk\n2∈R1are learnable parameters achieved through\na fully connected layer, and Aijis the ( i,j)-th entry of the adjacency matrix.\nFinally, the nodes are updated by:\nvk+1\ni=relu(Wk\n3∗vk\ni+bk\n3)+N/summationdisplay\nj=1 ,j/negationslash=irelu(Aij∗hk\nj+bk\n1) (8)\nwhere bk\n1∈Rdk+1\nvis a learnable bias, Wk\n3∈Rdk+1\nv∗dk\nvandbk\n3∈Rdk+1\nvare learnable\nparameters achieved through a fully connected layer.\n4 Experiments\nIn order to verify the eﬀectiveness of our proposed network, we evaluate it on\nthe large-scale HICO-Det dataset. In this section, we introduce experimental\ndetails including dataset and evaluate metric, implement details, comparison\nwith state-of-the-art methods and ablation study.\n4.1 Dataset and Evaluate Metric\nDataset. HICO-Det is a dataset containing 80 object classes, 117 predicate classes\nand 600 HOI classes. Each HOI is annotated with a human bounding box, anobject bounding box, and binary labels for HOI classes. It has the same 80 object\nclasses as MS-COCO.\nEvaluation Metrics. Following the standard settings in HICO-Det, we use mean\naverage precision (mAP) as the evaluation metrics. A HOI detection is considered\na true positive if both the human overlap IOU\nhand object overlap IOU oare\ngreater than 0.5. In default HOI experiments, we reported performances for three\ndiﬀerent HOI sets: (a) all 600 classes (Full), (b) 138 classes with less than 10\ntraining samples (Rare), and (c) the remaining 462 classes with more than 10training samples (Non-Rare).\n4.2 Implementation Details\nTraining Data Preparation. We use a Faster R-CNN which is pre-trained on MS-\nCOCO dataset as the object detector. All instances with conﬁdence over 0.8 are\nreserved. For each human-object pair, we match it with the HOI annotations\nin the dataset. If both the human overlap IOU\nhand object overlap IOU oare\ngreater than 0.5 and the object classes are same, we apply the annotation to the\npair. Mismatched pairs are considered to be negative samples for all HOI classes,\nand unmatched annotations are added to the training dataset as new pairs.', 'Multi-branch Graph Network for Learning Human-Object Interaction 433\nFig. 3. HOI detection results on HICO-Det test images. Human and objects\nare shown in red and blue rectangles, respectively. (Color ﬁgure online)\nNetwork. In the graph construction step, We ﬁx the Res1-4 block during training.\nFor each instance, we ﬁrst crop the region from the outputs of Res4 block, then\nresize it to 14 ∗14 and pool to 7 ∗7. All feature maps of instances are sent to\nthe following residual block, which is initialized with the parameters of the Res5\nblock. Nodes in the initial graph are of 2048 dimension, and 1024 dimension in\nfollowing graph. All αandγof focal loss are set to 0.8 and 2. In our experiments,\nwe use a four branch network, and the corresponding cluster number is set as\nK={4,7,10,13}.\nTraining. We train the network for 800K iterations on the HICO-Det training set\nwith a learning rate of 0.001, a batch size of 1, a weight decay of 0.0001 and a\nmomentum of 0.9. All experiments are implemented on a single NVIDIA 1080Ti\nGPU.\n4.3 Comparison with State-of-the-Art Methods\nWe compare our model with several state-of-the-art HOI detection approaches.\nA ss h o w ni nT a b l e 1, we achieve 20.45% for full classes, 16.78% for rare classes\nand 21.54% for non-rare classes, which outperforms all existing works. It shows\nthat our proposed method has a good learning ability for learning human-object\ninteraction. This is mainly because decomposing predicate into several seman-tic groups decrease the intra-class variance, and merging semantic-related HOI\nclasses increases the available number of training samples of each class. This not\nonly avoids the negative eﬀects for feature and predicate classiﬁer learning causedby the predicate ambiguity, but also turns ambiguity into useful information for\nenhancing model learning. Figure 3shows some HOI detection results.\n4.4 Ablation Study\nIn this section, we provide further analysis of our proposed method from the two\naspects, merging semantic-related HOI classes and the number of branches. We', '434 T. Wu et al.\nTable 1. mAP (%) in default setting for the HICO-Det dataset. Higher values indicate\nbetter performance. The best scores are marked in bold.\nMethods Full Rare Non-rare\nShen et al.[19] 6.46 4.24 7.12\nHO-RCNN + IP [ 2] 7.30 4.68 8.08\nHO-RCNN + IP + S [ 2]7.81 5.37 8.54\nInteractNet [ 6] 9.94 7.16 10.77\niHOI [ 22] 9.97 7.11 10.83\nGPNN [ 16] 13.11 9.34 14.23\nXu et al.[23] 14.70 13.26 15.13\niCAN [ 5] 14.84 10.45 16.15\nBansal et al.[1] 16.96 11.73 18.52\nGupta et al.[7] 17.18 12.17 18.68\nInteractiveness prior [ 11]17.22 13.51 18.32\nPeyre et al.[15] 19.40 15.40 20.75\nOurs 20.45 16.78 21.54\nTable 2. mAP (%) in the ablation study of our proposed method. A single branch net-\nwork without HOI merging are used as baseline. The number in () represents clusternumber, and (+) represents multi-branch.\nMethods Full Rare Non-rare\nbaseline 15.26 10.05 16.82\nbaseline+(4) 19.15 13.99 20.69\nbaseline+(4+7) 20.16 16.12 21.36\nbaseline+(4+7+10) 20.30 16.22 21.52\nbaseline+(4+7+10+13) 20.45 16.78 21.54\nuse a single branch network without HOI graph attention and HOI merging as\nbaseline. Table 2shows the details of results.\nEﬀect of Merging Semantic-Related HOI Classes. We investigate the inﬂuence of\nmerge semantic-related HOI classes on the model performance. From Table 2,w e\ncan see that this method improves 19 .15%−15.26% = 3 .89%, 13 .99%−10.05% =\n3.94% and 20 .69%−16.82% = 3 .87% on the three HOI sets. It is a signiﬁcant\nimprovement. This shows that redeﬁne the classiﬁcation classes is a key to solve\nthe HOI task.\nEﬀect of Number of Branches. We investigate the inﬂuence of the number of\nbranches. From the results in Table 2, we can see that multi-branch network\nhave better performance than single-branch network. Using multi-branch network\nachieves 20 .45%−19.15 = 1 .30%, 16 .78−13.99% = 2 .79% and 21 .54%−20.69% =', 'Multi-branch Graph Network for Learning Human-Object Interaction 435\n0.85% improvement on the three HOI sets. The improvement are considerable,\nespecially for the Rare classes. This conﬁrms our idea that combining multiplelevels of decomposition with diﬀerent granularities can provide richer auxiliary\ninformation which leads to better performance.\n5 Conclusion\nIn this paper, we propose a novel multi-branch graph network for learning human-object interaction. In order to resolve the data sparsity issue and data variance\nissue in HOI learning, we decompose HOI classes of a same predicate into severalsemantic groups and merge semantic-related HOI classes based on the appear-\nance, semantic information and function of the objects. In the graph network, we\nadopt multiple levels of decompositions in diﬀerent granularities, and each branchcorresponds to one level decomposition. We evaluate this network on HICO-Det\ndataset, and achieve state-of-the-art performance. We hope that our work can pro-\nvide reference for future research.\nAcknowledgement. This work is supported by National Key Research and Develop-\nment Project Grant, Grant/Award Number: 2018AAA0100802.\nReferences\n1. Bansal, A., Rambhatla, S.S., Shrivastava, A., Chellappa, R.: Detecting human-\nobject interactions via functional generalization. arXiv preprint arXiv:1904.03181\n(2019)\n2. Chao, Y.W., Liu, Y., Liu, X., Zeng, H., Deng, J.: Learning to detect human-object\ninteractions. In: 2018 IEEE Winter Conference on Applications of Computer Vision(WACV), pp. 381–389. IEEE (2018)\n3. Chao, Y.W., Wang, Z., He, Y., Wang, J., Deng, J.: HICO: a benchmark for recogniz-\ning human-object interactions in images. In: Proceedings of the IEEE International\nConference on Computer Vision, pp. 1017–1025 (2015)\n4. Fang, H.S., Cao, J., Tai, Y.W., Lu, C.: Pairwise body-part attention for recognizing\nhuman-object interactions. In: Proceedings of the European Conference on Com-puter Vision (ECCV), pp. 51–67 (2018)\n5. Gao, C., Zou, Y., Huang, J.B.: iCAN: instance-centric attention network for human-\nobject interaction detection. arXiv preprint arXiv:1808.10437 (2018)\n6. Gkioxari, G., Girshick, R., Doll´ ar, P., He, K.: Detecting and recognizing human-\nobject interactions. In: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 8359–8367 (2018)\n7. Gupta, T., Schwing, A., Hoiem, D.: No-frills human-object interaction detection:\nfactorization, layout encodings, and training techniques. In: Proceedings of theIEEE International Conference on Computer Vision, pp. 9677–9685 (2019)\n8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npp. 770–778 (2016)\n9. Kipf, T.N., Welling, M.: Semi-supervised classiﬁcation with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907 (2016)', '436 T. Wu et al.\n10. Krishna, R., et al.: Visual genome: connecting language and vision using crowd-\nsourced dense image annotations. Int. J. Comput. Vis. 123(1), 32–73 (2017).\nhttps://doi.org/10.1007/s11263-016-0981-7\n11. Li, Y.L., et al.: Transferable interactiveness knowledge for human-object interaction\ndetection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 3585–3594 (2019)\n12. Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll´ ar, P.: Focal loss for dense object\ndetection. In: Proceedings of the IEEE International Conference on Computer\nVision, pp. 2980–2988 (2017)\n13. Lin, T.-Y., et al.: Microsoft COCO: common objects in context. In: Fleet, D., Pajdla,\nT., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp. 740–755.Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10602-1\n48\n14. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed repre-\nsentations of words and phrases and their compositionality. In: Advances in NeuralInformation Processing Systems, pp. 3111–3119 (2013)\n15. Peyre, J., Laptev, I., Schmid, C., Sivic, J.: Detecting unseen visual relations using\nanalogies. arXiv preprint arXiv:1812.05736 (2018)\n16. Qi, S., Wang, W., Jia, B., Shen, J., Zhu, S.C.: Learning human-object interactions\nby graph parsing neural networks. In: Proceedings of the European Conference on\nComputer Vision (ECCV), pp. 401–417 (2018)\n17.ˇReh˚uˇrek, R., Sojka, P.: Software framework for topic modelling with large cor-\npora. In: Proceedings of the LREC 2010 Workshop on New Challenges for NLP\nFrameworks, pp. 45–50. ELRA, Valletta, May 2010. http://is.muni.cz/publication/\n884893/en\n18. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object detec-\ntion with region proposal networks. In: Advances in Neural Information ProcessingSystems, pp. 91–99 (2015)\n19. Shen, L., Yeung, S., Hoﬀman, J., Mori, G., Fei-Fei, L.: Scaling human-object inter-\naction recognition through zero-shot learning. In: 2018 IEEE Winter Conference onApplications of Computer Vision (WACV), pp. 1568–1576. IEEE (2018)\n20. Veliˇ ckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y.: Graph\nattention networks. arXiv preprint arXiv:1710.10903 (2017)\n21. Wang, S., Cheng, Z., Deng, X., Chang, L., Duan, F., Lu, K.: Leveraging 3D blend-\nshape for facial expression recognition using CNN. Sci. China Inf. Sci 63(2020).\nArticle number: 120114. https://doi.org/10.1007/s11432-019-2747-y\n22. Xu, B., Li, J., Wong, Y., Zhao, Q., Kankanhalli, M.S.: Interact as you intend:\nIntention-driven human-object interaction detection. IEEE Trans. Multimed.\n22(6), 1423–1432 (2019)\n23. Xu, B., Wong, Y., Li, J., Zhao, Q., Kankanhalli, M.S.: Learning to detect human-\nobject interactions with knowledge. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (2019)\n24. Yang, J., Lu, J., Lee, S., Batra, D., Parikh, D.: Graph R-CNN for scene graph gen-\neration. In: Proceedings of the European Conference on Computer Vision (ECCV),pp. 670–685 (2018)\n25. Yang, Z., Qin, Z., Yu, J., Hu, Y.: Scene graph reasoning with prior visual relationship\nfor visual question answering. arXiv preprint arXiv:1812.09681 (2018)\n26. Zhuang, B., Wu, Q., Shen, C., Reid, I., van den Hengel, A.: HCVRD: a benchmark\nfor large-scale human-centered visual relationship detection. In: Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence (2018)', 'FDEA: Face Dataset with Ethnicity\nAttribute\nJun Chen, Ting Liu, Fu-Zhao Ou, and Yuan-Gen Wang(B)\nSchool of Computer Science and Cyber Engineering, Guangzhou University,\nGuangzhou 510006, China\nwangyg@gzhu.eud.cn\nAbstract. Face attributes play an important role in face-related appli-\ncations. However, existing face attributes (such as expression, age, and\nskin color) are subject to change. The ethnicity attribute is precious due\nto its invariance over time, but has not been developed well. This is partlybecause there is no large enough dataset and labeled accurately with eth-\nnicity attribute. This paper proposes a new Face Dataset with Ethnicity\nAttribute (FDEA), intended for ethnicity recognition benchmark. Forthis purpose, we ﬁrst collect an initial face dataset from CelebA and\nLFWA [ 10], MORPH [ 13], UTKFace [ 20], FairFace [ 8], and the web. The\nsamples extracted from CelebA are not labeled with ethnicity attribute.To this end, we employ nine annotators to label these samples from\nCelebA, while cleaning the remaining samples manually. Finally, our\nFDEA contains 157,801 samples and is divided into three classes: Cau-casian (54,438), Asian (61,522), and African (41,841). Moreover, we carry\nout a benchmark experiment by testing eight mainstream backbones on\nthe proposed FDEA. The baseline results of the three-classiﬁcation accu-racy are all over 0.92. FDEA is publicly available at https://github.com/\nGZHU-DVL/FDEA .\nKeywords: Face dataset\n·Deep convolution neural network ·\nEthnicity classiﬁcation\n1 Introduction\nWith the emergence of big data and the rapid development of hardware, deep\nlearning has made tremendous progress. Deep learning algorithms have been suc-\ncessfully applied in various ﬁelds, such as video surveillance, object detection, andbiometric recognition. In biometric recognition, more and more researchers have\ndrawn their attention to face-related studies since face images contain salient\nand unique biometric information. These studies include face detection [ 4,19],\nface recognition on gender [ 1,3,5,12], face attribute classiﬁcation [ 9,16], and so\non. To enable these studies, a number of face datasets have been created, such\nas MegaFace [ 11] and IMDB-WIKI [ 14] for face recognition, and CelebA and\nLFWA [ 10], MORPH [ 13], UTKFace [ 20], FairFace [ 8], RFW [ 17]a n dW F L W\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 437–446, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_36', '438 J. Chen et al.\n(a) Asian (b) African\n (c) Caucasian\nFig. 1. Illustration of random samples from our FDEA dataset.\n[18] for face attribute classiﬁcation. All of these datasets are labeled with vari-\nous face attributes. These face attributes include age, gender, race, eyeglasses,\npointy nose, wearing lipstick, beard, narrow eyes, blurry, big lips, smiling, and so\non, which play a vital role in face-related applications. Among these attributes,the ethnicity attribute is critical due to its merit of invariance over time. Unfor-\ntunately, there is little focus on ethnic attribute to date.\nFor example, CelebA [ 10] is a large-scale dataset with 202,599 face images,\neach of which is labeled with 40 attributes, such as hair, oval face, pale skin,\npointy nose, beard, and high cheekbones. However, it lacks the ethnicity label.Unlike CelebA, the LFWA dataset [ 10] is annotated with 73 face attributes,\nincluding a speciﬁc race attribute. The non-commercial release of MORPH [ 13]\nconsists of 55,000 unique images of more than 13,000 subjects. This datasetrecords four ethnicity categories including African, European, Asian, and His-\npanic. However, the distribution of the four categories is hugely unbalanced.\nUTKFace [ 20] consists of over 20,000 face images in the wild, and is labeled\nby age, gender, and race. Their race includes White, Black, Asian, Indian, and\nOthers. We notice that this dataset contains 8,493 White and only 2,387 Asian.\nHence, it is also an unbalanced dataset. FairFace [ 8] contains 108,501 images\nwith an emphasis on balanced race composition. As far as we know, FairFace\nis the largest dataset with race annotation so far. However, the experimental\nresults on the FairFace dataset show that the accuracy of race classiﬁcation issomewhat low. This implies that the race annotation in FairFace might not be\naccurate. For the sake of comparison, the details of the above-reviewed datasets\nare illustrated in Table 1. It is essential for ethnicity classiﬁcation tasks to create\na large-scale dataset with balanced ethnicity categories and accurately annotated\nethnicity attribute. It becomes a signiﬁcant reason that motivates us to carryout this work.', 'FDEA: Face Dataset with Ethnicity Attribute 439\nTable 1. Statistics of race attribute datasets.\nDataset Source # of faces White Asian Black Balanced?\nMORPH [ 13] Public Data 55,000 Yes Yes Yes No\nIMDB-WIKI [ 14]IMDB, WIKI 523,051 No No No No\nCelebA [ 10] CelebFace, LFW 202,599 No No No No\nLFW A [ 10] LFW (Newspapers) 13,143 Yes Yes Yes No\nUTKFace [ 20] MORPH, CACD Web 23,708 Yes Yes Yes No\nFairFace [ 8] Flicker, Twitter, Web 108,501 Yes Yes Yes Yes\nThis paper proposes a new Face Dataset with Ethnicity Attribute (FDEA)\nemphasizing the amount, balance, and accurate annotation. FDEA contains\n157,801 face images, which, to our best knowledge, is the largest scale face imagedataset with ethnicity attributes. FDEA deﬁnes three ethnic groups: Caucasian\n(54,438), Asian (61,522), and African (41,841). The numbers of samples in the\nthree ethnic groups are in good balance. As shown in Fig. 1, we can observe that\nthese three ethnic groups have distinctive features. That is: 1) Asian is often\ncharacterized by yellowish buﬀ skins, straight black hair, and high cheekbones.\nIt mainly includes Southeast Asians in Southeast Asia, East Asians in the East,and Indians in the continent of North and South America. 2) Unlike Asians,\ndark skins, small curly black hair, thick and protruding lips are the signiﬁcant\nAfrican features. African is generally divided into two types: South Africa (sub-\nSaharan Africa) and North Africa. 3) The main feature with pale skins, blonde\nhair, blue eyes, and the Roman nose appears in Caucasians, who are initiallydistributed in Europe. After a long period of evolution and settlement, they\nspread to North Africa, West Asia, Central Asia, South Asia, whole Oceania,\nand North and South America. With this clear deﬁnition [ 2], our ethnicity anno-\ntation can be more accurately performed compared with the existing ethnicity\nannotations. Eight mainstream backbones are tested on FDEA and achieve over\n0.92 classiﬁcation accuracy.\nThe rest of this paper is organized as follows. In Sect. 2,w ep r e s e n tt h ec o n -\nstruction process of FDEA in detail. Section 3provides several baseline experi-\nments, followed by our conclusion in Sect. 4.\n2 Proposed FDEA\n2.1 Images Extracted from CelebA and Annotation\nIt is well known that CelebA has been widely used in face recognition. It con-\ntains many face attributes. Unfortunately, some valuable attributes are not yet\nannotated in CelebA, such as ethnicity and brown eyes. These attributes are\ngenerally very robust in an unconstrained environment. In this paper, we adoptthe manual annotation to pick out three types of face images from the CelebA\ndataset according to the deﬁnition of ethnicity attribute. These three ethnic', '440 J. Chen et al.\nAnnotate\nGoogle3:0:0 / 2:1:0CelebA FDEA\nManual VoteSearch EngineConfused Images\n1:1:1\nFig. 2. Pipeline of extracting samples from CelebA. Here, the ratio represents the\nnumber of votes that a speciﬁc ethnicity obtains. For instance, 3:0:0 and 2:1:0 indicatethat the image belongs to a particular class with a majority vote, while 1:1:1 indicates\nthat the votes of three annotators are diﬀerent from each other and thus the image is\nconsidered as a confusing image.\nclasses are Asian, African, and Caucasian. We employ nine annotators (students\nand staﬀ from the university), who have been instructed with an hour tutorial of\nfacial knowledge on ethnicity for face ethnicity annotation assignment, to clas-\nsify the face images into the most apparent one of three classes. However, it is\ntoo labor-intensive and expensive for one person to annotate the overall dataset.Therefore, we divide this dataset into three parts. Each part contains the same\nnumber of images, namely 67,533 images. Then, each part is assigned to every\nthree annotators. For each image, annotators need to take about ﬁve seconds tojudge. If three or two annotators recognize an image as the same class, this image\nis annotated as an ethnicity ground truth. If three annotators do not agree on\nthe class of an image, we use the online mapping engine to determine its classfurther. This kind of images is considered as confusing images in this paper. The\ndetailed process is shown in Fig. 2.\nFor the above-mentioned confusing images (about two thousand images), we\nfurther use the online mapping engine to judge their class. We ﬁrst upload the\nimages with 1:1:1 vote onto the Google images engine\n1to grant the identities of\nthe confusing images. Then we can further determine their ethnicity based onthe information from Wikipedia\n2. Our strategy is based on the fact that humans\nare always subjective in the classiﬁcation judgment task. In other words, there\nmust be some biases towards the ethnic class of a face image. For example, afacial image with a Roman nose may be easily divided into Caucasian, and a\nman with dark skin is naturally considered as African. In fact, the Roman nose is\nalso presented in some Asians. Therefore, the judgment of an ethnicity attribute\n1https://www.google.com/imghp .\n2https://www.wikipedia.org .', 'FDEA: Face Dataset with Ethnicity Attribute 441\n(a) (b) (c)\n (d)\nFig. 3. Wrong annotated images in three categories. Here, noise images mean non-facial\nones. We delete all of them during data cleaning. (a) Misclassiﬁed images in Asians.\n(b) Misclassiﬁed images in African. (c) Misclassiﬁed images in Caucasians. (d) Noiseimages.\nshould refer to multiple features. Besides, we think that it is not very meaningful\nif a mixed-race individual is deemed to be the fourth class (i.e. other class) exceptfor our three main classes, as done in MORPH [ 13] and UTKFace [ 20]. This is\nbecause even if a mixed-race is classiﬁed to the fourth class, we still do not know\nwhich races are mixed. In other words, the fourth class does not provide helpful\ninformation for race recognition applications. Therefore, for a mixed-race face,\nwe classify it into one of three main classes according to its salient race feature.\nFinally, we converge the three parts as the ﬁrst ingredient of our dataset with\nethnicity ground truth. The ﬁrst ingredient includes 30,152 images from CelebA,\nand the numbers of Caucasian, Asian, and African are 10,358, 9,852, and 9,924,respectively.\n2.2 Image Collection from Annotated Datasets\nSome DCNNs such as ResNet101 [ 7] and DenseNet161 are apt to result in over-\nﬁtting problems when being trained on small-scale datasets. We notice that the\nnumber of samples extracted from CelebA is insuﬃcient to train these deep and\nbroad networks. To construct a larger scale dataset for deep learning, we focus\non the annotated face datasets with race attribute. There have been some main-\nstream datasets such as LFWA [ 10], FairFace [ 8], UTKFace [ 20], and MORPH\n[13], which are all labeled with race attribute. Next, we extract face image sam-\nples from these four datasets.\nFirst of all, we collect face images from LFWA [ 10]. The images labeled\nwith White, Asian, or Black attribute are output to three categories, respec-\ntively. However, we ﬁnd that the label ﬁle provided by the authors [ 10] contains\nmany wrong labels. For instance, a face image is labeled with two diﬀerent raceattributes, and various samples of an identity are labeled with diﬀerent race\nattributes. Thus, this is not a well-labeled dataset. We seek out the identities\nof the problematic annotations, and then all the samples of these identities arediscarded. Finally, we get 6,785 face images from LFWA.\nAs done in LFWA, we then collect face images from the FairFace dataset [ 8].\nNote that all the images in FairFace have no identiﬁcations. Hence, the above-mentioned problem with LFWA will not appear in FairFace. We only need to\ncollect the images with annotations of three categories according to the label', '442 J. Chen et al.\nﬁle provided by the authors [ 8]. After collecting the samples from FairFace,\nwe further collect images from UTKFace [ 20]. Interestingly, for UTKFace, the\nlabels of each face image are written into its ﬁle name. So we can easily catego-\nrize images by their ethnicity labels. Nevertheless, some serious problems exist\nin UTKFace and FairFace, e.g. both the noise images (non-facial image) andmisclassiﬁed images are also included. We show such bad examples in Fig. 3.\nTherefore, in order to obtain a high-quality dataset, we decide to delete all these\nsamples manually. Then, we obtain 110,388 images from CelebA, LFWA, Fair-Face, and UTKFace. And the numbers of Caucasians, Asians, and Africans are\n42,622, 38,648, and 28,967, respectively. By now, we notice that the number of\nthe collected African faces is relatively tiny. Hence, we consider one more datasetcalled MORPH [ 13], which records subject ethnicity. The non-commercial release\nof MORPH dataset includes 55,000 unique images of more than 13,000 subjects.\nSince the MORPH dataset contains nearly four ﬁfths of Africans, we collect\nmore African images from MORPH. After that, we get 130,846 images from\nCelebA, LFWA, FairFace, UTKFace, and MORPH. And the numbers of Cau-casians, Asians, and Africans are 50,206, 38,799, and 41,841, respectively. Note\nthat all the datasets we used in this paper are publicly available for academic\nstudy.\n2.3 Images Crawled from the Internet\nTo make the source of face image samples more diverse, we further use crawler\ntechnology to collect more face images that are publicly released on the Internet.\nNote that until now, the number of Asians is not large enough. This is becauseboth the CelebA and LFWA datasets contain a relatively smaller number of\nAsians. Hence, in this stage, we mainly collect Asian star faces. The detailed\nstrategy is described as follows. First, we get the names of famous stars byquerying “star name” in Google search engine\n3. Then the names of Asian stars\nare manually selected as keywords. Next, we crawl star face images from Bing\nsearch engine4based on the keywords. Taking the balance of face image samples\nof each star into consideration, we choose no more than ten face images for each\nstar in a random way. After that, we manually remove the misclassiﬁed, too\nblurred, or obscured face images. In the end, we obtain 26,955 face images fromthe Internet.\nSo far, our FDEA dataset consists of 157,801 face images. The details of the\ncomposition are shown in Table 2. Note that the sizes of our collected images\nare not the same, even diﬀerent in width and height. This is inconvenient for\ndeep learning backbones. To release a more helpful dataset, the size of each face\nimage is aligned to 224 ×224, which is right the input standard of ResNet. To\nthis end, we use the package of Dlib\n5to detect the faces in images with a square\nbox, and then crop them. Finally, all the face images are cropped to 224 ×224\nsize.\n3https://www.google.com .\n4https://www.bing.com .\n5dlib.net .', 'FDEA: Face Dataset with Ethnicity Attribute 443\nTable 2. The composition of our FDEA dataset.\nDataset Caucasian Asian African Grand total\nLFWA 6,006 514 265 6,785\nUTKFace 8,493 2,336 4,124 14,953\nMORPH 7,584 151 12,874 20,609\nCelebA 10,358 9,852 9,942 30,152\nFairFace 17,765 25,946 14,636 58,347\nOur FDEA 54,438 61,522 41,841 157,801\n3 Experimental Results\nThis section shows the advantages of our dataset over the existing race face\ndatasets from the following three aspects. The compared datasets include LFWA[10], MORPH [ 13], UTKFace [ 20], and FairFace [ 8]. First of all, we demonstrate\nthe balance in the numbers of three ethnic categories. The bar chart of all the\ncompared datasets is plotted in Fig. 4(b). We can see from Fig. 4( b )t h a to u r\nFDEA obtains a more balanced ratio of numbers of three ethnic categories than\nthe other four datasets. It can also be observed that MORPH is heavily unbal-\nanced in the three categories, and the ratio of Asians is too small to be present\nin the bar chart. Besides, we can see that LFWA is highly biased towards Cau-\ncasians. Next, we show a visualization result in quantity. The bar chart is shownin Fig. 4(a). It is clear that our FDEA has much more face samples than others. A\nlarger dataset will signiﬁcantly beneﬁt the training of deeper and wider networks\n[6,15]. Finally, to provide a benchmark study, several typical networks including\nResNet18, ResNet50, ResNet101, DenseNet161, AlexNet, VGG16, MobileNet,\nand GoogleNet are selected for training and testing. Hyperparameters setup of\nvarious DCNNs are listed in Table 3. Three classiﬁcation accuracies of the four\ncompared datasets are shown in Table 4and the baseline results on the proposed\ndataset are shown in Table 5. In the experiment, stochastic gradient descent\n(SGD) optimization is adopted. Table 4shows that our FDEA obtains the best\nbalance in three classiﬁcation accuracy among all the compared datasets. We\ncan also see from Table 5that most of the mainstream DCNNs on our FDEA\ncan achieve 94% classiﬁcation accuracy. This indicates that FDEA is suitablefor the benchmark study of ethnicity classiﬁcation based on deep learning.', '444 J. Chen et al.\nTable 3. Hyperparameters setup of various DCNNs.\nDCNNs BatchSizes Learning rate Epochs\nResNet18 512 10e −3 30\nResNet50 128 10e −3 30\nResNet101 128 10e −3 30\nDenseNet161 64 10e −3 30\nAlexNet 1024 10e −3 30\nVGG16 64 10e −3 30\nMobileNet 512 10e −3 30\nGoogleNet 256 10e −3 30\nTable 4. Three categories accuracies of the compared datasets.\nDataset Caucasian Asian African\nLFWA 0.987 0.827 0.729\nUTKFace 0.972 0.889 0.939\nFairFace 0.939 0.917 0.877\nFDEA 0.915 0.960 0.949\nTable 5. Baseline results of the mainstream DCNNs on our FDEA.\nNetworks ResNet18 ResNet50 ResNet101 DenseNet161 AlexNet VGG16 MobileNet GoogleNe\nSize (MB) 44.8 94.3 170.6 107.0 228.1 228.1 9.1 22.5\nAccuracy 0.943 0.946 0.945 0.950 0.930 0.943 0.923 0.944\nFDEA FairFace UTKFace MORPH LFWA02468the number of images104\nCaucasian\nAsian\nAfrican\n(a)FDEA FairFace UTKFace MORPH LFWA020406080100RatioCaucasian\nAsian\nAfrican\n(b)\nFig. 4. (a) Illustration of the dataset scale. Here, the numbers of FDEA, FairFace,\nUTKFace, MORPH, and LFWA are 157,801, 58,347, 14,953, 20,609, and 6,785 respec-tively. (b) Illustration of Ratio of three categories in face datsets. Here, the bars from\nleft to right are FDEA, FairFace, UTKFace, MORPH, and LFWA.', 'FDEA: Face Dataset with Ethnicity Attribute 445\n4 Conclusion\nIn this paper, we have created a new Face Dataset with Ethnicity Attributes\n(FDEA). FDEA contains 54,438 Caucasian images, 61,522 Asian images, and\n41,841 African images, adding up to 157,801 face images. In addition, we test theperformance of the mainstream DCNNs on FDEA for the benchmark establish-\nment. We believe that the release of FDEA could promote the study of ethnicity\nclassiﬁcation tasks. The major contributions of the paper can be summarized asfollows: 1) To our knowledge, FEDA has been the largest scale face image dataset\nwith ethnicity attributes so far, which can help train the deeper and wider net-\nworks. 2) FDEA shows a good balance in the numbers of three categories. Thisfacilitates the establishment of an unbiased model when being trained on the\nFDEA dataset. 3) FDEA is a high-quality dataset with sophisticated annota-\ntion. For this reason, most mainstream backbones tested on the FDEA datasetachieve competitive classiﬁcation accuracy.\nIn addition, we notice that several more mixed-race classes would contribute\nto the identiﬁcation of ethnic groups. However, identifying a mixed-race class isshown to be highly complex and controversial. For instance, Chinese mixed with\nRussian look like Caucasian with Chinese features. Which kind of mixed-race\nclasses should it be identiﬁed? In this sense, considering a clear three-class is\nof great interest for an initial and at-a-glance race determination in many face-\nrelated applications. In the future, we plan to collect some mixed-race classesand conduct more comprehensive benchmarking experiments.\nAcknowledgement. The authors would like to thank Peixin Tian for his help in\nusing online mapping engine for the class judgment on confusing images. This paper issupported in part by the National Natural Science Foundation of China under Grant\n61872099, in part by the Science and Technology Program of Guangzhou under Grant\n201904010478, and in part by the Scientiﬁc Research Project of Guangzhou Universityunder Grant YJ2021004.\nReferences\n1. Aﬁﬁ, M., Abdelhamed, A.: AFIF4: deep gender classiﬁcation based on AdaBoost-\nbased fusion of isolated facial features and foggy faces. J. Vis. Commun. Image\nRepresent. 62, 77–86 (2019)\n2. Banks, M.: Ethnicity: Anthropological Constructions. Routledge, New York (1996)3. Boutellaa, E., Hadid, A., Bengherabi, M., Ait-Aoudia, S.: On the use of Kinect\ndepth data for identity, gender and ethnicity classiﬁcation from facial images. Pat-\ntern Recogn. Lett. 68, 270–277 (2015)\n4. Chaudhuri, B., Vesdapunt, N., Wang, B.: Joint face detection and facial motion\nretargeting for multiple faces. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 9719–9728 (2019)\n5. Das, A., Dantcheva, A., Bremond, F.: Mitigating bias in gender, age and ethnicity\nclassiﬁcation: a multi-task convolution neural network approach. In: Proceedings\nof the European Conference on Computer Vision Workshops (ECCVW). pp. 1–13\n(2018)', '446 J. Chen et al.\n6. Donahue, J., Ji, Y., Vinyals, O., Hoﬀman, J., Zhang, N., Tzeng, E., Darrell, T.:\nDeCAF: a deep convolutional activation feature for generic visual recognition. In:\nProceedings of the 31st International Conference on Machine Learning (ICML),pp. 647–655 (2013)\n7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), pp. 770–778 (2016)\n8. Karkkainen, K., Joo, J.: FairFace: face attribute dataset for balanced race, gender,\nand age. arXiv:1908.04913 (2019)\n9. Li, S., Deng, W., Du, J.: Reliable crowdsourcing and deep locality-preserving learn-\ning for expression recognition in the wild. In: Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR), pp. 2852–2861 (2017)\n10. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In:\nProceedings of the IEEE International Conference on Computer Vision (ICCV),pp. 3730–3738 (2015)\n11. Miller, D., Brossard, E., Seitz, S., Kemelmacher-Shlizerman, I.: MegaFace: a million\nfaces for recognition at scale. arXiv:1505.02108 (2019)\n12. Narang, N., Bourlai, T.: Gender and ethnicity classiﬁcation using deep learning in\nheterogeneous face recognition. In: Proceedings of the IEEE International Confer-\nence on Biometrics (ICB), pp. 1–8 (2016)\n13. Ricanek, K., Tesafaye, T.: MORPH: a longitudinal image database of normal adult\nage-progression. In: Proceedings of the 7th International Conference on Automatic\nFace and Gesture Recognition (FG06), pp. 341–345 (2006)\n14. Rothe, R., Timofte, R., Gool, L.V.: DEX: deep expectation of apparent age from a\nsingle image. In: Proceedings of the IEEE International Conference on Computer\nVision (ICCV), pp. 10–15 (2015)\n15. Rothe, R., Timofte, R., Gool, L.V.: Deep expectation of real and apparent age\nfrom a single image without facial landmarks. Int. J. Comput. Vis. 126, 144–157\n(2018)\n16. Sun, Y., Yu, J.: General-to-speciﬁc learning for facial attribute classiﬁcation in the\nwild. J. Vis. Commun. Image Represent. 56, 83–91 (2018)\n17. Wang, M., Deng, W., Hu, J., Tao, X., Huang, Y.: Racial faces in the wild: reducing\nracial bias by information maximization adaptation network. In: Proceedings of the\nIEEE International Conference on Computer Vision (ICCV), pp. 692–702 (2019)\n18. Wu, W., Qian, C., Yang, S., Wang, Q., Cai, Y., Zhou, Q.: Look at boundary: a\nboundary-aware face alignment algorithm. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 2129–2138 (2018)\n19. Zhang, S., Chi, C., Lei, Z., Li, S.Z.: ReﬁneFace: reﬁnement neural network for high\nperformance face detection. arXiv:1909.04376 (2019)\n20. Zhang, Z., Song, Y., Qi, H.: Age progression/regression by conditional adversarial\nautoencoder. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 5810–5818 (2017)', 'TMD-FS: Improving Few-Shot Object\nDetection with Transformer Multi-modal\nDirecting\nYing Yuan1,2,3, Lijuan Duan1,2,3(B), Wenjian Wang1,2,3, and Qing En1,2,3\n1Faculty of Information Technology, Beijing University of Technology,\nBeijing 100124, China\nljduan@bjut.edu.cn\n2Beijing Key Laboratory of Trusted Computing, Beijing 100124, China\n3National Engineering Laboratory for Critical Technologies of Information Security\nClassiﬁed Protection, Beijing 100124, China\nAbstract. Few-shot object detection (FSOD) is a vital and challenging\ntask in which the aim is to detect unseen object classes with a few anno-\ntated samples. However, the discriminative semantic information existing\nin the new category is not well represented in most existing approaches.To address this issue, we propose a new few-shot object detection model\nnamed TMD-FS with Transformer multi-model directing, where the lost\ndiscriminative information is mined by adapting multi-modal semantic\nalignment. Speciﬁcally, we transfer the multi-model information into a\nmixed sequence and map the visual and semantic information into theembedding space. Moreover, we propose a Semantic Visual Transformer\n(SVT) module to incorporate and align the visual and semantic embed-\nding. Finally, the distance in terms of the visual and semantic embeddingis minimized on the basis of the attention loss. Experimental results\ndemonstrate that the performance of the model signiﬁcantly with few\nsamples. In addition, it achieves state-of-the-art performance when theamount of samples increases.\nKeywords: Few shot object detection\n·Transformer ·Multi-modal\nsemantic fusion\n1 Introduction\nObject detection is one of the most fundamental research problems in the\nﬁeld of computer vision. By fully utilizing accurate box-level annotated train-ing data, existing state-of-the-art approaches have the ability to predict sat-\nisfying bounding boxes. However, their performance drops signiﬁcantly when\nfacing less annotated unseen classes. Consequently, few-show object detection(FSOD) [ 6,8,9,14,24,26] methods are proposed in tackling the issue. Diﬀer-\nent from the fully supervised object detection methods, the objective of FSOD\nis to utilize one or a few annotated samples to detect new classes. Most\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 447–458, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_37', '448 Y. Yuan et al.\napproaches [ 22,27] use a large number of labeled classes datasets for pre-training\nand add a few novel samples to ﬁne tune the model. In this way, the generaliza-tion ability of the model can be obtained from only a few new labeled samples.\nTMD-FS\n cupcup\ncup\n bird brid brid\nAB\nFig. 1. The variation in object detection results before and after using our TMD-FS.\nA is the embedding spaces of diﬀerent categories without the guidance of multi-modalinformation, and B is the embedding space using TMD-FS.\nIt is observed that the key reason for misclassiﬁcation and missed detection in\nFSOD is that only a few samples of each category cannot cover all the attributes\nof this category. Most of the existing work [ 22,27] tends to learn the common\nattributes of categories on a large number of base class samples, and then transfer\nthem to the novel class samples during the ﬁne-tuning stage. In this way, unseen\nnovel classes can be identiﬁed. However, due to the visual diversity of the novelclass between test stage and ﬁne-tuning stage, it is easy to cause class confusion.\nAs shown in Fig. 1, when we only use TFA [ 22] to ﬁne tune the model, the result\nin A shows that there is intersection between diﬀerent classes in embeddingspace. To solve this issue, we introduce semantic information to supplement\nvisual information. The experimental results show that this method can better\ndistinguish diﬀerent categories.\nInspired by [ 19,25], word embedding that learned from hundreds of textual\ndatasets is applied as semantic information, which could measure the semantics\nof various categories eﬀectively. We map visual features and semantic featuresinto the same space by transferring them into the mixed sequence. In order to\nmake better use of semantic information to make up for the lack of visual infor-\nmation, we introduced Transformer [ 21] to integrate multi-modal information.', 'TMD-FS: Improving FSOD with Transformer Multi-modal Directing 449\nThe multi-modal information of the mixed sequence is aggregated into fusion\nblock by SVT. Meanwhile, in order to make the word vector guide the visualinformation as desired, we add an attention loss to guide the learning of SVT.\nThe contributions of this paper are as follows:\n•Proposing a novel few-shot object detection model named TMD-FS by adapt-\ning multi-modal semantic alignment for mining the discriminative informa-\ntion.\n•Proposing a Semantic and Visual Alignment module (SVA) and a Semantic\nVisual Transformer module (SVT) to fuse and align the visual and semantic\ninformation. Proposing an attention loss to constrain the visual and semantic\nembedding.\n•Gaining outstanding performance using the TMD-FS on two datasets: PAS-\nCAL VOC dataset [ 5] and Microsoft COCO dataset [ 11].\n2 Related Work\nAt present, the previous few-shot object detection is mostly based on the tradi-\ntional object detection models, such as SSD [ 12], YOLO series [ 1,15–17], Faster\nRCNN [ 18] and DETR [ 2,28] etc. FSOD are mostly based on meta learning\nmethod and ﬁne-tuning method. Meta learning [ 20,23] mainly constructs sup-\nport set and query set to form meta task when data is loaded. The model trainedon a large number of meta tasks has generalization ability when facing the meta\ntask composed of novel classes. Meta R-CNN [ 26] combines Faster RCNN [ 18]\nwith meta learning [ 20] to locate and classify. Due to the large maneuverabil-\nity of the ﬁne-tuning method [ 22,24], great progress has been made at present.\nTFA [ 22] makes the ﬁne-tuning approach for FSOD go beyond the meta learning\nmethod. [ 22] keeps the entire feature extractor of Faster RCNN [ 18] ﬁxed dur-\ning the ﬁne-tuning stage, only ﬁne-tuning the box classiﬁcation and regression\nnetworks. It can be found that the method of ﬁne-tuning by freezing parameters\ncan achieve higher accuracy, and the operation is relatively simple. Therefore,we use the method of ﬁne-tuning to train TMD-FS.\n3 Methodology\nIn this section, we mainly introduce the proposed TMD-FS based on Trans-former [ 21] with multi-modal information fusion. TMD-FS is built on Faster\nRCNN [ 18], as shown in Fig. 2. Firstly, in order to align the visual feature and\nthe semantic feature, we design the Semantic and Visual Alignment (SVA) mod-\nule. SVA maps the base feature obtained by backbone and the word embeddingto the same space to obtain a mixed sequence. Then, Semantic Visual Trans-\nformer (SVT) is the fusion module that converges the multi-modal information\nof the mixed sequence into the fusion block. The RPN module processes theV-S feature generated by the fusion block to generate regional proposals, which\ngenerates the ROI feature through the ROI module. Finally, the ROI feature is\nclassiﬁed and regressed by connecting two full connection layers.', '450 Y. Yuan et al.\nBackbone \nBase FeatureSV A\nInput Image V-S Feature Regressor  ROI  RPN  Classifier  \nSVTSelf  AttentionFeed Forward\nMixed SequenceFusion Block\nhhhNew Fusion Block\nFig. 2. The framework of TMD-FS\n3.1 FSOD Fine-Tuning\nOur TMD-FS is based on a ﬁne-tuning method. We pre-train the network on\nan abundant labeled dataset Dbase, which derives from the public dataset and\ncontains only base classes. The pre-training stage is similar to the traditional\nobject detection training. After that, the model initially has the ability to extractand classify detection boxes. Then the model is ﬁne-tuned using a few-shot\ndataset D\nnovel, which contains not only the pre-trained base classes but also\nthe unseened novel classes. Because the base class instances are far more thanthe novel class instances, we randomly sampled Kinstances from each base\nclass and novel class respectively for Kshot ﬁne-tuning, Kis 1, 2, 3, 5, 10.\nInspired by [ 22], the backbone, RPN and ROI module parameters are class-\ngeneric. Hence, we frozen them during the ﬁne-tuning stage and only ﬁne tune\nthe SVT and the last layers of the detection model (the box classiﬁcation and\nregression networks).\nWord Blocks...\nMixed Sequence{"dog", "cat", ... ,"person"}\nVisual Blocks\nFig. 3. SVA module', 'TMD-FS: Improving FSOD with Transformer Multi-modal Directing 451\n3.2 Semantic and Visual Alignment (SV A)\nIn order to align visual and semantic information eﬀectively, we map them into\nthe same space. The Semantic and Visual Alignment (SVA) module is shown\nin Fig. 3. Firstly, we get word embeddings S∈RN×Cfrom hundreds of text\ndatasets by using GloVe [ 13], where Nis the number of classes(including back-\nground to help distinguish objects from background), Cis word embedding\ndimension. In order to acquire word blocks WS∈RN×(12×D), where 1 ×1i s\nthe size of a word block and D is channel of base feature, we use the full connec-\ntion layer to map CtoD. Then, we take the base feature V∈RD×H×Wobtained\nby backbone as the visual feature, where D,Hand Wrepresent channel, height\nand weight respectively. We can obtain visual blocks VS∈RP×(12×D)by rear-\nranging V, P=H×Wis the number of visual blocks. Finally, we concatenate\nWSand VSto get a mixed sequence M∈R(P+N)×D.\nThe mixed sequence obtained above is similar to the token embedding of\nVIT [4] network. The diﬀerence is that we do not process the 2D image directly\nbut handle the base feature obtained through the backbone. Because the base\nfeature already contains the location attribute, linear mapping or location infor-mation in VIT [ 4] network is not necessary. In this way, the mixed sequence is\ntransferred to SVT (Semantic Visual Transformer) module for further attention\nfusion.\n3.3 Semantic Visual Transformer (SVT)\nIn order to make word embedding more targeted to guide the visual feature,\nwe propose a Semantic Visual Transformer (SVT) module for attention fusion.\nAs shown in Fig. 4, we add an additional fusion block F∈R\n1×(12×D)to aggre-\ngate all the visual features and word embeddings information. Meanwhile, the\nmixed sequence Mobtained by the SVA module combined with Fis normal-\nized and input into multi-head self-attention together. The number of heads\nof self-attention is adjusted according to the variable parameter h. Then, the\nself-attention module linearly transforms the mixed sequence and Finto Q,K,\nVaccording to the variable parameter d.Qand Kare used to generate the\nattention value Aa c c o r d i n gt oE q .( 1). Finally, Adot Vfor attention fusion.\nA=sof tmax (QKT\\√\nd) (1)\nThe ultimate output of self attention is skip connected with the normalized\nmixed sequence and F. Then, it is transferred to the feedforward layer. The\nfully connected layer of Gelu activation function is used twice in the feedforward\nlayer. Meanwhile, the output and input of feedforward layer adopt the same skipconnection as before. The above operation is performed h times, we get the new\nmixed sequence ˆMand the new ˆFwith multi-modal information. Finally, we\nseparate ˆFfrom the mixed sequence ˆMand repeat it to the size of the base\nfeature R\nD×H×W, which is the V-S feature in Fig. 2.\nSVT acquires the attention weight according to the advanced visual feature\nextracted from the image and the guidance information of word embeddings.', '452 Y. Yuan et al.\n MCONCATSoftmaxQ\nLinear \nTransformationKV\nReshapeSelf\nAttention\nAdd &NormLinearLinear\nGELU\nFeed\nForward\nAdd &Norm\n....Attention \nLossAF\nV-S  feature...\n×h\nFig. 4. SVT module\nThen, it adjusts all the blocks in the mixed sequence and Fadaptively. Finally,\nthe multi-modal information in the mixed sequence is integrated into ˆF. The V-S\nfeature obtained by ˆFcontains both advanced visual information and semantic\nguidance.\n3.4 Loss\nAlthough we introduce SVT to fuse visual and semantic information, it is diﬃcult\nto control the learning direction of the model by simply integrating them. Inorder to make the word embedding guide the visual information according to\nthe desired result, we propose the attention loss L\nattention of multi-modal fusion.\nSpeciﬁcally, we map the fusion block ˆFinto a vector with dimension N(the\nnumber of classes), that is ˆF=[f1,f2, ..., f N]. The label is Y=[y1,y2, ..., y N].\nThen, the cross entropy loss function is used to calculate attention loss seen as\nEq. (2).\nLattention =−N/summationdisplay\nn=1ynlog(fn) (2)\nThe total loss function is shown in Eq. ( 3).\nL=Lrpn+Lcls+Lreg+Lattention (3)\nThe ﬁrst three are RPN loss, classiﬁcation loss and regression loss of Faster\nRCNN [ 18] respectively.', 'TMD-FS: Improving FSOD with Transformer Multi-modal Directing 453\n4 Experiments\nIn this section, we validate TMD-FS on PASCAL VOC [ 5] and COCO [ 11]\ndatasets. By comparing with previous methods, our approach can reach the\ncurrent high precision. Especially when the number of samples is rare, such as\n1 shot and 2 shot, our method is more eﬀective than most. Then we performablation experiments and visualization.\n4.1 Implementation Details\nOur model is based on Faster R-CNN [ 18], using ResNet-101 [ 7] and Feature\nPyramid Network [ 10] as the backbone with SGD. In SVA, we choose F6 feature\nvector of FPN as visual feature and acquire the word embedding from abundant\ntext datasets by using GloVe [ 13]. For self attention in SVT, we set the number\nof heads is 3, h is 1 and d is 256. In the pre-training phase, we follow the same\ntraining method as TFA [ 22]. In the few-shot ﬁne-tuning phase, we add the SVT\nmodule and Attention Loss to train together. All the experiments are carriedout using two GPUs with the batch size as 16, the learning rate as 0.008, the\nmomentum as 0.9 and the weight decay as 0.0001.\n4.2 Existing Benchmarks\nResults on PASCAL VOC. For PASCAL VOC dataset, we use voc 2007\nand 2012 train sets for training and 2007 test set for testing including twenty\nclasses. Fifteen classes are divided into base classes, while the other ﬁve are novel\nclasses. We divide the novel classes into three splits according to [ 22], and we\nconducted diﬀerent kshot experiments on these three splits respectively and\ncompared them with the previous work [ 3,8,9,22–24,26].kcould be selected\nfrom 1,2,3,5,10. The speciﬁc experimental results are shown in Table 1.\nTable 1. FSOD performance (mAP50) on three splits of PASCAL VOC dataset\nMethod \\shot Novel split 1 (%) Novel split 2 (%) Novel split 3 (%)\n1 2 3 5 10 1 2 3 5 10 1 2 3 5 10\nFSRW [ 8] 14.8 15.5 26.7 33.9 47.2 15.7 15.3 22.7 30.1 40.5 21.3 25.6 28.4 42.8 45.9\nMetaDet [ 23] 18.9 20.6 30.2 36.8 49.6 21.8 23.1 27.8 31.7 43.0 20.6 23.9 29.4 43.9 44.1\nMeta R-CNN [ 26]19.9 25.5 35.0 45.7 51.5 10.4 19.4 29.6 34.8 45.4 14.3 18.2 27.5 41.2 48.1\nRepMet [ 9] 26.1 32.9 34.4 38.6 41.3 17.2 22.1 23.4 28.3 35.8 27.5 31.1 31.5 34.4 37.2\nAttFDNet [ 3] 29.6 34.9 35.1 – – 16.0 20.7 22.1 – – 22.6 29.1 32.0 – –\nTFA [ 22] 39.8 36.1 44.7 55.7 56.0 23.5 26.9 34.1 35.1 39.1 30.8 34.8 42.8 49.5 49.8\nMPSR [ 24] 41.7 – 51.4 55.2 61.8 24.4 – 39.2 39.9 47.8 35.6 – 42.3 48.0 49.7\nOurs 44.6 45.6 43.9 55.8 56.3 24.8 27.5 34.7 35.6 39.3 35.8 37.3 42.9 49.5 50.3\nAs can be seen from Table 1, when the number of samples is fewer, our method\ncan improve the detection performance signiﬁcantly. It is 5–9% higher than the\nbaseline [ 22] and has a greater advantage than other methods, which is consistent', '454 Y. Yuan et al.\nwith our previous analysis. When samples are rare, the discriminative semantic\ninformation existing in the novel class is not well represented. We use multi-modal semantic information to supplement lost discriminative information, so\nas to distinguish the diﬀerent categories. As the number of samples gradually\nincreases (e.g., 10 shot), visual features are already representative to a certainextent. The network has preliminarily been able to identify this category by\ngathering the information of 10 images. Therefore, the improvement of detection\nperformance with TMD-FS is more obvious in fewer samples.\nWe further verify whether the model forgets the representation of the base\nclass after the transfer learning of the novel class. We ﬁnd that the performance\nof the model on the base class did not decrease signiﬁcantly after adding novelclasses for ﬁne-tuning. This shows that TMD-FS is stable.\nResults on COCO. COCO dataset has 80 classes. We chose twenty classes\nthat overlap with VOC as novel classes and the others as base classes. Meanwhile,\nwe follow the same training strategy as [ 22], choosing k = 10, 30 for comparison,\nTable 2shows that our method has been improved compared with previous work.\nTable 2. FSOD performance on COCO dataset\nMethod Novel AP (%) Novel AP75 (%)\n10 shot 30 shot 10 shot 30 shot\nFSRW [ 8] 5.6 9.1 4.6 7.6\nMetaDet [ 23] 7.1 11.3 6.1 8.1\nMeta RCNN [ 26]8.7 12.4 6.6 10.8\nTFA [ 22] 10.0 13.7 9.3 13.4\nMPSR [ 24] 9.8 14.1 9.7 14.2\nOurs 10.6 14.3 9.5 13.9\nBecause the COCO dataset is relatively complex, it is more diﬃcult to detect\nobjects with only a small number of samples. However, as shown in Table 2,o u r\nmethod is still eﬀective. This indicates that when faced with more complexscenes, the problem of insuﬃcient class information can be alleviated by the\nassistance of multi-modal information.\n4.3 Ablation Study\nIn this part, we examine the contribution of each module. Our experiment is\nbased on 1 shot of novel split1 on Pascal VOC dataset. We use TFA [ 22]a s\nbaseline and add SVT with L\nattention progressively to compare the eﬀects of\neach module. The results are shown in Table 3. Then, we compare the fusion\nstrategy of word vectors and visual features with general attention and SVT,\nwhich are shown in Table 4.', 'TMD-FS: Improving FSOD with Transformer Multi-modal Directing 455\nTable 3. The ablation experiment compares the eﬀects of each module\nMethod \\module SVT Lattention mAP50 (%)\nBaseline (TFA) [ 22]/enc-37/enc-37 39.8\nOurs /enc-33/enc-37 43.8\nOurs /enc-33/enc-33 44.6\nResults in Table 3conﬁrm that the two modules are valid. When we fuse\nthe word vector and the visual feature through SVT module, the accuracy rate\nincreased by 4%, which indicates that the multi-modal information fusion canimprove the discrimination error of FSOD eﬀectively. Besides, it could avoid the\nconfusion of multiple classes, and make sure the supplement of word vector for\nfew-shot information is correct. The performance of the model with L\nattention\nis further improved, which certiﬁcates that the addition of loss in the process of\nmultimodal information fusion could constrain the learning direction of fusion\nfeatures.\nTable 4. The inﬂuence of diﬀerent ways of attention fusion on experimental results\nAttention method Without attention General attention SVT\nmAP50 (%) 39.8 41.9 44.6\nIn Table 4, the general attention method maps visual features to seman-\ntic space simply. Then, it generates attention weights with semantic features\n(word embedding) to further obtain fusion features for mapping them back tovisual space. We contrast the general attention method with SVT, and ﬁnd\nthat SVT can get better results. Speciﬁcally, SVT carries out similarity search\nbetween visual blocks and word blocks. In this way, the multi-modal informa-tion is gathered into the fusion block. The similarity matching between blocks is\nmore eﬀective than the general attention fusion method to supplement the lost\nidentiﬁcation information.\n4.4 Visualization\nWe visualize the 1 shot results of TMD-FS and baseline [ 22] on novel split 1 of\nPascal VOC dataset, and compare them with ground truth. As shown in Fig. 5,\nsection A and section B are visualizations of part of the image using baseline [ 22]\nand TMD-FS respectively. C shows the ground truth of images. In the ﬁrst threegroups of images, section A can not detect the novel object in the image but\nsection B can. In the last two groups of images, section A mistakenly identiﬁes\nthe novel object into other categories. On the contrary, section B distinguishes', '456 Y. Yuan et al.\nthe category correctly. This is because multi-modal information can pull apart\nimages which are similar but belong to diﬀerent categories, which makes up forthe lack of visual information when there are few samples.\nAB C\nFig. 5. Visualization of baseline, TMD-FS and ground truth.', 'TMD-FS: Improving FSOD with Transformer Multi-modal Directing 457\n5 Conclusion\nIn this paper, we propose a few-shot object detection model TMD-FS based on\nTransformer with multi-modal information fusion. The gaps of few-shot visual\ninformation are ﬁlled through multi-modal semantic information. We design SVAand SVT module to align and incorporate the visual and semantic embedding.\nCompared with others, our work is particularly eﬀective on rare samples. We\nhope the proposed method could assist in this ﬁeld.\nAcknowledgement. The research is partially sponsored by The Beijing Municipal\nEducation Commission Project (No. KZ201910005008), The National Natural Science\nFoundation of China (No.62176009).\nReferences\n1. Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M.: YOLOv4: optimal speed and accuracy\nof object detection. arXiv preprint arXiv:2004.10934 (2020)\n2. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\nto-end object detection with transformers. In: Vedaldi, A., Bischof, H., Brox, T.,\nFrahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12346, pp. 213–229. Springer, Cham\n(2020). https://doi.org/10.1007/978-3-030-58452-8 13\n3. Chen, X., Jiang, M., Zhao, Q.: Leveraging bottom-up and top-down attention for\nfew-shot object detection. arXiv preprint arXiv:2007.12104 (2020)\n4. Dosovitskiy, A., et al.: An image is worth 16 ×16 words: transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929 (2020)\n5. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The Pascal\nvisual object classes (VOC) challenge. Int. J. Comput. Vis. 88(2), 303–338 (2010).\nhttps://doi.org/10.1007/s11263-009-0275-4\n6. Fan, Q., Zhuo, W., Tang, C.K., Tai, Y.W.: Few-shot object detection with\nattention-RPN and multi-relation detector. In: Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pp. 4013–4022 (2020)\n7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npp. 770–778 (2016)\n8. Kang, B., Liu, Z., Wang, X., Yu, F., Feng, J., Darrell, T.: Few-shot object detection\nvia feature reweighting. In: Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pp. 8420–8429 (2019)\n9. Karlinsky, L., et al.: RepMet: representative-based metric learning for classiﬁcation\nand few-shot object detection. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 5197–5206 (2019)\n10. Lin, T.Y., Doll´ a r ,P . ,G i r s h i c k ,R . ,H e ,K . ,H a r i h a r a n ,B . ,B e l o n g i e ,S . :F e a t u r e\npyramid networks for object detection. In: Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pp. 2117–2125 (2017)\n11. Lin, T.-Y., et al.: Microsoft COCO: common objects in context. In: Fleet, D.,\nPajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp.\n740–755. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10602-1\n48\n12. Liu, W., et al.: SSD: single shot multibox detector. In: Leibe, B., Matas, J., Sebe,\nN., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9905, pp. 21–37. Springer, Cham\n(2016). https://doi.org/10.1007/978-3-319-46448-0 2', '458 Y. Yuan et al.\n13. Pennington, J., Socher, R., Manning, C.D.: GloVe: global vectors for word repre-\nsentation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pp. 1532–1543 (2014)\n14. Perez-Rua, J.M., Zhu, X., Hospedales, T.M., Xiang, T.: Incremental few-shot\nobject detection. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 13846–13855 (2020)\n15. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: uniﬁed,\nreal-time object detection. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 779–788 (2016)\n16. Redmon, J., Farhadi, A.: YOLO9000: better, faster, stronger. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 7263–7271(2017)\n17. Redmon, J., Farhadi, A.: YOLOv3: an incremental improvement. arXiv preprint\narXiv:1804.02767 (2018)\n18. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object\ndetection with region proposal networks. In: Advances in Neural Information Pro-\ncessing Systems, vol. 28, pp. 91–99 (2015)\n19. Schwartz, E., Karlinsky, L., Feris, R., Giryes, R., Bronstein, A.M.: Baby\nsteps towards few-shot learning with multiple semantics. arXiv preprint\narXiv:1906.01905 (2019)\n20. Sun, Q., Liu, Y., Chua, T.S., Schiele, B.: Meta-transfer learning for few-shot learn-\ning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 403–412 (2019)\n21. Vaswani, A., et al.: Attention is all you need. In: Advances in Neural Information\nProcessing Systems, pp. 5998–6008 (2017)\n22. Wang, X., Huang, T.E., Darrell, T., Gonzalez, J.E., Yu, F.: Frustratingly simple\nfew-shot object detection. arXiv preprint arXiv:2003.06957 (2020)\n23. Wang, Y.X., Ramanan, D., Hebert, M.: Meta-learning to detect rare objects. In:\nProceedings of the IEEE/CVF International Conference on Computer Vision, pp.9925–9934 (2019)\n24. Wu, J., Liu, S., Huang, D., Wang, Y.: Multi-scale positive sample reﬁnement for\nfew-shot object detection. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.)ECCV 2020. LNCS, vol. 12361, pp. 456–472. Springer, Cham (2020). https://doi.\norg/10.1007/978-3-030-58517-4\n27\n25. Xing, C., Rostamzadeh, N., Oreshkin, B., Pinheiro, P.O.O.: Adaptive cross-modal\nfew-shot learning. In: Advances in Neural Information Processing Systems, vol. 32,\npp. 4847–4857 (2019)\n26. Yan, X., Chen, Z., Xu, A., Wang, X., Liang, X., Lin, L.: Meta R-CNN: towards gen-\neral solver for instance-level low-shot learning. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 9577–9586 (2019)\n27. Yang, Z., Wang, Y., Chen, X., Liu, J., Qiao, Y.: Context-transformer: tackling\nobject confusion for few-shot detection. In: Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, vol. 34, pp. 12653–12660 (2020)\n28. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable DETR: deformable\ntransformers for end-to-end object detection. arXiv preprint arXiv:2010.04159\n(2020)', 'Feature Matching Network\nfor Weakly-Supervised Temporal Action\nLocalization\nPeng Dou, Wei Zhou, Zhongke Liao, and Haifeng Hu(B)\nSchool of Electronics and Information Technology, Sun Yat-sen University,\nGuangzhou 510006, Guangdong Province, China\n{doup,zhouw75,liaozhk5 }@mail2.sysu.edu.cn, huhaif@mail.sysu.edu.cn\nAbstract. Weakly supervised temporal action localization needs to get\nthe category of the action as well as the time period when the action\nof corresponding category occurs with only video level annotation dur-ing training. Currently, how to eﬀectively localize the actions with weak\ndiscriminative information and distinguish background activities is the\nmajor challenge. To address these issues, we propose a feature matchingnetwork (FMNet) that can produce attention scores and perform diﬀer-\nent operations on them to represent diﬀerent actions and background\nfeatures. Moreover, we modify the cross-entropy loss to match diﬀerentfeatures. Experimental results on two standard datasets THUMOS14 and\nActivityNet1.2 show the superiority of our methods.\nKeywords: Weakly-supervised action localization\n·Feature matching\nmechanism ·Attention\n1 Introduction\nIn the case of using video classiﬁcation label only, the weakly-supervised tempo-\nral action localization (WTAL) task not only needs to determine the category\nof the action instance in the videos, but also needs to localize the time when thecorresponding action instance occurs. Since only classiﬁcation labels are needed,\nthe weakly-supervised methods greatly reduce the labeling cost compared to the\nfully-supervised methods.\nThe previous weakly-supervised temporal action localization methods utilize\nmultiple instance learning (MIL) [ 11,17,22] to solve this problem. These meth-\nods ﬁrst divide the video into segments, and then generate the segment-levelclass scores to obtain the class activation sequence (CAS). Finally, the top-k\nmechanism is used to combine the time series to get the video-level class scores.\nHowever, the methods based on multiple instance learning has a signiﬁcant prob-lem: when performing temporal action localization, the network can only locate\nthe highly discriminative actions, while others with lower discriminativeness are\nignored.\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 459–471, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_38', '460 P. Dou et al.\nBackground Discrimina Ɵve AcƟonWeak Discrimina Ɵve\n AcƟonBackgroundWeak Discrimina Ɵve\n AcƟon\nFig. 1. Examples of challenges faced by the MIL framework. Both weak discriminative\nand strong discriminative actions needs to be captured. However, the MIL frameworkonly has a good performance in capturing strong discriminative actions.\nA ss h o w ni nF i g . 1, the multiple instance learning methods can only capture\nthe most discriminative parts of the action, making the temporal predicitions\nincomplete. Moreover, the existing methods commomly fail to model the back-ground well, which makes the background segments more easy to be misclassiﬁed\nas actions.\nTo address the above limitations, we propose a feature matching network\n(FMNet) for weakly-supervised temporal action localization task. The network\nis divided into classiﬁcation branch and attention branch. The attention branch\npredicts the attention value in the time dimension, and then performs four dif-ferent strategies to obtain four types of attentions scores to represent diﬀerent\naction or background features. In particular, these four attention scores modu-\nlate the CAS generated by the classiﬁcation branch, and ﬁnally are aggregatedin the time dimension to obtain the video-level class score. Since the four atten-\ntion scores can represent four diﬀerent features, we design diﬀerent loss functions\nbased on the cross-entropy loss, which can learn diﬀerent features in a targetedmanner.\nIn summary, the main contributions in this work are as follows: (i) We design\na multi-attention branch to allow the network to focus on diﬀerent parts of the\naction in the videos. (ii) We modify the cross-entropy loss so that the features\nof each branch of attention can be learned speciﬁcally. (iii) Extensive experi-ments on two benchmark datasets on the THUMOS14 [ 6] and ActivityNet [ 2]\ndemonstrate the superiority of the FMNet model. Speciﬁcally, our FMNet model\nachieves 32.7% mAP when the IoU threshold on the THUMOS14 dataset is 0.5.When the IoU threshold on the ActivityNet1.2 data set is 0.75, 25.2% mAP is\nachieved.\n2 Related Work\nFully-Supervised Temporal Action Localization. The fully-supervised\naction localization methods are mainly divided into two categories. The ﬁrst cat-\negory is proposal-based methods. In order to generate proposals, sliding window\ntechnology is mainly used [ 4,24,26,29,33]. The second category is frame-based\nmethods, including [ 1,12–16,37], which directly predicts the frame-level action\ncategory and location. Recently, some graph structure methods [ 31,36]a r ep r o -\nposed. Since the time nodes of the beginning and ending of each action instance', 'FMNet for Weakly-Supervised Temporal Action Localization 461\nneed to be accurately labelled, the annotation cost of fully supervised methods\nare very expensive.\nWeakly-Supervised Temporal Action Localization. Compared with the\nfully-supervised methods, the weakly-supervised methods only use video-level\naction category labels, which greatly reduce the cost of annotating. In particular,weakly-supervised temporal action localization methods are mainly divided into\ntwo categories. The ﬁrst kind of methods [ 18,20,21,32,35] leverage foreground-\nbackground separation attention branches to construct video-level features, andthen apply the action classiﬁer to recognize the videos. The second kind of meth-\nods [11,17,22,34] treat the problem as a multi-instance learning task [ 39], that\ntreat the untrimmed video as a bag containing positive and negative instances.They ﬁrst obtain the class activation sequence (CAS), and then use the top-k\nmechanism to obtain the video-level classiﬁcation score.\nCurrently, in order to make the performance of weakly-supervised methods\nclose to that of fully-supervised methods, some works [ 10,13] attempt to model\nthe action completeness. Moreover, HAMNet [ 7] proposes a hybrid attention\nmechanism, which separately processes the foreground, background, and dis-criminative action features. However, all branches of HAMNet are optimized\nwith the loss function in the form of cross entropy, which is not conductive\nto learning each type of feature. To address these issues, we modify the cross-\nentropy loss to ﬁt the characteristics of each type of feature. In the meanwhile,\nwe design four attention branches to model more complete actions.\n3 Network Structure\n3.1 Problem Statement\nFor a training video Vcontaining several action instances, each instance belongs\nto one of ncaction classes. Denote these action instances as y∈{0,1}nc, where\nyj= 1 only if there is at least one instance of the jthaction class in the video,\nandyj= 0 if there is no instance of the jthaction. Given such a training video,\nwe need to predict which action appears in the video as well as the temporal\nlocalization of its instance, i.e., to output ( ts,te,ψ,c), where cis the action cate-\ngory,ts,te,ψare the start frame, end frame and prediction score corresponding\nto action crespectively.\n3.2 Network Structure\nThe overall framework of our feature matching network (FMNet) is shown in\nFig.2. Following the two-stream strategy [ 3,5] for action recognition, we extract\nsnippet-level features for both the RGB and ﬂow streams. Specially, we divideeach video into several non-overlapping snippets. These snippets are sent into\na two-stream I3D extractor to obtain RGB features and ﬂow features, both of\nwhich are 1048 dimensions. The two features are concatenated to a 2048 ×T\nv', '462 P. Dou et al.\nFig. 2. Overall framework of our proposed FMNet architecture. RGB features and ﬂow\nfeatures are extracted by the I3D extractor, then concatenated together and sent to the\nclassiﬁcation branch and the attention branch. The attention branch calculates four\nattention scores and uses diﬀerent loss functions for training.\ndimensional feature, where Tvrepresents the number of segments in the video.\nThe concatenated features are sent to the classiﬁcation branch and the attention\nbranch to localize the start and end time of the action.\nFollowing the previous strategy, the output of the classiﬁcation branch is the\nclass activation sequence (CAS) [ 25]. The class activation sequence is a snippet-\nlevel representation, and each snippet has c+ 1 CAS. Formally, c+ 1 means c\naction categories and one background, and the CAS of the ithsnippet is rep-\nresented by si∈ Rc+1. In order to integrate the snippet-level representation\ninto the video-level scores, we adopt a top-k strategy for the CAS value of each\ncategory [ 22]:\nvj= max\nl⊂1,2,...,T\n|l|=k1\nk/summationdisplay\ni∈lsi(j) (1)\nwhere j=1,2,...,c + 1. Then, we exploit the softmax to vjalong the category\ndimension to get the class score of the video level:\npj=exp (vj)/summationtextc+1\nj/prime=1exp (vj/prime)(2)\nFinally, we calculate the cross-entropy loss between the real video-level class\nscore yand the predicted score pto obtain the classiﬁcation loss of the classiﬁ-\ncation branch:\nLCLSL=−/summationtextc+1\nj=1yjlog (pj) (3)\n3.3 Attention Branch\nForeground Attention Score. We denote the score obtained through the\nattention branch as ai. Since the aicorresponding to the background snippets\nare low or even almost zero, we can use aito initially represent the foreground\nattention score. In order to establish a connection between aiand category, we', 'FMNet for Weakly-Supervised Temporal Action Localization 463\nmultiply the CAS si(j)a n d aito obtain the foreground snippet-level class score\nsfore\ni(j)=si(j)⊗ai. Following Eqs. 1and2, we change the variable si(j)t o\nsfore\ni(j). After the top-k strategy and softmax calculation, we get the substitute\npfore\njofpj. Afterwards, following Eq. 3, we calculate the cross-entropy loss.\nSince we focus on the foreground, we only use the foreground label yf\njwhen\ncalculating the cross entropy, i.e., when j=c+1 ,yf\nc+1= 0. For video-level\nannotation, the number of background categories is much greater than thatof action instance categories. After removing the background categories, the\nnetwork can no longer learn background information, we re-weight the instance\nlabels to reduce possibility of the network learning mistake information:\ny\nf/prime\nj=( 1−/epsilon1)yf\nj+/epsilon1/K (4)\nwhere /epsilon1=0.005,k= 20. Foreground attention loss is as follows:\nLFAL=−/summationtextc+1\nj=1yf/prime\njlog(pfore\nj) (5)\nWeak Discriminative Attention Score. The foreground attention score ai\nincludes the foreground with strong discrimination and the foreground with weak\ndiscrimination. By setting the threshold γ, we keep the foreground with relatively\nweak discrimination and screen out the foreground with strong discrimination:\naweak\ni=/braceleftbigg\nai,i f a i<γ\n0, otherwise(6)\nwhere aweak\niis the attention score with only weak discriminative foreground,\nandγ∈[0,1]. Since only the most discriminative parts of aiis suppressed, while\nothers stay consistent, the subsequent calculation process still exploits yf/prime\njas real\nlabel. Follow the calculation of foreground attention loss, the weak discriminative\nattention loss is as follows:\nLWDAL =−/summationtextc+1\nj=1yf/prime\njlog/parenleftbig\npweak\nj/parenrightbig\n(7)\nComplementary Strong Attention Score. In order to maximize the contri-\nbution of diﬀerent parts of the attention value, we follow Eq. 6and set the value\nthat meets the threshold condition to 1:\na/prime\ni=/braceleftbigg\n1,i f a i<γ\n0, otherwisea/prime/prime\ni=/braceleftbigg\n1,i f a i>γ\n0, otherwise(8)\nIn the above formula, a/prime\nirepresents the strong attention score of the weak dis-\ncriminative regions and the background, and a/prime/prime\nirepresents the strong attention\nscore of the strong discriminative regions.\nFollowing Eq. 1and Eq. 2, we can get p/prime\njandp/prime/prime\nj. Inspired by focal loss, we do\nthe following calculations for p/prime\njandp/prime/prime\njin order to solve the problem of sample\nimbalance after retaining only part of the attention regions.', '464 P. Dou et al.\nL/prime\nCSAL=−/summationtextc+1\nj=1yj(1−p/prime\nj)γlog(p/prime\nj) (9)\nL/prime/prime\nCSAL=−/summationtextc+1\nj=1yf/prime\nj(p/prime/prime\nj)γlog(1−p/prime/prime\nj) (10)\nAs discussed in the previous section, L/primeCSALpays attention to the background\nregion, so the label ywith background category is used, i.e., yc+1=1 .L/prime/primeCSAL\nignores the background area, so the label yf/primeof Eq. 4is used. Then we get the\ncomplementary strong attention loss:\nLCSAL=L/prime\nCSAL+L/prime/prime\nCSAL (11)\nLoss Functions. After introducing the above loss function, we leverage the\nfollowing joint loss function to train the FM-Net:\nL=λ0LCLSL+λ1LFAL+λ2LWDAL +λ3LCSAL+αLsparse +βLguide (12)\nwhere λ0,λ1,λ2,λ3,αandβare hyper-parameters, and Lsparse means sparse\nloss,Lguide denotes guide loss respectively. According to [ 7], the sparsity loss\nand the guide loss are valid which can be written as:\nLsparse =/summationtextT\ni=1|ai| (13)\nLguide=/summationtextT\ni=1|1−ai−¯sc+1| (14)\nwhere\n¯sc+1=exp (sc+1)/summationtextc\nj=1exp (sj)(15)\n3.4 Temporal Action Localization\nFirstly, we discard the classes whose video-level class scores are lower than the\nthreshold 0.1. Then, in order to discard the background snippets, we threshold\nthe foreground attention scores aiof the remaining classes. Finally, we select\nthe one-dimensional components of the remaining snippets to get classagnostic\naction proposals. As shown in Sect. 3.1, we represent the candidate action loca-\ntions as ( t(s),t(e),ψ,c). Following the outer-inner score of AutoLoc [ 25], we\ncalculate ψ, which is the classiﬁcation score corresponding to class c. Notably,\nwe should leverage foreground snippet-level class logits sfore\ncto calculate the\nscore of a speciﬁc category:\nψ=ψinner−ψouter+ζpfore\nc (16)\nψinner= Avg/parenleftbig\nsfore\nc(ts:te)/parenrightbig\n(17)\nψouter= Avg/parenleftbig\nsfore\nc(ts−lm:ts)+sfore\nc(te:te+lm)/parenrightbig\n(18)\nwhere ζis a hyper-parameter, pfore\ncdenotes the video-level score for class c,a n d\nlm=(te−ts)/4. For action proposals, we obtain them by applying diﬀerent\nthresholds. In addition, we exploit non-maximum suppression to remove the\noverlapping segments.', 'FMNet for Weakly-Supervised Temporal Action Localization 465\n4 Experiments\n4.1 Datasets and Evaluation Metrics\nTo evaluate the eﬀectiveness and superiority of our FMNet model, we conduct\nexperiments on THUMOS14 [ 6] and ActivityNet1.2 [ 2]. THUMOS14 contains a\ntotal of 20 action categories, including 200 validation videos for task training,\nand it contains 213 testing videos for task testing. ActivityNet1.2 contains 200\naction categories, a total of 4,819 videos for training and 2,382 videos for testing.We leverage ActivityNet oﬃcial code [ 2] to calculate the evaluation metrics.\nWe follow the standard evaluation and report that the mean Average Precision\n(mAP) at diﬀerent intersection over union (IoU) threshold.\n4.2 Implementation Details\nWe exploit the I3D network [ 3] pre-trained on the Kinetics dataset [ 8]t oe x t r a c t\nRGB and ﬂow features. RGB and ﬂow features should be divided into 16 frame\nchunks that do not overlap each other. The ﬂow features are created by using the\nTV-L1 algorithm [ 28]. After extracting RGB and ﬂow features, we concatenate\nthem to obtain the snippet-level features with 2048 dimensions. When using the\nTHUMOS14 dataset for training, we randomly sample 500 snippets for eachvideo; when using the ActivityNet dataset for training, we randomly sample\n80 snippets; and during testing, we take all the snippets for evaluation. The\nfeatures are then sent to the classiﬁcation branch and the attention branch. Theclassiﬁcation branch is composed of two temporal convolution layers with kernel\nsize 3. Each temporal convolution layer is followed by LeakyReLU activation.\nAt the end of the layer, a linear fully-connected layer is used to predict the classlogits. The attention branch is composed of two temporal convolutions with a\nkernel size 3, and ﬁnally followed by sigmoid activation, and an attention score\nbetween 0 and 1 is predicted.\nWe train 100 epochs for THUMOS14 and 20 epochs for ActivityNet. Hyper-\nparameters are determined by grid search, and we set λ\n0=λ1=0.8,λ2=λ3=\n0.2,α=β=0.8,γ=0.2 on THUMOS14, and for top-k temporal pooling, k=50.\nIn addition, we set λ0=λ1=0.4,λ2=λ3=0.6 and k=4 on ActivityNet, and\napply additional average pooling to post-processing the ﬁnal CAS. We use the\nAdam optimizer [ 9] with a learning rate of 0.00001. For action localization, we\nset the threshold from 0.1 to 0.9, the step size is 0.05, and perform non-maximum\nsuppression to remove overlapping segments.\n4.3 Ablation Studies\nWe conduct a set of ablation studies on the THUMOS14 dataset to analyze\nthe contributions made by each of our branches. The experimental results are\nshown in Table 1.F o rE q . 12, when LFAL,LWDAL ,LCSAL are normal form of\ncross entropy (i.e., LFAL,LWDAL ,L/primeCSAL andL/prime/primeCSAL useyf\nj,yf\nj,yjandyf\njto\nmodulate log( pfore\nj),log(pweak\nj),log(p/prime\nj) and log( p/prime/prime\nj), respectively), 29.6% mAP', '466 P. Dou et al.\nTable 1. Comparison of diﬀerent losses on Thumos14. “-” means the corresponding loss\nfunction is the normal form of cross-entropy, and “ /check” means that the corresponding\nloss function is trained with the loss function proposed in this article.\nMethods mAP@IoUs\nLFA L LWDAL LCSAL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 AVG\n- - - 64.6 58.3 50.7 40.0 29.6 20.0 10.5 39.1\n/check - - 65.4 59.0 51.3 41.6 31.5 21.1 11.5 40.2\n- - /check 65.6 59.0 51.2 41.5 30.8 21.0 11.2 40.0\n/check /check - 65.2 59.0 51.1 41.6 31.3 21.0 11.7 40.1\n/check - /check 65.8 59.5 51.6 42.3 32.7 22.1 11.9 40.8\n- /check /check 65.9 59.1 51.3 41.5 30.9 21.1 11.1 40.1\n/check /check /check 66.1 59.5 51.6 42.2 32.7 22.1 12.0 40.9\nTable 2. Recent detection results on the ActivityNet1.2 dataset. The results includes\nboth fully supervised methods and weakly supervised mothods. The results are evalu-\nated on mAP at IoU threshold [0.5:0.05:0.95, AVG]. The AVE is the average mAP atIoU threshold [0.5:0.05:0.95].\nSupervision Method IoU\n0.5 0.75 0.95AVG\nFull SSN [38] 41.3 27.0 6.126.6\nWeak UntrimmedNets [ 27]7.4 3.2 0.73.6\nAutoLoc [ 25] 27.3 15.1 3.316.0\nW-TALC [ 22] 37.0 12.7 1.518.0\nTSM [ 32] 28.6 17.0 3.517.1\n3C-Net [ 19] 35.4 - -21.1\nBaSNet [ 11] 34.5 22.5 4.922.2\nDGAM [ 23] 41.0 23.5 5.324.4\nHAMNet [ 7] 41.0 24.8 5.325.1\nFM-Net (ours) 41.0 25.0 5.3 25.2\ncan be achieved when the IoU value is 0.5, and AVG mAP is 39.1%. Table 1\nshows the performance when we change the cross entropy to the loss functionproposed in this article. Compared with all branches using normal form of cross\nentropy loss for optimization, our method improves mAP by 3.1% when the IoU\nvalue is 0.5, and AVG mAP increases by 1.8%. Notably, when two or all of the\nimproved loss functions are combined, mAP can be improved.', 'FMNet for Weakly-Supervised Temporal Action Localization 467\nTable 3. Recent detection results on the Thumos14 dataset. The results includes both\nfully supervised methods and weakly supervised methods. The results are evaluated onmAP at IoU threshold [0.1:0.1:0.7].\nSupervision Method Feature IoU\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 AVG\nFull S-CNN [ 26] – 47.7 43.5 36.3 28.7 19.0 10.3 5.3 –\nR-C3D [ 30] – 54.5 51.5 44.8 35.6 28.9 – – –\nSSN [ 38] – 66.0 59.4 51.9 41.0 29.8 – – –\nTAL-Net [ 4] – 59.8 57.1 53.2 48.5 42.8 33.8 20.8 –\nBSN [ 15] – – – 53.5 45.0 36.9 28.4 20.0 –\nG-TAD [ 31] – – – 54.5 47.6 40.2 30.8 23.4 –\nP-GCN [ 36] – 69.5 67.8 63.6 57.8 49.1 – – –\nWeak HaS [ 10] – 36.4 27.8 19.5 12.7 6.8 – – –\nUntrimmedNets [ 27]– 44.4 37.7 28.2 21.1 13.7 – – –\nAutoLoc [ 25] UNT – – 35.8 29.0 21.2 13.4 5.8 –\nSTPN [ 20] I3D 52.0 44.7 35.5 25.8 16.9 9.9 4.3 26.4\nMAAN [ 34] I3D 59.8 50.8 41.1 30.6 20.3 12.0 6.9 31.6\nW-TALC [ 22] I3D 55.2 49.6 40.1 31.1 22.8 – 7.6 –\n3C-Net [ 19] I3D 56.8 49.8 40.9 32.3 24.6 – 7.7 –\nNguyen et al. [ 21] I3D 60.4 56.0 46.6 37.5 26.8 17.6 9.0 36.3\nDGAM [ 23] I3D 60.0 54.2 46.8 38.2 28.8 19.8 11.4 37.0\nHAM-Net [ 7] I3D 65.4 59.0 50.3 41.1 31.0 20.7 11.1 39.8\nFMNet (ours) I3D 66.1 59.5 51.6 42.2 32.7 22.1 12.0 40.9\nPerformance Comparison to State-of-the-Art. We summarise the com-\nparison between our method and the state-of-the-art fully and weakly super-\nvised TAL methods on the THUMOS14 in Table 3. Under weak supervision, the\nmAP corresponding to each IoU threshold has achieved excellent results, reach-\ning 32.7% mAP when the IoU threshold is 0.5. With reference to the Table 2,\nwe also evaluate our method on the ActivityNet1.2 dataset, and also achievedexcellent results, verifying the eﬀectiveness of our FMNet model.\nQualitative Performance. We show some representative examples in Fig. 3.\nFor each video, the top row shows example frames, the next row represents\nground-truth localization, “Ours” is our prediction, and “CE’ means that L\nFAL,\nLWDAL ,LCSAL are only optimized by cross entropy. Figure 3shows that our\nmethod captures more action segments than “CE”, and reduces the possibility\nof mistaking background for action.', '468 P. Dou et al.\nFig. 3. Visualization of the FMNet model. On the vertical axis, we plot the ground\ntruth, the detection results with the loss function in the form of cross entropy, and thedetection results with our loss function. The results show that our FMNet model has\na more accurate localization eﬀect.\n5 Conclusion\nFor the task of weakly supervised temporal action localization, we propose a\nnew framework called FM-Net. We construct four attention branches and designdiﬀerent loss functions for each branch to capture diﬀerent features of the video,\nand then localize the complete time boundary of the action in the video. We con-\nduct extensive experiments and analysis to prove the eﬀectiveness of our method.Our method achieves excellent performance on THUMOS14 and ActivityNet1.2.\nAcknowledgement. This work was supported in part by the National Natural Sci-\nence Foundation of China (62076262, 61673402, 61273270, 60802069), the Natural Sci-ence Foundation of Guangdong Province (2017A030311029), the Science and Technol-\nogy Program of Huizhou of China (2020SC0702002).\nReferences\n1. Buch, S., Escorcia, V., Ghanem, B., Fei-Fei, L., Niebles, J.C.: End-to-end, single-\nstream temporal action detection in untrimmed videos. In: Procedings of the\nBritish Machine Vision Conference 2017. British Machine Vision Association (2019)\n2. Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: ActivityNet: a\nlarge-scale video benchmark for human activity understanding. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 961–970(2015)\n3. Carreira, J., Zisserman, A.: Quo Vadis, action recognition? A new model and the\nkinetics dataset. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 6299–6308 (2017)', 'FMNet for Weakly-Supervised Temporal Action Localization 469\n4. Chao, Y.W., Vijayanarasimhan, S., Seybold, B., Ross, D.A., Deng, J., Sukthankar,\nR.: Rethinking the faster R-CNN architecture for temporal action localization. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pp. 1130–1139 (2018)\n5. Feichtenhofer, C., Pinz, A., Zisserman, A.: Convolutional two-stream network\nfusion for video action recognition. In: Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pp. 1933–1941 (2016)\n6. Idrees, H., et al.: The THUMOS challenge on action recognition for videos “in the\nwild.” Comput. Vis. Image Underst. 155, 1–23 (2017)\n7. Islam, A., Long, C., Radke, R.: A hybrid attention mechanism for weakly-\nsupervised temporal action localization. arXiv preprint arXiv:2101.00545 (2021)\n8. Kay, W., et al.: The kinetics human action video dataset. arXiv preprint\narXiv:1705.06950 (2017)\n9. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)\n10. Kumar Singh, K., Jae Lee, Y.: Hide-and-seek: forcing a network to be meticulous\nfor weakly-supervised object and action localization. In: Proceedings of the IEEEInternational Conference on Computer Vision, pp. 3524–3533 (2017)\n11. Lee, P., Uh, Y., Byun, H.: Background suppression network for weakly-supervised\ntemporal action localization. In: Proceedings of the AAAI Conference on ArtiﬁcialIntelligence, vol. 34, pp. 11320–11327 (2020)\n12. Lin, C., et al.: Fast learning of temporal action proposal via dense boundary gen-\nerator. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 34,pp. 11499–11506 (2020)\n13. Lin, T., Liu, X., Li, X., Ding, E., Wen, S.: BMN: boundary-matching network for\ntemporal action proposal generation. In: Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pp. 3889–3898 (2019)\n14. Lin, T., Zhao, X., Shou, Z.: Single shot temporal action detection. In: Proceedings\nof the 25th ACM international conference on Multimedia, pp. 988–996 (2017)\n15. Lin, T., Zhao, X., Su, H., Wang, C., Yang, M.: BSN: boundary sensitive network\nfor temporal action proposal generation. In: Ferrari, V., Hebert, M., Sminchisescu,\nC., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11208, pp. 3–21. Springer, Cham(2018). https://doi.org/10.1007/978-3-030-01225-0\n1\n16. Long, F., Yao, T., Qiu, Z., Tian, X., Luo, J., Mei, T.: Gaussian temporal awareness\nnetworks for action localization. In: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 344–353 (2019)\n17. Luo, Z., et al.: Weakly-supervised action localization with expectation-\nmaximization multi-instance learning. In: Vedaldi, A., Bischof, H., Brox, T., Frahm,J.-M. (eds.) ECCV 2020. LNCS, vol. 12374, pp. 729–745. Springer, Cham (2020).\nhttps://doi.org/10.1007/978-3-030-58526-6\n43\n18. Min, K., Corso, J.J.: Adversarial background-aware loss for weakly-supervised tem-\nporal activity localization. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M.\n(eds.) ECCV 2020. LNCS, vol. 12359, pp. 283–299. Springer, Cham (2020). https://\ndoi.org/10.1007/978-3-030-58568-6 17\n19. Narayan, S., Cholakkal, H., Khan, F.S., Shao, L.: 3C-Net: category count and center\nloss for weakly-supervised action localization. In: Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pp. 8679–8687 (2019)\n20. Nguyen, P., Liu, T., Prasad, G., Han, B.: Weakly supervised action localization\nby sparse temporal pooling network. In: Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pp. 6752–6761 (2018)', '470 P. Dou et al.\n21. Nguyen, P.X., Ramanan, D., Fowlkes, C.C.: Weakly-supervised action localization\nwith background modeling. In: Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pp. 5502–5511 (2019)\n22. Paul, S., Roy, S., Roy-Chowdhury, A.K.: W-TALC: weakly-supervised temporal\nactivity localization and classiﬁcation. In: Ferrari, V., Hebert, M., Sminchisescu,\nC., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11208, pp. 588–607. Springer, Cham(2018). https://doi.org/10.1007/978-3-030-01225-0\n35\n23. Shi, B., Dai, Q., Mu, Y., Wang, J.: Weakly-supervised action localization by gener-\native attention modeling. In: Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pp. 1009–1019 (2020)\n24. Shou, Z., Chan, J., Zareian, A., Miyazawa, K., Chang, S.F.: CDC: convolutional-\nde-convolutional networks for precise temporal action localization in untrimmed\nvideos. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 5734–5743 (2017)\n25. Shou, Z., Gao, H., Zhang, L., Miyazawa, K., Chang, S.-F.: AutoLoc: weakly-\nsupervised temporal action localization in untrimmed videos. In: Ferrari, V.,\nHebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11220, pp.162–179. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01270-0\n10\n26. Shou, Z., Wang, D., Chang, S.F.: Temporal action localization in untrimmed videos\nvia multi-stage CNNs. In: Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 1049–1058 (2016)\n27. Wang, L., Xiong, Y., Lin, D., Van Gool, L.: Untrimmednets for weakly super-\nvised action recognition and detection. In: Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pp. 4325–4334 (2017)\n28. Wedel, A., Pock, T., Zach, C., Bischof, H., Cremers, D.: An improved algorithm for\nTV- L\n1optical ﬂow. In: Cremers, D., Rosenhahn, B., Yuille, A.L., Schmidt, F.R.\n(eds.) Statistical and Geometrical Approaches to Visual Motion Analysis. LNCS,\nvol. 5604, pp. 23–45. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-\n642-03061-1 2\n29. Xiong, Y., Zhao, Y., Wang, L., Lin, D., Tang, X.: A pursuit of temporal accuracy\nin general activity detection. arXiv preprint arXiv:1703.02716 (2017)\n30. Xu, H., Das, A., Saenko, K.: R-C3D: region convolutional 3D network for tem-\nporal activity detection. In: Proceedings of the IEEE International Conference on\nComputer Vision, pp. 5783–5792 (2017)\n31. Xu, M., Zhao, C., Rojas, D.S., Thabet, A., Ghanem, B.: G-TAD: sub-graph local-\nization for temporal action detection. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 10156–10165 (2020)\n32. Yu, T., Ren, Z., Li, Y., Yan, E., Xu, N., Yuan, J.: Temporal structure mining for\nweakly supervised action detection. In: Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pp. 5522–5531 (2019)\n33. Yuan, J., Ni, B., Yang, X., Kassim, A.A.: Temporal action localization with pyra-\nmid of score distribution features. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pp. 3093–3102 (2016)\n34. Yuan, Y., Lyu, Y., Shen, X., Tsang, I.W., Yeung, D.Y.: Marginalized average atten-\ntional network for weakly-supervised learning. arXiv preprint arXiv:1905.08586\n(2019)\n35. Zeng, R., Gan, C., Chen, P., Huang, W., Wu, Q., Tan, M.: Breaking winner-takes-\nall: iterative-winners-out networks for weakly supervised temporal action localiza-\ntion. IEEE Trans. Image Process. 28(12), 5797–5808 (2019)', 'FMNet for Weakly-Supervised Temporal Action Localization 471\n36. Zeng, R., et al.: Graph convolutional networks for temporal action localization. In:\nProceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n7094–7103 (2019)\n37. Zhao, P., Xie, L., Ju, C., Zhang, Y., Wang, Y., Tian, Q.: Bottom-up temporal\naction localization with mutual regularization. In: Vedaldi, A., Bischof, H., Brox,\nT., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12353, pp. 539–555. Springer,Cham (2020). https://doi.org/10.1007/978-3-030-58598-3\n32\n38. Zhao, Y., Xiong, Y., Wang, L., Wu, Z., Tang, X., Lin, D.: Temporal action detec-\ntion with structured segment networks. In: Proceedings of the IEEE International\nConference on Computer Vision, pp. 2914–2923 (2017)\n39. Zhou, Z.H.: Multi-instance learning: a survey. Technical report, Department of\nComputer Science & Technology, Nanjing University, 2 (2004)', 'LiDAR-Based Symmetrical Guidance\nf o r3 DO b j e c tD e t e c t i o n\nHuazhen Chu, Huimin Ma(B), Haizhuang Liu, and Rongquan Wang\nSchool of Computer and Communication Engineering, University of Science\nand Technology Beijing, 100083 Beijing, China\nmhmpub@ustb.edu.cn\nAbstract. Object detection from 3D point clouds is an essential task\nfor autonomous driving. Current approaches usually focus on the charac-\nteristics of the point cloud while ignoring the structural characteristics of\nthe object itself. This paper designs a new symmetric structure enhance-ment network SS-PV-RCNN according to the symmetry of car struc-\nture, inspired by the human symmetry visual cognition system. Using\nthe symmetrically strengthened point cloud to help the network learn\nthe features of the symmetric structure, improve the robustness of the\nnetwork in the occlusion area, and then improve the accuracy of 3Dobject detection. This method is veriﬁed on the KITTI dataset, and it\nhas a particular improvement compared to the baseline method.\nKeywords: 3D object detection\n·Point cloud ·Symmetrical structure\n1 Introduction\n3D object detection has been receiving increasing attention from both industry\nand academia thanks to its wide applications in various ﬁelds such as autonomous\ndriving and robotics. LiDAR sensors are widely adopted in autonomous driving\nvehicles and robots for capturing 3D scene information as sparse and irregularpoint clouds, which provide vital cues for 3D scene perception and understand-\ning. This paper proposes a new method for 3D object detection on point clouds\nto achieve high performance by designing a novel symmetrical strengthening\nPV-RCNN (SS-PV-RCNN) network.\nAlthough the point cloud can not be aﬀected by the weather and light, the\npoint cloud has very serious sparsity. There are still problems of missed detec-\ntion, false detection, and inaccurate positioning for the objects with occlusion.\nTherefore, we study the human visual system and ﬁnd the symmetry visual per-ception in the human visual system. As a result, we apply the symmetry visual\nperception to the point cloud-based detection task.\nThe human visual system is susceptible to the processing of bilateral symme-\ntry of visual input stimuli. Many studies [ 1–3] show that the perceptual organi-\nzation of visual input is directly involved in symmetry processing. For example,\nsymmetry inﬂuences the ﬁgure-background separation process, one of the critical\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 472–483, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_39', 'LiDAR-Based Symmetrical Guidance for 3D Object Detection 473\nsteps in object perception formation. Driver et al. [ 1] found that when ambiguous\nshapes were used as visual stimuli, subjects tended to regard perceived symmet-rical shapes as graphics and asymmetric shapes as backgrounds.\nFrom a geometric point of view, the concept of symmetry involves isometric\nisomorphisms, mirror symmetry (also known as mirror symmetry or bilateralsymmetry), rotational symmetry, translational symmetry (also known as repeti-\ntion), and their composite forms. Although the human visual system is sensitive\nto all of these forms of symmetry, bilateral symmetry is probably the single mostsigniﬁcant, most frequently mentioned, and most relevant to humans. From a\nphenomenological point of view, the outstanding feature of symmetry is its abil-\nity to highlight the object’s structure when the observer inspects it.\nInspired by the human visual cognitive system, we use bilateral symmetry\nas the starting point of our research and propose a new symmetric-enhanced\nnetwork. The symmetric branch of the network is trained with the symmetrically\nreinforced data. Then the original branch is constrained to learn the content as\nclose as possible to the symmetric branch to promote the branch to learn thestructural information about the symmetry. In this way, the original network can\nlearn symmetric structure information from the original point cloud to improve\nits performance of 3D detection.\nWe make the following three contributions in this work:\ni Based on human symmetry visual perception, we propose a symmetric struc-\nture enhancement network to help the network learn the structural infor-\nmation about symmetry. Moreover, we verify the validity of the symmetricstructure by using the existing algorithm on KITTI dataset.\nii According to the symmetry of car structure, we propose a method to generate\nsymmetrically enhanced car point cloud data. Furthermore, use the symmet-rically enhanced point cloud data to help the network learn the symmetric\nstructure information.\niii We innovatively proposed feature consistency loss to constrain the network\nat the feature level to learn symmetric information.\n2 Related Work\n3D object detection aims to predict three-dimensional rotated bounding boxes.\nThere are three main methods in this ﬁeld:\nMulti-modal 3D Object Detection. MV3D [ 4] converts the point cloud to\nthe birdview and front view, integrates the information from multiple perspec-\ntives accordingly, and inferences the 3D information of objects. AVOD [ 5] takes\nRGB image and point cloud birdview as the network’s input uses FPN network\nto extract features and re-projects a small amount of 3D physical similarity\nregions onto the feature maps of diﬀerent modes accurate 2D candidate regionfeatures. Finally, accurate 3D object detection is achieved through secondary\nfusion. Pixor [ 6] and Contfuse [ 7] take point cloud projection map as input and', '474 H. Chu et al.\nimprove the detection framework and fusion strategy, thus improving the detec-\ntion performance.\nSingle-Stage 3D Object Detection. VoxelNet [ 8] ﬁrst discretized the three-\ndimensional space and divided the three-dimensional space into multiple voxel\nunits of the same size. Then, such normalized point clouds were successively sent\ninto 3D CNN and 3D RPN to output the three-dimensional detection results.\nIn order to solve the problem of high time cost, Second [ 9] took advantage of\nthe sparsity of the point cloud and proposed the method of sparse 3D convolu-\ntion to accelerate 3DCNN. The high eﬃciency of sparse convolution improves\nresearchers’ conﬁdence in the use of 3DCNN. 3DSSD [ 10] improves the distance\ncalculation method of FPS by taking advantage of the diﬀerence between the\nforeground and the background points of outdoor scenes and improves the recall\nrate of foreground object instance level, thus further improving the network eﬃ-ciency. SA-SSD [ 11] used 3DCNN to extract the backbone network features, and\nthe secondary network features were transformed to the BEV plane and then\nsent to the detection head. By constructing additional tasks, the constraints ofthe network were enhanced. CIA-SSD [ 12] proposed the spatial-Semantic Feature\nAggregation (SSFA) module for more robust feature extraction in 3D convolu-\ntion and introduced classiﬁcation fraction and location fraction to represent theconﬁdence degree of the frame jointly, thus improving NMS. SE-SSD [ 13]p r o -\nposes a self-integrated single-stage object detector, which is optimized by soft\ntarget consistency constraint.\nTwo-Stage 3D Object Detection. PointNet [ 14] and PointNet++ [ 15]a r e\nclassical feature extraction methods in 3D detection. In recent years, many 3Dtarget detection algorithms based on PointNet have also emerged. F-PointNet\n[16] uses 2D detection box and cone geometry to input the point cloud inside\nthe cone into the PointNet to segment the point cloud instances, and obtains thesemantic category of each point cloud, and then uses a PointNet to predict the\n3D detection box. PointRCNN [ 17] uses PointNet++ to segment the foreground\npoint of the point cloud and puts forward global and local coordinate systems\nto form a more stable feature expression. VoteNet [ 18] adopted Pointnet++\nas the backbone network of point cloud and added a voting mechanism in theaggregation stage of point cloud features to stabilize the network and extract\nfeatures. PV-RCNN [ 19] is a combination of point cloud and voxelization method,\nwhich combines features at all levels after voxelization to improve the network’slearning ability.\n3M e t h o d\nCompared with the original point cloud, the symmetrically completed point\ncloud has complete structural information. The original point cloud has an\nincomplete structure due to occluding, distance, and other factors. If the sym-metrically strengthened point cloud is sent into the network, the network can\nlearn the symmetric structure information. In contrast, if the missing point cloud', 'LiDAR-Based Symmetrical Guidance for 3D Object Detection 475\nis directly sent into the network, the network will correspondingly lose the sym-\nmetric structure information. This section describes in detail how the networklearns symmetric structure information from the original point cloud.\n3.1 Generation of Symmetric Annotations\nIn the driving scene, the incomplete point cloud is mainly caused by occlusion.\nIn this paper, we use the symmetry of the object structure to supplement the\ninvisible part of the point cloud to generate complete point cloud information. A\ncar is a rigid object, and its structure is symmetrical about the central axis of the\ndriving direction. Therefore, by using the symmetry of the car and the annotation\nof the data set itself, a complete annotation of the car can be generated tosupplement the point cloud information of the occluded part.\nFirst, according to the label of the detection box, the symmetric point cloud\nin the three-dimensional box is calculated according to its symmetry axis. Thesymmetrical point cloud is added to the original point cloud data, and a relatively\ncomplete symmetrical point cloud can be obtained.\nFig. 1. Visualization of the (a) original point cloud and (b) symmetrically enhanced\npoint cloud.\nA ss h o w ni nF i g . 1, the top is the original point cloud, and the bottom is\nthe point cloud strengthened by symmetry. After symmetry enhancement, the\nstructural features of cars at diﬀerent positions will be more obvious. In addition,part of the point cloud will be completed in the shaded places according to their\nsymmetrical structures. Therefore, the analysis of the shaded part is no longer', '476 H. Chu et al.\nblank. Point cloud strengthened by symmetry retains more surface information of\nthe car, and the occlusion part is clearer so that the car’s position can be locatedmore accurately. Validity veriﬁcation experiment of symmetrically enhance is\nintroduced in detail in Sect. 4.1.\n3.2 SS-PV-RCNN\nFigure 2shows the framework of SS-PV-RCNN, which consists of the original\nbranch (top) and the symmetric branch (bottom), both of which use the samePV-RCNN [ 19] network. The input point cloud is voxelized and sent to the four-\nlayer sparse convolution to extract voxel-based features. Then, the last layer of\nthe convolution feature is projected onto the top view to get 2D features, and2D detection is carried out. In addition, the original point cloud extracts the\nkey points, then aggregates and splices the key points in the features of diﬀerent\nconvolutional layers and the top view, and sends them into the detection headto output the three-dimensional detection results.\nFig. 2. Overview of our proposed SS-PV-RCNN framework. SS-PV-RCNN mainly con-\nsists of two parts, the yellow primitive branch, and the blue symmetric branch. Bothbranches adopt the same PV-RCNN structure. The yellow branch inputs the original\npoint cloud, while the blue branch inputs the symmetrically strengthened point cloud.\nBetween the two branches, feature consistency loss, center consistency loss, and classconsistency loss help the original branch learn the symmetric structure information\nfrom the symmetric branch. (Color ﬁgure online)\nIn training, we ﬁrst trained the symmetric branch, ﬁxed its parameters, and\ntrained the original branch. When training the original branch, we add feature\nconsistency loss, center consistency loss, and class consistency loss to constrain-ing the original branch to learn symmetry-related information from the symmet-\nric branch. Symmetry awareness of the original branch is developed by enforcing\nthe consistency of the output of the original branch and the symmetric branch.', 'LiDAR-Based Symmetrical Guidance for 3D Object Detection 477\n3.3 Consistency Loss\nSymmetry enhancement is generated according to ground truth, but the object to\nbe detected does not have a position label. To this end, we propose three diﬀerentconsistency losses to help the network learn symmetrical structure information\nfrom the original point cloud.\nWe adopt a structure similar to knowledge distillation to make the network\nlearn information about symmetry independently. Teacher networks and student\nnetworks share the same network structure. As shown in Fig. 2, in our design,\nthe symmetrically strengthened yellow branch is the teacher network, while theunstrengthened blue branch is the student branch. Since the input of the point\ncloud is strengthened by symmetry, the teacher branch can learn information\nabout symmetry that cannot be learned by the student branch in the processof feature learning. In order to make the student network learn symmetrical\ninformation in the process of training, the feature consistency loss is added into\nthe last layer of the feature, which constrains the feature output of the studentnetwork and the teacher network to be as similar as possible. The calculation of\nthe consistency loss is shown in Formula 1:\nL\nf eature =1\nN×/summationtextN\ni=1Lc\nfi\nand Lc\nfi=/braceleftbigg\n0.5∗(fsi−fri)2,i f |fsi−fri|<1\n|fsi−fri|−0.5, otherwise(1)\nWhere fsirepresents the feature of the symmetric branch, frirepresents the\nfeature of the original branch, and Nrepresents the number of features. The\nprototype for consistency loss is Smooth L1 [ 20] Loss.\nIn addition, the results of the symmetrical branching test are more accurate\nand have a high recall rate. Therefore, in the output results, we constrained the\ncategory of output results to be consistent with the center point of the box.\nFor this reason, we added the category consistency and center point consistencyconstraints to encourage the student network to learn more accurately center\nand category.\nInspired by SESS [ 21],C\nr={cr}is used to represent the predicted 3D\nbounding boxes by student network, and Cs={cs}is used to represent the\n3D bounding boxes of symmetric teacher network prediction. For each cs∈Cs,\nwe do the alignment by searching for the its nearest neighbor in Crbased on\nthe minimum Euclidean distance between the centers of the bounding boxes.\nWe further use C/prime\nrto denote the elements from Crthat are aligned with each\nelement in Cs.\nC/prime\nr=/braceleftbig\n···,c/prime\nri,···/bracerightbig\nand c/prime\nri= arg min\ncr/bardblcr−csi/bardbl2,∀cr∈Cr(2)', '478 H. Chu et al.\nWe deﬁne center consistency as:\nLcenter =2∗/summationtext\ncs/bardblcs−c/prime\nr/bardbl2\n|Cr|+|Cs|(3)\nIn addition, we use Kullback-Leibler (KL) divergence to calculate category\nconsistency, and the prediction category of symmetric branches is the learning\nobjective of the original branch prediction category. PsandPra r eu s e dt or e p -\nresent the category probability of the predicted target of the symmetric branch\nand the original branch, respectively.\nLclass=1\n|Ps|/summationdisplay\nDKL(pr/bardblps) (4)\nFinally, the total consistency loss is a weighted sum of all the three consis-\ntency terms described earlier:\nLconsistency =λ1Lfeature +λ2Lcenter +λ3Lclass (5)\nwhere λ1,λ2,a n d λ3are the weights to control the importance of the corre-\nsponding consistency term.\n4 Experiments\nWe evaluate our SS-PV-RCNN method on challenging KITTI [ 22] object detec-\ntion benchmark. The KITTI dataset provides 7481 training samples and 7518test samples, where the training samples are generally divided into the train split\n(3712 samples) and the val split (3769 samples). We compare SS-PV-RCNN with\nstate-of-the-art methods on val split.\n4.1 Symmetry Enhances Data Analysis\nIn order to prove the eﬀectiveness of symmetry for point cloud detection, we\ntook the truth values of all cars in 7481 training samples and strengthened the\nsymmetry of all internal point clouds of cars according to their central axis planes\nto form a new data set after symmetry. Then, PV-RCNN was trained and veriﬁedon the data sets before and after symmetric reinforcement, respectively. The\ndivision of training and veriﬁcation sets was consistent with that of KITTI, and\nthe setting of the PV-RCNN network’s hyperparameter was also consistent. Theexperimental results are shown in Table 1. When trained on the original KITTI\ndata and veriﬁed on the symmetrically enhanced veriﬁcation set, the accuracy\nincreased by 5.78 on Hard, 2.91 on Moderate, and 0.45 on Easy. After symmetricenhancement of training data, the accuracy rate increased by 9.92 on Hard, 5.37\non Moderate, and 1.07 on Easy. This result fully shows that the enhancement\nof symmetry signiﬁcantly aﬀects the improvement of detection accuracy under\nchallenging samples.', 'LiDAR-Based Symmetrical Guidance for 3D Object Detection 479\nWe use the symmetrically strengthened point cloud to train the symmet-\nric branch model. The trained model has ﬁxed parameters. As our symmetricteacher branch, both teacher and student branches are a complete PV-RCNN\nnetwork structure. The original point cloud and the symmetrically strengthened\npoint cloud are sent to the original student PV-RCNN branch and the symmet-ric teacher PV-RCNN branch. In order to make the student network learn more\ninformation about symmetry in the training process, the feature consistency loss,\nthe signiﬁcant consistency loss of the ﬁnal result, and the category consistencyloss are added. In the training process, only the weights of students’ branches\nare updated, but not the weights of teachers’ branches. In the prediction process,\nonly the parameters of students’ branches are used for prediction.\nTable 1. Comparison of validation results of PV-RCNN method in KITTI and sym-\nmetrically enhanced KITTI datasets. RT and RV respectively represent the train set\nand val set of the original KITTI dataset. In contrast, ST and SV represent the trainset and val set of the symmetrically strengthened KITTI dataset.\nMethod Train ValCar-3D detection Car-BEV detection\nEasy Mod Hard Easy Mod Hard\nPV-RCNNRT RV89.43 83.26 78.76 90.03 87.88 87.43\nRT SV89.88 86.17 84.54 90.26 88.81 88.31\n↑0.45 ↑2.91 ↑5.78 ↑0.23 ↑0.93 ↑0.88\nST SV90.50 88.99 88.68 90.67 89.97 89.81\n↑1.07 ↑5.37 ↑9.92 ↑0.64 ↑2.09 ↑2.41\nTable 2. Comparison with PV-RCNN detectors on KITTI val split for car detection,\nin which “R40” means 40 sampling recall points for AP.\nMethod MetricCar-3D detection Car-BEV detection\nEasy Mod Hard Easy Mod Hard\nPV-RCNNAP 89.43 83.26 78.76 90.03 87.88 87.43\nAPR4091.43 84.47 82.22 92.97 90.56 88.41\nSS-PV-RCNNAP 89.62 84.25 79.13 90.44 88.49 87.94\nAPR4092.36 85.10 82.82 94.98 91.26 88.91\n4.2 Comparison with State-of-the-Arts\nAs shown in Fig. 2, we used PV-RCNN [ 19] as the baseline, the yellow branch is\na separate PV-RCNN network, and the blue branch as well. The setting of its', '480 H. Chu et al.\nTable 3. Performance comparison on the moderate level car class of KITTI val split\nwith mAP calculated by 11 recall positions.\nMethod Reference Modality 3D mAP\nMV3D [ 4] CVPR 2017 RGB + LiDAR 62.68\nConFuse [ 7] ECCV 2018 RGB + LiDAR 73.25\nAVOD-FPN [ 5] IROS 2018 RGB + LiDAR 74.44\nF-PointNet [ 16] CVPR 2018 RGB + LiDAR 70.92\nVoxelNet [ 8] CVPR 2018 LiDAR only 65.46\nSECOND [ 9] Sensors 2018 LiDAR only 76.48\nPointRCNN [ 17] CVPR 2019 LiDAR only 78.63\nFast Point R-CNN [ 23]ICCV 2019 LiDAR only 79.00\nSTD [ 24] ICCV 2019 LiDAR only 79.80\nPV-RCNN [ 19] CVPR2020 LiDAR only 83.90\nSS-PV-RCNN (ours) - LiDAR only 84.28\nFig. 3. Visualization of our results on the KITTI val split set. The ground-truth 3D\nboxes and the predicted 3D boxes of the baseline method and our method are drawnin green, blue, and purple, respectively, in the LiDAR phase. The ﬁrst row shows RGB\nimages, and the second shows the LiDAR pattern. (Color ﬁgure online)', 'LiDAR-Based Symmetrical Guidance for 3D Object Detection 481\nTable 4. Recall of diﬀerent proposal generation networks on the car class at moderate\ndiﬃculty level of the KITTI val split set.\nMethod PointRCNN [ 17]STD [ 24]PVRCNN [ 19]SS-PV-RCNN\nRecall (IoU = 0.7) 74.8 76.8 75.2 85.4\nsuper parameters and the detailed implementation of the network was based on\nPV-RCNN. We empirically set hyperparameters λ1=1 ,λ2=0.6, and λ3=0.6.\nMetric. All results are evaluated by the mean average precision and recall with\nIoU threshold of 0.7 for cars. The mean average precisions on the test set are\ncalculated with 40 recall positions on the oﬃcial KITTI test server. The resultson the val set in Table 2are calculated with 11 recall positions to compare with\nthe results of the previous works.\nOur PV-RCNN framework is trained from scratch in an end-to-end manner\nwith the ADAM optimizer. For the KITTI dataset, we train the entire network\nwith the batch size 2, learning rate 0.01 for 80 epochs on 4 TITAN Xp GPUs,\nwhich takes around 23 h. In order to make a more unbiased experimental com-parison, we trained PV-RCNN several times on our equipment and selected the\nbest results as the baseline. All PV-RCNN results presented in this paper are\nour reproduction results.\nA ss h o w ni nT a b l e 2, we compared the detection results of SS-PV-RCNN and\nPV-RCNN on the KITTI val split for car detection. Compared to baseline, our\nmethod has nearly 1 point improvement on the mod of 3D Detection and alsohas some improvement on other levels. Table 3, we compared our method with\nthe prior works on the KITTI val split. Although there is still a particular gap\nbetween our method and the latest method, there is still a certain improvement\nin detection accuracy compared with the baseline method.\nFigure 3shows the comparison between our method and PV-RCNN in the\ndetection results. In the LIDAR diagram, green represents groundtruth, blue\nrepresents the detection results of PV-RCNN, and purple represents the detec-\ntion results of our method. In order to show it more clearly, only the groundtruth(green) and the detection results of our method (purple) are shown in the RGB\nﬁgure. In the two ﬁgures in the ﬁrst row, although SS-PV-RCNN detected some\nboxes without labeling, these boxes were all actual cars in the distance, whichwere not labeled due to the distance when labeling. It can be seen that our\nmethod is still robust to small distant targets. In the two ﬁgures in the second\nrow, some error boxes detected by PV-RCNN are not found in our method, andour method is also more accurate in orientation prediction.\nSince some small targets without labeling can also be detected by our method,\nthe accuracy of our method will be relatively low. In order to further verify theeﬀectiveness of our method, we compared the recall rate of SS-PV-RCNN and\nsome typical methods on the val split when IOU = 0.7. As shown in Table 4,\ncompared with PV-RCNN, the recall rate of our method has been improved by\nnearly 10 points, indicating that our method can detect more truth boxes.', '482 H. Chu et al.\n4.3 Ablation Study\nAs can be seen from Table 1, the symmetry of the point cloud plays a signiﬁcant\nrole in 3D object detection based on point cloud. However, compare Table 1and\nTable 2, it can be seen that the symmetrical information in the SS-PV-RCNN\nnetwork we designed does not have such a prominent eﬀect as that in Table 1.\nIn future research work, we will further improve the design of network structure,\nhoping to make more eﬀective use of symmetric structure to help improve theperformance of 3D object detection. At the same time, we also hope that more\nresearchers can pay attention to the critical role of symmetrical structure and\nwork with us to introduce symmetry into 3D object detection better.\n5 Conclusion\nInspired by the human symmetric visual cognitive system, we propose a sym-metrically enhanced network SS-PV-RCNN, which can learn about symmetricstructures from the point cloud by using symmetrically enhanced point clouds.\nFurthermore, we verify that symmetrical structure plays a signiﬁcant role in the\n3D detection of point clouds. SS-PV-RCNN can improve the accuracy of 3Dobject detection by using the learned symmetric structure information. Exper-\nimental results on the KITTI benchmark dataset demonstrate the eﬀectiveness\nand robustness of our SS-PV-RCNN. In the future, we will try to introduce sym-metric structure information into the single-stage model to further improve the\naccuracy of 3D object detection.\nAcknowledgments. This work was supported by the National Natural Science\nFoundation of China (No.U20B2062), the fellowship of China Postdoctoral Science\nFoundation (No.2021M690354), the Beijing Municipal Science & Technology Project(No.Z191100007419001).\nReferences\n1. Driver, J., Baylis, G., Rafal, R.: Preserved ﬁgure-ground segregation and symmetry\nperception in visual neglect. Nature 360, 73–75 (1992)\n2. Machilsen, B., Pauwels, M., Wagemans, J.: The role of vertical mirror symmetry\nin visual shape detection. J. Vis. 9(12), 11 (2009)\n3. Helm, P.A., Treder, M.: Detection of (anti)symmetry and (anti)repetition: percep-\ntual mechanisms versus cognitive strategies. Vis. Res. 49, 2754–2763 (2009)\n4. Chen, X., Ma, H., Wan, J., Li, B., Xia, T.: Multi-view 3D object detection network\nfor autonomous driving. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 1907–1915 (2017)\n5. Ku, J., Moziﬁan, M., Lee, J., Harakeh, A., Waslander, S.L.: Joint 3D proposal\ngeneration and object detection from view aggregation. In: 2018 IEEE/RSJ Inter-\nnational Conference on Intelligent Robots and Systems (IROS), pp. 1–8. IEEE(2018)\n6. Yang, B., Luo, W., Urtasun, R.: PIXOR: real-time 3D object detection from point\nclouds. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pp. 7652–7660 (2018)', 'LiDAR-Based Symmetrical Guidance for 3D Object Detection 483\n7. Liang, M., Yang, B., Wang, S., Urtasun, R.: Deep continuous fusion for multi-\nsensor 3D object detection. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss,\nY. (eds.) ECCV 2018. LNCS, vol. 11220, pp. 663–678. Springer, Cham (2018).https://doi.org/10.1007/978-3-030-01270-0\n39\n8. Zhou, Y., Tuzel, O.: VoxelNet: end-to-end learning for point cloud based 3D object\ndetection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 4490–4499 (2018)\n9. Yan, Y., Mao, Y., Li, B.: SECOND: sparsely embedded convolutional detection.\nSensors 18(10), 3337 (2018)\n10. Yang, Z., Sun, Y., Liu, S., Jia, J.: 3DSSD: point-based 3d single stage object detec-\ntor. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 11037–11045 (2020)\n11. He, C., Zeng, H., Huang, J., Hua, X., Zhang, L.: Structure aware single-stage 3D\nobject detection from point cloud. In: 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 11870–11879 (2020)\n12. Zheng, W., Tang, W., Chen, S., Jiang, L., Fu, C.-W.: CIA-SSD: conﬁdent IoU-\naware single-stage object detector from point cloud. In: AAAI (2021)\n13. Zheng, W., et al.: SE-SSD: Self-Ensembling Single-Stage Object Detector From\nPoint Cloud. In: Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition(CVPR), pp. 14494–14503 (2021)\n14. Qi, C., Su, H., Mo, K., Guibas, L.: PointNet: deep learning on point sets for 3d\nclassiﬁcation and segmentation. In: 2017 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 77–85 (2017)\n15. Qi, C., Yi, L., Su, H., Guibas, L.: PointNet++: deep hierarchical feature learning\non point sets in a metric space. In: NIPS (2017)\n16. Qi, C.R., Liu, W., Wu, C., Su, H., Guibas, L.J.: Frustum PointNets for 3D object\ndetection from RGB-D data. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 918–927 (2018)\n17. Shi, S., Wang, X., Li, H.: PointRCNN: 3D object proposal generation and detection\nfrom point cloud. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 770–779 (2019)\n18. Ding, Z., Han, X., Niethammer, M.: VoteNet: a deep learning label fusion method\nfor multi-atlas segmentation. In: Shen, D., et al. (eds.) MICCAI 2019. LNCS, vol.11766, pp. 202–210. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-\n32248-9\n23\n19. Shi, S., et al.: PV-RCNN: point-voxel feature set abstraction for 3D object detec-\ntion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pp. 10529–10538 (2020)\n20. Liu, W., et al.: SSD: single shot MultiBox detector. In: Leibe, B., Matas, J., Sebe,\nN., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9905, pp. 21–37. Springer, Cham(2016). https://doi.org/10.1007/978-3-319-46448-0\n2\n21. Zhao, N., Chua, T.S., Lee, G.H.: SESS: self-ensembling semi-supervised 3D object\ndetection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 11079–11087 (2020)\n22. Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: the KITTI\ndataset. Int. J. Robot. Res. 32(11), 1231–1237 (2013)\n23. Chen, Y., Liu, S., Shen, X., Jia, J.: Fast point R-CNN. In: Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 9775–9784 (2019)\n24. Yang, Z., Sun, Y., Liu, S., Shen, X., Jia, J.: STD: sparse-to-dense 3D object detector\nfor point cloud. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pp. 1951–1960 (2019)', 'Few-Shot Segmentation\nvia Complementary Prototype Learning\nand Cascaded Reﬁnement\nHanxiao Luo, Hui Li, Qingbo Wu(B), Hongliang Li, King Ngi Ngan,\nFanman Meng, and Linfeng Xu\nUniversity of Electronic Science and Technology of China, Chengdu, 611731, China\n{lhx,huili }@std.uestc.edu.cn, {qbwu,hlli,knngan,fmmeng,lfxu }@uestc.edu.cn\nAbstract. Prototype learning has been widely explored for few-shot\nsegmentation. Existing methods typically learn the prototype from theforeground features of all support images, which rarely consider the\nbackground similarities between the query images and the support\nimages. This unbalanced prototype learning strategy limits its capa-bility to mutually correct the segmentation errors between the fore-\nground and background. In this paper, we propose a Complementary\nPrototype Learning and Cascaded Reﬁnement (CPLCR) network forfew-shot segmentation. Firstly, both the foreground and background fea-\ntures of the support images are used to learn our complementary proto-\ntypes. Then, the foreground and background similarity maps are jointlyderived between the query image feature and our complementary pro-\ntotypes, which capture more comprehensive prior information. Finally,\nwe fuse the query image feature, foreground prototype and the fore-ground/background similarity maps together, and feed them to a cas-\ncaded reﬁnement module, which recursively reuses the output of previ-\nous iteration to reﬁne the segmentation result. Extensive experimentalresults show that the proposed CPLCR model outperforms many state-\nof-the-art methods for 1-shot and 5-shot segmentation.\nKeywords: Few-shot segmentation\n·Semantic segmentation ·\nPrototype learning\n1 Introduction\nDeep learning methods based on convolutional neural networks have achieved\nprogressive success in most visual tasks, which depends on a large amount of\nlabeled data to supervise the training process [ 1]. However, the cost of pixel-\nlevel annotation is very expensive especially for the dense prediction tasks such\nas semantic segmentation tasks [ 8,18,24]. Under the situation that the labeled\ndata is not enough or the model is adapted to unseen categories, the segmentationperformance will decreases sharply. Though additional data available can help\nfor training, the ﬁne-tune process still consumes training time and computing\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 484–495, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_40', 'Few-Shot Segmentation via Complementary Prototype Learning 485\nresources. Few-shot segmentation methods [ 9,16,19,20] are proposed to alleviate\nexpensive annotation cost and improve the model generalizability for unseenclasses, which can only leverage a small amount of labeled samples to deal with\nnew classes.\nMost of the current few-shot segmentation methods usually adopt shared\nfeature extractor to perform feature extraction on support images and query\nimages, and leverage average pooling strategy to generate the prototype vector\nfor each category. Then, segmentation of query images is achieved by analysingthe relationship between the query feature and the prototype vector, such as\ncalculating the cosine similarity matrix, generating cascaded feature maps, or\nfeature fusion. For example, Shaban et al. proposed a parallel two-branch net-work OSLSM [ 5], which solves the few-shot segmentation problem by learning\ndiﬀerent classiﬁer weights for diﬀerent categories. Co-FCN [ 12] directly cascaded\nthe feature of support set and query set to achieve segmentation. In order to\nextract the foreground and background information in support set eﬀectively,\nSG-One [ 11] used masked average pooling to obtain the guidance features, and\ncalculated the cosine similarity between the guidance features and the query\nfeatures. PANet [ 13] optimized prototype representations by adopting proto-\ntype alignment regularization for support branch and query branch, and fullymined the information of support images. FWB [ 15] used the diﬀerence between\nforeground and background of support images to generate a more discrimina-\ntive prototype vector, and obtained a similarity mask by calculating the cosinesimilarity between the query feature and the prototype vector.\nFor a training episode in few-shot segmentation, the areas annotated at pixel-\nwise is seen as the foreground of support images and the other areas is seen as thebackground. Existing prototypical feature based few-shot segmentation methods\n[3,15,22,23] typically learn the prototype from the foreground features of all sup-\nport images, which rarely consider the background similarities between the queryfeature and the support feature. However, due to the diversity of visual content,\nthe background segmentation may be easier than the foreground segmentation\nand vice versa, which provides important complementary information for mutual\ncorrection.\nIn this paper, we propose a Complementary Prototype Learning and Cas-\ncaded Reﬁnement (CPLCR) network for few-shot segmentation. Adopting com-\nplementary prototypes and leveraging the background similarities between query\nfeatures and support features, CPLCR can eﬀectively correct the segmentationerrors between the foreground and background, so that it adapt to unseen cat-\negories well and achieve better segmentation performance. Firstly, the comple-\nmentary prototypes including foreground prototype and background prototypeare extracted from support features. Then, the foreground and background simi-\nlarity maps are jointly derived between the query feature and our complementary\nprototypes, which capture more comprehensive prior information. Finally, thequery image feature, foreground prototype and the foreground/background sim-\nilarity maps are fused together, and we feed the fusion feature to a cascaded\nreﬁnement module to reﬁne the segmentation result.', '486 H. Luo et al.\nTo summarize, this paper has following contributions:\n1. We propose a Complementary Prototype Learning and Cascaded Reﬁnement\n(CPLCR) network for few-shot segmentation, which can mutually correct\nthe segmentation errors between the foreground and background by gener-ating complementary prototypes and exploiting the background similarities\nbetween query features and support features.\n2. In CPLCR, we design a Prototype Relationship Module (PRM) to predict\nthe foreground and background similarity maps which can better represent\nthe similarities between support prototypes and query features. Besides, we\nadopt a cascaded reﬁnement module, which recursively reuses the output ofprevious iteration to reﬁne the segmentation result.\n3. Extensive experimental results show that our CPLCR outperforms many\nstate-of-the-art methods for 1-shot and 5-shot segmentation by eﬀectively\ncapturing the background similarities between support images and query\nimages.\n2M e t h o d\n2.1 Method Overview\nThe overall structure of our CPLCR is shown in Fig. 1. Depending on the support\npixel-wise annotation mask, we generate complementary prototypes including\nthe foreground prototype and the background prototype from support features\nby the Prototype Generation Module (PGM). Then the foreground and back-ground similarity maps are jointly derived between the query feature and our\ncomplementary prototypes by the Prototype Relationship Module (PRM). The\nnext feature fusion module fuses query feature, support foreground prototypeand foreground/background similarity maps to generate segmentation features.\nLastly, the cascaded reﬁnement module is designed to generate segmentation\nmasks for query images and reﬁne the segmentation result.\n2.2 Complementary Prototype Learning and Cascaded Reﬁnement\nFeature Extraction Network. Features in shallow layers usually contain low-\nlevel characteristic, such as texture, edge and color. In contrast, deep features\nimply high-level semantic information such as object categories. In order toenhance extracted features, we elaborate the feature extraction network which\nperforms feature fusion on the shallow feature and the deep feature to contain\nlow-level image characteristic and high-level semantic information.\nFirstly, the input support image or query image Iis transformed to feature\nF\nIwhose size is a quarter of the input image by sequentially passing the convo-\nlutional layer, the batch normalization layer, the ReLU activation layer and themax pooling layer. More speciﬁcally, the convolutional layer has a kernel size of\n7×7, a padding size of 2 and a step size of 1. Then, F\nIis sent to Block 1a n d\nBlock 2 to obtain the low-level features Flow. The next Block 3c o n v e r t Flowto', 'Few-Shot Segmentation via Complementary Prototype Learning 487\nFig. 1. The schematic illustration of the Complementary Prototype Learning and Cas-\ncaded Reﬁnement (CPLCR) network.\nFhigh. Note that the Block 1−3 have a similar structure with the feature extrac-\ntion part of ResNet, but the ﬁrst convolutional layer of Block 3 has a kernel size\nof 3×3, a step size of 1, a padding of 2, and a void rate of 2.\nTo make the feature contains low-level characteristic and high-level semantic\ninformation, we cascade Flowand Fhighalong channel dimension to acquire the\nfused feature Ffu s i o n . Finally, a convolutional layer with a kernal size of 3 ×3, a\nstep size of 1, a padding of 2, and a void rate of 2 is used to reduce the dimension\nofFfu s i o n , and the ﬁnal output feature Fis obtained through another ReLU\nactivation function and Dropout layer.\nPrototype Generation Module. In this section, we explain the generation of\ncomplementary prototypes. The inputs of prototype generation module are sup-\nport pixel-wise annotation mask MSand support feature FS. The corresponding\noutputs is the complementary prototypes including the support foreground pro-totype P\nFand the background prototype PB. For the foreground prototypes,\nwe ﬁrst perform dimensional transformation for MSto make it consistent with\nthe spatial size of support feature. Then FSand the transformed MSconduct\nelement-wise multiplication to obtain Ffo re, which presents the foreground cate-\ngories of support images. Finally we conduct average pooling on the foregroundfeatures F\nfo reto get the foreground prototype PF. Note that, Tand APin the\nfollowing descriptions indicate dimensional transformation and average pooling\nrespectively.\nFfo re=FS⊙T(MS) (1)\nPF=AP(Ffo re) (2)', '488 H. Luo et al.\nFor the background prototypes, we ﬁrst reverse MSand perform dimensional\ntransformation to make it consistent with the spatial size of support feature, thenconduct element-wise multiplication on F\nSand the transformed MSto obtain\nFback. Finally we perform average pooling to the background features Fbackto\nget background prototype PB.\nFback=FS⊙T(1−MS) (3)\nPB=AP(Fback) (4)\nPrototype Relationship Module. We design a prototype relationship mod-\nule (PRM) to predict the foreground and background similarity maps, whichanalyses the background similarities between query features and support proto-\ntype. As shown in Fig. 1, the query feature F\nQ, foreground prototype PFand\nbackground prototype PBare inputs of the module, the foreground similarity\nmap MFand the background similarity map MBare corresponding outputs.\nWe ﬁrst transfer the query feature FQ∈RB×C×H×WtoF/prime\nQ∈RB×N×C, then\nconcatenate PFand PBalong the channel dimension. Next we multiply the\nconcatenation result with F/prime\nQ∈RB×N×Cto obtain F/prime/prime\nQ, the batch matrix multi-\nplication is denoted as BMin the following equations. After a Softmax layer and\nanother dimension transformation, the prototype relationship module generatesforeground similarity map M\nFand background similarity map MBwhich have\nthe same size as query features.\nF/prime/prime\nQ=BM/parenleftbig\nF/prime\nQ,[PF,P B]/parenrightbig\n(5)\nPr o b F=Softmax/parenleftbig\nF/prime/prime\nQ/parenrightbig\n(6)\nMF,M B=T(Pr o b F) (7)\nFeature Fusion Module. The feature fusion module aims to perform feature\nfusion on query features, foreground prototypes, and foreground/background\nsimilarity maps, which can promote the network to eﬀectively leverage the sim-ilarities between support prototypes and query features. We ﬁrst upsample P\nF\nto get P/prime\nF, then concatenate P/prime\nF,FQ,MFand MBto obtain new feature F/prime.\nNext, perform feature transformation and channel reduction on F/primeby the con-\nvolutional layer with a kernel size of 3 ×3, a step size of 1, a padding of 2, and a\nvoid rate of 2. The ﬁnal segmentation feature Fsegare generated after a ReLU\nactivation function and a Dropout layer.\nFseg=Dropout (ReLU (Conv (F/prime))) (8)', 'Few-Shot Segmentation via Complementary Prototype Learning 489\nFig. 2. Illustration of the cascaded reﬁnement module.\nCascaded Reﬁnement Module. In order to acquire more reﬁned predictions\nfor query images, the segmentation prediction module adopts a cascaded struc-\nture, which recursively reuses the output of previous iteration to reﬁne the seg-mentation result. The schematic diagram is shown in Fig. 2. The segmentation\nfeature F\nsegis concatenated with prediction mask M/prime\nQ, and the concatenation\nresult is sent into three cascaded residual blocks to obtain F/prime/prime\nseg.E a c ho ft h e\nblock is consist of two 3 ×3 convolution layers and a ReLU activation function\nlayer. Then F/prime/prime\nsegare sent into a Atrous Spatial Pyramid Pooling module (ASPP)\nproposed in [ 2] to capture multi-scale contextual information. The output F/prime/prime/prime\nsegof\npyramid pooling module passes a 1 ×1 convolution layer whose channel amount\nis the number of classes. In the end, the ﬁnal prediction mask M/prime\nQare gener-\nated after dimension transform and Softmax operation. Note that the initial\nsegmentation probability mask M/prime\nQis initialized to 0.\n2.3 Loss Function\nIn the proposed Complementary Prototype Learning and Cascaded Reﬁnement\n(CPLCR) network, we adopt cross-entropy loss function to measure the diﬀer-\nence between the predicted segmentation spectrum M/prime\nQand the groundtruth\nMQof the query image.\nloss seg=LCrossEntropy/parenleftbig\nM/prime\nQ,M Q/parenrightbig\n(9)\n3 Experiments\nTo evaluate the performance of our proposed method, we conduct extensive\nexperiments on the Pascal-5idataset [ 12]. More speciﬁcally, we not only eval-\nuate the segmentation performance of the proposed method quantitatively and\nqualitatively, but also analyze the complexity of the model.\n3.1 Datasets and Evaluation Metric\nIn this paper, we choose Pascal-5ito evaluate the proposed method. Pascal-5i\nis consists of images from PASCAL VOC 2012 [ 4] and extra annotations from', '490 H. Luo et al.\nSBD [ 6]. It contains a total of 20 classes and is evenly divided into 4 subsets\n(i∈0,1,2,3), each of which contains 5 classes. In the speciﬁc experimental pro-\ncess, we adopt a cross-validation strategy which trains the model on 3 subsets\nand perform evaluation on the remaining subset. Following the inference strategy\nof few-shot segmentation, 1000 support-query pairs from the test subset are ran-domly sampled for evaluation. Besides, we adopt mean intersection-over-union\n(mIoU) as the evaluation metric for objective performance and we compare the\nproposed method with 1-NN [ 12], LogReg [ 12], OSLSM [ 12], co-FCN [ 11], SG-\nOne [23], AMP [ 7], PANet [ 14], FWB [ 10], CANet [ 22], PGNet [ 21], PPNet [ 7]\nand RPMMs [ 17].\n3.2 Implementation Details\nThe proposed method is implemented on the framework of Pytorch and all the\nexperiments run on a workstation with a single NVIDIA Titan Xp GPU. During\nthe training process, input images are ﬁrstly transformed with horizontal ﬂip\nand random scale from [1 −1.5], all the images are normalized with a mean\nvalue of 0 .456 and 0 .406 and a variance of 0 .229, 0 .224 and 0 .225, ﬁnally the\nimages are cropped to 321 ×321 as training samples. For the testing process,\nwe do not adopt any additional data augmentation and crop the test images to\n321×321 directly. SGD optimizer with a momentum of 0 .9 and a weight decay\nof 0 .0005 is adopted to optimize the model. The learning rate is set to 0 .00025\nand the batch size is 4. The model is trained for a total of 200 epochs. When\nverifying the 5-shot segmentation performance, for a query image of class c,w e\nrandomly sample 5 support images and corresponding annotation masks with\nthe same class, and concatenate them for model input.\n3.3 Comparison to State-of-the-Art\nAt ﬁrst, we quantitatively compare the 1-shot performance of our model with\nthe state-of-the-art methods. From Table 1, it’s obvious that our method has\nthe best overall performance, which has an average mIoU of 57 .61 on 4 subsets\nand achieves an increase of 1 .27 compared with the best-performing RPMMs.\nThe mIoU on Pascal-5\n0and Pascal-53are 56 .26 and 53 .73, which are 1 .11 and\n3.05 higher than RPMMs respectively, and the performance on Pascal-51is only\n0.29 lower than RPMMs. Although the mIoU on Pascal-52is lower than that of\nPPNet, the performance of our method on the other three subsets is much higherthan PPNet, more speciﬁcally, the average mIoU is 4 .77 higher than PPNet.\nIn Tabel 2, we compare our model with the state-of-the-art methods in\n5-shot experiment setting. Due to the increase of supporting samples, the aver-age mIoU on 4 subsets increased by 1 .45, which is better than that of 1-shot.\nBesides, performance of the proposed method is still the best, with an average\nmIoU of 59 .06, which gets an increase of 0 .56 compared to the best-performing\nPGNet in the compared methods.\nTo intuitively present the eﬀectiveness of our method, we also visualizes the\nprediction results of 1-shot segmentation. The segmentation results of birds,', 'Few-Shot Segmentation via Complementary Prototype Learning 491\nTable 1. Comparison of 1-shot performance with the state-of-the-art on Pascal-5i.\nMethod Pascal-50Pascal-51Pascal-52Pascal-53Mean\n1-NN [ 12] 25.30 44.90 41.70 18.40 32.60\nLogReg [ 12]26.90 42.90 37.10 18.40 31.40\nOSLSM [ 12]33.60 55.30 40.90 33.50 40.80\nco-FCN [ 11]36.70 50.60 44.90 32.40 41.10\nSG-One [ 23]40.20 58.40 48.40 38.40 46.30\nAMP [ 7] 41.90 50.20 46.70 34.70 43.40\nPANet [ 14] 42.30 58.00 51.10 41.20 48.10\nFWB [ 10] 47.04 59.64 52.51 48.27 51.90\nCANet [ 22] 52.50 65.90 51.30 51.90 55.40\nPGNet [ 21] 56.00 66.90 50.60 50.40 56.00\nPPNet [ 7] 48.58 60.58 55.71 46.47 52.84\nRPMMs [ 17]55.15 66.91 52.61 50.68 56.34\nOurs 56.26 66.62 53.83 53.73 57.61\nTable 2. Comparison of 5-shot performance with the state-of-the-art on Pascal-5i.\nMethod Pascal-50Pascal-51Pascal-52Pascal-53Mean\n1-NN [ 12] 34.50 53.00 46.90 25.60 40.00\nLogReg [ 12]35.90 51.60 44.50 25.60 39.30\nOSLSM [ 12]35.90 58.10 42.70 39.10 43.95\nCo-FCN [ 11]37.50 50.00 44.10 33.90 41.40\nSG-One [ 23]41.90 58.60 48.60 39.40 47.10\nAMP [ 7] 41.80 55.50 50.30 39.90 46.90\nPANet [ 14] 51.80 64.60 59.80 46.05 55.70\nFWB [ 10] 50.87 62.86 56.48 50.09 55.08\nCANet [ 22] 55.50 67.80 51.90 53.20 57.10\nPGNet [ 21] 57.70 68.70 52.90 54.60 58.50\nRPMMs [ 17]56.28 67.34 54.52 51.00 57.30\nOurs 58.64 67.46 54.28 55.85 59.06\ntrains, cats, airplanes, cars, dogs, boats, bottles, and sheep are respectively shown\nin Fig. 3. For better presentation of segmentation results, the pixel-wise anno-\ntation masks and the predicted mask for query images are marked with redforeground. When the objects in query images have diﬀerent sizes (birds in the\nﬁrst row, cats in the third row, and boats in the seventh row), our method can\neﬀectively segment the foreground objects. When the query image has multipleinstances (such as the sheep in the fourth, ﬁfth and sixth columns of the ninth\nrow), our method can also segment all the foreground objects without missing\ndetection.', '492 H. Luo et al.\nFig. 3. Visualizations of the 1-shot predicted segmentation results.', 'Few-Shot Segmentation via Complementary Prototype Learning 493\n3.4 Ablation Study\nWe conduct ablation experiments on Pascal-5ito exploit the inﬂuence of diﬀerent\ncascaded layers on the segmentation performance. The comparison results ares h o w ni nT a b l e 3and Table 4. Concretely, in Table 3, when the number of\ncascaded layers increases from 1 to 4, the average mIoU of 1-shot segmentation on\n4 subsets gradually increases from 55 .21 to 57 .61. When the number of cascaded\nlayers is 5, the mIoU on Pascal-5\n0and Pascal-52subsets are 55 .18 and 53 .81,\nand the performance decreases by 1 .08 and 0 .02 respectively, which leads to the\naverage mIoU performance decreasing by 0 .05.\nTable 3. 1-shot segmentation performance for diﬀerent cascading layers on Pascal-5i.\nCascaded layers Pascal-50Pascal-51Pascal-52Pascal-53Mean\n1 53.49 66.16 51.15 50.02 55.21\n2 54.66 66.35 52.77 52.53 56.58\n3 55.43 66.64 53.16 53.51 57.19\n4 56.26 66.62 53.83 53.73 57.61\n5 55.18 66.97 53.81 54.29 57.56\nFor 5-shot setting, the results present similar changes with 1-shot performance\nin Table 3. In Table 4, when the number of cascaded layers increases from 1 to 4,\nthe average mIoU of 5-shot segmentation on 4 subsets gradually increases from56.76 to 59 .06, and the performance improves by 2 .3. However, when the number\nof cascaded layers is 5, the mIoU on Pascal-5\n1and Pascal-53subsets are 67 .02 and\n55.58, and the performance decreases by 0 .44 and 0 .27 respectively, which leads to\nthe average mIoU performance decreasing by 0 .08. As the number of cascaded lay-\ners increases, the training time and resource consumption will gradually increase.\nConsidering the trade-oﬀ between segmentation performance and resource con-sumption, the number of cascaded layers in our experiments is set to 4.\nTable 4. 5-shot segmentation performance for diﬀerent cascading layers on Pascal-5i.\nCascaded layers Pascal-50Pascal-51Pascal-52Pascal-53Mean\n1 56.37 66.11 51.82 52.75 56.76\n2 57.99 66.79 53.50 53.96 58.06\n3 57.96 66.68 54.23 54.83 58.43\n4 58.64 67.46 54.28 55.85 59.06\n5 58.86 67.02 54.41 55.58 58.98\nIn addition, we also make comparison on the number of training parame-\nters. In Table 5, OSLSM [ 12] has a total of 276 .7M parameters, with the largest', '494 H. Luo et al.\nnetwork complexity and calculation amount, followed by AMP [ 7], co-FCN [ 11],\nPPNet [ 7] and RPMMs [ 17]. On the contrary, the proposed method has a param-\neter amount of 19 .0M, which is equivalent to the parameter amount of SG-One\n[23] and CANet [ 22], and rank only second only to PANet [ 14].\nTable 5. Comparison of diﬀerent methods on the number of parameters.\nMethod OSLSM co-FCN SG-One AMP PANet CANet PPNet RPMMs Ours\nParams(M) 276.7 34.2 19.0 34.7 14.7 19.0 23.5 19.6 19.0\n4 Conclusion\nIn this paper, we propose a Complementary Prototype Learning and Cascaded\nReﬁnement (CPLCR) network for few-shot segmentation. By leveraging the\ncomplementary prototypes and the foreground/background similarity maps, themodel can eﬀectively exploit the background similarities between the query\nimages and the support images to promote generalization ability for unseen\nclasses. Extensive experiments and ablation studies on 1-shot and 5-shot seg-mentation have demonstrated the superiority of our proposed method.\nAcknowledgement. This work was partially supported by National Natural Science\nFoundation of China (No. 61971095, 61871078, 61831005, and 61871087).\nReferences\n1. Badrinarayanan, V., Kendall, A., Cipolla, R.: SegNet: a deep convolutional\nencoder-decoder architecture for image segmentation. IEEE Trans. Pattern Anal.Mach. Intell. 39(12), 2481–2495 (2017)\n2. Chen, L.C., Papandreou, G., Schroﬀ, F., Adam, H.: Rethinking atrous convolution\nfor semantic image segmentation. arXiv preprint arXiv:1706.05587 (2017)\n3. Dong, N., Xing, E.P.: Few-shot semantic segmentation with prototype learning. In:\nBMVC, vol. 3 (2018)\n4. Everingham, M., Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The pascal\nvisual object classes (VOC) challenge. Int. J. Comput. Vis. 88, 303–338 (2009)\n5. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal\nvisual object classes (VOC) challenge. Int. J. Comput. Vis. 88(2), 303–338 (2010)\n6. Hariharan, B., Arbel´ aez, P., Bourdev, L.D., Maji, S., Malik, J.: Semantic contours\nfrom inverse detectors. In: 2011 International Conference on Computer Vision, pp.991–998 (2011)\n7. Liu, Y., Zhang, X., Zhang, S., He, X.: Part-aware prototype network for few-shot\nsemantic segmentation. ArXiv abs/2007.06309 (2020)\n8. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 3431–3440 (2015)', 'Few-Shot Segmentation via Complementary Prototype Learning 495\n9. Luo, K., Meng, F., Wu, Q., Li, H.: Weakly supervised semantic segmentation by\nmultiple group cosegmentation. In: 2018 IEEE Visual Communications and Image\nProcessing (VCIP), pp. 1–4 (2018)\n10. Nguyen, K.D.M., Todorovic, S.: Feature weighting and boosting for few-shot seg-\nmentation. In: 2019 IEEE/CVF International Conference on Computer Vision\n(ICCV), pp. 622–631 (2019)\n11. Rakelly, K., Shelhamer, E., Darrell, T., Efros, A.A., Levine, S.: Conditional net-\nworks for few-shot semantic segmentation. In: ICLR (2018)\n12. Shaban, A., Bansal, S., Liu, Z., Essa, I., Boots, B.: One-shot learning for semantic\nsegmentation. ArXiv abs/1709.03410 (2017). arXiv:1709.03410\n13. Siam, M., Oreshkin, B.N., Jagersand, M.: AMP: adaptive masked proxies for few-\nshot segmentation. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pp. 5249–5258 (2019)\n14. Wang, K., Liew, J., Zou, Y., Zhou, D., Feng, J.: PANet: few-shot image semantic\nsegmentation with prototype alignment. In: 2019 IEEE/CVF International Con-\nference on Computer Vision (ICCV), pp. 9196–9205 (2019)\n15. Wang, K., Liew, J.H., Zou, Y., Zhou, D., Feng, J.: PANet: few-shot image semantic\nsegmentation with prototype alignment. In: Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pp. 9197–9206 (2019)\n16. Xu, X., Meng, F., liang Li, H., Wu, Q., Ngan, K.N., Chen, S.: A new bounding box\nbased pseudo annotation generation method for semantic segmentation. In: 2020\nIEEE International Conference on Visual Communications and Image Processing\n(VCIP), pp. 100–103 (2020)\n17. Yang, B., Liu, C., Li, B., Jiao, J., Ye, Q.: Prototype mixture models for few-shot\nsemantic segmentation. ArXiv abs/2008.03898 (2020)\n18. Yang, M., Yu, K., Zhang, C., Li, Z., Yang, K.: DenseASPP for semantic segmenta-\ntion in street scenes. In: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 3684–3692 (2018)\n19. Yang, Y., Meng, F., Li, H., Ngan, K., Wu, Q.: A new few-shot segmentation network\nbased on class representation. In: 2019 IEEE Visual Communications and Image\nProcessing (VCIP), pp. 1–4 (2019)\n20. Yang, Y., Meng, F., Li, H., Wu, Q., Xu, X., Chen, S.: A new local transformation\nmodule for few-shot segmentation. ArXiv abs/1910.05886 (2020)\n21. Zhang, C., Lin, G., Liu, F., Guo, J., Wu, Q., Yao, R.: Pyramid graph networks\nwith connection attentions for region-based one-shot semantic segmentation. 2019IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9586–9594\n(2019)\n22. Zhang, C., Lin, G., Liu, F., Yao, R., Shen, C.: CANet: class-agnostic segmentation\nnetworks with iterative reﬁnement and attentive few-shot learning. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n5217–5226 (2019)\n23. Zhang, X., Wei, Y., Yang, Y., Huang, T.: SG-One: similarity guidance network for\none-shot semantic segmentation. IEEE Trans. Cybern. 50(9), 3855–3865 (2020)\n24. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npp. 2881–2890 (2017)', 'Couple Double-Stage FPNs with Single\nPipe-Line for Solar Speckle Images Deblurring\nFuhai Li1, Murong Jiang1(B), and Lei Yang2\n1School of Information Science and Engineering, Y unnan University, Kunming, China\njiangmr@ynu.edu.com\n2Y unnan Observatories, Chinese Academy of Sciences, Kunming, China\nAbstract. Solar speckle images acquired by ground-based optical telescope usu-\nally are blurred or degraded seriously with more noise and fuzzy local details. Most\nof the deep learning deblurring methods are suitable for the natural images whichhave sufﬁcient contextualized and gradient information, but for solar speckle\nimages may cause some problems such as over-smoothing, high-frequency loss,\nand artifacts generated. In this paper, we propose a deblurring method based oncoupling double-stage feature pyramid networks (FPN) with a single pipe-line\n(DSFSP) to reconstruct high-resolution solar speckle images. In stage1, one FPN\nis used to recover structure features; In stage2, another FPN is used to enhance thestructural contextualized, and the single pipe-line coupled with this FPN is used\nto extract gradient information. After fusing these to generate a reconstructed\nimage, discriminators are used to make it closer to the reference. Experimentsshow that DSFSP has a strong ability to strengthen the gradient spatial and con-\ntextualized information, improve image clarity, restore high-frequency details and\ndrop artifacts.\nKeywords: Double-stage ·FPN·Single pipe-line ·Gradient spatial ·\nContextualized information ·Solar speckle image\n1 Introduction\nDue to the inﬂuence of atmospheric turbulence and atmospheric disturbance, the solar\nactivity observation image by ground-based optical telescope would be seriously blurred\nor degraded, which needs image restoration method for reconstruction. As deep learningis widely used in computer vision and image processing, how to use deep learning\nmethods to reconstruct solar speckle images has become one of the research hotspots\nin astronomical image processing. In recent years, two blind deblurring methods basedon deep learning have been widely used. These two methods are the kernel estimation\nmethod and the image-to-image regression method.\nFor the method of kernel estimation, there has been lots of work with great results\n[1–4]. This kind of method is often used in super-resolution and deblurring tasks. Since\nGT kernels are usually needed in training, it is generally active on synthetic datasets.\nHowever, the degradation process of blurred images taken naturally is unknown, whichgreatly increases the difﬁculty of deblurring.\n© Springer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 496–507, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_41', 'Couple Double-Stage FPNs with Single Pipe-Line 497\nFor the method of image-to-image regression, some work [ 5,6] has found that\nskipping the step of blur kernel estimation would omit the connection of prior hypothesis,and get a more natural deblurred image. As discovered by Ledig et al. [ 7], a generative\nadversarial network (GAN) can reconstruct sharp edges effectively. However, improper\nhandling of the adversarial process may distort the local detail in the generated image.To solve this problem, Ma et al. [ 8] use a gradient-guided approach to help the generator\npay more attention to local detail. Kupyn et al. [ 9] introduce a multi-scale generator\nbased on a feature pyramid network (FPN) [ 10] to obtain features at different scales.\nAnd they constructed a two-scale discriminator, focusing on local and global features\nrespectively. The structure allows them to obtain brilliant results on the public datasets.\nSome reconstruction works on the astronomical image also use a deep learning network.Ren et al. [ 11] apply Cycle-GAN [ 12] to the restoration of solar images. Based on\nthe original network structure, they improve the content loss and add a perceptual loss\nto make the deblurred image more realistic. Correspondingly, Jia et al. [ 13]a l s ou s e\nCycle-GAN for the reconstruction of solar images. But the difference is that they use a\nframe selection algorithm to select multiple parts with large information content from alarge number of blurred images and use them as the training set of the neural network.\nThis approach allows the network to have a strong generalization ability and can restore\nimages with great visual effects.\nThe above-mentioned work has made great progress in reconstruction, but it is not\nsuitable for the reconstruction task of solar speckle images. The main reason is that solar\nspeckle images usually contain single structural features, low contrast and fuzzy localdetails.\nIn order to strengthen gradient spatial and contextualized features, a deblurring\nmethod based on coupling double-stage FPNs with a single pipe-line (DSFSP) is pro-posed. The paper content is arranged as follows: In Sect. 2, three parts including design\nideas, generator and discriminator, and loss function are introduced in detail; In Sect. 3,\nexperiments and analysis prove the effectiveness of DSFSP; In Sect. 4, advantages and\ndisadvantages of this method are summarized.\n2 Method Description\n2.1 Main Idea\nIn order to make the results obtained by DSFSP closer to the reconstruction effect of the\nLevel1+ method [ 14] used by the Y unnan Observatory, part of reconstruction results of\nthe Level1+ method are used as the supervised image (GT Sharp), and the paired blurredimage be the input image (Blur). The main framework of our method DSFSP is shown in\nFig. 1. The Sharps (Stage1 Sharp, Stage2 Sharp) are the reconstruction results of stage1\nand stage2, and the Gradient is the gradient information captured by the stage2. The GTGradient is the gradient image of GT Sharp.\nDSFSP has two contributions for solar speckle images reconstruction:\n1) Redesign the gradient structure which is proposed by Ma et al. [ 8] to better extract\nthe spatial information. It uses a single pipe-line method to obtain features from the', '498 F. Li et al.\ndecoder of FPN2 step by step. This design is quite different from theirs and greatly\nreduce the dependence on the gradient information of original input images.\nFig. 1. The main framework of our method (DSFSP).\n2) Design a double-stage generator structure based on FPN. With the help of FPN’s\npowerful multi-scale capabilities, it can easily extract the contextualized informa-tion. This double-stage structure can split complex deblurring problem into multiple\nsub-problems. After the ﬁrst stage processed, the gradient structure of the second\nstage can capture more accurate gradient information. Therefore, this double-stagestructure can be more effective than a single-stage.\nAbove two contributions can enhance the model’s ability to acquire contextualized\nand spatial information at the same time. But this requires that gradient information\nmust be sufﬁciently accurate, so the gradient information as a gradient image would be\noutput separately, a discriminator used to optimize it. Therefore, DSFSP should have\ntwo discriminators, one for discriminating generated images (Dis1) and another for\ndiscriminating gradient images (Dis2).\n2.2 Description of the Generator\nDouble-Stages FPNs and Single-Pipe Gradient Structure. To prevent the generator\nfrom acquiring noisy gradient information, we construct a double-stage FPNs structure.\nAnd this structure inevitably increases the difﬁculty of model training, so we introduce\nthe supervised attention module (SAM) proposed by Zamir et al. [ 15] to help the model\nenable progressive learning. More precisely, we use SAM to connect two FPN networksthat can reconstruct the image step by step. Then we output features of different scales\nfrom the decoder of the second FPN network, and import them into a single-pipe structure\nto capture gradient information. This single-pile design without down sampling cangreatly retain the spatial information of the image. And we fuse outputs of single-pipe\nand FPN to get the ﬁnal reconstruction result. In addition, to reduce the artifacts caused\nby the generator after feature stacking, we adopt the Residual in Residual Dense Block(RRDB) module proposed by Wang et al. [ 16], which can improve its stability during\ngenerator training and produce fewer artifacts. The generator structure of our method is\nshown in Fig. 2.', 'Couple Double-Stage FPNs with Single Pipe-Line 499\nGradient Spatial and Contextualized Information. With gradients, powerful spatial\ninformation can be captured. Unlike public datasets, the gradient information of solarspeckle images is very difﬁcult to obtain. Therefore, our method gradually captures\nthe gradient information from the reconstruction process. The gradient information\nacquisition can refer to Fig. 3.\nFig. 2. The structure of generator.\nFig. 3. The gradient information.\nInspired by Kupyn et al. [ 9], we also use the FPN as an encoder-decoder for blurred\nimages. The powerful multi-scale design of FPN can focus on image features of different\nscales and positions, so as to obtain rich contextualized information. Different from the\nrich contextualized information of natural images, the contextualized information of\nsolar speckle images is mainly reﬂected in the rice grain’s shape of the speckle and thegap between the rice grains. It can be seen from the feature heatmaps (Fig. 4) that FPN\ncan effectively pay attention to these, so as to extract the contextualized information of\nsolar speckle images.\nLoss Function of the Generator . Since the loss functions used in the generated sharp\nimages of stage1 and stage2 are the same, we take the loss of stage2 as an example. For\nthe convenience of naming, we call the “Stage1 Sharp” as “Sharp1” and the “Stage2Sharp” as “Sharp2”.\nPixel Content Loss. We use the mean absolute error (MAE) as the pixel content loss for\nthe reconstructed sharp image. Consistent with the consideration of the sharp image, we', '500 F. Li et al.\nFig. 4. Part of the feature heatmaps from FPN.\nalso use MAE as the pixel content loss for the reconstructed gradient image. In order to\nensure that the sharp image and the supervised image have similar edges, this paper usesthe MSE to compare the gradient images of these two. Therefore, the pixel content loss\nconsists of three parts, we name these losses as LReconSharp2\nMAE,LReconGradient\nMAE, and LEdgeSharp2\nMSE\nrespectively. The superscript “Sharp2” indicates the loss for the reconstructed image of\nstage2. They are represented by formula ( 1), formula ( 2), and formula ( 3) respectively.\nLReconSharp2\nMAE=1\nNN/summationdisplay\ni=1||Gsharp2(IBlur\ni)−IGTsharp\ni||1 (1)\nLReconGradient\nMAE =1\nNN/summationdisplay\ni=1||Ggradient (IBlur\ni)−M(IGTsharp\ni)||1 (2)\nLEdgeSharp2\nMSE=1\nNN/summationdisplay\ni=1||M(Gsharp2(IBlur\ni))−M(IGTsharp\ni)||2\n2 (3)\nThe IBlur\niand IGTsharp\niindicate the i-th blurry input images and the i-th paired super-\nvised images reconstructed by the Level1 +method respectively. The Gsharp2(IBlur\ni)\nand Ggradient (IBlur\ni)respectively represent the sharp image reconstructed in stage2 of the\ngenerator and the gradient image reconstructed by the generator. The M(•)represents\nthe gradient operator of the image [ 8].\nPerceptual Loss. To make better perceptual effect, we introduce perceptual loss [ 17].\nThe calculation formula of the perceptual loss is shown in formula ( 4). The /Phi1(•)\nrepresents the VGG19 network [ 18] for capturing perceptual features.\nLSharp2\nPerceptual=1\nNN/summationdisplay\ni=1||/Phi1(Gsharp2(IBlur\ni))−/Phi1(IGTsharp\ni)||1 (4)\nAdversarial Loss. In order to obtain more high-quality reconstructed images and more\nefﬁcient gradient images, we use relativistic average least squares GAN loss (RaLsGAN)\n[19] as our adversarial loss. It can acquire higher-quality perception effects and realistic\nhigh-frequency information. The calculation formula of its generator part is shown informula ( 5).\nLRaLsGAN\nG=Ez∼pz(z)[(D(G(z))−Ex∼pdata(x)D(x)−1)2]+ Ex∼pdata(x)[(D(x)−Ez∼pz(z)D(G(z))+1)2]\n(5)', 'Couple Double-Stage FPNs with Single Pipe-Line 501\nThe pdata(x)represents the target (real) distribution, and the pz(z)represents the generated\n(fake) distribution. In this relative adversarial loss, the purpose of the discriminator Dis\nto identify the probability that one sample is more real than another. Therefore, as the\nnumber of training iterations increases, the discriminator D(G(z))would increase, while\nthe discriminator D(x)would decrease, and eventually, the two would reach a balance.\nT otal Loss of the Generator . Finally, adding the above loss functions can get the total\nloss of the generator. It is worth mentioning that the same loss except the generatedgradient loss (formula ( 2)) must be used in the stage1 of the generated image (Sharp1).\nThe calculation formula of the total loss function of the generator is shown in formula\n(6).\nLG=min\nG(α( LReconSharp1\nMAE+LReconSharp2\nMAE)+β(LReconGradient\nMAE )+χ(LEdgeSharp1\nMSE+LEdgeSharp2\nMSE)\n+δ(LSharp1\nPerceptual+LSharp2\nPerceptual)+ε(LRaLsGAN\nGSharp1+LRaLsGAN\nGSharp2)+φ(LRaLsGAN\nGGradient)) (6)\nThe LRaLsGAN\nGSharp1,LRaLsGAN\nGSharp2, and the LRaLsGAN\nGGradientrespectively represents the adversarial loss\nfor the Sharp1, the adversarial loss for the Sharp2, and the adversarial loss for the gradient\nimage generated by the generator. It is worth mentioning that the Sharp1 and the Sharp2\nuse one discriminator, and the gradient image uses another different discriminator. Thevalue of the α,β,χ,δ,ε,φare set to 0.01, 0.5, 0.01, 0.05, 0.05, 0.05.\n2.3 Discriminators\nTo obtain a more realistic generated image, we introduce a multi-scale discriminator\ndesigned by Chen et al. [ 20], which can obtain different features at three scales for\nidentiﬁcation. And we use two discriminators of the same structure to respectively dis-criminate the generated images including Sharp1 and Sharp2, and the generated gra-\ndient image. Since stacking convolutional layers would obtain a larger receptive ﬁeld,\nthis stacking method of three different scales can identify areas of different sizes in theimage. Then combining the losses of the three scales as a comprehensive decision helps\nto more accurately identify the reality.\nCorresponding to the generator’s adversarial loss, the RaLsGAN calculation formula\nused by the discriminator is shown in formula ( 7).\nL\nRaLsGAN\nD =Ex∼pdata(x)[(D(x)−Ez∼pz(z)D(G(z))−1)2]+ Ez∼pz(z)[(D(G(z))\n−Ex∼pdata(x)D(x)+1)2] (7)\nTherefore, the loss functions of the two discriminators used in this paper are shown in\nformula ( 8) and formula ( 9) respectively.\nLDSharp=min\nDSharp(LRaLsGAN\nDSharp1+LRaLsGAN\nDSharp2) (8)\nLDGradient= min\nDGradientLRaLsGAN\nDGradient(9)\nThe DSharp represents the discriminator for generated sharp images including the Sharp1\nand the Sharp2. And the DGradient represents the discriminator for the generated gradient\nimage.', '502 F. Li et al.\n3 Experiments and Analysis\n3.1 Solar Speckle Datasets\nIn this paper, we use the continuous solar speckle images taken by the 1m New V acuum\nSun Telescope (NVST) from Fuxian Lake as the datasets. Although the amount of\nobservation data is huge, not all captured images are suitable for reconstruction work.Thanks to the work of Jia et al. [ 13], they ﬁnd that the deep learning network can complete\nthe model training only by using the same wavelength images and a small number of\nhigh-deﬁnition reference images. And it can recover most of the captured images at thiswavelength. Following their work of datasets, we select the blurred images (Blur) and\nthen crop them together with the paired supervised images (GT Sharp) reconstructed by\nLevel1+. Finally, we obtained more than 50000 patches with the size of 256 ×256 as\nour training datasets.\n3.2 Implementation Details\nTo reduce the memory usage and back propagation problems, we separate the training of\nstage1 and stage2. Speciﬁcally, stage1 trains separately and froze the stage2 parameters.\nDuring stage2 training, the parameters of stage1 should be frozen. All experiments inthis paper are trained and tested on a single Nvidia 1080ti GPU.\n3.3 Experimental Evaluation\nWe compare our method with several current mainstream methods [ 3,8,9] on public\ndatasets and astronomical methods [ 11,13]. Since the method proposed by Kupyn et al.\n[9] contains two methods, for the convenience of naming, we call his mobile method\nas mobile and call his inception method as inception. In addition, our method is namedDSFSP in the evaluation results.\nQuantitative Comparison. In order to ensure the fairness of the comparison, this paper\nadopts the evaluation indicators that have appeared in the above research: Peak Signal-\nto-Noise Ratio (PSNR) and Structural SIMilarity (SSIM). The quantitative comparisonresults are shown in Table 1. It can be seen that the method proposed in this paper has\nachieved good results on both PSNR and SSIM.\nQualitative Comparison. From a more intuitive point of view, DSFSP can effectively\napproach the supervised image (GT Sharp). And it can be found from Fig. 5that the\ndouble-stage FPNs and single pipe-line gradient structure method can clearly restore the\noverall edges of the image. In addition, it can also show that DSFSP effectively reducesthe generated artifacts and restores the realistic local high-frequency information.', 'Couple Double-Stage FPNs with Single Pipe-Line 503\nTa b l e 1 . PSNR and SSIM comparison on the Solar Speckle Datasets\nMethod PSNR/dB SSIM\nGuo et al. [ 3] 24.2742 0.6524\nMa et al. [ 8] 22.9655 0.6383\nKupyn et al. [ 9]-mobile 24.4764 0.6526\nKupyn et al. [ 9]-inception 24.0640 0.6741\nRen et al. [ 11] 25.1237 0.7442\nJia et al. [ 13] 23.6201 0.6947\nDSFSP 27.8010 0.8510\n(b)GT Sharp (d)Ma et al. [8] (e)Kupyn et al. [9]\n-Mobile\n(g)Ren et al.[11] (h)Jia et al.[13] (i)DSFSP(c)Guo et al.[3]\n(f)Kupyn et al. [9]\n-Inception(a)Blur\nFig. 5. Comparison of different methods.\n3.4 Ablation Experiment\nGradient Information. It is worth mentioning that we also use the gradient structure to\nenhance the spatial information like Ma et al. [ 8], but our design is quite different from\ntheirs. Due to the different task, their gradient information is obtained by the encoder of\nthe generator. Such a design heavily depends on the edge features of the input image.It can be found from (b) in Fig. 6that we get less information if we obtain the gradient\ndirectly from the input image. Therefore, depending on the original gradient information\nare not suitable for solar speckle images. To avoid this problem, our generator capturesthe gradient information from the decoder which is after the feature bridge of the second\nFPN. Through the purpose of this design, more effective spatial features can be obtained\nfrom the decoder, rather than the blurry features extracted from the encoder. For the faircomparison with Ma et al. [ 8], our structure retains only the second stage. That means\nthe blurred image directly inputs to stage2 and this structure is named single-stage. It can\nbe seen from (c) and (d) in Fig. 6that after we redesign the gradient structure, the ﬁnal', '504 F. Li et al.\ngradient image information is signiﬁcantly greater than the result of Ma et al. [ 8]. And\nto prove that this different is not caused by FPN, we modify the gradient structure on ourmethod. The speciﬁc results can refer to Fig. 7. In addition, we design a double-stage\nFPNs, which can further improve the utilization rate of the gradient structure. It can be\nseen from the comparison between (d) and (e) in Fig. 6.\n(a)Blur (b)Gradient of \nblur(c)Gradient of \nMa et al.[8](d)Gradient of \nsingle-stage(e)Gradient of \ndouble-stage(f)GT gradient\nFig. 6. Ablation experiment of gradient information.\n(a)GT (b)Gradient structure \nconnects to the \ndecoder of FPN(c)Gradient structure \nconnects to the \nencoder of FPN(d)Remove the \ngradient \nstructureReconstruction:\nGradient:\nFig. 7. Different construction methods of gradient structure.\nMoreover, the gradient acquisition is related to the clarity of the input image. If the\ninput image has obvious gradient information, the gradient structure is not even needed\nanymore. And it can get a good effect without a double-stage structure, which we willprove in the experiment of public datasets later.\nMulti-stage Structure. Multi-Stage can signiﬁcantly improve the acquisition of gradi-\nent information, which means that model can acquire more spatial information. But itis not that the more stages, the better effects. When the number of FPNs increases to\n3, the reconstruction effect decreases instead. This is because as the number of stages\nincreases, the parameters of the model are rising rapidly. More parameters mean that themodel is more difﬁcult to train and overﬁtting is more serious.\nTherefore, we use 2 stages of FPNs to construct the generator. This design can\nimprove the effectiveness of the single-pipe gradient, thereby helping us to obtain morespatial information. The comparison of the number of stages can refer to Fig. 8.\n3.5 Comparison Results on Public Datasets\nTo verify whether the proposed method is applicable to general blurry, this paper also\ncompares the public datasets. The public datasets we choose include DVD datasets [ 21]', 'Couple Double-Stage FPNs with Single Pipe-Line 505\nFig. 8. Comparison with different number of stages.\nand GoPro datasets [ 5]. The quantitative evaluation results are shown in Table 2and\nTable 3, and the qualitative evaluation results are shown in Fig. 9and Fig. 10. Since\npublic datasets usually have obvious gradient and contextualized information, we try to\nextract the second stage separately for experimentation.\nTa b l e 2 . PSNR and SSIM comparison on the DVD datasets.\nMethod PSNR/dB SSIM\nKupyn et al. [ 9]-mobile 28.54 0.9294\nKupyn et al. [ 9]-inception 28.85 0.9327\nDSFSP (only second stage) 28.69 0.9312\nDSFSP 28.94 0.9306\nTa b l e 3 . PSNR and SSIM comparison on the GoPro datasets.\nMethod PSNR/dB SSIM\nSolar et al. [ 1] 24.64 0.8419\nNah et al. [ 5] 29.08 0.9135\nTao et al. [ 6] 30.26 0.9342\nKupyn et al. [ 9]-mobile 28.17 0.9254\nKupyn et al. [ 9]-inception 29.55 0.9344\nDSFSP (only second stage) 28.85 0.9232\nDSFSP 28.92 0.9212\nIt can be seen from the results above that the method proposed in this paper is also\ncompetitive in the public datasets and can effectively restore the local details of the\nimage. However, it can also be seen from the evaluation results that the double-stage and\ngradient structure has not signiﬁcantly improved in terms of indicators. This is because', '506 F. Li et al.\n(a)blurry (b)local \nblurry(c)local \nsharp(d)Solar [1] (e)Nah [5] (f)Tao [6] (g)Kupyn [9]\n-mobile(h)Kupyn [9]\n-inception(i)DSFSP \n(only second \nstage)(j)D SFSP\nFig. 9. Visual comparison of different methods (1)\n(a)blurry (b)local \nblurry(c)local \nsharp(d)Solar [1] (e)Nah [5] (f)Tao [6] (g)Kupyn [9]\n-mobile(h)Kupyn [9]\n-inception(i)DSFSP \n(only second \nstage)(j)DSFSP\nFig. 10. Visual comparison of different methods (2)\nthe blurry images in the public datasets generally have clear spatial and contextualized\nfeatures, so some multi-scale networks are sufﬁcient to capture spatial and contextualized\ninformation. This is different from our solar speckle datasets, which do not have obviousinformation like that. Our method is more focused on solving this complicated situation.\n4 Conclusion\nBecause of solar speckle images having single structural features, low contrast andfuzzy local details, we use FPNs to enhance the structural contextualized, use the singlepipe-line coupled with FPN to extract gradient information, and ﬁnally reconstruct high-\nresolution images. Experimental results show that our method has a strong ability in the\nreconstruction of solar speckle images, and its visual quality and evaluation indicatorsare signiﬁcantly better than other existing mainstream deep learning methods.\nAlthough DSFSP can be very close to the GT images reconstructed by the Level1+\nmethod, it would also learn some noise and artifacts in GT, and its time efﬁciency needsto be further improved. In future work, we will try to introduce an unsupervised method\nto reconstruct the solar speckle images and reduce the dependence on GT.\nAcknowledgments. This work is supported by National Nature Science Foundation under Grant\n11773073, the Supported by program for Innovative Research Team in University of Y unnan\nProvince (IRTSTYN).\nReferences\n1. Solar, J., Cao, W., Xu, Z., et al.: Learning a convolutional neural network for non-uniform\nmotion blur removal. In: IEEE Conference on Computer Vision and Pattern Recognition,\npp. 769–777 (2015)', 'Couple Double-Stage FPNs with Single Pipe-Line 507\n2. Gu, J., Lu, H., Zuo, W., et al.: Blind super-resolution with iterative kernel correction. In:\nConference on Computer Vision and Pattern Recognition, pp. 1604–1613 (2019)\n3. Guo, Y ., Chen, J., Wang, J., et al.: Closed-loop matters: dual regression networks for single\nimage super-resolution. In: IEEE Conference on Computer Vision and Pattern Recognition,\npp. 5406–5415 (2020)\n4. Kaufman, A., Fattal, R.: Deblurring using analysis synthesis networks pair. In: IEEE\nConference on Computer Vision and Pattern Recognition, pp. 5810–5819 (2020)\n5. Nah, S., Kim, T., Lee, K.: Deep multi-scale convolutional neural network for dynamic scene\ndeblurring. In: IEEE Conference on Computer Vision and Pattern Recognition, pp. 257–265\n(2017)\n6. Tao, X., Gao, H., Shen, X., et al.: Scale-recurrent network for deep image deblurring. In:\nIEEE Conference on Computer Vision and Pattern Recognition, pp. 8174–8182 (2018)\n7. Ledig, C., Theis, L., Huszar, F., et al.: Photo-realistic single image super-resolution using\na generative adversarial network. In: IEEE Conference on Computer Vision and PatternRecognition, pp. 4681–4690 (2017)\n8. Ma, C., Rao, Y ., Cheng, Y ., et al.: Structure-preserving super resolution with gradient guidance.\nIn: IEEE Conference on Computer Vision and Pattern Recognition, pp. 7766–7775 (2020)\n9. Kupyn, O., Martyniuk, T., Wu, J., et al.: DeblurGAN-v2: deblurring (orders-of-magnitude)\nfaster and better. In: IEEE International Conference on Computer Vision and Pattern\nRecognition, pp. 8877–8886 (2019)\n10. Lin, T., Dollar, P ., Girshick, R., et al.: Feature pyramid networks for object detection. In: IEEE\nConference on Computer Vision and Pattern Recognition, pp. 936–944 (2017)\n11. Ren, Y ., Jiang, M., Yang, L., et al.: Reconstruction of single-frame solar speckle image\nwith cycle consistency loss and perceptual loss. In: IEEE 6th International Conference on\nInformation Science and Control Engineering, pp. 439–443 (2019)\n12. Zhu, J., Park, T., Isola, P ., et al.: Unpaired image-to-image translation using cycle-consistent\nadversarial networks. In: IEEE International Conference on Computer Vision, pp. 2242–2251\n(2017)\n13. Jia, P ., Huang, Y ., Cai, B., et al.: Solar image restoration with the CycleGAN based on\nmulti-fractal properties of texture features. Astrophys. J. Lett. 881(2), L30 (2019)\n14. Xiang, Y .: Research on high-resolution and high-speed solar reconstruction algorithm\ndissertation. University of Chinese Academy of Sciences, Beijing, pp. 19–22 (2016)\n15. Zamir, S., Arora, A., Khan, S., et al.: Multi-stage progressive image restoration. In: CVPR\n(2021)\n16. Wang, X., et al.: ESRGAN: enhanced super-resolution generative adversarial networks. In:\nLeal-Taixé, L., Roth, S. (eds.) ECCV 2018. LNCS, vol. 11133, pp. 63–79. Springer, Cham\n(2019). https://doi.org/10.1007/978-3-030-11021-5_5\n17. Johnson, J., Alahi, A., Li, F.: Perceptual losses for real-time style transfer and super-resolution.\narXiv preprint arXiv:1603.08155 (2016)\n18. Simonyan, K., Zisserman, A.: A very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 (2014)\n19. Jolicoeur-Martineau, A.: The relativistic discriminator: a key element missing from standard\nGAN. arXiv preprint arXiv:1807.00734 (2018)\n20. Chen, R., Huang, W., Huang, B., et al.: Reusing discriminators for encoding: towards unsu-\npervised image-to-image translation. In: IEEE International Conference on Computer Vision\nand Pattern Recognition, pp. 8165–8174 (2020)\n21. Su, S., Delbracio, M., Wang, J., et al.: Deep video deblurring for hand-held cameras. In: IEEE\nInternational Conference on Computer Vision and Pattern Recognition, pp. 237–246 (2017)', 'Multi-scale Image Partitioning and Saliency\nDetection for Single Image Blind Deblurring\nJiaqian Yan1,2,3, Y u Shi1,2,3(B),X i aH u a1,2,3, Zhigao Huang1,2,3, and Ruzhou Li1,2,3\n1School of Electrical and Information Engineering,\nWuhan Institute of Technology, Wuhan 430205, China\n2Hubei Key Laboratory of Optical Information and Pattern Recognition, Wuhan 430205, China\n3Laboratory of Hubei Province Video Image and HD Projection Engineering Technology\nResearch Center, Wuhan 430205, China\nAbstract. Solving the problem of the blurred image degraded by natural envi-\nronment or human induced camera exposure has always been a challenge. The\nresearches on blind deblurring are generally to use the statistical prior of the\nimage as the regularization term, which can extract the critical information fromthe image itself. However, such methods are unable to achieve desired results\nbecause the blur kernel will be non-uniform in practice, so the inconsistencies of\nkernels in different image regions should be considered. Many patch-wise deblur-ring algorithms have certain limitations, such as the unreasonableness of partition.\nIn this paper, we propose an adaptive multi-scale image partitioning method from\ncoarse to ﬁne, which provides a more accurate partition for kernel estimationlocally, and uses the structural similarity value of the kernel as the criterion of\npartitioning. Then, we introduce the image saliency detection to obtain more edge\ndetails from the image, which can contribute to the better kernel estimation. Inaddition, a weighted window function is applied to the joint of image patches\nwith different sizes to obtain the ﬁnal restored image. Extensive experiments on\nspace-invariant and space-variant blurred images demonstrate that the proposed\nmethod achieves better performance against many classical methods.\nKeywords: Image blind deblurring ·Non-uniform ·Multi-scale partitioning ·\nSaliency detection\n1 Introduction\nSingle image deblurring is one of the fundamental issues in image processing, which\nis caused by several reasons, such as the physical limitations of the camera system,\nthe relative motion between the camera and the scene during exposure, atmospheric\nturbulence and so on. Image deblurring can be divided into space-invariant deblurringand space-variant deblurring, and the ultimate aim of both is to recover the latent image\nfrom the input blurred image. For the space-invariant image deblurring problem, given\na space-invariant blurred image, the process of image degradation can be deﬁned as\ny=k∗x+n (1)\n© Springer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 508–523, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_42', 'Multi-scale Image Partitioning and Saliency Detection 509\nwhere ∗is the convolution operator, kis the blur kernel, xis the desired latent image,\nand nis the noise. In practice, such a problem is generally the blind deblurring with\nunknown blur kernel. It requires to estimate the latent image and the blur kernel together\nfrom the given blurred image, which is a highly ill-posed problem because the number\nof unknowns to be solved is more than the number of known ones.\n2 Related Work\nIn recent years, the image blind deblurring method has received increasing attention,many researchers use the image statistical prior as regularization term, such as image\ngradient, image edge information, total variation and so on. Cho et al. [ 1] introduce the\nbilateral ﬁltering function to estimate the strong edges in image, which can improve therobustness of the kernel estimation and the convergence speed. Xu et al. [ 2] consider\nto select the image edges that can be more beneﬁcial to kernel estimation, and use the\ntotal variation to obtain the optimal restoration. Pan et al. [ 3] use both image gradient\nprior and intensity prior to remove the blur effectively. Then, Pan et al. [ 4] introduce a\nL\n0regularization term to enhance the sparsity of the dark channel in the image, which\ncan effectively perform blind image deblurring. In reference [ 5], the local maximum\ngradient is used as a prior to establish a new framework, which has a good effect.\nDifferent with the space-invariant deblurring, the image degradation of the space-\nvariant deblurring is non-uniform due to the camera shake or the random interference\nof atmospheric turbulence dynamic environment [ 6,7]. The space-variant degraded\nproblem has a seriously impact on the target recognition, detection and positioning inpractical application. Because of the non-uniformity of the blur kernel, estimating the\nkernel accurately becomes more difﬁcult, which can’t be solved as the same as the\nsolution in the space-invariant deblurring.\nTherefore, the researches on space-variant image restoration have attracted wide\nattention. There are many methods based on global deblurring, the SVD is applied to\ndecompose the kernel, and TV regularization is used to solve the deblurring problem in[8]. Gupta et al. [ 9] model the blur kernel from three degrees in the camera motion as a\nmotion density function, including the horizontal and vertical rotation and translation.\nCho et al. [ 10] use a set of plane perspective projections to describe the process of 3D\nnon-uniform blur. However, these methods require a large amount of computation and\nmemory. In the previous work, the efﬁcient ﬁlter ﬂow (EFF) method proposed in [ 11]\ndivides the blurred image into patches according to the set numbers of partitioning. Themethod considers that the patches are space-invariant, and uses the blind deblurring\nmethod to restore the image, but it will result in the problems such as expensive compu-\ntation and inaccurate kernel estimation. Harmeling et al. [ 12] use global constraints to\nensure the similarity between blur kernels in kernel estimation after image partitioning,\nbut such global constraints lead to the problem that kernels cannot be solved in closedform. Inspired by the same patch-wise method, in [ 13], the blurred image is divided\ninto overlapping patches, and then a pixel level blur matrix is constructed to restore the\nimage. In [ 14], the similarity of adjacent kernel is considered and the kernel mapping\nregularization is used to improve the kernel estimation accuracy. Barnsley et al. [ 15]\napply the phase diversity method to the kernel estimation of each patch, thus the image', '510 J. Yan et al.\ncan be restored globally. In [ 16], after the image is divided into patches, the kernel of\neach patch is estimated by prior knowledge combined with dictionary to solve the non-uniform deblurring problem in real life. From the above methods, it can be seen that\napplying the appropriate method to image partitioning is important for accurate kernel\nestimation.\nIn this paper, to achieve this goal, we propose an image blind deblurring method\nbased on multi-scale image adaptive partitioning, which consists of three parts: ﬁrst,\nwe can achieve the adaptive image partitioning without predetermined image patches,and using the structural similarity of the kernel as the criterion to decide whether to\ndivide or merge image patches. Second, a saliency detection is introduced as a prior\nto estimate the kernel more accurately and preserve more details. Third, the adjacentdeblurred patches of different sizes can be jointed with the weighted window function\nto obtain the ﬁnal restored image. The experimental results verify that, compared with\nseveral existing classical deblurring methods, the proposed method performs well on\nspace-invariant and space-variant blurred images.\n3 Multi-scale Adaptive Image Partition and Merging\nThis section introduces the overall steps of adaptive image partition and merging, and weuse the structural similarity evaluation (SSIM) as the criterion to measure the similarity\nof two estimated kernels.\nWe know that the visual effect of human eyes is a gradual blurring process generally\nwhen observing an object at close and long distances, and the scale of the observedimage is decreasing, specially, different scales can reﬂect the different details in image.\nHere, if the scale of the input image is not lower than a preset value (set as C=128 in\nthis paper), we make the scale variation on the image. We adopt a multi-scale adaptiveimage partitioning and merging method from coarse to ﬁne, as shown in Fig. 1.\n(1) First, assuming that the size of input blurred image I\n0isIh×Iv(Ih,Iv>C), and\nreducing the scale of image in half proportion until the image size is just close to\nC. Reduced scale can get images of different scales I1,I2,..., In,nis the counter\nof reduced scale;\n(2) The image with the smallest size is divided into average patches to estimate the blur\nkernel of each patch. Calculating the SSIM of the blur kernels between the adjacent\nimage patches to decide whether to partition or not. The structures of kernels aremore similar if the value is closer to 1.\n(3) When the value cannot satisfy the threshold (we set 0.91), it means that the image\nneeds to be partitioned at the maximum scale n. Then returning to the image I\nn−1\nof the previous scale and carrying out the partitioning once, next according to step\n(1), judging whether each patch requires to continue to partition at current scale.If the criterion satisﬁes the threshold or the image has returned to the original size\nat a certain scale, it is judged that the image is no longer divided and the result of\nimage partitioning can be obtained.', 'Multi-scale Image Partitioning and Saliency Detection 511\nFig. 1. The process ﬂow of the image adaptive partition.\n4 Blind Deblurring Based on Saliency Detection\nWhatever it is to judge the SSIM of the kernels in the image partition, and to restore\nthe obtained image patches, it is necessary to accurately estimate the kernels of image\npatches. This section discusses the blind deblurring based on saliency detection.\n4.1 Saliency Detection Based on SUN\nAs the signiﬁcant areas of the image contain a large amount of image information and\nedge details, we desire a method that can pay more attention to obtain these areas. Inorder to achieve this goal, we introduce a saliency detection [ 17] based on the Bayesian\nprobability framework to deﬁne the bottom-up saliency which can be naturally expressed\nas the self-information of visual features, and the overall saliency can be obtained by\ncombining it with the top-down information. First, the saliency of each pixel in the image\ncan be deﬁned using the logarithmic form of probability\nlog s\nz=− log p(F=fz) (2)\nwhere zrepresents the pixel in the image, Fis the observed image feature, fzis the\neigenvalues at pixel z. It shows that the rarer a feature is, the more attention it can get.\nSecondly, the response of the linear ﬁlter is used to calculate the pixels’ features in\nthe image, and fzcan be further understood as the response of the ﬁlter at the pixel z.\nTherefore, we use the Gaussian differential ﬁlter (DOG), which is deﬁned as\nDOG(a,b)=1\nσ2exp/parenleftbigg\n−a2+b2\nσ2/parenrightbigg\n−1\n(1.6σ)2exp/parenleftbigg\n−a2+b2\n(1.6σ)2/parenrightbigg\n(3)\nwhere (a,b)is the location. We perform the convoluting operation on the color channels\nof the image with four different ﬁlters (supposing that they are all independent) so as to\ngenerate Dresponses ( Dmeans twelve if color channels are three). Using the exponential\npower distribution to get the estimated distribution of each response\np(f;σ,θ)=θ\n2σ/Gamma1/parenleftbig1\nθ/parenrightbigexp/parenleftBigg\n−/vextendsingle/vextendsingle/vextendsingle/vextendsinglef\nσ/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ/parenrightBigg\n(4)', '512 J. Yan et al.\nwhere /Gamma1is the gamma function, θis the shape parameter, and frepresents the response of\nthe ﬁlter. Taking the logarithm form of Eq. ( 4) to obtain the probability of each possible\neigenvalue, and the sum of total eigenvalues is deﬁned as the saliency value at the pixel\nz. In addition, for the simplicity of expression, the term irrelevant to the characteristic\nresponse is classiﬁed as the constant const .\nlog sz=− log p(F=fz)=12/summationdisplay\ni=1logθi−log 2−logσi−log/Gamma1/parenleftbigg1\nθi/parenrightbigg\n−/vextendsingle/vextendsingle/vextendsingle/vextendsinglef\ni\nσi/vextendsingle/vextendsingle/vextendsingle/vextendsingleθi\n=12/summationdisplay\ni=1/vextendsingle/vextendsingle/vextendsingle/vextendsinglef\ni\nσi/vextendsingle/vextendsingle/vextendsingle/vextendsingleθi\n+const (5)\n4.2 Blind Deblurring Model and Solution\nAfter saliency detection, we can obtain the saliency map of the input image and denote\nit as P·x. Because of the sparsity of saliency value, /bardblP·x/bardbl0can be used as a constraint\nterm to extract the saliency structure in the image. Besides, the gradient constraint ofthe image /bardbl∇x/bardbl\n0and the intensity constraint of the image /bardblx/bardbl0are introduced for better\ndeblurring, the overall model can be established as\nmin\nx,k/bardblx⊗k−y/bardbl2\n2+α/bardblP·x/bardbl0+β/bardbl∇x/bardbl0+γ/bardblx/bardbl0+λ/bardblk/bardbl2\n2 (6)\nwhere x,yand kdenote the latent image, blurred image and kernel, respectively, /bardblk/bardbl2\n2is\nthe L 2norm about k,∇=(∇h,∇v)Tis the image gradient operator, ⊗is the convolution\noperator, ·is the point multiplication operation between matrix elements, and α,β,γ\nandλare the positive weight parameters.\nIn Eq. ( 6), it is necessary to estimate two unknowns xand k,s oE q .( 6) can be divided\ninto two steps by alternating iteration scheme\nmin\nk/bardblx⊗k−y/bardbl2\n2+λ/bardblk/bardbl2\n2 (7)\nminx/bardblx⊗k−y/bardbl2\n2+α/bardblP·x/bardbl0+β/bardbl∇x/bardbl0+γ/bardblx/bardbl0 (8)\nSolving kwith given x\nFor sub-problem ( 7), the latent image is ﬁxed to solve the blur kernel. In addition, to\nensure the accuracy of the kernel estimation, L2norm is used to estimate the kernel in\nthe gradient space\nmin\nk/bardbl∇x⊗k−∇ y/bardbl2\n2+λ/bardblk/bardbl2\n2 (9)', 'Multi-scale Image Partitioning and Saliency Detection 513\nAfter derivation, FFT transform is used to solve the problem in frequency domain\nk=F−1/parenleftBigg\nF(∇x)F(∇y)\nF(∇x)F(∇x)+λ/parenrightBigg\n(10)\nSolving xwith given k\nFor sub-problem ( 8), the latent image is estimated with ﬁxed blur kernel. According to\nthe basic idea of split-Bregman algorithm, three auxiliary variables d1,d2and d3are\nintroduced and corresponded to P·x,∇x, and x, respectively. Thus, by applying Bregman\niteration to strictly execute constraints, Eq. ( 8) can be converted into unconstrained\nminimization form\nmin\nx,d1,d2,d3/bardblx⊗k−y/bardbl2\n2+α/bardbld1/bardbl0+β/bardbld2/bardbl0+γ/bardbld3/bardbl0+μ1\n2/bardbld1−P·x−q1/bardbl2\n2\n+μ2\n2/bardbld2−∇ x−q2/bardbl2\n2+μ3\n2/bardbld3−x−q3/bardbl2\n2 (11)\nwhere μ1,μ2andμ3are the penalty parameters, q1,q2and q3are the Bregman variables,\nand Eq. ( 11) can be divided into three simpler sub-problems.\n•Sub-problem related to x:\nminx/vextenddouble/vextenddouble/vextenddoublex⊗kl−y/vextenddouble/vextenddouble/vextenddouble2\n2+μ1\n2/vextenddouble/vextenddouble/vextenddoubledl\n1−P·x−ql\n1/vextenddouble/vextenddouble/vextenddouble2\n2+μ2\n2/vextenddouble/vextenddouble/vextenddoubledl\n2−∇ x−ql\n2/vextenddouble/vextenddouble/vextenddouble2\n2+μ3\n2/vextenddouble/vextenddouble/vextenddoubledl\n3−x−ql\n3/vextenddouble/vextenddouble/vextenddouble2\n2\n(12)\nwhich is the least squares problem, and can be solved as\nxl+1=2/parenleftbig\nkl/parenrightbigTy+μ1P·/parenleftbig\ndl\n1−ql\n1/parenrightbig\n+μ2∇T/parenleftbig\ndl\n2−ql\n2/parenrightbig\n+μ3/parenleftbig\ndl\n3−ql\n3/parenrightbig\n2/parenleftbig\nkl/parenrightbigTkl+μ1P2+μ2∇T∇+μ3(13)\nFor the Eq. ( 13), it can be solved effectively in frequency domain by FFT.\n•Sub-problem related to d1,d2and d3:\nmin\nd1α/bardbld1/bardbl0+μ1\n2/vextenddouble/vextenddouble/vextenddoubled\n1−P·xl+1−ql\n1/vextenddouble/vextenddouble/vextenddouble2\n2(14)\nThe problem of ( 14) can be solved by shrinking operator as follows\ndl+1\n1=shrink/parenleftbigg\nP·xl+1+ql\n1,α\nμ1/parenrightbigg\n(15)\nwhere the shrink operator can be expressed as\nshrink (e,ξ)=e\n|e|∗max(e−ξ,0) (16)', '514 J. Yan et al.\nWhen xl+1is estimated, we can also obtain the values of d2and d3, which can be\nexpressed as\n⎧\n⎨\n⎩dl+1\n2=shrink/parenleftBig\n∇xl+1+ql\n2,β\nμ2/parenrightBig\ndl+1\n3=shrink/parenleftBig\nxl+1+ql\n3,γ\nμ3/parenrightBig (17)\nFinally, we can update the variables q1,q2and q3by following formula\n⎧\n⎪⎪⎪⎨\n⎪⎪⎪⎩ql+1\n1=ql\n1+/parenleftBig\nP·xl+1−dl+1\n1/parenrightBig\nql+1\n2=ql\n2+/parenleftBig\n∇xl+1−dl+1\n2/parenrightBig\nql+1\n3=ql\n3+/parenleftBig\nxl+1−dl+1\n3/parenrightBig(18)\n5 Weighted Window Function for Image Patches Joint\nwith Different Sizes\nSuppose that we can obtain mrestored patches. In the image patches joint, ﬁrstly, the\nsizes of two patches are supposed to be r1×v1and r2×v2, in which v1>v2, and their\nexpanded patches R1and R2can be obtained. Here, we only consider the horizontal joint,\nand the sizes of the expanded patches are (r1+c1)×v1and(r2+c2)×v2,c1and c2are\nthe padding values in the horizontal direction. In practice, because of v1>v2, we only\nconsider the actual joint part in R1,w h o s es i z ei s (r1+c1)×v2. For each image patch\nRt(1≤t≤m), assuming that its size is ut×zt, then establishing a two-dimensional\nweighted window function, which can be expressed as\nwt(i,j)=w(i,ut)w(j,zt)T(19)\nwhere i,jrepresent the row and col in the wt, and w(·)can be deﬁned as\nw(n,N)=/braceleftBigg\n0.42−0.5 cos/parenleftBig\n2πn\nN−1/parenrightBig\n+0.08 cos/parenleftBig\n4πn\nN−1/parenrightBig\n,0≤n≤N−1\n0, otherwise(20)\nwhere Ncorresponds to the image patch size utorzt, and nrepresents a discrete variable\nspeciﬁed by the location of the element in wt.A n d wtis only non-zero in the region\nof the actual partitioning image, and is set to 0 otherwise. Finally, the window function\nis multiplied by the current expanded image patch to achieve the mitigated boundary,\nwhich can be described as wt(i,j).∗Rt(i,j). Figure 2shows the joint method in the\nhorizontal direction, and the joint in the vertical direction can be understood similarly.', 'Multi-scale Image Partitioning and Saliency Detection 515\nFig. 2. The joint process of image patches in horizontal direction\n6 Experiments and Results\nAll the experiments in this paper are implemented in MA TLAB 2018a, and the computer\nis conﬁgured with 2.90 Ghz CPU and 8GB RAM. The parameters setting involved are:\nα∈[2e−4,4e−3],β∈[2e−4,2e−3],γ∈[2e−42e−3],λ=2, and the maximum values\nofμ1,μ2andμ3are 24,1e3and 1 e4, respectively. The parameters of the compared\nalgorithms are set according to the way suggested in their papers.\n6.1 The Comparisons with the State-of-the-Art Methods on the Space-Invariant\nBlurred Image\nIn order to prove the effectiveness of our method, we select four natural test images for\nsimulation, as shown in Fig. 3. In our experiment, we set two different degrees of motion\nblur and convolute them with the clear images to generate four space-invariant blurredimages for simulation. And their motion angles theta and motion pixels lenare (theta =\n10, len =15) and (theta =15, len =20), respectively.\nWe compare the performance of our method on the test images with methods [ 2,3].\nTo evaluate the quality of results correctly, the mean square error (MSE), multi-scale\nstructure similarity index (MS-SSIM) and visual information ﬁdelity (VIF) are used asthe reference. Note that the lower value of MSE indicates the better effect of image\nrestoration, the higher value of MS-SSIM shows the better structural similarity between\nthe restored image and the original image, and the higher value of VIF shows the highervisual ﬁdelity of the image. The quality evaluation data of the compared methods and\nthe proposed method are shown in Table 1.\n(a) street1 (b) street2 (c) tree (d) floor\nFig. 3. Original images. (a)–(d) are four natural test images.', '516 J. Yan et al.\nTa b l e 1 . Quality evaluation comparison of restored images with [ 2]a n d[ 3].\nImages Motion blur size Indices [2] [3] Ours\nstreet1 len=15, theta =10 MSE 0.0213 0.0279 0.0103\nMS-SSIM 0.9256 0.9240 0.9842\nVIF 0.2467 0.2715 0.5270\nlen=20, theta =15 MSE 0.0218 0.0240 0.0129\nMS-SSIM 0.9019 0.9168 0.9717\nVIF 0.2107 0.2424 0.4277\nstreet2 len=15, theta =10 MSE 0.0343 0.0408 0.0224\nMS-SSIM 0.8782 0.8863 0.9466\nVIF 0.1801 0.2170 0.3132\nlen=20, theta =15 MSE 0.0337 0.0361 0.0211\nMS-SSIM 0.8365 0.8892 0.9526\nVIF 0.1573 0.2221 0.3441\ntree len=15, theta =10 MSE 0.0424 0.0514 0.0304\nMS-SSIM 0.7778 0.8384 0.9128\nVIF 0.1079 0.1656 0.2299\nlen=20, theta =15 MSE 0.0453 0.0452 0.0325\nMS-SSIM 0.7212 0.8520 0.8867\nVIF 0.0870 0.1594 0.2047\nﬂoor len=15, theta =10 MSE 0.0251 0.0194 0.0140\nMS-SSIM 0.8959 0.9512 0.9698\nVIF 0.2725 0.3783 0.4557\nlen=20, theta =15 MSE 0.0297 0.0270 0.0181\nMS-SSIM 0.8269 0.9137 0.9519\nVIF 0.1972 0.2863 0.3937\nIt can be observed in Table 1that proposed method outperforms the other two algo-\nrithms on MSE, MS-SSIM and VIF value. We show the deblurred results on the simulated\nblurred image of Fig. 3(a). In this group of experiment, the blurred image shown in Fig. 4\n(b) is generated by the second blur, the deblurred results demonstrate that the proposedmethod achieves a competitive performance, as shown in Fig. 4(f), the proposed method\nobtains relatively clearer result in detail, which appear more similar to the original clear\nimage.', 'Multi-scale Image Partitioning and Saliency Detection 517\n(a)clear image (b)blurry image (c)Pan et al. \n(d)Xu et al. (e)Ours \n(f)\nFig. 4. Restored results of street1 image. (a), (b) are clear image and blurry image, (c)–(e) are\nrestorations of [ 2,3] and the proposed method respectively, (f) are the close-up views of (a)–(e)\nimage regions extracted from this example.\n6.2 Quantitative Evaluations\nFurthermore, in order to verify the effectiveness of the saliency prior used in this method,\nwe compare the results from the methods in [ 1,2,4] and [ 19] with the proposed method\non Levin’s dataset [ 18]. The dataset totally consists of 32 images, which are synthesized\nby four clear images and eight kinds of kernels. We calculate the average PSNR and MS-SSIM values of eight different blurred images corresponding to each clear image, and\nthe average PSNR and MS-SSIM values of all images in the whole dataset. The results\nare shown in Fig. 5, it can be seen that the proposed method boost the performance in\nall compared methods. Figure 6shows the restoration of one of the degraded images,\nwe can also observe that compared with other methods, the proposed method achieves\nclearer restoration and better visual effect to a certain extent.', '518 J. Yan et al.\n  \n(a)Comparison chart of average PSNR (b)Comparison chart of average MS-SSIM\nFig. 5. Quantitative evaluation on Levin’s dataset, in terms of PSNR and MS-SSIM.\n(a)clear image (b)blurry image (c)Cho et al.( d ) X u  et al.\n(e)Krishnan et al.( f ) P a n  et al. (g)The proposed method\n(h)\nFig. 6. Visual comparison of image restoration. (a) and (b) are the clear image and the blurry\nimage in [ 18]. (c)–(g) are the restored images obtained by the four methods used for comparison\nand the proposed method. (h) are the close-up views in (a)–(g).\n6.3 Comparison with the State-of-the-Art Methods on the Space-V ariant Blurred\nImage\nIn this section, we demonstrate that the proposed method is applicable and effective\nfor the space-variant blurred images. We perform the proposed method compared with\nseveral algorithms on the space-variant blurred image, and these images and comparisonresults are provided in [ 23]. Figure 7shows several restored results of papers [ 20–22] and\nthe proposed method. From the experimental results, we can observe that the proposed', 'Multi-scale Image Partitioning and Saliency Detection 519\n(a) clear image (b) blurry image (c) Zhang et al. [20]\n(d) Michael et al. [21] (e) Perrone et al. [22] (f) the proposed method\n(a) clear image (b) blurry image (c) Zhang et al. [20]\n(d) Michael et al. [21] (e) Perrone et al. [22] (f) the proposed method\nFig. 7. The visual comparison on the synthetic images. (a) and (b) are clear image and blurry\nimage respectively, (c)–(f) are comparisons of restored results respectively.', '520 J. Yan et al.\nmethod is capable of removing the blur better. And from the close-up views, the proposed\nmethod can show more details clearly, and can better suppress the artifacts.\nWe also make the numerical comparison on the experimental results, and apply the\nnon-reference metric evaluation method in [ 24] to objectively evaluate the effects of\ndeblurred results, as shown in Table 2. It can be observed that the proposed method also\nperforms better in numerical value.\nTa b l e 2 . Quality evaluation with [ 20,21]a n d[ 22].\nImages Blurry [20] [21] [22] Proposed\nFigure 7(1) −12.5491 −9.9714 −9.4379 −8.7361 −8.6186\nFigure 7(2) −21.5568 −13.4973 −11.1008 −11.9871 −10.9307\nIn Fig. 8, we show the deblurred comparison on a set of blurred images. And these\nimages are from one of the frames of the blurred images in the dynamic scene. We com-\npare the restored results with several methods, from which we can see that the proposed\nmethod can remove the blur to a certain extent and achieve the better performance.\nFigure 9shows the several real blurred images and their results generated by the\nproposed method and methods in [ 27,28], and the restored results for comparison come\nfrom [ 13]. We can observe from Fig. 9that the other two methods have certain artifacts\ndue to the imperfect blur kernel estimation, the proposed method performs better and\nproduces less artifacts, and the results are more natural visually.\n(a) blurry image (b) Tao et al. [25] (c) Kupyn et al. [26] (d) the proposed method\nFig. 8. Visual comparison with [ 25]a n d[ 26]. From left to right: blurry image, the restored results\nby [ 25,26] and the proposed method.', 'Multi-scale Image Partitioning and Saliency Detection 521\n(a) blurry image (b) Whyte et al. [27] (c) Xu et al. [28] (d)the proposed method\nFig. 9. Comparison on the real blurred images. From left to right: blurry image, the restored\nimages by [ 27,28] and the proposed method, respectively.\n7 Conclusion\nIn this work, we propose an effective approach for adaptive multi-scale image partitioning\nby viewing the structural similarity as the criterion, so that the image can be dividedadaptively from coarse to ﬁne. In addition, the saliency detection is introduced as the\nprior to obtain better performance of image restoration. And the image patches with\ndifferent sizes can be joined with the weighted window function. The experiments showthat the proposed approach can be applied to both the space-invariant blurred image and\nthe space-variant blurred image and get an improvement of image deblurring.\nAcknowledgement. This work was supported by a project of the National Science Foundation\nof China (61701353, 61801337).\nReferences\n1. Cho, S., Lee, S.: Fast motion deblurring. ACM Trans. Graph. 28(5), 89–97 (2009)\n2. Xu, L., Jia, J.: Two-phase kernel estimation for robust motion deblurring. In: Daniilidis,\nK., Maragos, P ., Paragios, N. (eds.) ECCV 2010. LNCS, vol. 6311, pp. 157–170. Springer,\nHeidelberg (2010). https://doi.org/10.1007/978-3-642-15549-9_12', '522 J. Yan et al.\n3. Pan, J., Hu, Z., Su, Z.: Deblurring text images via L0-regularized intensity and gradient\nprior. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npp. 2901–2908 (2014)\n4. Pan, J., Sun, D., Pﬁster, H.: Blind image deblurring using dark channel prior. In: Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1628–1636 (2016)\n5. Chen, L., Fang, F., Wang, T.: Blind image deblurring with local maximum gradient prior. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1742–1750 (2019)\n6. Yan, J., Bai, X., Xiao, Y ., Zhang, Y ., Lv, X.: No-reference remote sensing image quality\nassessment based on gradient-weighted natural scene statistics in spatial domain. J. Electron.\nImaging 28(1), 013033 (2019)\n7. Chen, G., Gao, Z., Wang, Q., Luo, Q.: U-net like deep autoencoders for deblurring atmospheric\nturbulence. J. Electron. Imaging 28(5), 053024 (2019)\n8. Sroubek, F., Kamenicky, J., Lu, Y .-M.: Decomposition of space-variant blur in image\ndeconvolution. IEEE Signal Process. Lett. 23(3), 346–350 (2016)\n9. Gupta, A., Josh, N., Lawrence, Z.C., Cohen, M., Curless, B.: Single image deblurring using\nmotion density functions. In: Daniilidis, K., Maragos, P ., Paragios, N. (eds.) ECCV 2010.\nLNCS, vol. 6311, pp. 171–184. Springer, Berlin (2010)\n10. Cho, S., Cho, H., Tai, Y .-W., Lee, S.: Registration based non-uniform motion deblurring.\nComput. Graph. Forum 31(7–2), 2183–2192 (2012)\n11. Hirsch, M., Sra, S., Scholkopf, B., Harmeling, S.: Efﬁcient ﬁlter ﬂow for space-variant mul-\ntiframe blind deconvolutions. In: Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 607–614 (2010)\n12. Harmeling, S., Hirsch, M., Scholkopf, B.: Space-variant single-image blind deconvolution\nfor removing camera shake. In: NIPS, pp. 1–9 (2010)\n13. Ji, H., Wang, K.: A two-stage approach to blind spatially-varying motion deblurring. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 73–80\n(2012)\n14. Shen, Z., Xu, T., Pan, J.: Non-uniform motion deblurring with Kernel grid regularization.\nSignal Process. Image Commun. 62, 1–15 (2018)\n15. Bardsley, J., Jefferies, S., Nagy, J., Plemmons, R.: A computational method for the restoration\nof images with an unknown, spatially-varying blur. Opt. Express 14(5), 1767–1782 (2006)\n16. Cao, X., Ren, W., Zuo, W., Guo, X., Hassan, F.: Scene text deblurring using text-speciﬁc\nmultiscale dictionaries. IEEE Trans. Image Process 24(4), 1302–1314 (2015)\n17. Zhang, L., Tong, M., Marks, T., Shan, H., Cottrell, G.: SUN: a bayesian framework for saliency\nusing natural statistics. J. Vis. 8(32), 1–20 (2008)\n18. Levin, A., Weiss, Y ., Durand, F., Freeman, W.T.: Understanding and evaluating blind decon-\nvolution algorithms. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 1964–1971 (2009)\n19. Krishnan, D., Tay, T., Fergus, R.: Blind deconvolution using a normalized sparsity measure. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 233–\n240 (2011)\n20. Zhang, H., Wipf, D., Zhang, Y .: Multi-image blind deblurring using a coupled adaptive sparse\nprior. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pp. 1051–1058 (2013)\n21. Michaeli, T., Irani, M.: Blind deblurring using internal patch recurrence. In: Fleet, D., Pajdla,\nT., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8691, pp. 783–798. Springer,\nCham (2014). https://doi.org/10.1007/978-3-319-10578-9_51\n22. Perrone, D., Favaro, P .: Total variation blind deconvolution: the devil is in the details. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2909–\n2916 (2014)', 'Multi-scale Image Partitioning and Saliency Detection 523\n23. Lai, W.-S., Huang, J.-B., Yang, M.-H.: A comparative study for single image blind deblurring.\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 64–\n72 (2016)\n24. Liu, Y ., Wang, J., Cho, S., Finkelstein, A., Rusinkiewicz, S.: A no-reference metric for\nevaluating the quality of motion deblurring. ACM SIGGRAPH Asia 32(175), 1–12 (2013)\n25. Tao, X., Gao, H., Wang, Y ., Shen, X., Wang, J., Jia, J.: Scale-recurrent network for deep\nimage deblurring. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 8147–8182 (2018)\n26. Kupyn, O., Budzan, V ., Mykhailych, M., Mishkin, D., Matas, J.: Deblurgan: blind motion\ndeblurring using conditional adversarial networks. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 8183–8192 (2018)\n27. Whyte, O., Sivic, J., Zisserman, A., Ponce, J.: Non-uniform deblurring for shaken images.\nInt. J. Comput. Vis. 98, 168–186 (2012)\n28. Xu, L., Zheng, S., Jia, J.: Unnatural L 0sparse representation for natural image deblurring. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8\n(2013)', 'CETransformer: Casual Eﬀect Estimation\nvia Transformer Based Representation\nLearning\nZhenyu Guo1,2, Shuai Zheng1,2, Zhizhe Liu1,2, Kun Yan1,2,\nand Zhenfeng Zhu1,2(B)\n1Beijing Jiaotong University, Beijing, China\n{zhyguo,zs1997,zhzliu,kunyan,zhfzhu }@bjtu.edu.cn\n2Beijing Key Laboratory of Advanced Information Science and Network Technology,\nBeijing, China\nAbstract. Treatment eﬀect estimation, which refers to the estimation\nof causal eﬀects and aims to measure the strength of the causal relation-\nship, is of great importance in many ﬁelds but is a challenging problem\nin practice. As present, data-driven causal eﬀect estimation faces twomain challenges, i.e., selection bias and the missing of counterfactual. To\naddress these two issues, most of the existing approaches tend to reduce\nthe selection bias by learning a balanced representation, and then toestimate the counterfactual through the representation. However, they\nheavily rely on the ﬁnely hand-crafted metric functions when learning\nbalanced representations, which generally doesn’t work well for the situa-tions where the original distribution is complicated. In this paper, we pro-\npose a CETransformer model for casual eﬀect estimation via transformer\nbased representation learning. To learn the representation of covariates(features) robustly, a self-supervised transformer is proposed, by which\nthe correlation between covariates can be well exploited through self-\nattention mechanism. In addition, an adversarial network is adopted tobalance the distribution of the treated and control groups in the represen-\ntation space. Experimental results on three real-world datasets demon-\nstrate the advantages of the proposed CETransformer, compared withthe state-of-the-art treatment eﬀect estimation methods.\nKeywords: Transformer\n·Casual eﬀect estimation ·Adversarial\nlearning\n1 Introduction\nCausal eﬀect estimation is an crucial task that can beneﬁt many domains includ-\ning health care [ 2,10], machine learning [ 14,28], business [ 24] and sociology sci-\nence [ 9]. For example, in medicine, if two pharmaceutical companies have both\ndeveloped anti-hyperlipidemic drugs, which one is more eﬀective for a given\npatient? Suppose we consider diﬀerent anti-hyperlipidemic drugs as diﬀerent\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 524–535, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_43', 'CETransformer: Casual Eﬀect Estimation via Transformer 525\ntreatments, the therapeutic eﬀects of the drugs can be obtained by estimat-\ning causal eﬀects. As described above, the causal eﬀect is used to measure thediﬀerence in outcomes under diﬀerent interventions.\nIn practice, we often obtain drug treatment eﬀects by means of randomized\ncontrolled trials(RCT), and similar methods such as A/B tests are used to obtainaverage eﬀects of a new feature in recommendation systems [ 27]. However, for\nindividual causal eﬀects, we cannot collect them by means of RCT because we\nlack counterfactual outcomes. Since counterfactuals are not directly available,causal eﬀect estimation through massive data has become an important task\nin the era of big data and has been widely adopted [ 19,25,26]. Nevertheless,\ndata-driven causal eﬀect estimation approaches face two main challenges, i.e.,treatment selection bias and missing counterfactuals outcomes .\nFirstly, in contrast to RCT, treatments in observational data are usually not\nrandomly assigned. In the healthcare setting, physicians consider a range of fac-\ntors when selecting treatment options, such as patient feedback on treatment,\nmedical history, and patient health status. Due to the presence of selection bias,the treated population may diﬀer signiﬁcantly from the controlled population.\nSecondly, in real life, we only observe the factual outcome and never all the\npotential outcomes that could have occurred if we had chosen a diﬀerent treat-ment option. However, the estimation of treatment eﬀects requires to compare\nthe results of a person under diﬀerent treatments. These two issues make it\nchallenging to obtain an assessment of the treatment eﬀect from the observeddata.\nTo address both of these challenges, existing approaches [ 13,19] project the\nobserved data into a balanced representation space where diﬀerent treatmentgroups are close to each other, and then train an outcome prediction model to\nestimate the counterfactual. To the best of our knowledge, existing methods use\nﬁnely hand-crafted metric functions to approximate the distribution of diﬀerenttreatment groups, and the network structures are the simplest fully connected\nneural networks. In [ 25], the authors considered propensity scores as the relative\nposition of individuals in the covariate space and performed the construction of\ntriplet pairs. They adopted a hand-crafted metric function between the midpoint\nfrom diﬀerent treatment groups to balance the distribution. By adopting an inte-gral probability metric (IPM) [ 26], the similarity between two distributions is\nmeasured. The network architecture used in all of the above approaches is the\nmost primitive fully connected network. To reduce the diﬀerence between diﬀer-ent distributions, using manually designed similarity metric functions alone is\nnot well adapted to the situation where the original distribution is complicated.\nMeanwhile, more attention should be paid to the correlation between covari-ates to generate a more discriminative representation, while the fully connected\nnetwork is shown to exploit only the relationship between individuals [ 7].\nAiming at solving the above problems, we propose an casual eﬀect estimation\nmodel via transformer based representation learning (CETransformer). The key\ncontributions of this work are as follows:', '526 Z. Guo et al.\n– The recently popular Transfomer network is adopted as a converter for map-\nping individuals into a latent representation space. Hence, the correlationbetween diﬀerent covariates can be eﬀectively exploited through self-attention\nmechanism to beneﬁt the representation learning.\n– To make the transformer trainable in the situation of limited number of indi-\nviduals, we take a form of self-supervision via auto-encoder to realize the\naugmentation of training data.\n– Rather than simply using a metric function, the adversarial learning is utilized\nto balance the distribution between the diﬀerent treatment groups.\nWe organize the rest of our paper as follows. Technical background including\nthe basic notations, deﬁnitions, and assumptions are introduced in Sect. 2. Our\nproposed framework is presented in Sect. 3. In Sect. 4, experiments on three\npublic datasets are provided to demonstrate the eﬀectiveness of our method.\nFinally, Sect. 5draws our conclusions on this work.\n2 Preliminary and Background\nSuppose that the observational data X={Xi∈ Rd}n\ni=1contain nunits\n(individual-s/samples) with each containing dfeature variables, and that each\nindividual received one of two treatments. Let Tidenote the binary treatment\nassignment on unit Xi, i.e., Ti= 0 or 1. For the unit Xiin the treated group,\nTi= 1, and it will belong to the control group if Ti= 0. Before the treatment\nassignment, any outcome Yi\n1(treated) or Yi\n0(control), is taken as a potential out-\ncome. After the intervention, the outcome Yi\nTiwill be the observed outcome or\nfactual outcome , and the other treatment’s outcome Yi\n1−Tiis the counterfactual\noutcome .\nThroughout this paper, we follow the potential outcome framework for esti-\nmating treatment eﬀects [ 17,21]. Speciﬁcally, the individual treatment eﬀect\n(ITE) for unit xiis deﬁned as the diﬀerence between the potential treated and\ncontrol outcomes:\nITE i=Yi\n1−Yi\n0,(i=1,···,n). (1)\nMeanwhile, the average treatment eﬀect (ATE) is the diﬀerence between the\npotential treated and control outcomes, which is deﬁned as:\nATE =1\nnn/summationdisplay\ni=1(Yi\n1−Yi\n0),(i=1,···,n). (2)\nUnder the potential outcome framework, the common assumptions to ensure\nthe identiﬁcation of ITE include: Stable Unit Treatment Value Assumption\n(SUTV A) ,Consistency ,Ignorability and Positivity [8,16,17]. With these four\nassumptions satisﬁed, we can successfully estimate the counterfactual outcome\nrequired by ITE.', 'CETransformer: Casual Eﬀect Estimation via Transformer 527\n3 Methodology\n3.1 Overview of the Proposed CETransformer Model\nIn [1], it has been shown that the bound for the error in the estimation of indi-\nvidual causal eﬀects mainly consists of the diﬀerence between the treated and\ncontrol groups and the loss of outcome prediction. Out of consideration of reduc-ing the diﬀerence between treated and control groups for robust estimation of\ncounterfactual, we propose a CETransformer model for casual eﬀect estimation\nvia transformer based representation learning. As shown in Fig. 1, the proposed\nframework of CETransformer contains three modules: 1) Self-supervised Trans-\nformer for representation learning which learns the balanced representation; 2)\nDiscriminator network for adversarial learning to progressively shrink the diﬀer-ence between treated and control groups in the representation space; 3) Outcome\nprediction that uses the learned representations to estimate all potential outcome\nrepresentations. For the details about each module, they will be presented in thefollowing sections.\nFig. 1. Illustration of the proposed CETransformer model for casual eﬀect estimation\nvia transformer based representation learning.\n3.2 Self-supervised Transformer\nExisting works, such as [ 25,26], learn representations of individuals through fully\nconnected neural networks, and their core spirit is to balance the distribution\nbetween diﬀerent treatment groups by means of carefully designed metric func-\ntions. Meanwhile, more attention should be paid to the correlation both betweencovariates and between individuals to generate more discriminative representa-\ntion. However, according to the theoretical analysis in [ 7], simple fully connected\nnetworks only approximate learning the similarity function between samples.\nBased on the above observations, we propose a CETransformer model that is', '528 Z. Guo et al.\nbuilt upon transformers [ 22] to learn robust and balanced feature-contextual\nrepresentation of individual features. Speciﬁcally, CETransformer models thecorresponding correlations between diﬀerent individuals features and obtains a\nrobust representation of the individual by means of self-attention mechanism.\nFig. 2. The architecture of the proposed adversarial transformer model.\nA ss h o w ni nF i g . 2, the CETransformer architecture comprises a stack of N\ntransformer blocks, a reconstruction feed-forward network, a discriminator net-work for adversarial learning, and an outcome prediction feed-forward network.\nCETransformer ﬁrst learns the embedding representation via the transformer\nX\nE=ftrans(X;Θtrans), where XEdenotes the embedding representation, and\nftrans(·;Θtrans) denotes the transformer with Θtransas its parameters. For a\ntransformer, its core capability is to capture arbitrary distance dependencies\nthrough a self-attention mechanism:\nAttention (Q,K,V )=softmax (Q·KT\n√dk)V (3)\nwhere Q,K,a n d Vrepresent Query, Key, Value, respectively, and dkstands for\nthe dimension of Key. For Q,K,a n dV, all of them are obtained from the input\nXthrough three diﬀerent mapping networks.\nSince the training of transformer requires a large amount of data, to learn\na robust representation for the transformer in the case of limited number ofindividuals is very diﬃcult and even untrainable. In view of this situation, the\nway of self-supervision is explored in the transformer framework to overcome\nthis limitation to some extent. In particular, we adopt a simple fully connected', 'CETransformer: Casual Eﬀect Estimation via Transformer 529\nnetwork frecon(·;Θdec) as a decoder to obtain a reconstructed representation\nˆX=frecon(XE;Θdec), where Θdecis the network parameters. Here, MSE is\nused as the loss function to measure the reconstruction error:\nLreco=/vextenddouble/vextenddouble/vextenddoubleX−ˆX/vextenddouble/vextenddouble/vextenddouble2\nF(4)\nCompared with existing fully connected network-based approaches, our pro-\nposed CETransformer has the following advantages: 1) with the help of the self-\nattentive mechanism in Transformer, the correlations among diﬀerent covariates\nare well exploited; 2) by the way of self-supervised learning via auto-encoder torealize the augmentation of training data, the transformer can be trainable in\nthe situation of limited number of individuals.\n3.3 Adversarial Learning for Distribution Balancing\nIn order to satisfy the theoretical analysis mentioned in [ 1] that the distribution\nof treated and control groups should overlap, the learned representations of the\ntwo groups should be balanced. Brieﬂy, if it is not, there is a problem of covari-\nates shift, which will lead to inaccurate estimation of potential outcomes. To\nthe best of our knowledge, all existing works on causal eﬀect estimation adopt ahand-crafted metric function to balance the distribution between the two groups.\nHowever, these approaches heavily rely on a carefully designed metrics. There-\nfore, it is not a trivial task to deal with the complicated original distribution.\nUnlike the previous methods, we take adversarial learning to balance the\ndistribution between the control and treated groups. Generative adversarial net-\nworks (GAN) is generally used to approximate the output distribution from thegenerator to the distribution of the real data. However, as far as causal reasoning\nis concerned, there is no such thing as real and generated data. To solve this\nproblem, a straightforward way is to take the representation of the treated groupas real data and the representation of the control group as generated data.\nFor this case, to train a generative adversarial network, let D(X\ni\nE) denote\nthe discriminator network that maps the embedding representations of treatedand control groups to the corresponding treatment assignment variables T\ni.T h e\ndiscriminator network consists of a fully connected network, and the generator\nG(·) is the aforementioned transformer model. Due to the contradictory problems\nof the objective function in original GAN, which can lead to training instability\nand mode collapse. Many works [ 3,11,18] tries to solve these problems and in this\npaper we adopt WGAN [ 3] as our framework. Technically, WGAN minimizes a\nreasonable and eﬃcient approximation of the Earth Mover (EM) distance, which\nis beneﬁt for the stability of training. The loss function adopted by WGAN:\nLadv= min\nGmax\nDEXE∼Pt[D(XE)]−E˜XE∼Pc[D(˜XE)] (5)\nwhere Ptand Pcrepresent the distributions of the treated and control groups,\nrespectively.', '530 Z. Guo et al.\n3.4 Outcome Prediction\nAfter obtaining the balanced representation, we employed a two-branch network\nto predict the potential output Yiafter a given Tibased on the representation\nXi\nEof the input Xi. Each branch is implemented by fully connected layers and\none output regression layer. Let ˜Yi=h(Xi\nE,T i) denote the corresponding output\nprediction network. We aim to minimize the mean squared error in predicting\nfactual outcomes:\nLp=1\nnn/summationdisplay\ni=1(˜Yi−Yi)2(6)\nUltimately, our total objective function can be expressed in the following form:\nL=αL reco+βL adv+γL p (7)\nwhere the hyper-parameter α,β,γ controls the trade-oﬀ between the three\nfunction.\n4 Experiments\n4.1 Datasets and Metric\nIn this section, we conduct experiments on three public datasets which is same\nas [29], including the IHDP, Jobs, and Twins. On IHDP and Twins datasets, we\naverage over 10 realizations with 61/27/10 ratio of train/validation/test splits.\nAnd on Jobs dataset, because of the extremely low treated/control ratio, we\nconduct the experiment on 10 train/validation/test splits with 56/24/20 splitratio, as suggested in [ 20].\nThe expected Precision in Estimation of Heterogeneous Eﬀect (PEHE) [ 12]\nis adopted on IHDP and Twins dataset. The lower the ε\nPE HE is, the better the\nmethod is. On Jobs dataset, only the observed outcomes are available and the\nground truth of ITE is unavailable. We adopt the policy risk [ 20] to measure the\nexpected loss when taking the treatment as the ITE estimator suggests. Policy risk\nreﬂects how good the ITE estimation can guide the decision. The lower the policy\nrisk is, the better the ITE estimation model can support the decision making.\n4.2 Competing Algorithms\nWe compare CETransformer with a total of 12 algorithms. First we evaluate least\nsquares regression using treatment as an additional input feature (OLS/LR 1),\nwe consider separate least squares regressions for each treatment (OLS/LR 2),\nwe evaluate balancing linear regression (BLR) [ 13], k-nearest neighbor (k-NN)\n[6], Bayesian additive regression trees (BART) [ 5], random forests (R-Forest)\n[4], causal forests (C-Forest) [ 23], treatment-agnostic representation network\n(TARNET), counterfactual regression with Wasserstein distance (CARW ASS)\n[20], local similarity preserved individual treatment eﬀect (SITE) [ 25], adap-\ntively similarity-preserved representation learning method for Causal Eﬀect esti-\nmation (ACE) [ 26], deep kernel learning for individualized treatment eﬀects\n(DKLITE) [ 29].', 'CETransformer: Casual Eﬀect Estimation via Transformer 531\nTable 1. Mean performance (lower better) of individualized treatment eﬀect estimation\nand standard deviation.\nIHDP(√εPE H E ) Twins(√εPE H E ) Jobs( Rpol(πf))\nIn-sample Out-sample In-sample Out-sample In-sample Out-sample\nOLS/LR 1 5.8±.3 5.8±.3 .319 ±.001 .318 ±.007 .22±.00 .23±.02\nOLS/LR 2 2.4±.1 2.5±.1 .320 ±.002 .320 ±.003 .21±.00 .24±.01\nBLR 5.8±.3 5.8±.3 .312 ±.003 .323 ±.018 .22±.01 .26±.02\nK-NN 2.1±.1 4.1±.2 .333 ±.001 .345 ±.007 .22±.00 .26±.02\nBART 2.1±.1 2.3±.1 .347 ±.009 .338 ±.016 .23±.00 .25±.02\nR-FOREST 4.2±.2 6.6±.3 .366 ±.002 .321 ±.005 .23±.01 .28±.02\nC-FOREST 3.8±.2 3.8±.2 .366 ±.003 .316 ±.011 .19±.00 .20±.02\nTARNET .88±.02 .95±.02 .317 ±.002 .315 ±.003 .17±.01 .21±.01\nCAR WA S S .72±.02 .76±.02 .315 ±.007 .313 ±.008 .17±.01 .21±.01\nSITE .60±.09 .65±.10 .309 ±.002 .311 ±.004 .22±.00 .22±.00\nACE .49±.04 .54±.06 .306 ±.000 .301 ±.002 .21±.00 .21±.00\nDKLITE .52±.02 .65±.03 .288 ±.001 .293 ±.003 .13±.01 .14±.01\nCETransformer (ours) .46±.02 .51±.03 .287 ±.001 .289 ±.002 .12±.01 .13±.00\n4.3 Prediction Performance Results\nWith the same settings as [ 29], we report in-sample and out-of-sample perfor-\nmance in Table 1. CETransformer adopts the transformer network as a backbone\nand learns balanced representations via adversarial learning. With this approach,\nwe outperform all competing algorithms on each benchmark dataset. Probablythe most relevant of these comparisons are the three works [ 25,26,29], which gen-\nerate overlapping representations of the treatment and control groups by means\nof neural networks and hand-designed inter-distributional similarity metric func-tions. Compared to them, the improvement in performance better highlights the\npredictive power of our representation. In addition to performance, we are also\ninterested in whether the learned representations are balanced. Figure 3shows\nthe visualization of the learned representations in the three datasets through\nt-SNE [ 15]. Stars and circles represent the two-dimensional representations of\nthe treated and control groups respectively, and we can ﬁnd that the distance\nbetween the two distributions is well approximated by the adversarial learning\nof CETransformer, which indicates that no covariate shift occurs.\nTable 2. Ablation on CETransformer: performance comparison on three datasets.\nDataset CETransformer Without transformer Without discriminator\nIHDP In-sample .46 ±.02 .50 ±.03 2.8 ±.13\nOut-sample .51 ±.03 .56 ±.03 2.9 ±.22\nTwins In-sample .287 ±.001 .292 ±.001 .335 ±.003\nOut-sample .289 ±.002 .295 ±.002 .317 ±.012\nJobs In-sample .12 ±.01 .13 ±.01 .18 ±.00\nOut-sample .13 ±.00 .15 ±.01 .20 ±.02', '532 Z. Guo et al.\nFig. 3. Visualization of the learned representations in the three datasets through t-\nSNE. By sampling the dataset and performing dimensionality reduction using t-SNE,\nwe can ﬁnd that the representations learned by CEtransformer for diﬀerent treatment\ngroups are highly overlapping, which is in accordance with our expectation.\nMeanwhile, quantitative metric, i.e., K-L divergence, of the diﬀerent treat-\nment groups is given in Fig. 4. We can ﬁnd that the diﬀerence between the treated\nand control groups in the representation space decreases rapidly with the num-ber of training iterations. As we inferred from the t-SNE visualization results,\nthe diﬀerences in the distributions between the diﬀerent treatment groups are\nwiped out under the constraints of the adversarial network.\n4.4 Ablation Study\nExperimental results on three datasets show that CETransformer is able to esti-\nmate causal eﬀects more accurately compared to existing representation-basedlearning methods. In this section, we further explore the extent to which the\nchanges made in CETransformer: Transformer backbone and adversarial learn-\ning aﬀect the results. We compare CETransformer with CETransformer withoutTransformer and CETransformer without adversarial learning. Table 2shows the\nresults.\nOur motivation for replacing the fully connected network with Transformer\nis that Transformer’s self-attention can better capture the correlation between\ndiﬀerent features, and that correlation is expected to yield a more predictive\nrepresentation. The results conﬁrm our conjecture, and we ﬁnd that replacing', 'CETransformer: Casual Eﬀect Estimation via Transformer 533\nFig. 4. KL divergence between control and treated groups in representation space.\nthe transformer with a fully connected network will cause a diﬀerent degree of\nperformance degradation. Then, the distribution of the two groups is balanced\nusing an adversarial learning rather than a hand-crafted metric function. When\nthe adversarial learning part is removed, the distribution imbalance should existin the representation space as shown by the theoretical analysis in [ 1]. The\nexperimental results also conﬁrm our conjecture, and it can be found that after\nremoving the adversarial learning part, the performance of CETransfoermer issimilar to that of using traditional supervised learning methods alone.\n5 Conclusion\nIn many domains, understanding the eﬀect of diﬀerent treatments on the indi-\nvidual level is crucial, but predicting their potential outcome in real-life is chal-\nlenging. In this paper, we propose a novel balanced representation distributionlearning model based on Transformer for the estimation of individual causal\neﬀects. We fully exploit the correlation information among the input features by\nthe Transformer structure and automatically balance the distribution betweenthe treated and control groups by the adversarial learning. Extensive experi-\nments on three benchmark datasets show that CETransformer outperforms the\nstate-of-the-art methods, which demonstrates the competitive level of CETrans-former in estimating causal eﬀects. We further demonstrate the eﬀectiveness of\ncomponents in CEtransformer through ablation study.\nReferences\n1. Alaa, A., Schaar, M.: Limits of estimating heterogeneous treatment eﬀects: guide-\nlines for practical algorithm design. In: International Conference on Machine Learn-\ning, pp. 129–138. PMLR (2018)', '534 Z. Guo et al.\n2. Alaa, A.M., van der Schaar, M.: Bayesian inference of individualized treatment\neﬀects using multi-task gaussian processes. arXiv preprint arXiv:1704.02801 (2017)\n3. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein GAN (2017)4. Breiman, L.: Random forests. Mach. Learn. 45(1), 5–32 (2001)\n5. Chipman, H.A., George, E.I., McCulloch, R.E., et al.: BART: Bayesian additive\nregression trees. Ann. Appl. Stat. 4(1), 266–298 (2010)\n6. Crump, R.K., Hotz, V.J., Imbens, G.W., Mitnik, O.A.: Nonparametric tests for\ntreatment eﬀect heterogeneity. Rev. Econ. Stat. 90(3), 389–405 (2008)\n7. Domingos, P.: Every model learned by gradient descent is approximately a kernel\nmachine. arXiv preprint arXiv:2012.00152 (2020)\n8. D’Amour, A., Ding, P., Feller, A., Lei, L., Sekhon, J.: Overlap in observational\nstudies with high-dimensional covariates. J. Econometrics 221(2), 644–654 (2021)\n9. Gangl, M.: Causal inference in sociological research. Ann. Rev. Sociol. 36, 21–47\n(2010)\n10. Glass, T.A., Goodman, S.N., Hern´ an, M.A., Samet, J.M.: Causal inference in public\nhealth. Annu. Rev. Public Health 34, 61–75 (2013)\n11. Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.: Improved\ntraining of Wasserstein GANs. arXiv preprint arXiv:1704.00028 (2017)\n12. Hill, J.L.: Bayesian nonparametric modeling for causal inference. J. Comput.\nGraph. Stat. 20(1), 217–240 (2011)\n13. Johansson, F., Shalit, U., Sontag, D.: Learning representations for counterfac-\ntual inference. In: International Conference on Machine Learning, pp. 3020–3029.\nPMLR (2016)\n14. Kuang, K., Cui, P., Li, B., Jiang, M., Yang, S.: Estimating treatment eﬀect in\nthe wild via diﬀerentiated confounder balancing. In: Proceedings of the 23rd ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, pp.265–274 (2017)\n15. Van der Maaten, L., Hinton, G.: Visualizing data using t-SNE. J. Mach. Learn.\nRes. 9(11) (2008)\n16. Pearl, J., et al.: Causal inference in statistics: an overview. Stat. Surv. 3, 96–146\n(2009)\n17. Rubin, D.B.: Estimating causal eﬀects of treatments in randomized and nonran-\ndomized studies. J. Educ. Psychol. 66(5), 688 (1974)\n18. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.:\nImproved techniques for training GANs. arXiv preprint arXiv:1606.03498 (2016)\n19. Schwab, P., Linhardt, L., Bauer, S., Buhmann, J.M., Karlen, W.: Learning coun-\nterfactual representations for estimating individual dose-response curves. In: Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 34, pp. 5612–5619(2020)\n20. Shalit, U., Johansson, F.D., Sontag, D.: Estimating individual treatment eﬀect:\ngeneralization bounds and algorithms. In: International Conference on Machine\nLearning, pp. 3076–3085. PMLR (2017)\n21. Splawa-Neyman, J., Dabrowska, D.M., Speed, T.: On the application of probability\ntheory to agricultural experiments. essay on principles. Section 9. Stat. Sci. 465–472\n(1990)\n22. Vaswani, A., et al.: Attention is all you need. arXiv preprint arXiv:1706.03762\n(2017)\n23. Wager, S., Athey, S.: Estimation and inference of heterogeneous treatment eﬀects\nusing random forests. J. Am. Stat. Assoc. 113(523), 1228–1242 (2018)', 'CETransformer: Casual Eﬀect Estimation via Transformer 535\n24. Wang, P., Sun, W., Yin, D., Yang, J., Chang, Y.: Robust tree-based causal infer-\nence for complex ad eﬀectiveness analysis. In: Proceedings of the Eighth ACM\nInternational Conference on Web Search and Data Mining, pp. 67–76 (2015)\n25. Yao, L., Li, S., Li, Y., Huai, M., Gao, J., Zhang, A.: Representation learning\nfor treatment eﬀect estimation from observational data. In: Advances in Neural\nInformation Processing Systems, vol. 31 (2018)\n26. Yao, L., Li, S., Li, Y., Huai, M., Gao, J., Zhang, A.: Ace: Adaptively similarity-\npreserved representation learning for individual treatment eﬀect estimation. In:\n2019 IEEE International Conference on Data Mining (ICDM), pp. 1432–1437.\nIEEE (2019)\n27. Yin, X., Hong, L.: The identiﬁcation and estimation of direct and indirect eﬀects\nin a/b tests through causal mediation analysis. In: Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining, pp.\n2989–2999 (2019)\n28. Zhang, K., Gong, M., Sch¨ olkopf, B.: Multi-source domain adaptation: a causal\nview. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 29\n(2015)\n29. Zhang, Y., Bellot, A., Schaar, M.: Learning overlapping representations for the esti-\nmation of individualized treatment eﬀects. In: International Conference on Artiﬁ-\ncial Intelligence and Statistics, pp. 1005–1014. PMLR (2020)', 'An Eﬃcient Polyp Detection Framework\nwith Suspicious Targets Assisted Training\nZhipeng Zhang1,2, Li Xiao1,2,3(B), Fuzhen Zhuang4,5(B),L i n gM a1(B),\nYuan Chang6(B), Yuanyuan Wang1, Huiqin Jiang1, and Qing He1,2\n1Henan Institute of Advanced Technology, Zhengzhou University, Zhengzhou, China\n{xiaoli,heqing }@ict.ac.cn, {ielma,iehqjiang }@zzu.edu.cn\n2Key Lab of Intelligent Information Processing of Chinese Academy of Sciences\n(CAS), Institute of Computing Technology, CAS, Beijing, China\n3Ningbo Huamei Hospital, University of the Chinese Academy of Sciences,\nNingbo, China\n4Institute of Artiﬁcial Intelligence, Beihang University, Beijing 100191, China\nzhuangfuzhen@buaa.edu.cn\n5Xiamen Institute of Data Intelligence, Xiamen, China\n6The First Aﬃliated Hospital of Zhengzhou University, Zhengzhou, China\nAbstract. Automatic polyp detection during colonoscopy is beneﬁcial\nfor reducing the risk of colorectal cancer. However, due to the various\nshapes and sizes of polyps and the complex structures in the intestinalcavity, some normal tissues may display features similar to actual polyps.\nAs a result, traditional object detection models are easily confused by\nsuch suspected target regions and lead to false-positive detection. Inthis work, we propose a multi-branch spatial attention mechanism based\non the one-stage object detection framework, YOLOv4. Our model is\nfurther jointly optimized with a top likelihood and similarity to reducefalse positives caused by suspected target regions. A similarity loss is\nfurther added to identify the suspected targets from real ones. We then\nintroduce a Cross Stage Partial Connection mechanism to reduce theparameters. Our model is evaluated on the private colonic polyp dataset\nand the public MICCAI 2015 grand challenge dataset including the CVC-\nClinic 2015 and Etis-Larib, both of the results show our model improvesperformance by a large margin and with less computational cost.\nKeywords: Polyp detection\n·Suspected target ·Semi-supervised\nlearning\n1 Introduction\nColorectal cancer is one of the most common malignancies of the digestive system\nin the world. Most colorectal cancers originate from adenomatous polyp, andcolonoscopy is an important way to screen for colorectal cancer [ 1]. Colonoscopy-\nbased polyp detection is a key task in medical image computing. In recent years,\nDeep learning detection models are widely used in polyp detection [ 2–4,8,16].\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 536–547, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_44', 'An Eﬃcient Polyp Detection Framework 537\nHowever, inﬂuenced by the complex environment of the intestinal tract, bubbles,\nlens reﬂection, residues, and shadows may display polyp-like features. Thosefeatures can form the suspected target and confuse the model. See Fig. 1below.\n(a) (b) (c)\n (d)\nFig. 1. (a) Bubbles; (b) Lens reﬂection; (c) Residues; (d) Virtual shadow\nCurrently two-stage [ 2–4,6] and one-stage [ 5,8,16,23] models are the most\nwidely used models in object detection. Faster R-CNN [ 6] as the most widely\nused two-stage object detection model, has been adopted in various polyp detec-\ntion tasks. Mo et al. [ 2] provide the ﬁrst evaluation for polyp detection using\nFaster R-CNN framework, which provides a good trade-oﬀ between eﬃciency\nand accuracy. Shin et al. [ 4] propose FP learning. They ﬁrst trained a network\nwith polyp images and generated FP samples with additional normal videos.Then retrained the network by adding back the generated FP samples. Sor-\nnapudi et al. [ 3] propose a modiﬁed region-based convolutional neural network\n(R-CNN) by generating masks around polyp detected from still frames. Onestage model such as You only look once (YOLO) [ 5] is also widely used for\nlesion detection with the advantage of its eﬃciency. Wang et al. [ 8]p r o p o s ea\nnew anchor free polyp detector, which can achieve real-time performance. Liu etal. [23] investigated the potential of the single shot detector (SSD) [ 18] frame-\nwork for detecting polyps in colonoscopy videos. Three diﬀerent feature extrac-\ntors, including ResNet50, VGG16, and InceptionV3 are assessed. Tian et al. [ 16]\npropose a one-stage detection and classiﬁcation approach for a new 5-class polyp\nclassiﬁcation problem.\nTo deal with the suspected target regions, some mechanisms such as attention\nmechanism (CBAM) [ 7] propose to make the model more focused on true target\nregions. Recently, Xiao et al. [ 10] propose a new sampling method based on the\nFaster R-CNN model to automatically learn features from the suspected target\nregions directly and eﬀectively reduce false positives. Guo et al. [ 24]p r o p o s ea\nmethod based on active learning to tackle false positives detected by the CADesystem. But both [ 24] and [ 4] methods add the FP samples to the training\nset after ﬁnding the false-positive region to retrain the network, this process is\nmore complicated. We design a semi-supervised method to automatically learnsuspicious targets to solve this problem.\nIn addition, there are other methods to detect polyps. Tajbakhsh et al. [ 22]\nis based on a hybrid context-shape approach, which utilizes context information', '538 Z. Zhang et al.\nto remove non-polyp structures and shape information to reliably localize polyps.\nTian et al. [ 25] integrate few-shot anomaly detection methods designed to perform\nthe detection of frames containing polyps from colonoscopy videos with a method\nthat rejects frames containing blurry images, feces and water jet sprays. Liu et\nal. [26] propose a consolidated domain adaptive detection and localization frame-\nwork to bridge the domain gap between diﬀerent colonosopic datasets eﬀectively.\nIn this paper, we propose a novel one-stage polyp detection model based on\nYOLOv4. Moreover, Our model is validated on both the private dataset and thepublic dataset of the MICCAI 2015 challenge [ 11] including CVC-Clinic 2015\nand Etis-Larib, brings signiﬁcant performance improvements and outperform\nmost cutting-edge models. To summarize, our main contributions include: (i) Amulti-branched spatial attention mechanism (MSAM) is proposed to make the\nmodel more focus on the polyp lesion regions. (ii) Design the Top likelihood loss\n(Tloss) with a multi-scale sampling strategy to reduce false positives by learn-\ning from suspected regions from the background. (iii) Further propose Cosine\nsimilarity loss (Csimloss) to improve the discrimination ability between positiveand negative images. (iv) A cross stage partial connection mechanism is further\nintroduced to make the model more eﬃcient. (v) Finally, from the large amount\nof experiments using the private and public datasets, we demonstrate that ourdetection model shows improved detection performance compared with other\nrecent studies in the colonoscopy image datasets.\n2 Methods\nOur detailed model is shown in Fig. 2. The proposed framework consists of three\nparts: (1) A multi-branch spatial attention mechanism (MSAM) is proposed tomake the model pay more attention to the polyp lesion regions (Sect. 2.1); (2)\nTop likelihood loss and cosine similarity loss are designed to the one-stage model\nfor false-positive reduction (Sect. 2.2); (3) Cross Stage Partial Connection is\nintroduced to reduce model parameters through feature fusion (Sect. 2.3). During\ntraining, the proposed model jointly optimizes positive and negative images. The\npositive images are trained by the original loss function, the negative images aretrained with the top likelihood loss added. The pairs of positive and negative\nimages are further optimized by the cosine similarity loss.\n2.1 Multi-branch Spatial Attention Mechanism\nIn order to make the model pay more attention to the polyp lesion regions\nand eliminate the eﬀect of background contents, inspired by the idea of spatialattention mechanism (SAM) [ 7] which locates the most important information\non the feature map, we propose a multi-branch spatial attention mechanism\n(MSAM). We put them in the three output positions of feature fusion, as shownin C-M-Block in Fig. 2, MSAM is a concrete structure. There are three diﬀerent\nscales of feature maps for feature fusion, the receptive ﬁelds of the three scales\nare targeted to diﬀerent sizes of objects.', 'An Eﬃcient Polyp Detection Framework 539\nFig. 2. The architecture of the model. C-Block is the structure after adding cross stage\npartial connection, and C-M-Block is the structure after adding cross stage partial con-\nnection and multi-branch spatial attention mechanism (MSAM), the number representthe convolution kernel size, setting k\n/prime∈{5,7,9}in our model, They correspond to the\nthree scales in the model.\nGiven an input F, we compute the MSAM map As=σ/parenleftBig/summationtext\nk/primefk/prime×k/prime(F)/parenrightBig\n.\nWhere, fk/prime×k/primerepresents the convolution operation with the kernel size of k/prime×k/prime,\nandσrepresents the sigmoid activation function. Setting k/prime∈{5,7,9}in our\nmodel, They correspond to the three scales in the model. The 9 ×9 convolution\nkernel corresponds to the smaller receptive ﬁeld, the 7 ×7 convolution kernel\ncorresponds to the middle scale receptive ﬁeld, and the 5 ×5 convolution kernel\ncorresponds to the larger receptive ﬁeld.\n2.2 Top Likelihood and Similarity Loss\nWe design the top likelihood loss and cosine similarity loss to reduce false posi-\ntives. The implementation details of the loss can be summarized in Fig. 3.\nTop Likelihood Loss. When optimizing negative samples, since those images\ndo not have any annotation information, this means that all areas will be ran-\ndomly sampled with equal chance. As a result, the suspected target regions willhave a small chance to get trained since it usually only occupies a small portion\nof the image. The prediction result may bias towards normal features, leading\nto some false positive detection. To solve this problem, we design top likelihoodloss with multi-scale sampling strategy in a one-stage model. When dealing with\nnegative images, we use top likelihood loss and select the proposals with top\nconﬁdence scores.\nDiﬀerent from two-stage models, YOLOv4 directly generates object conﬁ-\ndence score, category probability, and border regression. When training negative', '540 Z. Zhang et al.\nFig. 3. The illustration of the multi-scale top likelihood loss and cosine similarity loss\nwhere the solid point represents the selected sample: (a) show top likelihood loss withmulti-scale sampling strategy, the K of each scale is set to 50. (b) In the same batch,\npositive and negative samples of the same scale calculate cosine similarity loss.\nimages, we compute the conﬁdence scores and select the top 50 anchor boxes\nscore negative anchor boxes on each scale (150 in total) to calculate the loss.\nThe boxes with high scores will be more likely to represent the suspected target\nregion, and as long as the boxes with high scores are minimized, all the boxeswould be optimized to be negative regions. This top likelihood loss is deﬁned as:\nL\ntloss=1\nobj/summationdisplay\ni∈topsLobj(pi,p∗\ni= 0) (1)\nHere, irepresents the index of anchor in a batch, and pirepresents the\npredicted score of the i-th anchors. Lobjis the cross-entropy loss.\nCosine Similarity Loss. We further propose the cosine similarity loss to\nimprove the discrimination ability between positive and negative images. To\nmake our model trained suﬃciently, we make use all of the pairs of positive and\nnegative images for computing the cosine similarity loss. Speciﬁcally, in each\nbatch, positive images and negative images are random. In order to fully learnthe characteristics between positive and negative images, we design a program\nto let the positive and negative images in the same batch size calculate the\nsimilarity loss between each other, and ﬁnally take the average. When the net-work processes the positive images, we take the positive samples with top K\nscores. Then, when the network processes negative images, we select the high-\nest predicted Kclassiﬁcation scores and pair them with positive ones. Assume\nApositive images and Bnegative images within one batch, there are A×B\npositive-negative pairs. The similarity loss is obtained by computing the cosine\nsimilarity of Kpaired eigen-vectors and summing over the A×Bpairs.', 'An Eﬃcient Polyp Detection Framework 541\nLcsimloss (X1,X2)=1\nA×BAxB/summationdisplay\nj/bracketleftBigg\n1\nKK/summationdisplay\ni=1csim/parenleftbig\nXi\n1,Xi\n2/parenrightbig/bracketrightBigg\n(2)\nWhere Xi\n1,Xi\n2are the feature vectors from positive and negative images, csim\nis cosine similarity loss, csim/parenleftbig\nXi\n1,Xi\n2/parenrightbig\n=Xi\n1·Xi\n2\n/bardblXi\n1/bardbl/bardbli2/bardbl=/summationtextn\ni=1Xi\n1×Xi\n2/radicalBig/summationtextn\ni=1(Xi\n1)2×/radicalBig/summationtextn\ni=1(Xi\n2)2.\n2.3 Cross Stage Partial Connection\nWe further introduce the Cross Stage Partial Network (CSPNet) [ 13]i no u r\nmodel. By dividing the gradient ﬂow, CSPNet can make the gradient ﬂow prop-\nagate through diﬀerent network paths, which can improve the reasoning speed.\nAs shown in Fig. 2, the feature fusion part includes ﬁve modules: three up-\nsampling and two down-sampling. As shown in C-Block and C-M-Block in the\nbottom right of Fig. 2, the Block represents the original connection, C-Block\nand C-M-Block represents the connection after adding CSP. through the split\nand merge strategy, the number of gradient paths can be doubled. Because of\nthe cross-stage strategy, which can alleviate the disadvantages caused by usingexplicit feature map copy for concatenation. As shown in Table 1, the number of\nparameters signiﬁcantly decrease by adding such an operation.\n3 Experiment\n3.1 Datasets\nIn order to verify the eﬀectiveness of the proposed method, we conduct experi-\nments on two datasets, the private colonic polyp dataset and the public dataset\nincluding CVC-Clinic 2015 and Etis-Larib.\nPrivate Polyp Dataset. A dataset of private colonic polyp dataset is collected\nand labeled from the Colorectal and Anorectal Surgery Department of a local\nhospital, which contains 175 patients with 1720 colon polyp images. The 1720\nimages are randomly divided into training and testing set with a ratio of 4:1. We\nsimulate the actual application scenes of colonoscopy and expand the datasetaccordingly, including the expansion of blur, brightness, deformation and so on,\nﬁnally expanding to 3582 images. The colon polyp images are combined with\n1000 normal images without annotation information to build the training set.The original image size is varied from 612 ×524 to 1280 ×720. And we resize all\nthe images to 512 ×512.\nMICCAI 2015 Colonoscopy Polyp Automatic Detection Classiﬁcation\nChallenge. The challenge contains two datasets, the model is trained on CVC-\nClinic 2015 and evaluated on Etis-Larib. The CVC-Clinic 2015 dataset contains\n612 standard well-deﬁned images extracted from 29 diﬀerent sequences. Each', '542 Z. Zhang et al.\nsequence consists of 6 to 26 frames and contains at least one polyp in a vari-\nety of viewing angles, distances and views. Each polyp is manually annotatedby a mask that accurately states its boundaries. The resolution is 384 ×288.\nThe Etis-Larib dataset contains 196 high-resolution images with a resolution of\n1225×966, including 44 distinct polyps obtained from 34 sequences.\n3.2 Evaluation and Results\nEvaluation Criteria. We use the same evaluation metrics presented in the\nMICCAI 2015 challenge to perform the fair evaluation of our polyp detector\nperformance.\nSince the number of false negative in this particular medical application is\nmore harmful, we also calculate the F1 and F2 scores as follows. The evaluation\ncriteria are as follows:\nPrecision =TP\nTP+FPRecall =TP\nTP+FN\nF1=2∗Precision ∗Recall\nPrecision +RecallF2=5∗Precision ∗Recall\n4∗Precision +Recall(3)\nwhere TP and FN denote the true positive and false negative patient cases. FP\nrepresents the false positive patient cases.\nImplementation Details. Our model uses the Pytorch framework and runs on\nNVIDIA GeForce RTX 2080Ti GPU servers. We set the batch size to 8. Duringtraining, we use the SGD optimization method, we also perform random angle\nrotation and image scaling data for data augmentation. The training contains\n2000 epochs with 574 iterations for each epoch, Normally the training processstarts with a high learning rate and then decreases every certain as the training\ngoes on. However, a large learning rate applies on a randomly initialized network\nmay cause instability for training. To solve this problem, we apply a smoothcosine learning rate learner [ 12]. The learning rate α\ntis computed as αt=\n1\n2/parenleftbig\n1 + cos/parenleftbigtπ\nT/parenrightbig/parenrightbig\nα, where trepresents the current epoch, Trepresents the epoch\nandαrepresents initial learning rate.\nAblation Experiments on Private Dataset. In order to study the eﬀect of\nMSAM and the new loss function, we conduct ablation experiments on our pri-\nvate dataset. As shown in Table 1, Compared to the YOLOv4 baseline, our pro-\nposed MSAM increases the Recall by 4.5%, resulting in a score increase of F1 andF2 by 2.2% and 4.0%, respectively. Adding the top likelihood loss only increases\nthe Precision by 4.4%, and combining top likelihood loss together increases both\nPrecision and Recall, leading to an increase of Precision by 2.9% and Recall by3.1%. Finally, the model achieves the performance boosting over all the met-\nrics when combining MSAM, Top likelihood and similarity loss, CSP module\ntogether, leading to increases of Precision by 4.4%, Recall by 3.7%, F1 by 4.0%,', 'An Eﬃcient Polyp Detection Framework 543\n(a) (b) (c) (d) (e)\n (f)\nFig. 4. (a) Origin image with ground truth label (solid line box); (b) Heatmap gener-\nated by the original YOLOv4; (c) Heatmap generated by YOLOv4+MSAM; (d) Originimage with ground truth label (solid line box) and suspected target regions (dashed\nline box); (e) Heatmap generated by YOLOv4+MSAM; (f) Heatmap generated by\nYOLOv4+MSAM+Tloss (top likelihood loss);\nand F2 by 3.8%. It is also worth noting that CSP makes the model more eﬃ-\ncient and leads decreases of FLOPs by 10.74% (8.66 to 7.73), and Parameters\nby 15.7% (63.94 to 53.9).\nWe also show some visualization results of the heatmap (last feature map of\nYOLOv4) for ablation comparison (shown in Fig. 4). The results demonstrate\nthat MSAM makes the model more focus on the ground truth areas, and the\ntop likelihood loss let the model better identify the suspected target regions andpay less attention to such areas.\nTable 1. The results on the private polyp datasets.\nSAM MSAM Tloss Csimloss CSP Precision Recall F1 F2\nYOLOv4 0.876 0.851 0.863 0.856\n/check 0.864 0.897 0.88 0.89\n/check 0.874 0.896 0.885 0.896\n/check 0.92 0.845 0.881 0.859\n/check 0.878 0.888 0.883 0.886\n/check 0.869 0.851 0.86 0.854\n/check/check 0.905 0.882 0.894 0.887\n/check/check 0.914 0.885 0.899 0.891\n/check/check /check 0.907 0.898 0.902 0.9\n/check/check /check/check 0.92 0.888 0.903 0.894\nModelFLOPs (GMac) Params (M)parameters\n8.66 63.94\n/check/check /check 8.82 65.32\n/check/check /check/check 7.73 53.9', '544 Z. Zhang et al.\nResults and Comparisons on the Public Dataset. The results on the\npublic dataset are shown in Table 2, we also test several previous models for the\nMICCAI 2015 challenges. The results show that our method improves perfor-\nmance on almost all metrics. Compare to the baseline, our proposed approach\nachieves a great performance boosting, yielding an increase of Precision by 11.8%(0.736 to 0.854), Recall by 7.5% (0.702 to 0.777), F1 by 9.5% (0.719 to 0.814),\nF2 by 8.2% (0.709 to 0.791). It is worth noting that the depth of CSPDarknet53\nbackbone for YOLOv4 is almost the same as Resnet50. However, our proposedapproach even signiﬁcantly outperforms the state-of-the-art model Sornapudi\net al. [ 3] with a backbone of Resnet101 and Liu et al. [ 23] with a backbone of\nInceptionv3. Comparison with Liu et al. [ 23], although it slightly decreases the\nRecall by 2.6% (0.803 to 0.777), it increases Precision by 11.5% (0.739 to 0.854),\nF1 by 4.6% (0.768 to 0.814), and F2 by 0.2% (0.789 to 0.791). We presented\nthe Frame Per Second (FPS) for each model. It shows that our one-stage model\nis much faster than other models. It is 5.3 times faster than the Faster R-CNN\n(37.2 vs 7), 11.6 times faster than Sornapudi et al. [ 3] (37.2 vs 3.2) and 1.2 times\nfaster than Liu et al. [ 23] (37.2 vs 32). Furthermore, The PR curve is plotted in\nFig.5. Comparison with baseline, our proposed approach increases the AP by\n5.1% (0.728 to 0.779).\nTable 2. Results of the diﬀerent modes on MICCAI 2015 challenge dataset.\nBackbone Precision Recall F1 F2 FPS\nOUS – 0.697 0.63 0.661 0.642 0.2\nCUMED – 0.723 0.692 0.707 0.698 5\nFaster RCNN [ 6] Resnet101 0.617 0.644 0.63 0.638 7\nZheng et al. [ 19] – 0.76 0.668 0.711 0.685 –\nYOLOv3 [ 17] Darknet53 0.764 0.577 0.658 0.607 37\nQadir et al. [ 21] Resnet50 0.8 0.726 0.761 0.74 –\nSornapudi et al. [ 3]Resnet50 0.632 0.769 0.694 0.737 –\nSornapudi et al. [ 3]Resnet101 0.729 0.803 0.764 0.787 3.2\nJia et al. [ 20] Resnet50 0.639 0.817 0.717 0.774 –\nXu et al. [ 27] Darknet53 0.832 0.716 0.77 0.736 35\nLiu et al. [ 23] Inceptionv3 0.739 0.803 0.768 0.789 32\nTian et al. [ 25] Resnet50 0.736 0.644 0.687 0.661 –\nYOLOv4 CSPDarknet53 0.736 0.702 0.719 0.709 36.9\nProposed approach CSPDarknet53 0.854 0.777 0.814 0.791 37.2', 'An Eﬃcient Polyp Detection Framework 545\nFig. 5. Precision-Recall curves for all the methods. The performance of Proposed app-\nroach is much better than the teams that attended the MICCAI challenge\n4 Conclusions\nIn this paper, we propose an eﬃcient and accurate object detection method to\ndetect colonoscopic polyps. We design a MSAM mechanism to make the model\npay more attention to the polyp lesion regions and eliminate the eﬀect of back-\nground content. To make our network more eﬃcient, we develop our methodbased on a one-stage object detection model. Our model is further jointly opti-\nmized with a top likelihood and similarity loss to reduce false positives caused by\nsuspected target regions. A Cross Stage Partial Connection mechanism is furtherintroduced to reduce the parameters. Our approach brings performance boost-\ning compare to the state-of-the-art methods, on both a private polyp detection\ndataset and public MICCAI 2015 challenge dataset. In the future, we plan toextend our model on more complex scenes, such as gastric polyp detection, lung\nnodule detection, achieving accurate and real-time lesion detection.\nAcknowledgments. The research work supported by the National Key Research and\nDevelopment Program of China under Grant No. 2018YFB1004300, the National Nat-\nural Science Foundation of China under Grant No. U1836206, U1811461, 61773361 andZhengzhou collaborative innovation major special project (20XTZX11020).\nReferences\n1. Zhang, P., Sun, X., Wang, D., Wang, X., Cao, Y., Liu, B.: An eﬃcient spatial-\ntemporal polyp detection framework for colonoscopy video. In: 2019 IEEE 31st\nInternational Conference on Tools with Artiﬁcial Intelligence, pp. 1252–1259.\nIEEE, Portland (2019). https://doi.org/10.1109/ICTAI.2019.00-93', '546 Z. Zhang et al.\n2. Mo, X., Tao, K., Wang, Q., Wang, G.: An eﬃcient approach for polyps detection\nin endoscopic videos based on faster R-CNN. In: 2018 24th International Confer-\nence on Pattern Recognition (ICPR), Beijing, China, 2018, pp. 3929–3934 (2018).https://doi.org/10.1109/ICPR.2018.8545174\n3. Sornapudi, S., Meng, F., Yi, S.: Region-based automated localization of\ncolonoscopy and wireless capsule endoscopy polyps. In: Applied Sciences (2019).https://doi.org/10.3390/app9122404\n4. Shin, Y., Qadir, H.A., Aabakken, L., Bergsland, J., Balasingham, I.: Auto-\nmatic colon polyp detection using region based deep CNN and post learning\napproaches. In: IEEE Access, vol. 6, pp. 40950–40962 (2018). https://doi.org/10.\n1109/ACCESS.2018.2856402\n5. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: uniﬁed,\nreal-time object detection. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 779–788 (2016). https://doi.org/10.1109/\nCVPR.2016.91\n6. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object\ndetection with region proposal networks. In: Advances in Neural Information Pro-cessing Systems, pp. 91–99 (2015)\n7. Woo, S., Park, J., Lee, J.-Y., Kweon, I.S.: CBAM: convolutional block attention\nmodule. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018.LNCS, vol. 11211, pp. 3–19. Springer, Cham (2018). https://doi.org/10.1007/978-\n3-030-01234-2\n1\n8. Wang, D., et al.: AFP-Net: realtime anchor-free polyp detection in colonoscopy.\nIn: 2019 IEEE 31st International Conference on Tools with Artiﬁcial Intelligence\n(ICTAI), Portland, OR, USA, pp. 636–643 (2019). https://doi.org/10.1109/ICTAI.\n2019.00094\n9. Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M.: YOLOv4: optimal speed and accuracy\nof object detection. In: arXiv:2004.10934 (2020)\n10. Xiao, L., Zhu, C., Liu, J., Luo, C., Liu, P., Zhao, Y.: Learning from suspected\ntarget: bootstrapping performance for breast cancer detection in mammography.\nIn: Shen, D., et al. (eds.) MICCAI 2019. LNCS, vol. 11769, pp. 468–476. Springer,\nCham (2019). https://doi.org/10.1007/978-3-030-32226-7 52\n11. Bernal, J., Sanchez, F.J., Fernandez-Esparrach, G., Gil, D., Rodrguez, C., Vilarino,\nF.: WM-DOVA maps for accurate polyp highlighting in colonoscopy: validation vs.\nsaliency maps from physicians. In: Computerized Medical Imaging and Graphics,vol. 43, pp. 99–111 (2015)\n12. Loshchilov, I., Hutter, F.: SGDR: stochastic gradient descent with warm restarts.\nIn: arXiv preprint arXiv:1608.03983 (2016)\n13. Wang, C.Y., Liao, H.Y.M., Wu, Y.H., et al.: CSPNet: a new backbone that can\nenhance learning capability of CNN. In: 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops (2020)\n14. Yuan, Z., IzadyYazdanabadi, M., Mokkapati, D., et al.: Automatic polyp detec-\ntion in colonoscopy videos. In: Medical Imaging 2017: Image Processing, Orlando,Florida, USA, vol. 2017 (2017)\n15. Tajbakhsh, S., Gurudu, R., Liang, J.: Automatic polyp detection in colonoscopy\nvideos using an ensemble of convolutional neural networks. In: 2015 IEEE 12thInternational Symposium on Biomedical Imaging (ISBI), Brooklyn, NY, USA, pp.\n79–83 (2015). https://doi.org/10.1109/ISBI.2015.7163821', 'An Eﬃcient Polyp Detection Framework 547\n16. Tian, Y., Pu, L.Z.C.T., Singh, R., Burt, A.D., Carneiro, G.: One-stage ﬁve-class\npolyp detection and classiﬁcation. In: 2019 IEEE 16th International Symposium\non Biomedical Imaging (ISBI 2019), Venice, Italy, pp. 70–73 (2019). https://doi.\norg/10.1109/ISBI.2019.8759521\n17. Redmon, J., Farhadi, A.: YOLOv3: an incremental improvement. arXiv preprint\narXiv:1804.02767 (2018)\n18. Liu, W., et al.: SSD: single shot multibox detector. In: Leibe, B., Matas, J., Sebe,\nN., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9905, pp. 21–37. Springer, Cham\n(2016). https://doi.org/10.1007/978-3-319-46448-0 2\n19. Zheng, Y., et al.: Localisation of colorectal polyps by convolutional neural network\nfeatures learnt from white light and narrow band endoscopic images of multipledatabases. In: EMBS, pp. 4142–4145 (2018)\n20. Jia, X., et al.: Automatic polyp recognition in colonoscopy images using deep learn-\ning and two-stage pyramidal feature prediction. IEEE Trans. Autom. Sci. Eng. 17,\n1570–1584 (2020). https://doi.org/10.1109/TASE.2020.2964827\n21. Qadir, H.A., Shin, Y., Solhusvik, J., Bergsland, J., Aabakken, L., Balasingham,\nI.: Polyp detection and segmentation using mask R-CNN: does a deeper featureextractor CNN always perform better? In: 2019 13th International Symposium on\nMedical Information and Communication Technology (ISMICT), pp. 1–6. IEEE\n(2019)\n22. Tajbakhsh, S., Gurudu, R., Liang, J.: Automated polyp detection in colonoscopy\nvideos using shape and context information. IEEE Trans. Med. Imag. 35(2), 630–\n644 (2016). https://doi.org/10.1109/TMI.2015.2487997\n23. Liu, M., Jiang, J., Wang, Z.: Colonic polyp detection in endoscopic videos with\nsingle shot detection based deep convolutional neural network. IEEE Access 7,\n75058–75066 (2019). https://doi.org/10.1109/ACCESS.2019.2921027\n24. Guo, Z., et al.: Reduce false-positive rate by active learning for automatic polyp\ndetection in colonoscopy videos. In: 2020 IEEE 17th International Symposium\non Biomedical Imaging (ISBI), pp. 1655–1658 (2020). https://doi.org/10.1109/\nISBI45749.2020.9098500\n25. Tian, Y., Pu, L., Liu, Y., et al.: Detecting, localising and classifying polyps from\ncolonoscopy videos using deep learning. arXiv preprint arXiv:2101.03285v1 (2021)\n26. Liu, X., Guo, X., Liu, Y., et al.: Consolidated domain adaptive detection and\nlocalization framework for cross-device colonoscopic images. Med. Image Anal.\n(2021). https://doi.org/10.1016/j.media.2021.102052\n27. Xu, J., Zhao, R., Yu, Y., et al.: Real-time automatic polyp detection in colonoscopy\nusing feature enhancement module and spatiotemporal similarity correlation unit.\nIn: Biomedical Signal Processing and Control, vol. 66 (2021). https://doi.org/10.\n1016/j.bspc.2021.102503 . ISSN 1746–8094', 'Invertible Image Compressive Sensing\nBingfeng Sun and Jian Zhang(B)\nPeking University Shenzhen Graduate School, Shenzhen, China\nzhangjian.sz@pku.edu.cn\nAbstract. Invertible neural networks (INNs) have been widely used to\ndesign generative models and solve inverse problems. However, there is\nlittle research on applying invertible models to compressive sensing (CS)\ntasks. To address this challenge, in this paper, a novel and memory-eﬃcient invertible image compressive sensing framework, dubbed InvICS,\nis proposed, which conducts a practical integration of optimization-based\nCS methods and INNs. In particular, InvICS is composed of three sub-nets: sampling subnet, initialization subnet, and invertible recovery sub-\nnet, and all the parameters in it are learned end-to-end, rather than\nhand-crafted. Through building eﬀective modules in each phase of the\ninvertible recovery subnet, the activations for most layers need not be\nstored in memory during backpropagation, which easily allows trainingour model with 100+ phases (1000+ layers) even on a single Nvidia 2070\nGPU. Moreover, extensive experiments on several common benchmark\ndatasets demonstrate that the proposed InvICS outperforms most state-of-the-art methods by a large margin.\nKeywords: Compressive sensing\n·Invertible neural networks ·Image\nreconstruction ·Deep network\n1 Introduction\nCompressive Sensing (CS) has been applied in a series of imaging applications,\nsuch as single-pixel camera [ 4], magnetic resonance imaging (MRI) [ 15], wireless\ntele-monitoring [ 48], cognitive radio communication [ 33] and snapshot compres-\nsive imaging [ 39],etc.Mathematically, given the measurement y=Φx+w,\nwhere x∈IRNis the signal, Φ∈IRM×Nis called the measurement matrix, and\nw∈IRMis the measurement noise, reconstructing xfromywhen M/lessmuchNis\ntypically ill-posed. However, CS theory [ 6,7,11] states that the signal xcan be\nrecovered perfectly even from a small number of M=O(slog(N\ns)) random linear\nmeasurements provided that the signal is s-sparse in some sparsifying domain.\nOver the past decade, several reconstruction methods have been proposed.\nModel-based methods [ 13,23,46,47,50–54] usually assume that natural images\ncan be sparsely represented by a dictionary, and thus learn the sampling matrix\nThis work was supported in part by National Natural Science Foundation of China\n(61902009).\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 548–560, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_45', 'Invertible Image Compressive Sensing 549\nand sparse dictionary simultaneously through solving a sparsity regularized opti-\nmization problem. However, their iterative nature renders the algorithms compu-tationally expensive. Deep network-based algorithms [ 8,17,29–31,35,37,38]p r o -\npose to learn the non-linear reconstruction operator through neural networks.\nBeneﬁting from the powerful learning ability of neural networks, these meth-ods achieve a good balance between the reconstruction performance and time\ncomplexity. In the earlier periods, network-based models are trained like a black-\nbox and have low interpretability. Researchers [ 41–45] combine these two kinds\nof methods, and their proposed methods enjoy the advantages of fast and accu-\nrate reconstruction with well-deﬁned interpretability. However, as networks grow\nwider and deeper, storing the activations will cause an increasing memory bur-den [14], which may possibly limit their performances.\nTo address this problem, we propose a new image compressive sensing frame-\nwork, dubbed InvICS (see Fig. 1). It’s inspired by the traditional Iterative\nShrinkage-Thresholding Algorithm (ISTA). In contrast to optimization-based\nnetworks, we don’t rely upon sparse assumptions, and use deep invertible layersto obtain the optimal estimation in an iterative manner. The proposed InvICS\nalgorithm is proved eﬀective and eﬃcient.\nContributions. The main contributions of this paper are summarized as\nfollows:\n– Our proposed InvICS, is an eﬀective integration of optimization-based meth-\nods and invertible networks. It easily allows training with 1000+ layers evenon a single Nvidia 2070 GPU without a memory bottleneck.\n– We use an order-preserving multi-channel fusion mechanism in the invertible\nrecovery subnet, which is proved as an eﬃcient method to boost performance.\n– Extensive experiments on natural image CS reconstruction clearly show that\nInvICS signiﬁcantly outperforms the state-of-the-art.\nThe paper is organized as follows. Section 2introduces some related works.\nSection 3describes InvICS in detail. Section 4is the experimental results. And\nin Sect. 5, we conclude this paper and discuss the future works.\n2 Related Work\n2.1 Compressive Sensing\nExisting learning-based CS methods can be generally grouped into two cate-\ngories.\nBlack Box End-to-End. The ﬁrst category is to learn the inverse transforma-\ntion from measurement vector yto original signal xusing a deep convolutional\nnetwork. A representative method is ReconNet [ 21] which employs a convolu-\ntional neural network for reconstruction in a block-wise manner. Similar schemes\nare SDA based methods [ 31], DeepInverse [ 29],etc.The main feature of this kind\nis that they are non-iterative, which can dramatically reduce time complexity.\nHowever, their lack of structural diversity, leading to low interpretability, is the\nbottleneck for further performance improvement.', '550 B. Sun and J. Zhang\nOptimization-Inspired Networks. The second category blends data-driven\nnetworks with sparse-signal-recovery algorithms. Methods, such as [ 28,44,45,49],\nare of this kind. Among them, OPINE-Net+[45] is a notable one. It can be\nviewed as a perfect fusion between optimization-based methods and network-\nbased methods. Simply put, it casts Iterative Shrinkage-Thresholding Algorithm(ISTA) into deep network form. Instead of using a ﬁxed sampling matrix, it\nadopts a learnable one, and jointly optimizes it and the non-linear recovery\noperator. What’s more, it utilizes pixel shuﬄe operation, which can exploit theinter-block relationship and thus eliminate block artifacts.\n2.2 Invertible Neural Network\nInvertible network [ 9,10] is born to perform inverse problems since it can map\na prior distribution to the desired one easily. It acts in an unsupervised man-\nner and relies on bijectivity to ensure the mappings validity. Given a latentvariable zfrom a simple distribution p\nz(z),e.g., isotropic Gaussian, it can gen-\nerate an image with more complex distribution. Due to such ﬂexibility, INNs\nhave attracted much more attention in recent years, and have been appliedsuccessfully in applications, e.g., classiﬁcation [ 3,26], generation [ 2,10,20], and\nrescaling [ 40].\nInvertible Recovery Output Sampling Initialization\nSelect first\nchannelConcat\nchannelsSplit\nchannelsConv PixelshuffleNon-linear\nfunction\nFig. 1. The illustration of our proposed InvICS framework. It consists of three sub-\nnets, i.e., sampling, initialization, and invertible recovery subnet, while the invertible\nrecovery subnet is composed of Kinvertible phases. InvICS can extend to 100+ phases,\nwhich enables us to achieve better recovery accuracy.\n3 Proposed InvICS\nGaining insight from OPINE-Net+[45], we make a bright new attempt to inte-\ngrate with the invertible networks. And with the help of memory-saving tech-nologies based on INNs, we can design deeper networks to fully tap the potential.\n3.1 Sampling Subnet\nAs inspired by other popular designs [ 36,45], we adopt a sampling matrix with\nnetwork. For an image block of shape√\nN×√\nN, we utilize a convolutional layer', 'Invertible Image Compressive Sensing 551\nwithMﬁlters of size√\nN×√\nNand stride [√\nN,√\nN] to mimic the sampling\nprocess with ratioM\nN, and we term it as φ(·). Thus, it plays the same role as\nthe traditional sampling matrix Φ∈IRM×N. Note that in our implementation,\nwe use N= 1089. Beneﬁting from this design, InvICS can adapt to input with\nany size, not ﬁxed to√\nN×√\nN. For larger image, by means of this manner, we\ncan exploit the inter-block information, and then eliminate the block artifacts\nto achieve better recovery accuracy. For simplicity, we limit the image to withsingle channel, therefore, the measurement ywill be of size M×1×1.\n3.2 Initialization Subnet\nSome existing deep networks [ 1,12,36] introduce extra N×Mparameters for\ninitialization, which causes exploding parameters and then increases the training\ntime. We use the similar scheme proposed in OPINE-Net\n+. It’s composed of\ntwo consecutive models: a 1 ×1 convolutional layer, termed φ/latticetop(·), and a pixel\nshuﬄe layer [ 34]. The convolutional layer shares the same parameter space as the\nsampling subnet and its weights can be obtained through a transpose operation.\nIt enriches the measurements with more feature maps, while the pixelshuﬄeblock rearranges these features to an image, leading to our initial estimation ˆx\n0.\nThe initial estimation ˆx0is of a single channel. To give full play to the role of\nthe INNs, we ﬁrst use a non-linear block which is composed of a combination oftwo linear convolutional operators (without bias terms) separated by a rectiﬁed\nlinear unit (ReLU), termed PB l o c k as demonstrated in Fig. 1, to extend it to\nexpected zchannels. It will later be concatenated with the initial ˆx\n0to form u0:\nu0= Concat( ˆx0,P(ˆx0)), (1)\nwhere u0is of shape z×√\nN×√\nN, while ˆx0is of 1 ×√\nN×√\nN.\n3.3 Invertible Recovery Subnet\nThe invertible recovery subnet is composed of Kinvertible phases, and each\nphase consists of two consecutive modules. The kthphase transforms uk−1to\nukin a deep multi-channel fusion manner. The ﬁnal optimal estimation ˆxKis\nobtained through extracting the ﬁrst channel from uK.\nGradient-Descent Module. Gaining insights from the ISTA algorithm, we\ndesign a gradient-descent block, termed GD, which forms the main part of thegradient-descent module (GDM). The GD block ﬁnds the local optimal estima-\ntionr\nkthrough the negative gradient direction:\nrk=ˆxk−1−ρkφ/latticetop(φ(ˆxk−1)−y), (2)\nwhere ˆxk−1=uk−1[0 : 1], and ρkstands for the learnable step size for each\nphase. Later, rkwill be concatenated again to form the input to the second\nstep:\nGDM k(uk−1)=ˆuk= Concat( rk,uk−1[1 :z]). (3)', '552 B. Sun and J. Zhang\nPhase  k\n1\nrestC S//2\n//2+\nX +CS\nDense\nblockResidualblock restRest channels\nexcept first//2Split byhalf XElements\nmul+Elementsadd 1x1\nconvGDM AACM\nFig. 2. The design of each phase, which contains two consecutive modules: a Gradient-\nDescent Module (GDM) and an Additive & Aﬃne Coupling Module (AACM). GDMtries to get a new optimal estimation through the gradient-descent method, and then\nforward to AACM to make it more accurate in a multi-channel fusion manner.\nThe GD block is invertible if ρk/negationslash= 1, and this can be ensured through employ-\ning an extra sigmoid operation with normalized range (1.0, 2.0) to the original\nlearned ρ∗\nk,i.e.,ρk=σ(ρ∗\nk)+1.0. Thus, the reverse formula of GDM k(·)a r e :\nˆxk−1=(I−ρkΦ/latticetopΦ)−1(ˆuk[0 : 1] −ρkφ/latticetop(y)), (4)\nGDM−1\nk(ˆuk)=uk−1= Concat( ˆxk−1,ˆuk[1 :z]), (5)\nwhere the Φis obtained from the weights of convolutional layer φ.\nAdditive and Aﬃne Coupling Module. As inspired by [ 24,40], the proposed\nAACM combines an additive coupling layer [ 45] and an aﬃne coupling layer [ 10]\ntogether. Moreover, we adopt the same permutation layer used in [ 32] to improve\nthe representation capability, and meanwhile, ﬁx the optimal recovery result in\nthe ﬁrst channel.\nMathematically, as demonstrated in Fig. 2, given the input ˆuk,t h eA A C M\nﬁrstly utilizes one orthogonal convolutional layer Uk(·) to rearrange the channel\norder, and then split it to equivalent two halves along channel dimension:\nζ(1)\nk=Uk(ˆuk)[0 :z\n2],ζ(2)\nk=Uk(ˆuk)[z\n2:z]. (6)\nAfterward, the combination of coupling layers processes on each half of ζkin\nan alternative manner. We denote this transform as Qk(·):ζk→ξk,\nξ(1)\nk=ζ(1)\nk+Fk(ζ(2)\nk),ξ(2)\nk=g(Gk(ξ(1)\nk))⊙ζ(2)\nk+Hk(ξ(1)\nk), (7)\nwhere Fk(·) stands for a density block, while Gk(·)a n d Hk(·) represent two\nresidual blocks, and ⊙is the element-wise product. Based on the insights of [ 5],\nwe adopt sigmoid function as the scaling function g(·), and the output of it is\nnormalized to range (0 .5,1) to enhance stability.', 'Invertible Image Compressive Sensing 553\nBy the way, the inverse of the above operation can be obtained easily as\nfollows, and we denote it as Q−1\nk(·):ξk→ζk,\nζ(2)\nk=(ξ(2)\nk−Hk(ξ(1)\nk))/circledivideg(Gk(ξ(1)\nk)),ζ(1)\nk=ξ(1)\nk−Fk(ζ(2)\nk), (8)\nwhere /circledivideis the element-wise divide operation.\nFinally, ( ξ(1)\nk,ξ(2)\nk) are concatenated along the channel dimension to ξk.T h e\nother orthogonal convolutional layer, termed as U/latticetop\nk(·), is adopted to recover ξk:\nξk= Concat( ξ(1)\nk,ξ(2)\nk),uk=U/latticetop\nk(ξk), (9)\nwhere weights of U/latticetop\nk(·) can be obtained through the exact transpose of Uk(·).\nTo sum up, since Uk(·),U/latticetop\nk(·)a n d Qk(·) are all invertible, the entire AACM k\nis also invertible, and the bidirectional operation can be formulated as follows.\nFurthermore, the entire kthrecovery phase is invertible as well.\nAACM k(ˆuk)=uk=U/latticetop\nk(Qk(Uk(ˆuk))), (10)\nAACM−1\nk(uk)=ˆuk=U/latticetop\nk(Q−1\nk(Uk(uk))). (11)\nMemory-Saving Skills. Many methods [ 14,18,22] have been proposed regard-\ning how to reduce GPU memory consumption. We adopt the framework proposedby van de Leemput et al., termed MemCNN [ 22]. In our implementation, we wrap\neach phase through the MemCNN framework. By means of it, the input activa-\ntionu\n0:K−1, plus the intermediate gradient graph during deep recovery, will not\nbe stored. Because each phase contains at least 10 convolutional layers, taking\n100 phases for example, almost 1000+ layers’ activations will be freed, whichwill reduce the total GPU memory consumption to a large extent.\n3.4 Network Parameters and Loss Function\nThe learnable parameter set in InvICS, denoted by Θ, consists of the equiv-\nalent sampling matrix Φ(the weights of the sampling module actually), the\nparameters P( ·)o ft h e PB l o c k , and the parameters in each recovery phase,\ni.e.,G D M\nk(·), AACM k(·). Thus, Θ={Φ,P(·),GDM k(·),AACM k(·)}.G i v e n\nthe training dataset {xi}N b\ni=1, InvICS tries to train the sampling matrix and the\nrecovery module synchronously, and then gets the ﬁnal estimation ˆxi\nK. It aims\nto reduce the discrepancy between xiandˆxi\nK, and maintain the orthogonality\nof the sampling matrix. Therefore, the loss function designed looks like follows:\nLtotal(Θ)= Ldisc+γ×Lorth, (12)\nwhere Ldisc=1\nN bN/summationtextN b\ni=1/bardblxi−ˆxi\nK/bardbl2\n2,Lorth=1\nM/bardblΦΦ/latticetop−I/bardbl2\n2,and the\nregularization parameter γis ﬁxed to 0 .01 by default.', '554 B. Sun and J. Zhang\n4 Analysis and Experiments\nThe training data are obtained through randomly extracting speciﬁed size of\nimage blocks based on the Train400 dataset, while for testing, we utilize threewidely used benchmark datasets: Set11 [ 21], BSD68 [ 27] and Urban100 [ 16]. In\ngeneral, We adopt a 2-step training strategy: a) at ﬁrst train with 33 ×33 block\nwith learning rate 1 e\n−4to 100 epochs; b) then ﬁne-tune it with extra 10 epochs\nwith larger image block size, i.e.,9 9×99, and lower learning rate 1 e−5.B y\nthis mean, we can train with faster speed, and meanwhile, take the advantage\nof inter-block training to eliminate block artifacts. We use the Adam [ 19] opti-\nmization method to train. All the experiments are performed on a workstation\nwith Intel Core i3-9100 CPU and RTX2070 GPU unless otherwise stated. In\nour implementation, we set z= 32 by default, while Kis set to 30. Note that\nthe recovered results are evaluated with Peak Signal-to-Noise Ratio (PSNR) and\nStructural Similarity Index (SSIM).\n0 20 40 60 80 100\nK30323436PSNR (dB)\n35.7 35.8 35.9 36.0\nPSNR (dB)0.95800.95850.95900.95950.96000.96050.9610SSIM#30\n#50\n#70\n#100\nFig. 3. Illustration of PSNR improvement as phase number Ki n c r e a s e so nS e t 1 1\ndataset at CR = 25%. Utilizing more phases than existing methods, InvICS can obtainhuge improvement on PSNR.\n4.1 Study of Phase Number\nAs stated in OPINE-Net+that recovery PSNR is almost stable when phase\nnumber Kis larger than 9, therefore OPINE-Net+setsK= 9 by default. Our\narchitecture, by contrast, can get obvious improvement with larger K. As demon-\nstrated in Fig. 3, through extending Kfrom 9 to 30, we can obtain near 0.5 dB\ngain. What’s more, as phase number Kincreases from 30 to 100, we can still\nobtain 0.3 dB improvements. However, considering the trade-oﬀ between com-\nputational complexity and recovery performance, we choose K= 30 by default.\n4.2 Comparison with State-of-the-Art\nWe compare InvICS with four other representative network-based methods, i.e.,\nBCS [ 1], AdapReconNet [ 25], ISTA-Net+[44], and OPINE-Net+[45]. As shown', 'Invertible Image Compressive Sensing 555\nin Table 1, InvICS achieves the best quality on all the test datasets and at all\nCRs. And we ﬁnd that InvICS performs especially better at high CRs, e.g.,i t\nobtains up to 1.5 dB gain over OPINE-Net+at CR = 50%. Visual comparison\nresults are provided in Fig. 4, and we ﬁnd that InvICS can obtain more structural\ndetails than other methods.\nTable 1. Quantitative results (PSNR/SSIM) on common benchmark datasets. Our\nframework outperforms most methods, especially at high CRs, in which cases InvICS\ncan obtain a signiﬁcant >1.0 dB gain.\nDataset CS ratio ISTA-Net+[44]BCS [ 1] AdapReconNet [ 25]OPINE-Net+[45]InvICS\nSet11 1% 17.42/0.403 19.15/0.441 19.63/0.485 20.15/0.534 20.40/0.550\n4% 21.32/0.604 23.19/0.663 23.87/0.728 25.69/0.792 26.08/0.803\n10% 26.64/0.809 26.04/0.797 27.39/0.852 29.81/0.888 30.34/0.899\n25% 32.59/0.925 29.98/0.893 31.75/0.926 34.86/0.951 35.68/0.958\n50% 38.11/0.971 34.61/0.944 35.87/0.963 40.17/0.980 41.24/0.983\nSet68 1% 19.14/0.416 21.24/0.462 21.50/0.483 22.11/0.514 22.18/0.525\n4% 22.17/0.549 23.94/0.619 24.30/0.649 25.20/0.683 25.48/0.697\n10% 25.32/0.702 26.07/0.754 26.72/0.782 27.82/0.805 28.21/0.818\n25% 29.36/0.853 29.18/0.873 30.10/0.890 31.51/0.906 32.21/0.918\n50% 34.04/0.942 33.18/0.940 33.60/0.948 36.35/0.966 37.45/0.973\nUrban100 1% 16.90/0.385 18.97/0.436 19.14/0.451 19.82/0.501 19.88/0.515\n4% 19.83/0.538 21.55/0.599 21.92/0.639 23.36/0.711 23.93/0.736\n10% 24.04/0.738 23.58/0.723 24.55/0.780 26.93/0.840 27.86/0.863\n25% 29.78/0.895 26.75/0.841 28.21/0.884 31.86/0.931 33.32/0.947\n50% 35.24/0.961 30.65/0.913 31.88/0.943 37.23/0.974 38.76/0.981\n4.3 Ablation Studies\nOrder-Preserving Multi-channel Fusion. A ss h o w ni nF i g . 5,w ec a n\nobserve that with larger zthe recovery accuracy increases signiﬁcantly during\neach iteration. By this means, we can obtain huge representational power for\nrecovery tasks, which forms a main contribution of our framework.\nDense Block vs. Residual Block. In this section, we provide a comparison\nbetween diﬀerent F block designs in each phase. The architecture with dense\nblock achieves higher recovery accuracy than a residual block (see Fig. 6), which\nfully meets our expectations, since the dense block can ensure maximum infor-\nmation ﬂow and feature map fusion.\nMemory-Saving. After utilizing memory-saving skills, the GPU memory con-\nsumption drops dramatically. To get more intuitive impressions, we experiment\non Nvidia 1080Ti GPU (see Fig. 6). We observe that our model can run with\nK= 60 almost if no memory-saving skill is involved, in which case it consumes\nup to 10.0 GB GPU memories. But after we enable the memory-saving, the total\nGPU memory consumption drops to 2.43 GB, which incredibly saves up to three\nquarters compared with disabling the memory-saving option.', '556 B. Sun and J. Zhang\nOriginal ISTA-Net+OPINE-Net+\nPSNR/SSIM 28.27/0.802 30.78/0.892 31.24/0.900 30.02/0.878 29.93/0.878BCS AdapReconNet InvICS\nPSNR/SSIM 31.67/0.962 35.26/0.984 36.94/0.990Original ISTA-Net+OPINE-Net+ BCS AdapReconNet\n32.79/0.974 32.95/0.974InvICS\nFig. 4. The visual comparison with four other competing methods on two images, one\nfrom Set68 at CR = 10%, while the another from Urban100 dataset at CR = 25%\nrespectively. We observe that InvICS can obtain more structural details than othermethods.\n50 100 150\nEpoch33.033.534.034.535.035.5PSNR (dB)#2\n#8\n#16\n#32\n#64\n35.0 35.2 35.4 35.6\nPSNR (dB)0.9520.9530.9540.9550.9560.9570.958SSIM#2\n#8\n#16\n#32\n#64\nFig. 5. Illustration of the eﬀect of order-preserving multi-channel fusion method on\nSet11 dataset. We observe that the PSNR increases a lot as zincreases. We choose\nz= 32 by default, through which we can achieve a perfect balance between performance\nand the model size. This special design is one big diﬀerence from other CS methods,\nand it provides another source of gain to InvICS.', 'Invertible Image Compressive Sensing 557\n50 100 150\nEpoch33.033.534.034.535.035.5PSNR (dB)F\nDense Block\nResidual Block\n20 40 60 80 100\nK246810Memory (GB)MemCNN\nw/o\nw/\nFig. 6. (Left) indicates the improvement when using dense block; (Right) shows the\neﬃcient memory-saving result.\n5 Conclusion and Future Work\nIn this paper, we propose a novel and memory-eﬃcient invertible image com-\npressive sensing framework, dubbed InvICS. By introducing invertible networks\nto the deep recovery subnet, the activations for most layers need not be stored\nin memory during backpropagation, which easily allows training InvICS with100+ phases (1000+ layers) even on a single Nvidia 2070 GPU. What’s more,\nwe adopt an order-preserving multi-channel fusion approach, which can make\nfull use of the multiple feature maps. Extensive experiments demonstrate thatInvICS signiﬁcantly improves both quantitative and qualitative performance.\nSince we only utilize the invertible layers in the deep recovery subnet, InvICS is\npartially invertible. Thus, Our future work will be focused on how to extend it\nto a fully invertible one, and we believe it will bring further improvements.\nReferences\n1. Adler, A., Boublil, D., Elad, M., Zibulevsky, M.: Block-based compressed sensing\nof images via deep learning. In: MMSP (2017)\n2. Ardizzone, L., L¨ uth, C., Kruse, J., Rother, C., K¨ othe, U.: Guided image generation\nwith conditional invertible neural networks. In: ICLR (2020)\n3. Ardizzone, L., Mackowiak, R., Rother, C., K¨ othe, U.: Training normalizing ﬂows\nwith the information bottleneck for competitive generative classiﬁcation. In:NeurIPS (2020)\n4. Baraniuk, R.G., et al.: Single-pixel imaging via compressive sampling. IEEE Signal\nProcess. Mag. 25, 83–91 (2008)\n5. Behrmann, J., Vicol, P., Wang, K.C., Grosse, R., Jacobsen, J.H.: Understand-\ning and mitigating exploding inverses in invertible neural networks. In: AISTATS\n(2021)\n6. Cand` es, E.J., Romberg, J., Tao, T.: Robust uncertainty principles: exact signal\nreconstruction from highly incomplete frequency information. IEEE Trans. Inf.Theory 52(2), 489–509 (2006)\n7. Candes, E.J., Tao, T.: Near-optimal signal recovery from random projections: uni-\nversal encoding strategies. IEEE Trans. Inf. Theory 52(12), 5406–5425 (2006)', '558 B. Sun and J. Zhang\n8. Chen, J., Sun, Y., Liu, Q., Huang, R.: Learning memory augmented cascading\nnetwork for compressed sensing of images. In: Vedaldi, A., Bischof, H., Brox, T.,\nFrahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12367, pp. 513–529. Springer, Cham(2020). https://doi.org/10.1007/978-3-030-58542-6\n31\n9. Dinh, L., Krueger, D., Bengio, Y.: NICE: non-linear independent components esti-\nmation. In: ICLR (2015)\n10. Dinh, L., Sohl-Dickstein, J., Bengio, S.: Density estimation using real NVP. In:\nICLR (2017)\n11. Donoho, D.L.: Compressed sensing. IEEE Trans. Inf. Theory 52(4), 1289–1306\n(2006)\n12. Du, J., Xie, X., Wang, C., Shi, G., Xu, X., Wang, Y.: Fully convolutional mea-\nsurement network for compressive sensing image reconstruction. Neurocomputing\n328, 105–112 (2019)\n13. Gao, X., Zhang, J., Che, W., Fan, X., Zhao, D.: Block-based compressive sensing\ncoding of natural images by local structural measurement matrix. In: DCC (2015)\n14. Gomez, A.N., Ren, M., Urtasun, R., Grosse, R.B.: The reversible residual network:\nbackpropagation without storing activations. In: NeurIPS (2017)\n15. Hot, E., Sekuli´ c, P.: Compressed sensing MRI using masked DCT and DFT mea-\nsurements. In: MECO (2015)\n16. Huang, J.B., Singh, A., Ahuja, N.: Single image super-resolution from transformed\nself-exemplars. In: CVPR (2015)\n17. Iliadis, M., Spinoulas, L., Katsaggelos, A.K.: Deep fully-connected networks for\nvideo compressive sensing. Digit. Signal Process. 72, 9–18 (2018)\n18. Jacobsen, J.H., Smeulders, A., Oyallon, E.: I-REVNet: deep invertible networks.\nIn: ICLR (2018)\n19. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. In: ICLR (2014)20. Kingma, D.P., Dhariwal, P.: Glow: generative ﬂow with invertible 1x1 convolutions.\nIn: NeurIPS (2018)\n21. Kulkarni, K., Lohit, S., Turaga, P., Kerviche, R., Ashok, A.: ReconNet: non-\niterative reconstruction of images from compressively sensed measurements. In:\nCVPR (2016)\n22. van de Leemput, S.C., Teuwen, J., Manniesing, R.: MemCNN: a framework for\ndeveloping memory eﬃcient deep invertible networks. In: ICLR (2018)\n23. Li, C., Yin, W., Jiang, H., Zhang, Y.: An eﬃcient augmented Lagrangian method\nwith applications to total variation minimization. Comput. Opt. Appl. 56(3), 507–\n530 (2013)\n24. Liu, Y., et al.: Invertible denoising network: a light solution for real noise removal.\nIn: CVPR (2021)\n25. Lohit, S., Kulkarni, K., Kerviche, R., Turaga, P., Ashok, A.: Convolutional neural\nnetworks for noniterative reconstruction of compressively sensed images. IEEE\nTrans. Comput. Imaging 4(3), 326–340 (2018)\n26. Mackowiak, R., Ardizzone, L., K¨ othe, U., Rother, C.: Generative classiﬁers as a\nbasis for trustworthy computer vision. In: CVPR (2021)\n27. Martin, D., Fowlkes, C., Tal, D., Malik, J.: A database of human segmented natural\nimages and its application to evaluating segmentation algorithms and measuring\necological statistics. In: ICCV (2001)\n28. Metzler, C.A., Mousavi, A., Baraniuk, R.G.: Learned D-AMP: principled neural\nnetwork based compressive image recovery. In: NeurIPS (2017)\n29. Mousavi, A., Baraniuk, R.G.: Learning to invert: signal recovery via deep convo-\nlutional networks. In: ICASSP (2017)', 'Invertible Image Compressive Sensing 559\n30. Mousavi, A., Dasarathy, G., Baraniuk, R.G.: DeepCodec: adaptive sensing and\nrecovery via deep convolutional neural networks. In: Annual Allerton Conference\non Communication, Control, and Computing (Allerton) (2017)\n31. Mousavi, A., Patel, A.B., Baraniuk, R.G.: A deep learning approach to structured\nsignal recovery. In: Annual Allerton Conference on Communication, Control, andComputing (Allerton) (2015)\n32. Putzky, P., Welling, M.: Invert to learn to invert. In: NeurIPS (2019)\n33. Sharma, S.K., Lagunas, E., Chatzinotas, S., Ottersten, B.: Application of compres-\nsive sensing in cognitive radio communications: a survey. IEEE Commun. Surv.\nTutor. 18(3), 1838–1860 (2016)\n34. Shi, W., et al.: Real-time single image and video super-resolution using an eﬃcient\nsub-pixel convolutional neural network. In: CVPR (2016)\n35. Shi, W., Jiang, F., Liu, S., Zhao, D.: Image compressed sensing using convolutional\nneural network. TIP 29, 375–388 (2019)\n36. Shi, W., Jiang, F., Liu, S., Zhao, D.: Scalable convolutional neural network for\nimage compressed sensing. In: CVPR (2019)\n37. Shi, W., Jiang, F., Zhang, S., Zhao, D.: Deep networks for compressed image\nsensing. In: ICME (2017)\n38. Sun, Y., Chen, J., Liu, Q., Liu, B., Guo, G.: Dual-path attention network for\ncompressed sensing image reconstruction. IEEE Trans. Image Process. 29, 9482–\n9495 (2020)\n39. Wu, Z., Zhang, Z., Song, J., Zhang, J.: Spatial-temporal synergic prior driven\nunfolding network for snapshot compressive imaging. In: ICME (2021)\n40. Xiao, M., et al.: Invertible image rescaling. In: Vedaldi, A., Bischof, H., Brox, T.,\nFrahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12346, pp. 126–144. Springer, Cham\n(2020). https://doi.org/10.1007/978-3-030-58452-8\n8\n41. Yang, Y., Sun, J., Li, H., Xu, Z.: Deep ADMM-Net for compressive sensing MRI.\nIn: NeurIPS (2016)\n42. You, D., Xie, J., Zhang, J.: ISTA-Net++: ﬂexible deep unfolding network for com-\npressive sensing. In: ICME (2021)\n43. You, D., Zhang, J., Xie, J., Chen, B., Ma, S.: COAST: COntrollable arbitrary-\nsampling NeTwork for compressive sensing. IEEE Trans. Image Process. 30, 6066–\n6080 (2021)\n44. Zhang, J., Ghanem, B.: ISTA-Net: interpretable optimization-inspired deep net-\nwork for image compressive sensing. In: CVPR (2018)\n45. Zhang, J., Zhao, C., Gao, W.: Optimization-inspired compact deep compressive\nsensing. IEEE J. Sel. Top. Signal Process. 14(4), 765–774 (2020)\n46. Zhang, J., Zhao, C., Zhao, D., Gao, W.: Image compressive sensing recovery using\nadaptively learned sparsifying basis via L0 minimization. Signal Process. 103, 114–\n126 (2014)\n47. Zhang, J., Zhao, D., Gao, W.: Group-based sparse representation for image restora-\ntion. IEEE Trans. Image Process. 23(8), 3336–3351 (2014)\n48. Zhang, Z., Jung, T.P., Makeig, S., Rao, B.D.: Compressed sensing for energy-\neﬃcient wireless telemonitoring of noninvasive fetal ECG via block sparse Bayesian\nlearning. IEEE Trans. Biomed. Eng. 60(2), 300–309 (2012)\n49. Zhang, Z., Liu, Y., Liu, J., Wen, F., Zhu, C.: AMP-Net: denoising-based deep\nunfolding for compressive image sensing. IEEE Trans. Image Process. 30, 1487–\n1500 (2020)\n50. Zhao, C., Ma, S., Gao, W.: Image compressive-sensing recovery using structured\nLaplacian sparsity in DCT domain and multi-hypothesis prediction. In: ICME\n(2014)', '560 B. Sun and J. Zhang\n51. Zhao, C., Ma, S., Zhang, J., Xiong, R., Gao, W.: Video compressive sensing recon-\nstruction via reweighted residual sparsity. IEEE Trans. Circ. Syst. Video Technol.\n27(6), 1182–1195 (2016)\n52. Zhao, C., Zhang, J., Ma, S., Gao, W.: Compressive-sensed image coding via stripe-\nbased DPCM. In: DCC (2016)\n53. Zhao, C., Zhang, J., Ma, S., Gao, W.: Nonconvex LP nuclear norm based ADMM\nframework for compressed sensing. In: DCC (2016)\n54. Zhao, C., Zhang, J., Wang, R., Gao, W.: CREAM: CNN-regularized ADMM frame-\nwork for compressive-sensed image reconstruction. IEEE Access 6, 76838–76853\n(2018)', 'Gradient-Free Neural Network Training\nBased on Deep Dictionary Learning with\nthe Log Regularizer\nYing Xie1, Zhenni Li1(B), and Haoli Zhao2\n1School of Automation, Guangdong University of Technology,\nGuangzhou 510006, China\nlizhenni@gdut.edu.cn\n2School of Computer Science and Engineering, Sun Yat-sen University,\nGuangzhou 510006, China\nAbstract. Gradient-free neural network training is attracting increas-\ning attentions, which eﬃciently to avoid the gradient vanishing issue in\ntraditional neural network training with gradient-based methods. The\nstate-of-the-art gradient-free methods introduce a quadratic penalty oruse an equivalent approximation of the activation function to achieve the\ntraining process without gradients, but they are hardly to mine eﬀective\nsignal features since the activation function is a limited nonlinear trans-formation. In this paper, we ﬁrst propose to construct the neural network\ntraining as a deep dictionary learning model for achieving the gradient-\nfree training of the network. To further enhance the ability of feature\nextraction in network training based on gradient-free method, we intro-\nduce the logarithm function as a sparsity regularizer which introducesaccurate sparse activations on the hidden layer except for the last layer.\nThen, we employ a proximal block coordinate descent method to forward\nupdate the variables of each layer and apply the log-thresholding opera-\ntor to achieve the optimization of the non-convex and non-smooth sub-\nproblems. Finally, numerical experiments conducted on several publicly\navailable datasets prove the sparse representation of inputs is eﬀectivefor gradient-free neural network training.\nKeywords: Deep dictionary learning\n·logregularizer ·Block\ncoordinate descent ·Sparse proximal operator ·Gradient-free network\n1 Introduction\nDeep neural networks (DNN) have achieved great success in many applications,\nsuch as image recognition [ 1] and speech recognition [ 2]. Backpropagation based\non Stochastic Gradient Descent (SGD) [ 3] is widely used for training neural net-\nworks due to its eﬃciency. However, SGD suﬀers from a variety of problems such\nas the gradient vanishing problem, which makes the training diﬃcult or unstable.To solve this problem, some methods introduce rectiﬁed linear units (ReLU) [ 4],\nc/circlecopyrtSpringer Nature Switzerland AG 2021\nH. Ma et al. (Eds.): PRCV 2021, LNCS 13022, pp. 561–574, 2021.https://doi.org/10.1007/978-3-030-88013-2\n_46', '562 Y. Xie et al.\nbatch normalization (BN) [ 5], or residual convolutional neural network (ResNet)\n[6]. However, they still have drawbacks such as diﬃculty in tuning and paralleliza-\ntion [7]. Other works [ 8–12] have been proposed gradient-free methods for the\nneural network training, which do not suﬀer from gradient vanishing issue due\nto the avoidance of using classical backpropagation. The work [ 10]p r o p o s e dt h e\nmethod of auxiliary coordinates (MAC) which introduced a quadratic penalty\nto the objective function to relax the equality constraints and trained the net-\nwork by alternating optimization over the parameters and the auxiliary coor-dinates. The works [ 8,9] used the augmented Lagrangian method to relax net-\nwork constraints by introducing penalty in the objective function and optimized\nit by the alternating direction method of multipliers (ADMM). The works [ 11,12]\nalso adopted the introduction of a quadratic penalty in the objective function to\nrelax the network constraints, but proposed the proximal block coordinate descent\n(BCD) method to solve the minimization optimization to train networks. The\nworks [ 7,13] employed the equivalent representation of the activation function to\nsolve the nonlinear constraint problems in the optimization problems and the net-work was trained by BCD method. However, the activation functions used in the\ntraining of gradient-free networks in these works are very limited non-linear trans-\nformations, which hardly mine eﬀective signal features.\nDeep dictionary learning (DDL) aims to learn latent representations of data\nby learning multi-level dictionaries, which can be considered as the gradient-free\nneural network training model. The work [ 14] showed that the output of the neu-\nral network using dictionary learning is more discriminative and class speciﬁc\nthan the traditional fully connected layer. The work [ 15] took the advantage of\nthe convolutional neural network for convolution feature extraction, by learningdictionary for sparse representation to achieve scene recognition. DDL normally\nintroduces sparsity constraint to restrict the learning procedure from the over-\ncomplete underdetermined system. The work [ 16] showed that compared to shal-\nlow dictionary learning, DDL learned sparse representation by applying l\n1-norm\non the features and using the linear classiﬁer for classiﬁcation can achieve better\nclassiﬁcation accuracy.\nIn this paper, we propose a new gradient-free network training model based\non DDL with the logsparsity regularizer, which trains the network by learn-\ning the multi-layer dictionaries and then obtains sparse activations in network\nby learning sparse representation of inputs. In details, ﬁrstly, we formulate the\nneural network training model as a DDL model by regarding the weight matrixin the neural network as a dictionary, the activations of the hidden layer corre-\nspond to the coeﬃcients of DDL. Secondly, in order to obtain a accurate sparse\nrepresentation of inputs, we employ the logsparsity regularizer on the hidden\nlayer neurons of network except for the last hidden layer, which could enforce\nstrong sparsity and obtain accurate estimation. The reason why not impose the\nsparsity constraint on the last hidden layer is as follows, we regard the functionof the last two layers as a classiﬁer, where a two-layer structure can perform\nbetter feature classiﬁcation than a single-layer structure. Thirdly, we propose\nto use the proximal BCD method to optimize a block variable by ﬁxing otherblock variables. Due to the nonconvexity of the logregularizer, we employ the', 'Gradient-Free Neural Network Training Based on Deep Dictionary Learning 563\nlog-thresholding operator to solve the corresponding optimization problem with\nrespect to the activations. Finally, numerical experiments verify that the pro-posed gradient-free training model based on DDL is eﬀective and superior to\nothers training methods.\nThe main contributions are summarized as follows:\n1) To overcome the gradient vanishing issue, a DDL-based gradient-free neural\nnetwork training model is proposed, where the dictionary and the sparse\ncoeﬃcient correspond to the weight matrix and activation respectively.\n2) To obtain accurate and sparse representation of inputs on proposed gradient-\nfree network training model, we propose to employ the nonconvex logarithm-\nbased function as sparsity regularizer which can enforce strong sparsity andobtain accurate representation. The model takes advantage of sparse represen-\ntation for feature extraction to improve the performance of the gradient-free\ntraining model.\n3) We apply the proximal block coordinate descent method to train the gradient-\nfree network and update the variables of each layer in a forward update man-\nner, and employ the proximal operator to solve the non-convex logregularized\nsubproblems, which developed our proposed LOG-PBCD algorithm.\nThe rest of this paper is organized as follows. Section 2describes some current\ngradient-free neural network training methods and the connection between DDL\nand DNN. Section 3shows how to apply the logregularizer to achieve the purpose\nof sparse activations, and solve the objective function containing logregularizer\nthrough proximal BCD optimization with log-thresholding operator. Section 4,\nwe verify the eﬀectiveness of proposed model through experiments. We conclude\nthis paper in Sect. 5.\n2 Related Work\n2.1 Notation\nWe brieﬂy summarize notations in this paper. A boldface uppercase letter such\nasAldenotes a matrix. A lowercase element such as acjdenotes the entry in\nthec-th row and the j-th column of A. A lowercase Greek letter ϕdenotes an\nactivation function that acts column-wise on a matrix, Idenotes the identity\noperator. Let dlbe the number of neurons in lthlayer. Deﬁne a neural network\nwith llayers, where l=0,1,2,...,L. The ﬁrst layer ( l= 0) is the input layer, the\ninput signals are composed of Mtraining samples. The last layer ( l=L)i st h e\noutput layer with Kcategories, namely Kneurons. Let A0∈Rd0×Mbe the input\ntraining data, Al∈Rdl×M,l=1,2,...,L −1 be the activations of hidden layer,\nWl∈Rdl×dl−1,l=1,2,...,L be the weight matrix between the lthand (l−1)th\nlayers, Y∈RdL×Kbe the one-hot matrix of labels, Bl∈Rdl×M,l=1,2,...,L\nbe the bias matrix of hidden layer and every column of it is same. k=1,2,...,N\nmeans iteration.', '564 Y. Xie et al.\n2.2 Neural Network Training and Deep Dictionary Learning\nNeural network produces predictive output ˆYthrough feed-forward recursion\nˆY=WLAL−1+BLgiven below,\nAl=ϕ(WlAl−1+Bl),l=1,2,3...,L−1 (1)\nwhere ϕis the activation function such as sigmoid function or ReLU function\nthat acts column-wise on a matrix. Without loss of generality, we can remove\nBlby adding an extra column to Wland a row of ones to Al−1. Then Eq. ( 1)\nsimpliﬁes to\nAl=ϕ(WlAl−1),l=1,2,3...,L−1 (2)\nDeﬁne an auxiliary variable Zl=WlAl−1. Then, Al=ϕ(Zl),l=1,2,3...,L−1,\ngradient-free network training problem can be formulated as the minimization\nproblem,\nAl,Wl,Zl←argmin\nAl,Wl,ZlL(ZL,Y)(3)\nsubject to Zl=WlAl−1,f o rl=1,2,...,L.Al=ϕ(Zl), for l=1,2,...,L −1.\nFollowing the method of auxiliary coordinates (MAC) method [ 10], instead\nof directly solving the Eq. ( 3), the constrained problem may be solved with\nquadratic-penalty methods using alternating optimization over the parameters\nand the auxiliary coordinates. Inspired by ADMM algorithm, the work [ 8] also\ntry to optimize above Eq. ( 3), they impose Lagrangian constraints on the output\nlayer. Unlike MAC method, the output layer adopts the linear activation func-tion rather than non-linear activation function. The proximal BCD proposed by\n[12] is another method to solve Eq. ( 3) in gradient-free network training problem,\nwhich they call it three-splitting formulation. Compared with the two-splittingformulation, the advantage of adopting the three-splitting formulation is that\nalmost all updates use simple proximal updates or just least-squares problems,\nbut the disadvantage is that more storage memory is required. Recent worksaccelerate gradient-free training based on the BCD method [ 17] and use the\nequivalent representation of the activation function to solve the nonlinear con-\nstraint problem in the optimization problem [ 7,13], they improve and accelerate\nthe training process but hardly employ sparse to learn the deep representation\nof the inputs.\nDDL can be seen as a special form of the gradient-free neural network since\ntheir structure and optimization methods are similar, that is, regarding the\nweight matrix as a dictionary and the activation of a certain layer of neurons as\nsparse coeﬃcients. In addition, both use the same activation function. For exam-ple, the activation function employed in multi-layer dictionary learning assures\nthat the Llevels of dictionaries are not collapsible into one, so that deep dic-\ntionary learning can learn deeper features. But unlike a neural network which\nis directed from the input to the representation, the dictionary learning kind of\nnetwork points in the other direction, from representation to the input [ 16].\nIn this paper, DDL for sparse representation and its similarity to DNN\ninspires us to learn the sparse representation of the inputs, which will lead to the', 'Gradient-Free Neural Network Training Based on Deep Dictionary Learning 565\nsparse activations in DNN. We propose a gradient-free network training model\nbased on DDL and employ the proximal block coordinate descent method totrain all variables. The proximal operator and the log-thresholding operator are\nemployed to update the activations. An overview of our algorithm is shown in\nFig.1.\nFig. 1. An overview of the structure of DNN. The Ydenotes labels. We apply log\nregularizer to the activation of hidden layer neurons of DNN from A1toAL−2except\nthe last hidden layer AL−1. The update of parameters adopts forward updating: Wl→\nBl→Zl→Al,l=1,2, ..., L.\n3 Gradient-Free Neural Network Training Based on Deep\nDictionary Learning with the Log Regularizer\nIn this section, we ﬁrst formulate our gradient-free network training problem\nwhere the logsparsity regularizer is employed for accurate and sparse represen-\ntation of inputs and simultaneously get sparse activations. Second, we employ\nthe proximal block coordinate descent method and the log-thresholding operator\nto solve the non-convex problems. For the problem of updating variables withlogsparse constraints, we employ the proximal gradient descent method and\nlog-thresholding operator to update, and through proximal step update vari-\nables without sparse constraints. In the third part, we summarize the proposedalgorithm in Algorithm 1.\n3.1 Problem Formulation\nIn order to achieve gradient-free neural network training based on DDL. we\nregard the weight matrix of the gradient-free neural network as a dictionary\nand the activations of hidden layer as coeﬃcients, retain the bias of the neural\nnetwork and select ReLU function as the activation function. Unlike [ 12], we\nmaintain the usual structure of the neural network which the ﬁnal output is the\nactivations without the nonlinear transformation. Unlike DDL, we introduce the', '566 Y. Xie et al.\nlogsparse constraint on the activations Al,l=1,2,...,L −2 of the remaining\nlayers, except for the activations AL−1of the last hidden layer, where we expect\nto keep the structure of the last two layers of the neural network and regard last\ntwo layers as a classiﬁer. Our problem of gradient-free network training based\nDDL can be formulated as follows,\nZ, W, A, B ←argmin\nZ,W,A,BL−1/summationdisplay\nl=1(γ\n2/bardblAl−ϕ(Zl)/bardbl2\nF+ρ\n2/bardblZl−WlAl−1−Bl/bardbl2\nF)\n+λL−2/summationdisplay\nl=1G(Al)+β\n2/bardblZL−WLAL−1−BL/bardbl2\nF+α\n2/bardblZL−Y/bardbl2\nF\n(4)\nwhere G(Al)=/summationtextC\nc=1/summationtextJ\nj=1(log(1 +|acj|\n/epsilon1)) denotes the logsparsity regularizer.\nLetacjbe an element of Al.\n3.2 Proposed Algorithm\nIn order to solve the above non-convex and non-smooth optimization problem\nEq. (4), we employ the proximal block coordinate descent method to update Al\norWlby ﬁxing all other blocks of variables and employ the log-thresholding\noperator to solve the subproblems regarding to logregularizer. We retain the\nnetwork bias and speciﬁc steps are as follows,\nUpdating W l,Bl,l=1 ,2, ..., L . The proximal BCD algorithm ﬁxes other\nblock variables and perform a proximal step to update Wl,Blvia as follows,\nWk+1\nl←argmin\nWk\nlρ\n2/bardblZk\nl−Wk\nlAk+1\nl−1−Bk\nl/bardbl2\nF+δ\n2/bardblWk\nl−Wk−1\nl/bardbl2\nF(5)\nBk+1\nl←argmin\nBk\nlρ\n2/bardblZk\nl−Wk\nlAk+1\nl−1−Bk\nl/bardbl2\nF+δ\n2/bardblBk\nl−Bk−1\nl/bardbl2\nF (6)\nThe Eq. ( 5)a n dE q .( 6) have closed-form solutions. Then, the update rule of\nEq. (5)a n dE q .( 6) can be rewritten as,\nWk+1\nl=(δWk\nl+ρ(Zk\nl−Bk\nl)Ak\nl−1)(δI+ρ(Ak\nl−1(Ak+1\nl−1)T))−1(7)\nBk+1\nl=(δBk\nl+ρ(Zk\nl−Wk\nlAk+1\nl−1))/(ρ+δ) (8)\nUpdating Zl,l=1 ,2, ..., L −1. The variables Zlare updated as follows,\nZk+1\nl←argmin\nZk\nlγ\n2/bardblAk\nl−ϕ(Zk\nl)/bardbl2\nF+ρ\n2/bardblZk\nl−Wk+1\nlAk+1\nl−1−Bk+1\nl/bardbl2\nF+δ\n2/bardblZk\nl−Zk−1\nl/bardbl2\nF\n(9)', 'Gradient-Free Neural Network Training Based on Deep Dictionary Learning 567\nSince Eq. ( 9) contains the ReLU activation function, we obtain the solutions by\nthe Lemma 13 in the appendix of [ 12]. Then Eq. ( 9) can be written as,\nZk+1\nl←argmin\nZk\nl1\n2/bardblAk\nl−ϕ(Zk\nl)/bardbl2\nF+ρ+δ\n2γ/bardblZk\nl−δZk−1\nl+ρ(Wk+1\nlAk+1\nl−1+Bk+1\nl)\nρ+δ/bardbl2\nF\n(10)\nWhere η=ρ+δ\nγ,Sk\nl=δZk−1\nl+ρ(Wk+1\nlAk+1\nl−1+Bk+1\nl)\nρ+δ.T h e nE q .( 10) is equivalent to\nthe following form:\nZk+1\nl←argmin\nZk\nl1\n2/bardblAk\nl−ϕ(Zk\nl)/bardbl2\nF+η\n2/bardblZk\nl−Sk\nl/bardbl2\nF (11)\nThen Eq. ( 11) reduces to the following one-dimensional minimization problem.\nThezk\ncj,akcj,skcjis an element of Zk\nl,Ak\nl,Sk\nl\nzk+1\ncj←argmin\nzk\ncj1\n2(ak\ncj−ϕ(zk\ncj))2+η\n2(zk\ncj−sk\ncj)2(12)\nThe iteration of zcjcan be obtained as,\nzk+1\ncj=prox 1\n2η(ak\ncj−ϕ(zk\ncj))2(sk\ncj)=⎧\n⎪⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎪⎩ak\ncj+ηsk\ncj\n1+η,ifak\ncj+ηsk\ncj≥0,sk\ncj≥0\nak\ncj+ηsk\ncj\n1+η,if−(/radicalbig\nη(η+1 )−η)ak\ncj≤ηsk\ncj<0\nsk\ncj,if−ak\ncj≤ηsk\ncj≤− (/radicalbig\nη(η+1 )−η)ak\ncj\nmin(sk\ncj,0),ifak\ncj+ηsk\ncj<0\n(13)\nUpdating Al,l=1 ,2, ..., L −2. Due to the introduction of the logregularizer,\nthe update of Alis as follows,\nAk+1\nl←argmin\nAk\nlγ\n2/bardblAk\nl−ϕ(Zk+1\nl)/bardbl2\nF+ρ\n2/bardblZk+1\nl−Wk+1\nlAk+1\nl−1−Bk+1\nl/bardbl2\nF+δ\n2/bardblAk\nl−Ak−1\nl/bardbl2\nF+λG(Ak\nl)\n(14)\nwhere G(Ak\nl)i st h e logsparsity regularizer. The iterations Eq. ( 14)c a nb e\nrewritten by using the proximal operator [ 18]. Then, the update rule of Eq. ( 14)\ncan be rewritten as,\nAk+1\nl←argmin\nAk\nlδ\n2/bardblAk\nl−Xk\nl/bardbl2\nF+λG(Ak\nl) (15)\nwhere Xk\nl=Ak\nl−1\nδ/parenleftBig\n(ρ(Wk+1\nl+1)TWk+1\nl+1+γI)Ak\nl−ρ(Wk+1\nl+1)T(Zk+1\nl+1−Bk+1\nl+1)−\nγϕ(Zk+1\nl)/parenrightBig\n.\nThen, employing the log-thresholding operator [ 18] to update Al, where\nG(Ak\nl)=/summationtextC\nc=1/summationtextJ\nj=1(log(1 +|acj|\n/epsilon1)). The variable Alis updated via as follows,\nAk+1\nl←argmin\nAk\nlδ\n2/bardblAk\nl−Xk\nl/bardbl2\nF+λK/summationdisplay\nk=1J/summationdisplay\nj=1(log(1 +|acj|\n/epsilon1)) (16)', '568 Y. Xie et al.\nThe update of Alcan be obtained by the log-thresholding operator to Eq. ( 17),\nac+1\ncj=[prox log,λ/δ (Xk\nl)]cj=⎧\n⎪⎪⎨\n⎪⎪⎩1\n2((xk\ncj−/epsilon1)+/radicalBig\n(xk\ncj+/epsilon1)2−4λ\nδ),i fxk\ncj>x 0\n1\n2((xk\ncj+/epsilon1)−/radicalBig\n(xk\ncj−/epsilon1)2−4λ\nδ),i fxk\ncj<−x0\n0 , otherwise\n(17)\nwhere x0=/radicalBig\n4λ\nδ−/epsilon1,xcjis an element of Xl,T h es y m b o l prox log,λ/δ is the\nproximal operator for the logregularizer.\nUpdating AL−1. Without introducing a sparsity constraint on the last hidden\nlayer, the iteration of AL−1is given as,\nAk+1\nL−1←argmin\nAk\nL−1γ\n2/bardblAk\nL−1−ϕ(Zk+1\nL−1)/bardbl2\nF+ρ\n2/bardblZk\nL−Wk\nLAk+1\nL−1−Bk\nL/bardbl2\nF+δ\n2/bardblAk\nL−1−Ak−1\nL−1/bardbl2\nF\n(18)\nThe Eq. ( 18) has closed-form solution. Then, the update rule of Eq. ( 18)c a nb e\nrewritten as,\nAk+1\nL−1=/parenleftBig\nρ(Wk\nL)T)Wk\nL+(ρ+δ)I/parenrightBig−1/parenleftBig\nδAk\nL−1+ρ((Wk\nL)T(Zk\nL−Bk\nL)) +γϕ(Zk\nL−1)/parenrightBig\n(19)\nUpdating ZL. Due to without activation function in output layer, the optimiza-\ntion problem of variable ZLin the output layer is diﬀerent from other layers.\nWe update ZLby using a simple proximal update via as follows,\nZk+1\nL←argmin\nZk\nLβ\n2/bardblZk\nL−Wk+1\nLAk+1\nL−1−Bk+1\nL/bardbl2\nF+α\n2/bardblZk\nL−Y/bardbl2\nF+δ\n2/bardblZk\nL−Zk−1\nL/bardbl2\nF\n(20)\nEq. (20) is a convex problem and has a closed-form solution,\nZk+1\nL=(βWk+1\nL Ak+1\nL−1+Bk+1\nL+αY+δZk\nL)/(α+β+δ) (21)\n3.3 The Overall Algorithm\nThe algorithm updates each layer sequentially, starting from the ﬁrst layer, and\nits parameters are updated according to Wl→Bl→Zl→Al. Only update\nWL→BL→ZLin the output layer. The structure of the proposed algorithm\nis outlined in Algorithm 1.\n4 Numerical Experiments\nIn this section, we conducted numerical experiments to evaluate the performance\nof the proposed algorithm. The ﬁrst experiment was to ﬁnd the proper hyperpa-rameters of the proposed algorithm on the MNIST dataset, the hyperparameters', 'Gradient-Free Neural Network Training Based on Deep Dictionary Learning 569\nAlgorithm 1. Proximal BCD algorithm with logsparsity regularizer (LOG-\nPBCD)\nInput: A0denotes training samples, the λ,γ,/epsilon1,ρ,α,βare hyperparameters, Ydenotes labels\nOutput: Wl,Zl,Bl,l=1,2, ..., L. Al,l=1,2, ..., L −1\n1: Solving the probelm:\nZ, W, A, B ← min\nZ,W,A,B/summationtextL−1\nl=1(γ\n2/bardblAl−ϕ(Zl)/bardbl2\nF+ρ\n2/bardblZl−WlAl−1−Bl/bardbl2\nF+λG(Al)+\nβ\n2/bardblZL−WLAL−1−BL/bardbl2\nF+α\n2/bardblZL−Y/bardbl2\nF,w h e r e G(Al)=/summationtextC\nc=1/summationtextJ\nj=1(log(1+|acj|\n/epsilon1)) ,acj∈Al\n2: Update all parameters until convergence.\n3:fork=1,2, ..., N do\n4: update the network parameters\n5: forl=1,2, ..., L −1do\n6: Wk+1\nl=(δWk\nl+ρ(Zk\nl−Bk\nl)Ak\nl−1)(δI+ρ(Ak\nl−1(Ak\nl−1)T))−1\n7: Bk+1\nl=(δBk\nl+ρ(Zk\nl−Wk\nlAkl−1))/(ρ+δ)\n8: Zk+1\nl=/bracketleftBigg\nprox 1\n2η(ak\nl−ϕ(zk\nl))2(Sk\nl)/bracketrightBigg\ncj,w h e r e Slk=δ(ˆZl)k+ρ(Wk+1\nlAk\nl−1+Bk+1\nl)\nρ+δ,η=ρ+δ\nγ\n9: ifupdate Ak+1\nl,l=1,2, ..L−2then\n10: Ak+1\nl=/bracketleftBig\nproxlog,λ/δ (Xk\nl)/bracketrightBig\ncj,w h e r e Xk\nl= Ak\nl−((ρ(Wk+1\nl+1)TWk+1\nl+1+γI)Ak\nl−\nρ(Wk+1\nl+1)T)(Zk+1\nl+1−Bk+1\nl+1)−γϕ(Zk+1\nl))/δ\n11: end if\n12: ifupdate Ak+1\nl,l=L−1then\n13: Ak+1\nL−1=/parenleftBig\nρ(Wk\nL)T)Wk\nL+(ρ+δ)I/parenrightBig−1/parenleftBig\nδAk\nL−1+ρ((Wk\nL)T(Zk\nL−Bk\nL)) +γϕ(Zk+1\nL−1)/parenrightBig\n14: end if\n15: end for\n16: forl=Ldo\n17: Wk+1\nL=(δWk\nL+ρ(Zk\nL−Bk\nL)Ak+1\nL−1)(δI+ρ(Ak+1\nL−1(Ak+1\nL−1)T))−1\n18: Bk+1\nL=(δBk\nL+ρ(Zk\nL−Wk\nLAk+1\nL−1))/(ρ+δ)\n19: Zk+1\nL=β(Wk\nLAk+1\nL−1+Bk+1\nL+αY+δZk\nL)/(α+β+δ)\n20: end for\n21: end for\nof the third experiment would be adjusted based on this series of hyperparame-\nters. The second experiment showed that proposed algorithm was eﬀective com-\npared to other training algorithms, and had better classiﬁcation accuracy andsparse activations. The third experiment veriﬁed that proposed algorithm was\nstill eﬀective on diﬀerent datasets. All experiments were conducted on pytorch\n1.5.1, Intel(R) Core(TM)i5-10400F CPU and GTX1080Ti GPU.\n4.1 Parameter Setting\nIn order to determine the proper choice of hyperparameters, we compared the\nexperimental results with diﬀerent hyperparameters in Algorithm 1 to ﬁnd the\nmost proper value of λorρby ﬁxing the remaining hyperparameters. From the\nEq. (17), the x\n0=/radicalBig\n4λ\nδ−/epsilon1is jointly determined by δ,/epsilon1andλ. So we determined\nthe remaining two parameters based on the λin the log-thresholding operator.\nAfter the numerical experiments (See in Fig. 2), we obtained an optimal set of\nhyperparameters, namely λ=3e–6,/epsilon1=0.001,ρ=1.7,δ=1 .', '570 Y. Xie et al.\n0 100 200 300 400 500 600\nIteration0.840.860.880.90.920.94Training Accuracy1e-4\n5e-5\n1e-5\n5e-6\n1e-6\n5e-7\n(a) Training accuracy with diﬀerent λvalues0 100 200 300 400 500 600\nIteration0.840.860.880.90.920.940.96Training Accuracy0.1\n0.4\n0.8\n1.2\n1.6\n2.0\n(b) Training accuracy with diﬀerent ρvalues\nFig. 2. Training accuracy under diﬀerent hyperparameters. The value of training accu-\nracy was obtained when other hyperparameters are ﬁxed and only one hyperparameter\nis changed. When λ=5e−6,ρ=1.6, the highest accuracy can be achieved.\n4.2 Classiﬁcation Experiments on MNIST\nWe compared the proposed training algorithms with other gradient-free training\nalgorithms: proximal BCD [ 11], ADMM [ 8] and the traditional gradient-based\ntraining algorithm: SGD. Proximal BCD method performs a proximal step for\neach parameter except the auxiliary variables in a Gauss-Seidel fashion. ADMM\nuses alternating direction methods and Bregman iterations to train networkswithout gradient descent steps. SGD is a common traditional gradient-based\nneural network training method. In order to prove the eﬀectiveness of sparse rep-\nresentation of inputs, we employed the logsparsity regularizer in other gradient-\nfree training methods. Just like LOG-PBCD algorithm, the LOG-ADMM algo-\nrithm is a training method that applies the combination of logsparsity reg-\nularizer and ADMM algorithm [ 8] rather than BCD algorithm. Our network\nadopted a structure of 784-500-500-10, where the numbers represented the num-\nber of neurons in each layer. The learning rate of SGD algorithm was 0.1. We\nmaintained the initialization parameters of the proposed LOG-PBCD algorithmand the proximal BCD algorithm consistent, and selected the proper hyperpa-\nrameters obtained in the parameter setting experiment. ADMM did the same.\nAll algorithms used the same training batch size of 60,000 samples and testing\nbatch size of 10,000 samples in work [ 11].\nFigure 3(a) and (b) shows the experimental results of diﬀerent training algo-\nrithms, but SGD converges the slowest. Although the classiﬁcation accuracy of\nSGD can be improved, the best result is less than the proposed LOG-PBCD algo-\nrithm. Figure 3(a) and (b) shows LOG-PBCD algorithm can exceed the highest\naccuracy of other algorithms in a few iterations and has always maintained the\nhighest accuracy, which beneﬁts from sparse activations. See in Table 1,T h e\nproposed LOG-PBCD algorithm achieves superior to other algorithms and hasthe highest training and testing accuracy. The proposed LOG-ADMM algorithm\nimproves the training and testing accuracy with ADMM algorithm, which shows\nthat sparse representation of inputs is eﬀective in other training algorithms. It is', 'Gradient-Free Neural Network Training Based on Deep Dictionary Learning 571\n0 200 400 600 800 1000\nIteration0.10.20.30.40.50.60.70.80.9Training AccuracySGD\nproximal BCD\nADMM\nLOG-ADMM\nLOG-PBCD\n(a) Training accuracy on MNIST0 200 400 600 800 1000\nIteration0.10.20.30.40.50.60.70.80.9Testing AccuracySGD\nproximal BCD\nADMM\nLOG-ADMM\nLOG-PBCD\n(b) Testing accuracy on MNIST\n0 200 400 600 800 1000\nIteration00.10.20.30.40.50.60.70.8Training AccuracySGD\nproximal BCD\nADMM\nLOG-ADMM\nLOG-PBCD\n(c) Training accuracy on Fashion MNIST0 200 400 600 800 1000\nIteration0.10.20.30.40.50.60.70.80.9Testing AccuracySGD\nproximal BCD\nADMM\nLOG-ADMM\nLOG-PBCD\n(d) Testing accuracy on Fashion MNIST\nFig. 3. Classiﬁcation accuracy with diﬀerent training algorithms on diﬀerent datasets.\n(a) and (b) respectively are training and testing accuracy with proposed algorithm andother algorithms on MNIST. (c) and (d) respectively are training and testing accuracy\nwith proposed algorithm and other algorithms on Fashion MNIST. The proposed LOG-\nPBCD achieves superior to other algorithms.\n(a) ADMM algorithm\n (b) Our LOG-ADMM algorithm\nFig. 4. The partial activations in some samples. (a) The activations after training\nby ADMM algorithm. (b) The activations after training by proposed LOG-ADMM\nalgorithm.', '572 Y. Xie et al.\nTable 1. Classiﬁcation accuracy with diﬀerent training algorithm on diﬀerent datasets.\nTraining methods MNIST Fashion MNIST\nTrain-acc Test-acc Train-acc Test-acc\nSGD 88.05% 89.10% 83.61% 82.53%\nProximal BCD 90.26% 89.91% 84.33% 82.44%\nADMM 84.91% 84.82% 82.06% 79.93%\nLOG-ADMM 91.10% 90.55% 84.64% 82.09%\nLOG-PBCD 94.79% 93.47% 87.89% 84.15%\nworth noting that the sparsity of activations in LOG-PBCD training algorithm\nis 23.4%, while the sparsity of activations in LOG-ADMM training algorithm\nhas reached 95 .21% (See Fig. 4). The reason may be that initialization setting of\nparameters of diﬀerent training methods, which is also associated to the methods\nof parameters updating and training. Figure 3(a) and (b) and Fig. 4demonstrate\nthat the logregularizer learns an accurate sparse activations and proves the eﬀec-\ntiveness of sparse representation of inputs in gradient-free network training.\n4.3 Classiﬁcation Experiments on Fashion MNIST\nTo verify the eﬀectiveness of our algorithm in diﬀerent datasets, the structure\nand initialization settings are the consistency with the previous experiments. We\nimplemented the proposed LOG-PBCD and LOG-ADMM algorithms on theFashion MNIST dataset. The experimental hyperparameter settings of LOG-\nPBCD here is ρ=1,/epsilon1=0.0001,λ=1e–7. The learning rate of SGD is 0 .3a n d\nthe hyperparameters of other methods remain unchanged. Figure 3(c) and (d)\nshows that SGD still maintains a very slow convergence rate, but its training and\ntest accuracy will surpass proximal BCD and ADMM methods with enough iter-\nations. See in Table 1and Fig. 3(c) and (d), the proposed LOG-PBCD algorithm\nstill achieves superior to other training algorithms in classiﬁcation accuracy. The\nsparsity of the LOG-PBCD training algorithm here is 21 .69%, while the spar-\nsity of the LOG-ADMM training algorithm is about 94 .15%. Experiments on\nFashion MNIST verify that diﬀerent algorithms and their initialization setting\nof parameters have an impact on the sparse activations.\n5 Conclusion and Future Work\nInspired by the similarity of the deep dictionary learning model to the gradient-free neural network model, we proposed a DDL-based gradient-free neural net-\nwork training model, which regards the weight matrix in the neural network\nas a dictionary and the activations of hidden layer corresponds to the coeﬃ-cients of DDL. To obtain accurate and sparse representation, we employ the\nnonconvex log function as sparsity regularizer which can enforce sparsity and', 'Gradient-Free Neural Network Training Based on Deep Dictionary Learning 573\naccurate estimation. In order to solve the non-convex non-smooth optimiza-\ntion problems involved in the training, we use the proximal BCD algorithmand the log-thresholding operator to optimize the objective function, achieving\nforward propagation training method without gradient. Numerical experiments\nshow that the proposed LOG-PBCD algorithm performs superior to other algo-rithms on public datasets, which proves that sparse representation of inputs is\neﬀective in the application of gradient-free networks. Future work includes accel-\nerating our algorithm while decreasing the memory consumption and consideringadditional datasets to verify the eﬀectiveness of proposed model.\nReferences\n1. Brock, A., De, S., Smith, S.L., Simonyan, K.: High-performance large-scale image\nrecognition without normalization. arXiv preprint arXiv:2102.06171 (2021)\n2. Wu, Z., Zhao, D., Liang, Q., Yu, J., Gulati, A., Pang, R.: Dynamic sparsity neural\nnetworks for automatic speech recognition. In: ICASSP 2021–2021 IEEE Inter-national Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.\n6014–6018. IEEE (2021)\n3. Woodworth, B., et al.: Is local SGD better than minibatch SGD? In: International\nConference on Machine Learning, pp. 10334–10343. PMLR (2020)\n4. Kristiadi, A., Hein, M., Hennig, P.: Being Bayesian, even just a bit, ﬁxes overcon-\nﬁdence in ReLU networks. In: International Conference on Machine Learning, pp.\n5436–5446. PMLR (2020)\n5. Yao, Z., Cao, Y., Zheng, S., Huang, G., Lin, S.: Cross-iteration batch normalization.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 12331–12340 (2021)\n6. Peng, S., Huang, H., Chen, W., Zhang, L., Fang, W.: More trainable inception-\nResNet for face recognition. Neurocomputing 411, 9–19 (2020)\n7. Li, J., Xiao, M., Fang, C., Dai, Y., Xu, C., Lin, Z.: Training neural networks by\nlifted proximal operator machines. IEEE Trans. Pattern Anal. Mach. Intell. (2020)\n8. Taylor, G., Burmeister, R., Xu, Z., Singh, B., Patel, A., Goldstein, T.: Training\nneural networks without gradients: a scalable ADMM approach. In: International\nConference on Machine Learning, pp. 2722–2731. PMLR (2016)\n9. Wang, J., Yu, F., Chen, X., Zhao, L.: ADMM for eﬃcient deep learning with global\nconvergence. In: Proceedings of the 25th ACM SIGKDD International Conferenceon Knowledge Discovery & Data Mining, pp. 111–119 (2019)\n10. Carreira-Perpinan, M., Wang, W.: Distributed optimization of deeply nested sys-\ntems. In: Artiﬁcial Intelligence and Statistics, pp. 10–19. PMLR (2014)\n11. Lau, T.T.K., Zeng, J., Wu, B., Yao, Y.: A proximal block coordinate descent\nalgorithm for deep neural network training. arXiv preprint arXiv:1803.09082 (2018)\n12. Zeng, J., Lau, T.T.K., Lin, S., Yao, Y.: Global convergence of block coordinate\ndescent in deep learning. In: International Conference on Machine Learning, pp.7313–7323. PMLR (2019)\n13. Gu, F., Askari, A., El Ghaoui, L.: Fenchel lifted networks: a lagrange relaxation of\nneural network training. In: International Conference on Artiﬁcial Intelligence and\nStatistics, pp. 3362–3371. PMLR (2020)\n14. Chen, Y., Su, J.: Dict layer: a structured dictionary layer. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition Workshops, pp.\n422–431 (2018)', '574 Y. Xie et al.\n15. Liu, Y., Chen, Q., Chen, W., Wassell, I.: Dictionary learning inspired deep net-\nwork for scene recognition. In: Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 32 (2018)\n16. Singhal, V., Aggarwal, H.K., Tariyal, S., Majumdar, A.: Discriminative robust\ndeep dictionary learning for hyperspectral image classiﬁcation. IEEE Trans. Geosci.\nRemote Sens. 55(9), 5274–5283 (2017)\n17. Qiao, L., Sun, T., Pan, H., Li, D.: Inertial proximal deep learning alternating\nminimization for eﬃcient neutral network training. In: ICASSP 2021–2021 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP),\npp. 3895–3899. IEEE (2021)\n18. Li, Z., Zhao, H., Guo, Y., Yang, Z., Xie, S.: Accelerated log-regularized convo-\nlutional transform learning and its convergence guarantee. IEEE Trans. Cybern.\n(2021)', 'Author Index\nBao, Jiayu 252\nChang, Liang 425\nChang, Y uan 536\nChao, Zhenzhen 348\nChen, Ji 16\nChen, Jiansheng 252\nChen, Jun 437\nChen, Qiang 42\nChen, Yingying 128\nCheng, Keyang 399\nCheng, Xijie 78\nChu, Huazhen 472\nDai, Qi 312\nDai, Qingyun 103\nDing, Zihao 78\nDong, Y uhan 336\nDou, Peng 459\nDuan, Fuqing 425\nDuan, Lijuan 16,447\nDuan, Y exin 152\nDuan, Y utai 53\nEn, Qing 447\nFan, Wen 42\nFeng, Qiantai 3\nFeng, Zunlei 201\nFu, Lele 276\nGao, Chengli 240\nGlossner, John 411\nGong, Zhenfei 263\nGuo, Jipeng 66\nGuo, Zhenyu 524\nGuo, Zijing 240\nHan, Yi 348\nHe, Jiaqi 165\nHe, Qing 536\nHe, Xiaohui 78\nHou, Shiyu 115\nHou, Xinwen 128Hu, Haifeng 459\nHu, Tianyu 336\nHu, Y ongli 66\nHua, Xia 508\nHuang, Jiahao 103\nHuang, Ning 240\nHuang, Sheng 276\nHuang, Yiqing 252\nHuang, Zhangcheng 140\nHuang, Zhigao 508\nJi, Yixin 201\nJi, Zexuan 42\nJia, Xibin 312\nJian, Lihua 214\nJiang, Huiqin 214 ,536\nJiang, Jingen 115\nJiang, Murong 496\nKong, Lingwei 140\nKong, Qi 176\nLei, Fangyuan 103\nLei, Y u 288\nLi, Bocen 263\nLi, Chunlei 240\nLi, Fuhai 496\nLi, Guozhen 263\nLi, Haoliang 90\nLi, Haolun 165\nLi, Hongliang 484\nLi, Hui 484\nLi, Li 387\nLi, Panle 78\nLi, Runchuan 78\nLi, Ruzhou 508\nLi, Xin 374\nLi, Y uhai 3\nLi, Zhenni 561\nLiang, Sai 399\nLiang, Tailin 411\nLiao, Zhongke 459\nLiu, Haizhuang 472\nLiu, Shuaiqi 288', '576 Author Index\nLiu, Ting 437\nLiu, Wenxue 361\nLiu, Xiang 361\nLiu, Ximeng 29\nLiu, Y u 128\nLiu, Y ushuang 165\nLiu, Zhizhe 524\nLiu, Zhoufeng 240\nLong, Xiao 387\nLu, Huchuan 263\nLuo, Hanxiao 484\nLuo, Wenfeng 188\nMa, Hongbing 252\nMa, Huimin 252 ,336 ,472\nMa, Ling 214 ,536\nMao, Y aobin 348\nMeng, Chunyun 399\nMeng, Fanman 484\nMi, Qing 312\nNgan, King Ngi 484\nNi, Rongrong 374\nOu, Fu-Zhao 437\nPan, Zhisong 152\nQiao, Mengjia 78\nQiao, Y uanhua 16\nQuan, Weize 115\nShen, Y eqing 336\nShi, Shaobo 411\nShi, Y u 508\nSong, Dingjun 78\nSong, Jie 201\nSong, Mingli 201\nSu, Siwei 300\nSun, Bingfeng 548\nSun, Guoying 188\nSun, Y anfeng 66\nSun, Y uxin 3\nTao, Y uting 227\nWang, Chaoqun 115\nWang, Dong 3\nWang, Haijian 300\nWang, Haiting 263\nWang, Jianming 53Wang, Jianzong 140\nWang, Jie 66\nWang, Lei 411\nWang, Lijun 263\nWang, Li-Na 361\nWang, Rongquan 472\nWang, Senhong 103\nWang, Shafei 387\nWang, Shiping 276\nWang, Song 103\nWang, Wenjian 447\nWang, Y aopeng 29\nWang, Y uan-Gen 437\nWang, Y uanyuan 214 ,536\nWang, Zhibing 374\nWu, Bin 399\nWu, Falin 165\nWu, Qingbo 484\nWu, Tongtong 425\nXiao, Jing 140\nXiao, Li 536\nXie, Lehui 29\nXie, Ying 561\nXu, Fan 16\nXu, Ke 3\nXu, Linfeng 484\nXu, Xin 176\nY an, Dong-Ming 115\nY an, Jiaqian 508\nYa n , K u n 524\nY ang, Gongliu 165\nY ang, Lei 496\nY ang, Meng 188 ,300\nY e, Jingwen 201\nYin, Baocai 66\nYin, Jia-Li 29\nY u, Cheng 252\nY uan, Songtao 42\nY uan, Ying 447\nZhang, Jian 548\nZhang, Liangliang 176\nZhang, Luyao 288\nZhang, Pei 411\nZhang, Wu 152\nZhang, Xiaohua 263\nZhang, Xiaotong 411\nZhang, Xiaowen 336', 'Author Index 577\nZhang, Xu 425\nZhang, Y an 227\nZhang, Y unhe 276\nZhang, Zhipeng 536\nZhang, Ziyue 42\nZhao, Haifeng 227\nZhao, Haoli 561\nZhao, Jie 288\nZhao, Ling 288\nZhao, Sanyuan 323\nZhao, Y ao 374Zhao, Zongji 323\nZheng, Huicheng 90\nZheng, Shuai 524\nZhong, Guoqiang 361\nZhou, Guopeng 165\nZhou, Wei 459\nZhou, Xingyu 152\nZhu, Zhenfeng 524\nZhuang, Fuzhen 536\nZhuang, Liansheng 387\nZou, Junhua 152']